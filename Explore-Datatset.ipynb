{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sps\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'your_file.txt' with your file path\n",
    "file_path = 'Dataset/yahoo_ymusic_v1/ydata-ymusic-rating-study-v1_0-train.txt'\n",
    "\n",
    "# Load the file into a DataFrame\n",
    "df = pd.read_csv(file_path, sep='\\t',names=[\"UserID\",\"SongID\",\"Rating\"], header=None)  # sep='\\t' for tab-separated values\n",
    "\n",
    "# Display the DataFrame\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSITIVE_THRESHOLD = 4\n",
    "\n",
    "df['ImplicitRating'] = np.where(df['Rating'] >= POSITIVE_THRESHOLD, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Making IDs 0-based**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"UserID Count: {np.unique(df['UserID']).shape[0]}\")\n",
    "print(f\"UserID Min: {df['UserID'].min()}\")\n",
    "print(f\"UserID Max: {df['UserID'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['UserID'] = df['UserID'] - 1\n",
    "print(f\"UserID Count: {np.unique(df['UserID']).shape[0]}\")\n",
    "print(f\"UserID Min: {df['UserID'].min()}\")\n",
    "print(f\"UserID Max: {df['UserID'].max()}\")\n",
    "max_user = df['UserID'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"SongID Count: {np.unique(df['SongID']).shape[0]}\")\n",
    "print(f\"SongID Min: {df['SongID'].min()}\")\n",
    "print(f\"SongID Max: {df['SongID'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['SongID'] = df['SongID'] - 1\n",
    "print(f\"SongID Count: {np.unique(df['SongID']).shape[0]}\")\n",
    "print(f\"SongID Min: {df['SongID'].min()}\")\n",
    "print(f\"SongID Max: {df['SongID'].max()}\")\n",
    "max_item = df['SongID'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[\"ImplicitRating\"] = df[\"ImplicitRating\"].values.astype(np.float32)\n",
    "df['UserID'].shape, df['SongID'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Creating a URM coo format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URM_all = sps.coo_matrix((df[\"ImplicitRating\"].values, \n",
    "                          (df[\"UserID\"].values, df[\"SongID\"].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URM_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Try To Split Global Wise**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating array for OpenRec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_QUOTA = 0.7\n",
    "\n",
    "#Get relevant interactions indexes\n",
    "indicesRelevantInteractions = np.where(df[\"ImplicitRating\"] == 1)[0]\n",
    "\n",
    "print(f\"Total positive interactions: {indicesRelevantInteractions.shape[0]}\")\n",
    "\n",
    "#Shuffle them\n",
    "np.random.shuffle(indicesRelevantInteractions)\n",
    "n_train_interactions = round(indicesRelevantInteractions.shape[0] * TRAINING_QUOTA)\n",
    "\n",
    "print(f\"Training sampled positive interactions: {n_train_interactions}\")\n",
    "\n",
    "#Sample training indexes\n",
    "indices_for_train = indicesRelevantInteractions[0:n_train_interactions]\n",
    "indices_for_test_validation = indicesRelevantInteractions[n_train_interactions:]\n",
    "\n",
    "print(f\"Training indeces length: {indices_for_train.shape[0]}\")\n",
    "print(f\"Training validation+test length: {indices_for_test_validation.shape[0]}\")\n",
    "\n",
    "\n",
    "#Split remaining\n",
    "n_validation_interactions = round(len(indices_for_test_validation) / 2)\n",
    "\n",
    "print(f\"Validation sampled positive interactions: {n_validation_interactions}\")\n",
    "\n",
    "indices_for_validation = indices_for_test_validation[:n_validation_interactions]\n",
    "indices_for_test = indices_for_test_validation[n_validation_interactions:]\n",
    "\n",
    "assert len(indices_for_train) + len(indices_for_validation) + len(indices_for_test) == indicesRelevantInteractions.shape[0]\n",
    "len(indices_for_train), len(indices_for_validation), len(indices_for_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.ones(indices_for_train.shape[0], dtype={'names':('user_id', 'item_id'),\n",
    "                          'formats':('i4', 'i4')})\n",
    "train_data['user_id'] = df[\"UserID\"][indices_for_train]\n",
    "train_data['item_id'] = df[\"SongID\"][indices_for_train]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = np.ones(indices_for_validation.shape[0], dtype={'names':('user_id', 'item_id'),\n",
    "                          'formats':('i4', 'i4')})\n",
    "validation_data['user_id'] = df[\"UserID\"][indices_for_validation]\n",
    "validation_data['item_id'] = df[\"SongID\"][indices_for_validation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.ones(indices_for_test.shape[0], dtype={'names':('user_id', 'item_id'),\n",
    "                          'formats':('i4', 'i4')})\n",
    "test_data['user_id'] = df[\"UserID\"][indices_for_test]\n",
    "test_data['item_id'] = df[\"UserID\"][indices_for_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Splitting URM **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import DaCrema's function from \n",
    "def split_train_in_two_percentage_global_sample(URM_all, train_percentage = 0.1):\n",
    "    \"\"\"\n",
    "    The function splits an URM in two matrices selecting the number of interactions globally\n",
    "    :param URM_all:\n",
    "    :param train_percentage:\n",
    "    :param verbose:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    assert train_percentage >= 0.0 and train_percentage<=1.0, \"train_percentage must be a value between 0.0 and 1.0, provided was '{}'\".format(train_percentage)\n",
    "\n",
    "\n",
    "    from  MFDLib.IncrementalSparseMatrix import IncrementalSparseMatrix\n",
    "\n",
    "    num_users, num_items = URM_all.shape\n",
    "\n",
    "    URM_train_builder = IncrementalSparseMatrix(n_rows=num_users, n_cols=num_items, auto_create_col_mapper=False, auto_create_row_mapper=False)\n",
    "    URM_validation_builder = IncrementalSparseMatrix(n_rows=num_users, n_cols=num_items, auto_create_col_mapper=False, auto_create_row_mapper=False)\n",
    "\n",
    "\n",
    "    URM_train = sps.coo_matrix(URM_all)\n",
    "\n",
    "    indices_for_sampling = np.arange(0, URM_all.nnz, dtype=int)\n",
    "    np.random.shuffle(indices_for_sampling)\n",
    "\n",
    "    n_train_interactions = round(URM_all.nnz * train_percentage)\n",
    "\n",
    "    indices_for_train = indices_for_sampling[indices_for_sampling[0:n_train_interactions]]\n",
    "    indices_for_validation = indices_for_sampling[indices_for_sampling[n_train_interactions:]]\n",
    "\n",
    "\n",
    "    URM_train_builder.add_data_lists(URM_train.row[indices_for_train],\n",
    "                                     URM_train.col[indices_for_train],\n",
    "                                     URM_train.data[indices_for_train])\n",
    "\n",
    "    URM_validation_builder.add_data_lists(URM_train.row[indices_for_validation],\n",
    "                                          URM_train.col[indices_for_validation],\n",
    "                                          URM_train.data[indices_for_validation])\n",
    "\n",
    "\n",
    "    URM_train = URM_train_builder.get_SparseMatrix()\n",
    "    URM_validation = URM_validation_builder.get_SparseMatrix()\n",
    "\n",
    "    URM_train = sps.csr_matrix(URM_train)\n",
    "    URM_validation = sps.csr_matrix(URM_validation)\n",
    "\n",
    "    user_no_item_train = np.sum(np.ediff1d(URM_train.indptr) == 0)\n",
    "    user_no_item_validation = np.sum(np.ediff1d(URM_validation.indptr) == 0)\n",
    "\n",
    "    if user_no_item_train != 0:\n",
    "        print(\"Warning: {} ({:.2f} %) of {} users have no train items\".format(user_no_item_train, user_no_item_train/num_users*100, num_users))\n",
    "    if user_no_item_validation != 0:\n",
    "        print(\"Warning: {} ({:.2f} %) of {} users have no sampled items\".format(user_no_item_validation, user_no_item_validation/num_users*100, num_users))\n",
    "\n",
    "\n",
    "    return URM_train, URM_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 15% for Test and 15% for Validation\n",
    "\n",
    "urm_train, urm_test_validation = split_train_in_two_percentage_global_sample(URM_all, train_percentage = 0.7)\n",
    "urm_test, urm_validation = split_train_in_two_percentage_global_sample(urm_test_validation, train_percentage = 0.5)\n",
    "urm_train_validation = urm_train + urm_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lets USE openrec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the same libraries of the Section 4\n",
    "#openrec.legacy now moved to openrec.tf1.legacy\n",
    "from openrec.tf1.legacy import ImplicitModelTrainer\n",
    "from openrec.tf1.legacy.utils import ImplicitDataset\n",
    "from openrec.tf1.legacy.recommenders import PMF\n",
    "from openrec.tf1.legacy.utils.evaluators import AUC, Recall, Precision, NDCG\n",
    "from openrec.tf1.legacy.utils.samplers import PointwiseSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = dict()\n",
    "raw_data['train_data'] = train_data\n",
    "raw_data['val_data'] = validation_data\n",
    "raw_data['test_data'] = test_data\n",
    "raw_data['max_user'] = max_user + 1\n",
    "raw_data['max_item'] = max_item + 1\n",
    "batch_size = 8000\n",
    "test_batch_size = 200\n",
    "display_itr = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImplicitDataset(raw_data['train_data'], raw_data['max_user'], raw_data['max_item'], name='Train')\n",
    "val_dataset = ImplicitDataset(raw_data['val_data'], raw_data['max_user'], raw_data['max_item'], name='Val')\n",
    "test_dataset = ImplicitDataset(raw_data['test_data'], raw_data['max_user'], raw_data['max_item'], name='Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.reset_default_graph()\n",
    "model = PMF(batch_size=batch_size, max_user=train_dataset.max_user(), max_item=train_dataset.max_item(), \n",
    "            dim_embed=50, opt='Adam', sess_config=None, l2_reg=0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = PointwiseSampler(batch_size=batch_size, dataset=train_dataset, pos_ratio=0.2, num_process=5)\n",
    "model_trainer = ImplicitModelTrainer(batch_size=batch_size, test_batch_size=test_batch_size, \n",
    "    train_dataset=train_dataset, model=model, sampler=sampler, \n",
    "    eval_save_prefix=\"./pmf-yahoo\")\n",
    "auc_evaluator = AUC()\n",
    "recall_evaluator = Recall(recall_at=[10, 20, 30, 40, 50, 60, 70, 80, 90, 100])\n",
    "dcg_evaluator = NDCG(ndcg_at=[10, 20, 30, 40, 50, 60, 70, 80, 90, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_trainer.train(num_itr=50001, display_itr=display_itr, eval_datasets=[val_dataset],\n",
    "#                    evaluators=[auc_evaluator, recall_evaluator, dcg_evaluator], num_negatives=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Explore TestSet**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We filter the testing set by retaining users who have at least a relevant and an irrelevant song in the testing set and two relevant songs in the training set (2,296 users satisfy these requirements). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'your_file.txt' with your file path\n",
    "file_path = 'Dataset/yahoo_ymusic_v1/ydata-ymusic-rating-study-v1_0-train.txt'\n",
    "\n",
    "# Load the file into a DataFrame\n",
    "df_train = pd.read_csv(file_path, sep='\\t',names=[\"UserID\",\"SongID\",\"Rating\"], header=None)  # sep='\\t' for tab-separated values\n",
    "\n",
    "# Display the DataFrame\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'your_file.txt' with your file path\n",
    "file_path = 'Dataset/yahoo_ymusic_v1/ydata-ymusic-rating-study-v1_0-test.txt'\n",
    "\n",
    "# Load the file into a DataFrame\n",
    "df_test = pd.read_csv(file_path, sep='\\t',names=[\"UserID\",\"SongID\",\"Rating\"], header=None)  # sep='\\t' for tab-separated values\n",
    "\n",
    "# Display the DataFrame\n",
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSITIVE_THRESHOLD = 4\n",
    "\n",
    "df_train['ImplicitRating'] = np.where(df_train[\"Rating\"] >= POSITIVE_THRESHOLD, 1, 0)\n",
    "\n",
    "df_test['ImplicitRating'] = np.where(df_test['Rating'] >= POSITIVE_THRESHOLD, 1, 0)\n",
    "#df[\"ImplicitRating\"] = df[\"ImplicitRating\"].values.astype(np.float32)\n",
    "df_test['UserID'].shape, df_test['SongID'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the DataFrame\n",
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select UserID of users with at least an irrelevant song in testset\n",
    "usersWithNegativeInteractionTestSet = df_test[df_test[\"ImplicitRating\"] == 0][\"UserID\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select UserID of users with at least a relevant song in testset\n",
    "usersWithPositiveInteractionTestSet = df_test[df_test[\"ImplicitRating\"] == 1][\"UserID\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select UserID of users with at least two relevant song in trainset\n",
    "valid_users_trainset = df_train[df_train[\"ImplicitRating\"] == 1].groupby(\"UserID\").filter(lambda x: len(x) >= 2)['UserID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting arrays to sets\n",
    "set1 = set(usersWithNegativeInteractionTestSet)\n",
    "set2 = set(usersWithPositiveInteractionTestSet)\n",
    "\n",
    "set_train = set(valid_users_trainset)\n",
    "\n",
    "# Finding the intersection\n",
    "valid_users_testset = set1 & set2 & set_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(valid_users_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_filtered = df_test[df_test[\"UserID\"].isin(valid_users_testset)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_filtered.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RecommenderEvaluation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
