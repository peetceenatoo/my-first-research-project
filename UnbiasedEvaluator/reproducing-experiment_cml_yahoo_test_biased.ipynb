{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **SETUP**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjYyDBmbcEL9"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nUxhCz5MkDZk"
      },
      "outputs": [],
      "source": [
        "from __future__ import division\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os\n",
        "from openrec.tf1.legacy import ImplicitModelTrainer\n",
        "from openrec.tf1.legacy.utils import ImplicitDataset\n",
        "from openrec.tf1.legacy.utils.evaluators import ImplicitEvalManager\n",
        "from openrec.tf1.legacy.recommenders import CML, BPR\n",
        "from openrec.tf1.legacy.utils.evaluators import AUC\n",
        "from openrec.tf1.legacy.utils.samplers import PairwiseSampler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "auc_results = []\n",
        "recall_results = []\n",
        "\n",
        "seed = 76424236\n",
        "np.random.seed(seed=seed)\n",
        "\n",
        "folder_name = f\"./Dataset/\"\n",
        "\n",
        "if os.path.exists(folder_name) == False:\n",
        "    os.makedirs(folder_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "g0C--vI7lUe2"
      },
      "outputs": [],
      "source": [
        "raw_data = dict()\n",
        "raw_data['train_data'] = np.load(folder_name + \"training_arr.npy\")\n",
        "raw_data['test_data_pos'] = np.load(folder_name + \"biased-test_arr_pos.npy\")\n",
        "raw_data['test_data_neg'] = np.load(folder_name + \"biased-test_arr_neg.npy\")\n",
        "raw_data['max_user'] = 15401\n",
        "raw_data['max_item'] = 1001\n",
        "batch_size = 8000\n",
        "test_batch_size = 1000\n",
        "display_itr = 1000\n",
        "\n",
        "train_dataset = ImplicitDataset(raw_data['train_data'], raw_data['max_user'], raw_data['max_item'], name='Train')\n",
        "test_dataset_pos = ImplicitDataset(raw_data['test_data_pos'], raw_data['max_user'], raw_data['max_item'])\n",
        "test_dataset_neg = ImplicitDataset(raw_data['test_data_neg'], raw_data['max_user'], raw_data['max_item'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c61lBOIqcawA"
      },
      "source": [
        "## Define Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Code to avoid tf using cached embeddings\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.reset_default_graph()\n",
        "\n",
        "cml_model = CML(batch_size=batch_size, max_user=train_dataset.max_user(), max_item=train_dataset.max_item(),\n",
        "    dim_embed=50, l2_reg=0.001, opt='Adam', sess_config=None)\n",
        "sampler = PairwiseSampler(batch_size=batch_size, dataset=train_dataset, num_process=4)\n",
        "model_trainer = ImplicitModelTrainer(batch_size=batch_size, test_batch_size=test_batch_size,\n",
        "                                     train_dataset=train_dataset, model=cml_model, sampler=sampler,\n",
        "                                     eval_save_prefix=folder_name+\"yahoo\",\n",
        "                                     item_serving_size=500)\n",
        "auc_evaluator = AUC()\n",
        "\n",
        "cml_model.load(folder_name+\"cml-yahoo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **DEFINE FUNCTION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def eq(infilename, infilename_neg, trainfilename, gamma=-1.0, K=30):\n",
        "\n",
        "    # Load the pickles\n",
        "    infile = open(infilename, 'rb')\n",
        "    infile_neg = open(infilename_neg, 'rb')\n",
        "    P = pickle.load(infile)\n",
        "    infile.close()\n",
        "    P_neg = pickle.load(infile_neg)\n",
        "    infile_neg.close()\n",
        "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
        "    \n",
        "    # Merge the two dictionaries\n",
        "    for theuser in P[\"users\"]:\n",
        "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
        "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
        "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
        "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
        "    \n",
        "    Zui = dict()\n",
        "    Ni = dict()\n",
        "\n",
        "    # Compute the frequency of each item in the training set\n",
        "    trainset = np.load(trainfilename)\n",
        "    for i in trainset['item_id']:\n",
        "        if i in Ni:\n",
        "            Ni[i] += 1\n",
        "        else:\n",
        "            Ni[i] = 1\n",
        "    del trainset\n",
        "\n",
        "    # Count the number of users with at least one positive item in the test set\n",
        "    # (not the smartest way)\n",
        "    nonzero_user_count = 0\n",
        "    for theuser in P[\"users\"]:\n",
        "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
        "        for pos_item in pos_items:\n",
        "            if pos_item in Ni:\n",
        "                nonzero_user_count += 1\n",
        "                break\n",
        "\n",
        "    # Compute the recommendations\n",
        "    for theuser in P[\"users\"]:\n",
        "        all_scores = np.array(P[\"results\"][theuser])\n",
        "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
        "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
        "        for i, pos_item in enumerate(pos_items):\n",
        "            pos_score = pos_scores[i]\n",
        "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
        "\n",
        "    # Compute the scores using AUC and compute the recall\n",
        "    sum_user_auc = 0.0\n",
        "    sum_user_recall = 0.0\n",
        "    for theuser in P[\"users\"]:\n",
        "        numerator_auc = 0.0\n",
        "        numerator_recall = 0.0\n",
        "        denominator = 0.0\n",
        "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
        "            if theitem not in Ni:\n",
        "                continue\n",
        "            pui = np.power(Ni[theitem], (gamma + 1) / 2.0)\n",
        "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser])) / pui\n",
        "            if Zui[(theuser, theitem)] < K:\n",
        "                numerator_recall += 1.0 / pui\n",
        "            denominator += 1 / pui\n",
        "        if denominator > 0:\n",
        "            sum_user_auc += numerator_auc / denominator\n",
        "            sum_user_recall += numerator_recall / denominator\n",
        "\n",
        "    return {\n",
        "        \"auc\"       : sum_user_auc / nonzero_user_count,\n",
        "        \"recall\"    : sum_user_recall / nonzero_user_count\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **TEST**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTBMf1BPcvgL"
      },
      "source": [
        "## Generate Raw Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_trainer._eval_manager = ImplicitEvalManager(evaluators=[auc_evaluator])\n",
        "model_trainer._num_negatives = 200\n",
        "model_trainer._exclude_positives([train_dataset, test_dataset_pos, test_dataset_neg])\n",
        "model_trainer._sample_negatives(seed=10)\n",
        "\n",
        "model_trainer._eval_save_prefix = folder_name+\"cml-yahoo-test-pos-biased\"\n",
        "model_trainer._evaluate_partial(test_dataset_pos)\n",
        "\n",
        "model_trainer._eval_save_prefix = folder_name+\"cml-yahoo-test-neg-biased\"\n",
        "model_trainer._evaluate_partial(test_dataset_neg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eq(folder_name+\"cml-yahoo-test-pos-biased_evaluate_partial.pickle\", folder_name+\"cml-yahoo-test-neg-biased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", gamma=1.5)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "RecSysEvaluation",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
