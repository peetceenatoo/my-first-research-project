{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **SETUP**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjYyDBmbcEL9"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nUxhCz5MkDZk"
      },
      "outputs": [],
      "source": [
        "from __future__ import division\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os\n",
        "from openrec.tf1.legacy import ImplicitModelTrainer\n",
        "from openrec.tf1.legacy.utils import ImplicitDataset\n",
        "from openrec.tf1.legacy.utils.evaluators import ImplicitEvalManager\n",
        "from openrec.tf1.legacy.recommenders import CML, BPR\n",
        "from openrec.tf1.legacy.utils.evaluators import AUC\n",
        "from openrec.tf1.legacy.utils.samplers import PairwiseSampler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "auc_results = []\n",
        "recall_results = []\n",
        "\n",
        "seed = 76424236\n",
        "np.random.seed(seed=seed)\n",
        "\n",
        "folder_name = f\"./Dataset/\"\n",
        "\n",
        "if os.path.exists(folder_name) == False:\n",
        "    os.makedirs(folder_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "g0C--vI7lUe2"
      },
      "outputs": [],
      "source": [
        "raw_data = dict()\n",
        "raw_data['train_data'] = np.load(folder_name + \"training_arr.npy\")\n",
        "raw_data['test_data_pos'] = np.load(folder_name + \"biased-test_arr_pos.npy\")\n",
        "raw_data['test_data_neg'] = np.load(folder_name + \"biased-test_arr_neg.npy\")\n",
        "raw_data['max_user'] = 15401\n",
        "raw_data['max_item'] = 1001\n",
        "batch_size = 8000\n",
        "test_batch_size = 1000\n",
        "display_itr = 1000\n",
        "\n",
        "train_dataset = ImplicitDataset(raw_data['train_data'], raw_data['max_user'], raw_data['max_item'], name='Train')\n",
        "test_dataset_pos = ImplicitDataset(raw_data['test_data_pos'], raw_data['max_user'], raw_data['max_item'])\n",
        "test_dataset_neg = ImplicitDataset(raw_data['test_data_neg'], raw_data['max_user'], raw_data['max_item'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c61lBOIqcawA"
      },
      "source": [
        "## Define Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:391: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:31: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:41: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:43: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:33: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/interactions/pairwise_eu_dist.py:71: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:596: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:75: The name tf.scatter_update is deprecated. Please use tf.compat.v1.scatter_update instead.\n",
            "\n",
            "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:144: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:365: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:148: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-02-16 16:10:50.852517: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2024-02-16 16:10:50.871522: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fbd79d820b0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2024-02-16 16:10:50.871536: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./Dataset/cml-yahoo\n"
          ]
        }
      ],
      "source": [
        "#Code to avoid tf using cached embeddings\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.reset_default_graph()\n",
        "\n",
        "cml_model = CML(batch_size=batch_size, max_user=train_dataset.max_user(), max_item=train_dataset.max_item(),\n",
        "    dim_embed=50, l2_reg=0.001, opt='Adam', sess_config=None)\n",
        "sampler = PairwiseSampler(batch_size=batch_size, dataset=train_dataset, num_process=4)\n",
        "model_trainer = ImplicitModelTrainer(batch_size=batch_size, test_batch_size=test_batch_size,\n",
        "                                     train_dataset=train_dataset, model=cml_model, sampler=sampler,\n",
        "                                     eval_save_prefix=folder_name+\"yahoo\",\n",
        "                                     item_serving_size=500)\n",
        "auc_evaluator = AUC()\n",
        "\n",
        "cml_model.load(folder_name+\"cml-yahoo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **DEFINE FUNCTION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def eq(infilename, infilename_neg, trainfilename, gamma=-1.0, K=30):\n",
        "    infile = open(infilename, 'rb')\n",
        "    infile_neg = open(infilename_neg, 'rb')\n",
        "    P = pickle.load(infile)\n",
        "    infile.close()\n",
        "    P_neg = pickle.load(infile_neg)\n",
        "    infile_neg.close()\n",
        "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
        "    #\n",
        "    for theuser in P[\"users\"]:\n",
        "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
        "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
        "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
        "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
        "    #\n",
        "    Zui = dict()\n",
        "    Ni = dict()\n",
        "    # fill in dictionary Ni\n",
        "    trainset = np.load(trainfilename)\n",
        "    for i in trainset['item_id']:\n",
        "        if i in Ni:\n",
        "            Ni[i] += 1\n",
        "        else:\n",
        "            Ni[i] = 1\n",
        "    del trainset\n",
        "    # count #users with non-zero item frequencies\n",
        "    nonzero_user_count = 0\n",
        "    for theuser in P[\"users\"]:\n",
        "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
        "        for pos_item in pos_items:\n",
        "            if pos_item in Ni:\n",
        "                nonzero_user_count += 1\n",
        "                break\n",
        "    # fill in dictionary Zui\n",
        "    for theuser in P[\"users\"]:\n",
        "        all_scores = np.array(P[\"results\"][theuser])\n",
        "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
        "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
        "        for i, pos_item in enumerate(pos_items):\n",
        "            pos_score = pos_scores[i]\n",
        "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
        "    # calculate per-user scores\n",
        "    sum_user_auc = 0.0\n",
        "    sum_user_recall = 0.0\n",
        "    for theuser in P[\"users\"]:\n",
        "        numerator_auc = 0.0\n",
        "        numerator_recall = 0.0\n",
        "        denominator = 0.0\n",
        "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
        "            if theitem not in Ni:\n",
        "                continue\n",
        "            pui = np.power(Ni[theitem], (gamma + 1) / 2.0)\n",
        "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser])) / pui\n",
        "            # Calcolo il Recall a 30, vedi nota 6 paper\n",
        "            if Zui[(theuser, theitem)] < K:\n",
        "                numerator_recall += 1.0 / pui\n",
        "            denominator += 1 / pui\n",
        "        if denominator > 0:\n",
        "            sum_user_auc += numerator_auc / denominator\n",
        "            sum_user_recall += numerator_recall / denominator\n",
        "\n",
        "    return {\n",
        "        \"auc\"       : sum_user_auc / nonzero_user_count,\n",
        "        \"recall\"    : sum_user_recall / nonzero_user_count\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **TEST**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTBMf1BPcvgL"
      },
      "source": [
        "## Generate Raw Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Subsampling negative items]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2062/2062 [00:02<00:00, 812.16it/s] \n",
            "100%|██████████| 2296/2296 [01:01<00:00, 37.14it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'AUC': [0.4911655405405405,\n",
              "  0.5114309764309763,\n",
              "  0.47823232323232323,\n",
              "  0.47508333333333325,\n",
              "  0.49118644067796613,\n",
              "  0.4970508474576271,\n",
              "  0.5332881355932203,\n",
              "  0.5419360269360269,\n",
              "  0.5159121621621622,\n",
              "  0.4557457627118644,\n",
              "  0.4485690235690237,\n",
              "  0.505051724137931,\n",
              "  0.5000242718446601,\n",
              "  0.5126166666666667,\n",
              "  0.4587068965517241,\n",
              "  0.49633333333333324,\n",
              "  0.5013545150501673,\n",
              "  0.5601890034364261,\n",
              "  0.5084113712374583,\n",
              "  0.5018791946308725,\n",
              "  0.4925418060200669,\n",
              "  0.5108644067796609,\n",
              "  0.4572986577181207,\n",
              "  0.4853703703703703,\n",
              "  0.5253716216216217,\n",
              "  0.4938144329896907,\n",
              "  0.47029209621993123,\n",
              "  0.5371237458193979,\n",
              "  0.5079180887372015,\n",
              "  0.5237710437710438,\n",
              "  0.5224161073825504,\n",
              "  0.5221666666666668,\n",
              "  0.5144655172413792,\n",
              "  0.5005183946488295,\n",
              "  0.48801003344481614,\n",
              "  0.49296296296296305,\n",
              "  0.5151833333333333,\n",
              "  0.5011904761904763,\n",
              "  0.5173244147157191,\n",
              "  0.5141750841750841,\n",
              "  0.5172727272727272,\n",
              "  0.4631358885017422,\n",
              "  0.4817457627118644,\n",
              "  0.5158053691275167,\n",
              "  0.4737290969899665,\n",
              "  0.5368624161073826,\n",
              "  0.4559531772575251,\n",
              "  0.5220903010033445,\n",
              "  0.49296296296296294,\n",
              "  0.5007575757575757,\n",
              "  0.49879598662207353,\n",
              "  0.47345762711864414,\n",
              "  0.4779729729729729,\n",
              "  0.5109698996655517,\n",
              "  0.4334146341463415,\n",
              "  0.5020302013422819,\n",
              "  0.5194127516778524,\n",
              "  0.4629251700680271,\n",
              "  0.5242642140468228,\n",
              "  0.5138144329896907,\n",
              "  0.45125,\n",
              "  0.45901666666666663,\n",
              "  0.5115771812080537,\n",
              "  0.4532943143812709,\n",
              "  0.49088926174496644,\n",
              "  0.46446127946127946,\n",
              "  0.5133277591973244,\n",
              "  0.4664548494983278,\n",
              "  0.4470469798657717,\n",
              "  0.5137080536912751,\n",
              "  0.46623333333333333,\n",
              "  0.5363590604026846,\n",
              "  0.5144333333333333,\n",
              "  0.46601398601398614,\n",
              "  0.568020134228188,\n",
              "  0.5382653061224489,\n",
              "  0.4147,\n",
              "  0.4706166666666667,\n",
              "  0.44731666666666675,\n",
              "  0.5023737373737374,\n",
              "  0.5208361204013378,\n",
              "  0.4709152542372882,\n",
              "  0.5044781144781144,\n",
              "  0.5468288590604027,\n",
              "  0.45015202702702706,\n",
              "  0.46268333333333334,\n",
              "  0.5027090301003344,\n",
              "  0.520603448275862,\n",
              "  0.5154931972789116,\n",
              "  0.48775510204081635,\n",
              "  0.5186577181208054,\n",
              "  0.553047138047138,\n",
              "  0.4714882943143813,\n",
              "  0.4853187919463087,\n",
              "  0.511097972972973,\n",
              "  0.47322147651006713,\n",
              "  0.5298333333333333,\n",
              "  0.5164093959731543,\n",
              "  0.520406779661017,\n",
              "  0.48570234113712374,\n",
              "  0.5408417508417508,\n",
              "  0.5097440273037542,\n",
              "  0.47539249146757684,\n",
              "  0.4580701754385965,\n",
              "  0.4844666666666666,\n",
              "  0.4899664429530202,\n",
              "  0.48711409395973143,\n",
              "  0.5195652173913043,\n",
              "  0.5301839464882943,\n",
              "  0.4863869863013698,\n",
              "  0.5006543624161074,\n",
              "  0.48322147651006714,\n",
              "  0.49011864406779654,\n",
              "  0.5114334470989762,\n",
              "  0.4751174496644295,\n",
              "  0.5001003344481606,\n",
              "  0.5609563758389261,\n",
              "  0.44483164983164986,\n",
              "  0.4744463087248322,\n",
              "  0.5081879194630873,\n",
              "  0.47133333333333327,\n",
              "  0.5034228187919464,\n",
              "  0.529074074074074,\n",
              "  0.5019630872483221,\n",
              "  0.4805593220338983,\n",
              "  0.5192229729729729,\n",
              "  0.4886622073578595,\n",
              "  0.5635521885521886,\n",
              "  0.5410067114093959,\n",
              "  0.5675585284280936,\n",
              "  0.4933559322033899,\n",
              "  0.536472602739726,\n",
              "  0.48785958904109583,\n",
              "  0.4466666666666666,\n",
              "  0.5106040268456375,\n",
              "  0.5096127946127946,\n",
              "  0.5230536912751678,\n",
              "  0.532020202020202,\n",
              "  0.46380471380471383,\n",
              "  0.4517845117845118,\n",
              "  0.5097147651006712,\n",
              "  0.5814455782312926,\n",
              "  0.5258813559322033,\n",
              "  0.5177609427609426,\n",
              "  0.5233724832214766,\n",
              "  0.44001666666666667,\n",
              "  0.4424161073825503,\n",
              "  0.5341864406779661,\n",
              "  0.5096333333333333,\n",
              "  0.5018729096989967,\n",
              "  0.46667241379310337,\n",
              "  0.47808333333333336,\n",
              "  0.47790969899665553,\n",
              "  0.4934006734006734,\n",
              "  0.49050167224080266,\n",
              "  0.5393456375838926,\n",
              "  0.48613712374581936,\n",
              "  0.5039632107023412,\n",
              "  0.5216107382550336,\n",
              "  0.4837710437710438,\n",
              "  0.5121790540540541,\n",
              "  0.49037542662116046,\n",
              "  0.4874149659863945,\n",
              "  0.43373333333333336,\n",
              "  0.45953736654804267,\n",
              "  0.49934121621621624,\n",
              "  0.5231040268456375,\n",
              "  0.5233788395904437,\n",
              "  0.5102833333333333,\n",
              "  0.43988135593220334,\n",
              "  0.4495333333333334,\n",
              "  0.4716107382550335,\n",
              "  0.4994406779661017,\n",
              "  0.4835016835016835,\n",
              "  0.49690235690235696,\n",
              "  0.46456228956228957,\n",
              "  0.5095454545454545,\n",
              "  0.455234899328859,\n",
              "  0.5354530201342282,\n",
              "  0.4731418918918918,\n",
              "  0.48145762711864415,\n",
              "  0.4523559322033898,\n",
              "  0.48796979865771817,\n",
              "  0.5371476510067114,\n",
              "  0.5280434782608695,\n",
              "  0.4856020066889632,\n",
              "  0.48331081081081084,\n",
              "  0.5178451178451178,\n",
              "  0.49209030100334444,\n",
              "  0.4645637583892617,\n",
              "  0.5245408163265306,\n",
              "  0.5036454849498327,\n",
              "  0.4619127516778524,\n",
              "  0.47082770270270274,\n",
              "  0.5177526132404181,\n",
              "  0.49275337837837835,\n",
              "  0.48850168350168355,\n",
              "  0.5257912457912458,\n",
              "  0.4769,\n",
              "  0.4960833333333333,\n",
              "  0.49108695652173906,\n",
              "  0.5005387205387206,\n",
              "  0.4732659932659933,\n",
              "  0.506505016722408,\n",
              "  0.5124664429530202,\n",
              "  0.5114455782312926,\n",
              "  0.47067567567567564,\n",
              "  0.5106587837837837,\n",
              "  0.46286912751677856,\n",
              "  0.5105852842809365,\n",
              "  0.4878885135135135,\n",
              "  0.49133898305084744,\n",
              "  0.45884228187919457,\n",
              "  0.47850340136054414,\n",
              "  0.5066216216216215,\n",
              "  0.4087919463087248,\n",
              "  0.5161241610738255,\n",
              "  0.5494127516778523,\n",
              "  0.5082935153583618,\n",
              "  0.4590436241610738,\n",
              "  0.4924149659863945,\n",
              "  0.554006734006734,\n",
              "  0.47704391891891884,\n",
              "  0.5207432432432432,\n",
              "  0.5062709030100334,\n",
              "  0.5073389830508476,\n",
              "  0.4749832775919733,\n",
              "  0.5169528619528619,\n",
              "  0.5294709897610921,\n",
              "  0.4894630872483221,\n",
              "  0.44110921501706485,\n",
              "  0.5246166666666666,\n",
              "  0.4837373737373738,\n",
              "  0.5259833333333334,\n",
              "  0.47755033557046983,\n",
              "  0.5343602693602693,\n",
              "  0.49867796610169485,\n",
              "  0.5237883959044367,\n",
              "  0.47274410774410763,\n",
              "  0.5209595959595961,\n",
              "  0.48508417508417506,\n",
              "  0.4763422818791946,\n",
              "  0.479496644295302,\n",
              "  0.5234006734006733,\n",
              "  0.42565656565656573,\n",
              "  0.4917892976588628,\n",
              "  0.5203780068728523,\n",
              "  0.45792642140468226,\n",
              "  0.5047157190635452,\n",
              "  0.497312925170068,\n",
              "  0.48472881355932207,\n",
              "  0.4778519855595668,\n",
              "  0.5351174496644296,\n",
              "  0.5437751677852349,\n",
              "  0.5239100346020762,\n",
              "  0.5277257525083613,\n",
              "  0.5220066889632107,\n",
              "  0.47023333333333334,\n",
              "  0.5529661016949152,\n",
              "  0.46469696969696966,\n",
              "  0.4991016949152543,\n",
              "  0.49888888888888894,\n",
              "  0.49808724832214757,\n",
              "  0.47988294314381263,\n",
              "  0.5127013422818792,\n",
              "  0.48275167785234896,\n",
              "  0.4801337792642141,\n",
              "  0.5034563758389262,\n",
              "  0.5400682593856656,\n",
              "  0.5208724832214765,\n",
              "  0.4987542087542087,\n",
              "  0.49344178082191775,\n",
              "  0.5159793814432989,\n",
              "  0.4735810810810811,\n",
              "  0.4954391891891892,\n",
              "  0.47853040540540537,\n",
              "  0.4951672240802676,\n",
              "  0.48761824324324327,\n",
              "  0.5020731707317073,\n",
              "  0.5373170731707316,\n",
              "  0.5116498316498316,\n",
              "  0.49541666666666667,\n",
              "  0.445625,\n",
              "  0.5227702702702703,\n",
              "  0.5168624161073826,\n",
              "  0.46624579124579124,\n",
              "  0.5020401337792642,\n",
              "  0.47689597315436244,\n",
              "  0.46092592592592596,\n",
              "  0.48952702702702694,\n",
              "  0.4631481481481482,\n",
              "  0.461322033898305,\n",
              "  0.5354697986577182,\n",
              "  0.5017060810810811,\n",
              "  0.5135570469798658,\n",
              "  0.49314814814814817,\n",
              "  0.49813559322033907,\n",
              "  0.5467666666666666,\n",
              "  0.5061734693877551,\n",
              "  0.5593097643097643,\n",
              "  0.5166949152542373,\n",
              "  0.5012207357859532,\n",
              "  0.5349498327759197,\n",
              "  0.5330236486486487,\n",
              "  0.5159013605442178,\n",
              "  0.49682432432432433,\n",
              "  0.5327586206896552,\n",
              "  0.5725503355704699,\n",
              "  0.4936332179930796,\n",
              "  0.49322033898305084,\n",
              "  0.49498327759197325,\n",
              "  0.5394557823129251,\n",
              "  0.5487919463087249,\n",
              "  0.5065939597315436,\n",
              "  0.48638795986622074,\n",
              "  0.45876666666666666,\n",
              "  0.5292226148409893,\n",
              "  0.474314381270903,\n",
              "  0.5104237288135592,\n",
              "  0.4467979452054794,\n",
              "  0.48880701754385963,\n",
              "  0.4962709030100334,\n",
              "  0.48084459459459455,\n",
              "  0.5093478260869565,\n",
              "  0.47060606060606064,\n",
              "  0.5025762711864407,\n",
              "  0.5145805369127516,\n",
              "  0.5496822742474917,\n",
              "  0.4870469798657718,\n",
              "  0.47839590443686003,\n",
              "  0.48964765100671137,\n",
              "  0.46413333333333334,\n",
              "  0.5319795221843003,\n",
              "  0.5386904761904762,\n",
              "  0.5421717171717171,\n",
              "  0.4692439862542956,\n",
              "  0.5473600000000001,\n",
              "  0.5132432432432432,\n",
              "  0.50375,\n",
              "  0.5296632996632997,\n",
              "  0.4965202702702703,\n",
              "  0.478578947368421,\n",
              "  0.47636518771331066,\n",
              "  0.4555629139072847,\n",
              "  0.48959731543624163,\n",
              "  0.4640301003344482,\n",
              "  0.45298333333333335,\n",
              "  0.5874166666666667,\n",
              "  0.49489898989898995,\n",
              "  0.5059259259259259,\n",
              "  0.5257432432432433,\n",
              "  0.49845637583892616,\n",
              "  0.4891608391608392,\n",
              "  0.5274666666666668,\n",
              "  0.5034060402684564,\n",
              "  0.5362157534246575,\n",
              "  0.4759152542372881,\n",
              "  0.5053020134228188,\n",
              "  0.5268394648829431,\n",
              "  0.4954833333333333,\n",
              "  0.5153846153846153,\n",
              "  0.4527926421404682,\n",
              "  0.4899664429530201,\n",
              "  0.5071380471380471,\n",
              "  0.46488175675675675,\n",
              "  0.5043229166666667,\n",
              "  0.5051174496644295,\n",
              "  0.46356187290969897,\n",
              "  0.5458952702702703,\n",
              "  0.5374493243243242,\n",
              "  0.5136241610738255,\n",
              "  0.47994983277591974,\n",
              "  0.5575083612040134,\n",
              "  0.48785234899328855,\n",
              "  0.5042953020134229,\n",
              "  0.5679362416107383,\n",
              "  0.511438127090301,\n",
              "  0.4945791245791246,\n",
              "  0.5053061224489795,\n",
              "  0.5153,\n",
              "  0.4635964912280702,\n",
              "  0.5172661870503598,\n",
              "  0.4979833333333333,\n",
              "  0.5073666666666667,\n",
              "  0.5246333333333333,\n",
              "  0.48238596491228064,\n",
              "  0.4733724832214765,\n",
              "  0.4758754208754209,\n",
              "  0.5018275862068965,\n",
              "  0.46550847457627126,\n",
              "  0.5089597315436242,\n",
              "  0.5215333333333334,\n",
              "  0.5344594594594594,\n",
              "  0.48602040816326525,\n",
              "  0.5236700336700336,\n",
              "  0.48030100334448156,\n",
              "  0.5008833333333332,\n",
              "  0.4462080536912751,\n",
              "  0.5274664429530201,\n",
              "  0.4931605351170568,\n",
              "  0.5034113712374582,\n",
              "  0.48000000000000004,\n",
              "  0.4696321070234114,\n",
              "  0.47827759197324415,\n",
              "  0.5293749999999999,\n",
              "  0.4711734693877551,\n",
              "  0.4910200668896321,\n",
              "  0.534158249158249,\n",
              "  0.4987751677852349,\n",
              "  0.4763468013468014,\n",
              "  0.42827759197324416,\n",
              "  0.4990033783783783,\n",
              "  0.49957912457912457,\n",
              "  0.5128666666666666,\n",
              "  0.5181605351170568,\n",
              "  0.5035570469798658,\n",
              "  0.5267666666666667,\n",
              "  0.4875255972696246,\n",
              "  0.48280936454849505,\n",
              "  0.5031525423728814,\n",
              "  0.5219932432432433,\n",
              "  0.46901754385964917,\n",
              "  0.47231543624161076,\n",
              "  0.514765100671141,\n",
              "  0.5161111111111111,\n",
              "  0.48959731543624163,\n",
              "  0.4778885135135136,\n",
              "  0.5172666666666667,\n",
              "  0.503047138047138,\n",
              "  0.5344576271186441,\n",
              "  0.5263701067615658,\n",
              "  0.539006734006734,\n",
              "  0.5024747474747475,\n",
              "  0.5847297297297297,\n",
              "  0.5285953177257525,\n",
              "  0.49934931506849317,\n",
              "  0.45631756756756753,\n",
              "  0.49772241992882565,\n",
              "  0.49659322033898307,\n",
              "  0.5309830508474577,\n",
              "  0.5376279863481228,\n",
              "  0.4736394557823129,\n",
              "  0.49710437710437716,\n",
              "  0.4889932885906041,\n",
              "  0.4759830508474577,\n",
              "  0.4945317725752509,\n",
              "  0.44537288135593217,\n",
              "  0.4679761904761905,\n",
              "  0.49750853242320825,\n",
              "  0.4819759450171821,\n",
              "  0.496077441077441,\n",
              "  0.4887883959044368,\n",
              "  0.4821380471380471,\n",
              "  0.44003367003367,\n",
              "  0.48910169491525424,\n",
              "  0.48018394648829427,\n",
              "  0.5362040133779264,\n",
              "  0.515448275862069,\n",
              "  0.500755033557047,\n",
              "  0.44824915824915834,\n",
              "  0.5251839464882944,\n",
              "  0.45313793103448263,\n",
              "  0.4666379310344827,\n",
              "  0.5026923076923077,\n",
              "  0.4983277591973244,\n",
              "  0.4983053691275168,\n",
              "  0.5064189189189189,\n",
              "  0.4641275167785235,\n",
              "  0.5312876254180602,\n",
              "  0.5292592592592592,\n",
              "  0.49906040268456375,\n",
              "  0.5428727272727274,\n",
              "  0.4883529411764706,\n",
              "  0.4906565656565656,\n",
              "  0.5124080267558528,\n",
              "  0.45056896551724146,\n",
              "  0.5290909090909091,\n",
              "  0.484180602006689,\n",
              "  0.5090301003344482,\n",
              "  0.4850333333333333,\n",
              "  0.5743265993265994,\n",
              "  0.45899999999999996,\n",
              "  0.5474247491638796,\n",
              "  0.4777871621621622,\n",
              "  0.4957070707070706,\n",
              "  0.4694147157190636,\n",
              "  0.5243050847457628,\n",
              "  0.5183724832214766,\n",
              "  0.5313804713804714,\n",
              "  0.5219360269360269,\n",
              "  0.5243311036789298,\n",
              "  0.46097602739726024,\n",
              "  0.46197986577181205,\n",
              "  0.5414965986394558,\n",
              "  0.49306397306397304,\n",
              "  0.4921626297577855,\n",
              "  0.4832833333333334,\n",
              "  0.53405,\n",
              "  0.5016666666666667,\n",
              "  0.48206375838926185,\n",
              "  0.5215551839464883,\n",
              "  0.49484797297297295,\n",
              "  0.4618918918918919,\n",
              "  0.5167725752508362,\n",
              "  0.5461744966442953,\n",
              "  0.5058614864864865,\n",
              "  0.4829026845637584,\n",
              "  0.4571186440677967,\n",
              "  0.46092592592592585,\n",
              "  0.48891304347826087,\n",
              "  0.48593959731543623,\n",
              "  0.48707070707070704,\n",
              "  0.45239864864864865,\n",
              "  0.506996644295302,\n",
              "  0.4936287625418061,\n",
              "  0.4944648829431437,\n",
              "  0.5095719178082191,\n",
              "  0.4756902356902356,\n",
              "  0.47297979797979794,\n",
              "  0.5166151202749141,\n",
              "  0.5512962962962963,\n",
              "  0.47488255033557053,\n",
              "  0.4978929765886287,\n",
              "  0.5722408026755854,\n",
              "  0.4856354515050167,\n",
              "  0.5051845637583893,\n",
              "  0.5299,\n",
              "  0.5032166666666666,\n",
              "  0.4287123745819398,\n",
              "  0.47069023569023577,\n",
              "  0.5128146853146854,\n",
              "  0.49897993311036787,\n",
              "  0.4832542372881356,\n",
              "  0.46282534246575346,\n",
              "  0.4983728813559322,\n",
              "  0.4912166666666667,\n",
              "  0.5066778523489932,\n",
              "  0.49242214532871975,\n",
              "  0.485484949832776,\n",
              "  0.46816498316498323,\n",
              "  0.5143728813559323,\n",
              "  0.47969798657718116,\n",
              "  0.47278523489932883,\n",
              "  0.4831649831649832,\n",
              "  0.5142307692307693,\n",
              "  0.5402666666666667,\n",
              "  0.5081166666666667,\n",
              "  0.48472881355932207,\n",
              "  0.5139666666666667,\n",
              "  0.5158249158249157,\n",
              "  0.4998825503355705,\n",
              "  0.49658703071672355,\n",
              "  0.5382659932659933,\n",
              "  0.5220333333333333,\n",
              "  0.4676342281879195,\n",
              "  0.5163087248322147,\n",
              "  0.5377181208053692,\n",
              "  0.4930666666666667,\n",
              "  0.520976430976431,\n",
              "  0.514234693877551,\n",
              "  0.48811864406779665,\n",
              "  0.49911262798634815,\n",
              "  0.5129461279461279,\n",
              "  0.48395973154362415,\n",
              "  0.5020069204152249,\n",
              "  0.5632833333333332,\n",
              "  0.4916329966329966,\n",
              "  0.5199498327759197,\n",
              "  0.4844932432432432,\n",
              "  0.4519152542372881,\n",
              "  0.46169520547945203,\n",
              "  0.5379898648648649,\n",
              "  0.48983050847457626,\n",
              "  0.47415,\n",
              "  0.4968493150684931,\n",
              "  0.4815488215488215,\n",
              "  0.49092905405405407,\n",
              "  0.5537024221453286,\n",
              "  0.4992517006802722,\n",
              "  0.5560273972602741,\n",
              "  0.4616329966329967,\n",
              "  0.5340133779264215,\n",
              "  0.4857407407407408,\n",
              "  0.5116220735785953,\n",
              "  0.4730338983050848,\n",
              "  0.5430067567567567,\n",
              "  0.43994983277591976,\n",
              "  0.4688294314381271,\n",
              "  0.47162751677852344,\n",
              "  0.5428020134228188,\n",
              "  0.4943288590604027,\n",
              "  0.4503511705685619,\n",
              "  0.46337248322147645,\n",
              "  0.5026408450704225,\n",
              "  0.52475,\n",
              "  0.5212881355932203,\n",
              "  0.5354194630872483,\n",
              "  0.5201003344481605,\n",
              "  0.4917796610169491,\n",
              "  0.5175083612040134,\n",
              "  0.5307692307692308,\n",
              "  0.49915824915824913,\n",
              "  0.4952516778523489,\n",
              "  0.5216267123287671,\n",
              "  0.5628355704697987,\n",
              "  0.4959,\n",
              "  0.5148829431438128,\n",
              "  0.4626510067114094,\n",
              "  0.46360269360269357,\n",
              "  0.49474916387959866,\n",
              "  0.5097,\n",
              "  0.4878209459459459,\n",
              "  0.47298333333333337,\n",
              "  0.5161111111111111,\n",
              "  0.5161538461538461,\n",
              "  0.49625418060200677,\n",
              "  0.5447491638795986,\n",
              "  0.5152013422818793,\n",
              "  0.4983053691275168,\n",
              "  0.4597153024911032,\n",
              "  0.49917508417508416,\n",
              "  0.5687583892617449,\n",
              "  0.46326013513513514,\n",
              "  0.459593220338983,\n",
              "  0.5050836120401339,\n",
              "  0.5369666666666667,\n",
              "  0.4934523809523809,\n",
              "  0.4723489932885906,\n",
              "  0.48255,\n",
              "  0.5183053691275168,\n",
              "  0.5298657718120806,\n",
              "  0.5257912457912458,\n",
              "  0.5239212328767123,\n",
              "  0.48101010101010105,\n",
              "  0.5167064846416382,\n",
              "  0.4664093959731543,\n",
              "  0.45555000000000007,\n",
              "  0.5237878787878788,\n",
              "  0.5149498327759197,\n",
              "  0.5441694915254237,\n",
              "  0.4962881355932203,\n",
              "  0.5118707482993198,\n",
              "  0.4753666666666667,\n",
              "  0.531677966101695,\n",
              "  0.484040404040404,\n",
              "  0.4946666666666666,\n",
              "  0.43809764309764315,\n",
              "  0.5078983050847458,\n",
              "  0.5026621160409557,\n",
              "  0.4760847457627118,\n",
              "  0.4793979933110368,\n",
              "  0.4944310344827586,\n",
              "  0.5414776632302405,\n",
              "  0.496304347826087,\n",
              "  0.48961409395973166,\n",
              "  0.49801003344481604,\n",
              "  0.47336734693877547,\n",
              "  0.4625174825174824,\n",
              "  0.46787162162162166,\n",
              "  0.4912372881355933,\n",
              "  0.5251194539249148,\n",
              "  0.4995973154362416,\n",
              "  0.5223310810810812,\n",
              "  0.4732993197278912,\n",
              "  0.4779830508474576,\n",
              "  0.48501683501683507,\n",
              "  0.5376013513513513,\n",
              "  0.5601845637583893,\n",
              "  0.5181208053691275,\n",
              "  0.5213833333333334,\n",
              "  0.45923333333333327,\n",
              "  0.5131740614334471,\n",
              "  0.5217517006802721,\n",
              "  0.47886440677966097,\n",
              "  0.4231543624161074,\n",
              "  0.5117785234899328,\n",
              "  0.5104697986577181,\n",
              "  0.4541095890410959,\n",
              "  0.4915166666666666,\n",
              "  0.5079598662207357,\n",
              "  0.5042976588628763,\n",
              "  0.4510833333333333,\n",
              "  0.4835402684563758,\n",
              "  0.514728813559322,\n",
              "  0.4927684563758388,\n",
              "  0.4428595317725753,\n",
              "  0.49643333333333334,\n",
              "  0.5225945017182131,\n",
              "  0.48718855218855217,\n",
              "  0.5243456375838926,\n",
              "  0.516996644295302,\n",
              "  0.5944147157190635,\n",
              "  0.49298657718120814,\n",
              "  0.48003367003367003,\n",
              "  0.52755,\n",
              "  0.44249163879598663,\n",
              "  0.5160570469798658,\n",
              "  0.48051839464882945,\n",
              "  0.5188500000000001,\n",
              "  0.46996621621621626,\n",
              "  0.5292708333333334,\n",
              "  0.5268013468013468,\n",
              "  0.5295748299319728,\n",
              "  0.48021739130434776,\n",
              "  0.5125257731958762,\n",
              "  0.43437710437710436,\n",
              "  0.4676006711409396,\n",
              "  0.49679054054054056,\n",
              "  0.5582828282828283,\n",
              "  0.5199832214765101,\n",
              "  0.5119630872483222,\n",
              "  0.46155357142857145,\n",
              "  0.5418729096989966,\n",
              "  0.5193074324324325,\n",
              "  0.5131462585034013,\n",
              "  0.5026006711409396,\n",
              "  0.4868074324324324,\n",
              "  0.46736577181208044,\n",
              "  0.5047826086956522,\n",
              "  0.4902356902356903,\n",
              "  0.5182666666666667,\n",
              "  0.5674242424242425,\n",
              "  0.5415529010238909,\n",
              "  0.4686912751677853,\n",
              "  0.4900841750841751,\n",
              "  0.5297474747474746,\n",
              "  0.5044314381270902,\n",
              "  0.5328282828282829,\n",
              "  0.48616438356164376,\n",
              "  0.5031462585034014,\n",
              "  0.523441780821918,\n",
              "  0.4733454545454545,\n",
              "  0.42665517241379314,\n",
              "  0.4771043771043771,\n",
              "  0.46966442953020127,\n",
              "  0.5171672354948805,\n",
              "  0.4930434782608695,\n",
              "  0.5597833333333333,\n",
              "  0.4861993243243243,\n",
              "  0.49915824915824913,\n",
              "  0.4901010101010101,\n",
              "  0.4865217391304349,\n",
              "  0.4753209459459459,\n",
              "  0.46611111111111114,\n",
              "  0.46842809364548493,\n",
              "  0.5173817567567568,\n",
              "  0.4738006756756757,\n",
              "  0.5343412162162163,\n",
              "  0.47237201365187714,\n",
              "  0.49872053872053873,\n",
              "  0.5098979591836734,\n",
              "  0.4948109965635739,\n",
              "  0.50251677852349,\n",
              "  0.49245000000000005,\n",
              "  0.4960606060606061,\n",
              "  0.5536486486486486,\n",
              "  0.521695205479452,\n",
              "  0.509421768707483,\n",
              "  0.522680412371134,\n",
              "  0.4985016835016835,\n",
              "  0.49188333333333334,\n",
              "  0.48801038062283747,\n",
              "  0.48381355932203396,\n",
              "  0.499486301369863,\n",
              "  0.5022895622895623,\n",
              "  0.5076520270270269,\n",
              "  0.46701013513513506,\n",
              "  0.5038408304498271,\n",
              "  0.5073913043478261,\n",
              "  0.5087455830388693,\n",
              "  0.522752613240418,\n",
              "  0.49604729729729735,\n",
              "  0.5263175675675675,\n",
              "  0.5418686868686868,\n",
              "  0.48927609427609425,\n",
              "  0.5388344594594595,\n",
              "  0.4806833333333333,\n",
              "  0.5506521739130434,\n",
              "  0.49655932203389835,\n",
              "  0.5231375838926174,\n",
              "  0.5256101694915254,\n",
              "  0.5380034129692832,\n",
              "  0.4919166666666666,\n",
              "  0.4754560810810811,\n",
              "  0.45010309278350524,\n",
              "  0.5352941176470588,\n",
              "  0.5264093959731544,\n",
              "  0.5227364864864865,\n",
              "  0.4979863481228669,\n",
              "  0.5204500000000001,\n",
              "  0.45692307692307693,\n",
              "  0.49680602006688956,\n",
              "  0.5259364548494984,\n",
              "  0.5484013605442176,\n",
              "  0.49581355932203386,\n",
              "  0.48983050847457626,\n",
              "  0.5163050847457628,\n",
              "  0.5418624161073825,\n",
              "  0.4764882943143814,\n",
              "  0.5061952861952862,\n",
              "  0.548271186440678,\n",
              "  0.4816838487972508,\n",
              "  0.5204545454545455,\n",
              "  0.4725925925925926,\n",
              "  0.5014310344827586,\n",
              "  0.488271186440678,\n",
              "  0.5083274021352312,\n",
              "  0.4639730639730639,\n",
              "  0.4688628762541806,\n",
              "  0.5112541806020067,\n",
              "  0.4989236111111111,\n",
              "  0.4757912457912458,\n",
              "  0.4936148648648649,\n",
              "  0.4952568493150685,\n",
              "  0.4615217391304348,\n",
              "  0.5263265306122449,\n",
              "  0.47578125000000004,\n",
              "  0.5380936454849498,\n",
              "  0.5236824324324324,\n",
              "  0.49381270903010027,\n",
              "  0.5121644295302014,\n",
              "  0.49306397306397304,\n",
              "  0.5556333333333333,\n",
              "  0.4517508417508418,\n",
              "  0.48285953177257523,\n",
              "  0.4941077441077441,\n",
              "  0.5059666666666668,\n",
              "  0.519728813559322,\n",
              "  0.481969696969697,\n",
              "  0.46498299319727887,\n",
              "  0.5062542372881356,\n",
              "  0.4557718120805369,\n",
              "  0.4617166666666666,\n",
              "  0.5254882154882154,\n",
              "  0.5292979452054793,\n",
              "  0.49048657718120814,\n",
              "  0.5034417808219178,\n",
              "  0.47675,\n",
              "  0.4706802721088435,\n",
              "  0.46969594594594594,\n",
              "  0.5138590604026846,\n",
              "  0.5453177257525084,\n",
              "  0.48486622073578595,\n",
              "  0.5129194630872483,\n",
              "  0.512053872053872,\n",
              "  0.5440878378378379,\n",
              "  0.5297986577181207,\n",
              "  0.5185785953177258,\n",
              "  0.48012411347517725,\n",
              "  0.48008361204013383,\n",
              "  0.5021717171717172,\n",
              "  0.5014237288135593,\n",
              "  0.45418644067796615,\n",
              "  0.5085999999999999,\n",
              "  0.5310590277777778,\n",
              "  0.538494983277592,\n",
              "  0.5206354515050167,\n",
              "  0.5235034013605442,\n",
              "  0.467809364548495,\n",
              "  0.4961379310344827,\n",
              "  0.5380993150684932,\n",
              "  0.5070735785953178,\n",
              "  0.48625838926174497,\n",
              "  0.508108108108108,\n",
              "  0.477891156462585,\n",
              "  0.485993265993266,\n",
              "  0.4961784511784512,\n",
              "  0.5470637583892618,\n",
              "  0.49836734693877555,\n",
              "  0.49909698996655527,\n",
              "  0.4723141891891892,\n",
              "  0.4788166666666667,\n",
              "  0.4986531986531986,\n",
              "  0.4561945392491467,\n",
              "  0.5314576271186441,\n",
              "  0.48588135593220344,\n",
              "  0.5450170648464163,\n",
              "  0.48251700680272114,\n",
              "  0.5285353535353535,\n",
              "  0.47854729729729734,\n",
              "  0.509708904109589,\n",
              "  0.5242905405405406,\n",
              "  0.5001355932203391,\n",
              "  0.5116329966329967,\n",
              "  0.44143812709030106,\n",
              "  0.49944444444444436,\n",
              "  0.5025767918088737,\n",
              "  0.5138461538461538,\n",
              "  0.5132280701754386,\n",
              "  0.47818027210884345,\n",
              "  0.5130743243243243,\n",
              "  0.48465,\n",
              "  0.4978523489932886,\n",
              "  0.515376712328767,\n",
              "  0.5196271186440677,\n",
              "  0.47318181818181826,\n",
              "  0.5094463087248322,\n",
              "  0.5231292517006804,\n",
              "  0.4846644295302014,\n",
              "  0.47336206896551714,\n",
              "  0.5126020408163265,\n",
              "  0.515157894736842,\n",
              "  0.4679292929292929,\n",
              "  0.4985785953177258,\n",
              "  0.5111993243243244,\n",
              "  0.49479591836734693,\n",
              "  0.5172727272727272,\n",
              "  0.4939429530201342,\n",
              "  0.501322033898305,\n",
              "  0.4696452702702702,\n",
              "  0.4607357859531772,\n",
              "  0.542190635451505,\n",
              "  0.4427966101694915,\n",
              "  0.49980902777777775,\n",
              "  0.5307525083612041,\n",
              "  0.5223232323232323,\n",
              "  0.5159863945578231,\n",
              "  0.5366891891891892,\n",
              "  0.49733108108108104,\n",
              "  0.5273745819397994,\n",
              "  0.4839932885906041,\n",
              "  0.5105218855218855,\n",
              "  0.4881543624161074,\n",
              "  0.4503401360544218,\n",
              "  0.482195945945946,\n",
              "  0.5141216216216217,\n",
              "  0.4916053511705686,\n",
              "  0.43255972696245737,\n",
              "  0.5011186440677966,\n",
              "  0.46430976430976434,\n",
              "  0.4762633451957295,\n",
              "  0.48705084745762717,\n",
              "  0.5274833333333333,\n",
              "  0.5073657718120805,\n",
              "  0.5323389830508475,\n",
              "  0.5105351170568562,\n",
              "  0.4948333333333334,\n",
              "  0.45664965986394557,\n",
              "  0.5681081081081082,\n",
              "  0.5214527027027027,\n",
              "  0.479496644295302,\n",
              "  0.5156925675675675,\n",
              "  0.4499333333333334,\n",
              "  0.47349662162162165,\n",
              "  0.4859228187919462,\n",
              "  0.4464237288135593,\n",
              "  0.5309106529209622,\n",
              "  0.5038888888888889,\n",
              "  0.5384013605442177,\n",
              "  0.5056925675675675,\n",
              "  0.5293898305084747,\n",
              "  0.48503367003367004,\n",
              "  0.5549496644295303,\n",
              "  0.4931270903010033,\n",
              "  0.4860996563573883,\n",
              "  0.4679096989966555,\n",
              "  0.5141137123745818,\n",
              "  0.476996644295302,\n",
              "  0.5365319865319864,\n",
              "  0.4565771812080537,\n",
              "  0.4295108695652174,\n",
              "  0.48854948805460746,\n",
              "  0.5025680272108843,\n",
              "  0.4658221476510067,\n",
              "  0.4925250836120401,\n",
              "  0.5044481605351171,\n",
              "  0.4701003344481605,\n",
              "  0.4655500000000001,\n",
              "  0.4870401337792642,\n",
              "  0.4948657718120805,\n",
              "  0.46704391891891894,\n",
              "  0.49466442953020134,\n",
              "  0.48308080808080817,\n",
              "  0.5517391304347826,\n",
              "  0.4826755852842809,\n",
              "  0.4672390572390572,\n",
              "  0.5259731543624161,\n",
              "  0.5486409395973154,\n",
              "  0.4590476190476191,\n",
              "  0.4808333333333334,\n",
              "  0.47634680134680135,\n",
              "  0.4517056856187291,\n",
              "  0.461625,\n",
              "  0.5044107744107744,\n",
              "  0.4861824324324323,\n",
              "  0.4825932203389831,\n",
              "  0.5144982698961937,\n",
              "  0.5081103678929766,\n",
              "  0.5059931506849316,\n",
              "  0.5195748299319729,\n",
              "  0.5208892617449664,\n",
              "  0.4800513698630137,\n",
              "  0.4658703071672355,\n",
              "  0.49003355704697976,\n",
              "  0.4454054054054054,\n",
              "  0.47639261744966444,\n",
              "  0.463477508650519,\n",
              "  0.5056375838926174,\n",
              "  0.5338983050847458,\n",
              "  0.5206879194630872,\n",
              "  ...]}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_trainer._eval_manager = ImplicitEvalManager(evaluators=[auc_evaluator])\n",
        "model_trainer._num_negatives = 200\n",
        "model_trainer._exclude_positives([train_dataset, test_dataset_pos, test_dataset_neg])\n",
        "model_trainer._sample_negatives(seed=10)\n",
        "\n",
        "model_trainer._eval_save_prefix = folder_name+\"cml-yahoo-test-pos-biased\"\n",
        "model_trainer._evaluate_partial(test_dataset_pos)\n",
        "\n",
        "model_trainer._eval_save_prefix = folder_name+\"cml-yahoo-test-neg-biased\"\n",
        "model_trainer._evaluate_partial(test_dataset_neg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'auc': 0.772736165138026, 'recall': 0.45398828104948435}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eq(folder_name+\"cml-yahoo-test-pos-biased_evaluate_partial.pickle\", folder_name+\"cml-yahoo-test-neg-biased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", gamma=1.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'auc': 0.7651120425208878, 'recall': 0.4394679767082}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eq(folder_name+\"cml-yahoo-test-pos-biased_evaluate_partial.pickle\", folder_name+\"cml-yahoo-test-neg-biased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", gamma=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'auc': 0.7550797295856848, 'recall': 0.42104252841483814}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eq(folder_name+\"cml-yahoo-test-pos-biased_evaluate_partial.pickle\", folder_name+\"cml-yahoo-test-neg-biased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", gamma=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'auc': 0.16086465413381434, 'recall': 0.34524124934431627}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#gamma 1.5\n",
        "{'auc': 0.8449884377871792-0.6841237836533649, 'recall': 0.5929405866566257-0.24769933731230948}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'auc': 0.1604140385488535, 'recall': 0.33882941011729906}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Gamma 2\n",
        "{'auc': 0.8409330479463847-0.6805190093975312, 'recall': 0.5829036824161936-0.24407427229889453}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'auc': 0.15989914034678232, 'recall': 0.33072891372211416}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Gamma 3\n",
        "{'auc': 0.8357291786502742-0.6758300383034919, 'recall': 0.5702390779177972-0.239510164195683}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def aoa(infilename, infilename_neg, trainfilename, K=30):\n",
        "    infile = open(infilename, 'rb')\n",
        "    infile_neg = open(infilename_neg, 'rb')\n",
        "    P = pickle.load(infile)\n",
        "    infile.close()\n",
        "    P_neg = pickle.load(infile_neg)\n",
        "    infile_neg.close()\n",
        "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
        "    #\n",
        "    for theuser in P[\"users\"]:\n",
        "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
        "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
        "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
        "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
        "    #\n",
        "    Zui = dict()\n",
        "    Ni = dict()\n",
        "    # fill in dictionary Ni\n",
        "    trainset = np.load(trainfilename)\n",
        "    for i in trainset['item_id']:\n",
        "        if i in Ni:\n",
        "            Ni[i] += 1\n",
        "        else:\n",
        "            Ni[i] = 1\n",
        "    del trainset\n",
        "    # count #users with non-zero item frequencies\n",
        "    nonzero_user_count = 0\n",
        "    for theuser in P[\"users\"]:\n",
        "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
        "        for pos_item in pos_items:\n",
        "            if pos_item in Ni:\n",
        "                nonzero_user_count += 1\n",
        "                break\n",
        "\n",
        "    # fill in dictionary Zui\n",
        "    for theuser in P[\"users\"]:\n",
        "        all_scores = np.array(P[\"results\"][theuser])\n",
        "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
        "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
        "        for i, pos_item in enumerate(pos_items):\n",
        "            pos_score = pos_scores[i]\n",
        "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
        "    # calculate per-user scores\n",
        "    sum_user_auc = 0.0\n",
        "    sum_user_recall = 0.0\n",
        "\n",
        "    counter = 0\n",
        "\n",
        "    for theuser in P[\"users\"]:\n",
        "        numerator_auc = 0.0\n",
        "        numerator_recall = 0.0\n",
        "        denominator = 0.0\n",
        "        #CHECK THIS IS ACTUALLY CORRECT\n",
        "        #denominator = len(raw_data['train_data'][raw_data['train_data']['user_id'] == theuser]['item_id'])\n",
        "\n",
        "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
        "            if theitem not in Ni:\n",
        "                continue\n",
        "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser]))\n",
        "            # Calcolo il Recall a 30, vedi nota 6 paper\n",
        "            if Zui[(theuser, theitem)] < K:\n",
        "                numerator_recall += 1.0\n",
        "            denominator += 1 \n",
        "\n",
        "        if denominator > 0:\n",
        "            counter+=1\n",
        "            \n",
        "            sum_user_auc += numerator_auc / denominator\n",
        "            sum_user_recall += numerator_recall / denominator\n",
        "\n",
        "    return {\n",
        "        \"auc\"       : sum_user_auc / nonzero_user_count,\n",
        "        \"recall\"    : sum_user_recall / nonzero_user_count\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'auc': 0.857080935955807, 'recall': 0.6323394943425948}"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "aoa(folder_name+\"cml-yahoo-test-pos-biased_evaluate_partial.pickle\", folder_name+\"cml-yahoo-test-neg-biased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "RecSysEvaluation",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
