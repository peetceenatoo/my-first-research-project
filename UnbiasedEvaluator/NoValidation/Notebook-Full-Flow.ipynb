{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **IMPORT LIBS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from openrec.tf1.legacy import ImplicitModelTrainer\n",
    "from openrec.tf1.legacy.utils.evaluators import ImplicitEvalManager\n",
    "from openrec.tf1.legacy.utils import ImplicitDataset\n",
    "from openrec.tf1.legacy.recommenders import CML, BPR, PMF\n",
    "from openrec.tf1.legacy.utils.evaluators import AUC\n",
    "from openrec.tf1.legacy.utils.samplers import PairwiseSampler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sps\n",
    "import os\n",
    "import pickle\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **GENERATE THE DATASET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2384795\n",
    "np.random.seed(seed=seed)\n",
    "\n",
    "folder_name = f\"../Dataset/\"\n",
    "\n",
    "if os.path.exists(folder_name) == False:\n",
    "    os.makedirs(folder_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>SongID</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>83</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>153</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>170</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>184</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>194</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  SongID  Rating\n",
       "0       1      14       5\n",
       "1       1      35       1\n",
       "2       1      46       1\n",
       "3       1      83       1\n",
       "4       1      93       1\n",
       "5       1      94       1\n",
       "6       1     153       5\n",
       "7       1     170       4\n",
       "8       1     184       5\n",
       "9       1     194       5"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'yahoo_ymusic_v1/ydata-ymusic-rating-study-v1_0-train.txt'\n",
    "\n",
    "# Load the training set into a DataFrame\n",
    "df_train = pd.read_csv(folder_name+file_path, sep='\\t',names=[\"UserID\",\"SongID\",\"Rating\"], header=None)  # sep='\\t' for tab-separated values\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to implicit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"We treat items rated greater than or equal to 4 as relevant, and others as irrelevant, as suggested by prior literature.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>SongID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>ImplicitRating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>83</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>153</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>170</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>184</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>194</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  SongID  Rating  ImplicitRating\n",
       "0       1      14       5               1\n",
       "1       1      35       1               0\n",
       "2       1      46       1               0\n",
       "3       1      83       1               0\n",
       "4       1      93       1               0\n",
       "5       1      94       1               0\n",
       "6       1     153       5               1\n",
       "7       1     170       4               1\n",
       "8       1     184       5               1\n",
       "9       1     194       5               1"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "POSITIVE_THRESHOLD = 4\n",
    "df_train['ImplicitRating'] = np.where(df_train['Rating'] >= POSITIVE_THRESHOLD, 1, 0)\n",
    "\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the number of users and items in the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The training set contains 300K ratings given by 15.4K users against 1K songs through natural interactions.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 15400)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_user = df_train[\"UserID\"].min()\n",
    "max_user = df_train[\"UserID\"].max()\n",
    "\n",
    "min_item = df_train[\"SongID\"].min()\n",
    "max_item = df_train[\"SongID\"].max()\n",
    "\n",
    "max_item, max_user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **GET UNBIASED TESTSET**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the unbiased testset and convert it to implicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>SongID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>ImplicitRating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>138</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>141</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>177</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>268</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>511</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>587</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>772</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>941</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  SongID  Rating  ImplicitRating\n",
       "0       1      49       1               0\n",
       "1       1     126       1               0\n",
       "2       1     138       1               0\n",
       "3       1     141       1               0\n",
       "4       1     177       1               0\n",
       "5       1     268       3               0\n",
       "6       1     511       1               0\n",
       "7       1     587       1               0\n",
       "8       1     772       5               1\n",
       "9       1     941       1               0"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = '../Dataset/yahoo_ymusic_v1/ydata-ymusic-rating-study-v1_0-test.txt'\n",
    "df_test = pd.read_csv(file_path, sep='\\t',names=[\"UserID\",\"SongID\",\"Rating\"], header=None)  # sep='\\t' for tab-separated values\n",
    "df_test['ImplicitRating'] = np.where(df_test['Rating'] >= POSITIVE_THRESHOLD, 1, 0)\n",
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the number of users and items in the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The testing set is collected by asking a subset of 5.4K users to rate 10 randomly selected songs.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5400, 1000, 10)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[\"UserID\"].max(), df_test[\"SongID\"].max(), int(df_test.shape[0]/df_test[\"UserID\"].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter unbiased testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"We filter the testing set by retaining users who have at least a relevant and an irrelevant song in the testing set and two relevant songs in the training set.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select users with at least an irrelevant song in the unbiased testset\n",
    "usersWithNegativeInteractionInTest = df_test[df_test[\"ImplicitRating\"] == 0][\"UserID\"].unique()\n",
    "\n",
    "# Select UserID of users with at least a relevant song in testset\n",
    "usersWithPositiveInteractionInTest = df_test[df_test[\"ImplicitRating\"] == 1][\"UserID\"].unique()\n",
    "\n",
    "# Select UserID of users with at least two relevant song in trainset\n",
    "usersWithTwoPositiveInteractions = df_train[df_train[\"ImplicitRating\"] == 1].groupby(\"UserID\").filter(lambda x: len(x) >= 2)['UserID'].unique()\n",
    "\n",
    "# Compute the intersection\n",
    "set1 = set(usersWithNegativeInteractionInTest)\n",
    "set2 = set(usersWithPositiveInteractionInTest)\n",
    "set3 = set(usersWithTwoPositiveInteractions)\n",
    "valid_users_testset = set1 & set2 & set3\n",
    "\n",
    "# Filter the testset\n",
    "df_test_filtered = df_test[df_test[\"UserID\"].isin(valid_users_testset)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"2296 users satisfy these requirements.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2296"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_users_testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape the unbiased test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the dataframe, for each row where ImplicitRating is 1, append [userID, itemID] to unbiased_pos_test_set\n",
    "# and for each row where ImplicitRating is 0, append [userID, itemID] to unbiased_neg_test_set\n",
    "\n",
    "unbiased_pos_test_set = df_test_filtered[df_test_filtered[\"ImplicitRating\"] == 1][[\"UserID\", \"SongID\"]].values\n",
    "unbiased_neg_test_set = df_test_filtered[df_test_filtered[\"ImplicitRating\"] == 0][[\"UserID\", \"SongID\"]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save unbiased test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbiased_pos_test_set_df = pd.DataFrame(unbiased_pos_test_set)\n",
    "unbiased_neg_test_set_df = pd.DataFrame(unbiased_neg_test_set)\n",
    "\n",
    "unbiased_pos_test_set_df.columns = [\"user_id\",\"item_id\"]\n",
    "unbiased_neg_test_set_df.columns = [\"user_id\",\"item_id\"]\n",
    "\n",
    "structured_data_pos_test_set_unbiased = unbiased_pos_test_set_df.to_records(index=False)\n",
    "structured_data_neg_test_set_unbiased = unbiased_neg_test_set_df.to_records(index=False)\n",
    "\n",
    "np.save(folder_name + \"unbiased-test_arr_pos.npy\", structured_data_pos_test_set_unbiased)\n",
    "np.save(folder_name + \"unbiased-test_arr_neg.npy\", structured_data_neg_test_set_unbiased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **GET BIASED TESTSET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>SongID</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>83</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>153</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>170</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>184</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>194</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  SongID  Rating\n",
       "0       1      14       5\n",
       "1       1      35       1\n",
       "2       1      46       1\n",
       "3       1      83       1\n",
       "4       1      93       1\n",
       "5       1      94       1\n",
       "6       1     153       5\n",
       "7       1     170       4\n",
       "8       1     184       5\n",
       "9       1     194       5"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'yahoo_ymusic_v1/ydata-ymusic-rating-study-v1_0-train.txt'\n",
    "\n",
    "# Load the training set into a DataFrame\n",
    "df_train = pd.read_csv(folder_name+file_path, sep='\\t',names=[\"UserID\",\"SongID\",\"Rating\"], header=None)  # sep='\\t' for tab-separated values\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['ImplicitRating'] = np.where(df_train['Rating'] >= POSITIVE_THRESHOLD, 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[df_train[\"UserID\"].isin(valid_users_testset)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 58799 entries, 0 to 129178\n",
      "Data columns (total 4 columns):\n",
      " #   Column          Non-Null Count  Dtype\n",
      "---  ------          --------------  -----\n",
      " 0   UserID          58799 non-null  int64\n",
      " 1   SongID          58799 non-null  int64\n",
      " 2   Rating          58799 non-null  int64\n",
      " 3   ImplicitRating  58799 non-null  int64\n",
      "dtypes: int64(4)\n",
      "memory usage: 2.2 MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the biased test set and shape it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"We additionally held out a biased testing set (biased-testing) from the training set by randomly sampling 300 songs for each user.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_858/2769174745.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mpos_test_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_test_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mneg_test_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg_test_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Precompute, for each user, the list of songs with a relevant rating\n",
    "user_positive_ratings = df_train[df_train[\"ImplicitRating\"] == 1].groupby(\"UserID\")[\"SongID\"].apply(set)\n",
    "\n",
    "# Initialize the range of indexes for the items\n",
    "items_ids = np.arange(min_item, max_item + 1)\n",
    "# Set the number of songs for each user\n",
    "SONGS_FOR_BIASED_TEST = 300\n",
    "\n",
    "#IPOTESI MAN\n",
    "\n",
    "pos_test_set = []\n",
    "neg_test_set = []\n",
    "\n",
    "for user_id in valid_users_testset:\n",
    "    np.random.shuffle(items_ids)\n",
    "    test_items = set(items_ids[-SONGS_FOR_BIASED_TEST:])\n",
    "    pos_ids = user_positive_ratings.get(user_id, set()) & test_items\n",
    "\n",
    "    #set those to 0 so that they will no longer be used in training set\n",
    "    df_train.loc[(df_train['SongID'].isin(pos_ids)) & (df_train['UserID'] == user_id), 'ImplicitRating'] = 0\n",
    "\n",
    "    for id in test_items:\n",
    "        if id in pos_ids:\n",
    "            pos_test_set.append([user_id, id])\n",
    "        else:\n",
    "            neg_test_set.append([user_id, id])\n",
    "\n",
    "pos_test_set = np.array(pos_test_set)\n",
    "neg_test_set = np.array(neg_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the biased test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_test_set_df = pd.DataFrame(pos_test_set)\n",
    "neg_test_set_df = pd.DataFrame(neg_test_set)\n",
    "\n",
    "pos_test_set_df.columns = [\"user_id\",\"item_id\"]\n",
    "neg_test_set_df.columns = [\"user_id\",\"item_id\"]\n",
    "\n",
    "structured_data_pos_test_set = pos_test_set_df.to_records(index=False)\n",
    "structured_data_neg_test_set = neg_test_set_df.to_records(index=False)\n",
    "\n",
    "np.save(folder_name + \"biased-test_arr_pos.npy\", structured_data_pos_test_set)\n",
    "np.save(folder_name + \"biased-test_arr_neg.npy\", structured_data_neg_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **STORE TRAINSET**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take couples user-item filtering out the irrelevant ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only take the couples (user, item) with relevant rating\n",
    "new_df = df_train[df_train['ImplicitRating'] != 0]\n",
    "new_df = new_df.drop(columns=['Rating', 'ImplicitRating'])\n",
    "\n",
    "# Define a dictionary for renaming columns\n",
    "rename_dict = {\n",
    "    'UserID': 'user_id',\n",
    "    'SongID': 'item_id'\n",
    "}\n",
    "\n",
    "# Rename the columns\n",
    "new_df = new_df.rename(columns=rename_dict)\n",
    "\n",
    "# Convert the DataFrame to a structured array\n",
    "structured_data = new_df.to_records(index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = structured_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(folder_name + \"training_arr.npy\", train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MODEL CHOICE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = dict()\n",
    "raw_data['train_data'] = np.load(folder_name + \"training_arr.npy\")\n",
    "raw_data['max_user'] = 15401\n",
    "raw_data['max_item'] = 1001\n",
    "batch_size = 8000\n",
    "test_batch_size = 1000\n",
    "display_itr = 1000\n",
    "\n",
    "train_dataset = ImplicitDataset(raw_data['train_data'], raw_data['max_user'], raw_data['max_item'], name='Train')\n",
    "\n",
    "MODEL_CLASS = CML\n",
    "MODEL_PREFIX = \"cml\"\n",
    "DATASET_NAME = \"yahoo\"\n",
    "OUTPUT_FOLDER = \"./Output/\"\n",
    "OUTPUT_PATH = OUTPUT_FOLDER + MODEL_PREFIX + \"-\" + DATASET_NAME + \"/\"\n",
    "OUTPUT_PREFIX = str(OUTPUT_PATH) + str(MODEL_PREFIX) + \"-\" + str(DATASET_NAME)\n",
    "\n",
    "\n",
    "if os.path.exists(OUTPUT_PATH) == False:\n",
    "    os.makedirs(OUTPUT_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TRAIN THE MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:391: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:31: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:41: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:43: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:33: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/interactions/pairwise_eu_dist.py:71: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:596: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:75: The name tf.scatter_update is deprecated. Please use tf.compat.v1.scatter_update instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:144: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:365: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:148: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-05 16:12:29.587918: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
      "2024-04-05 16:12:29.594707: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 4192029999 Hz\n",
      "2024-04-05 16:12:29.595095: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55feb119e2a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2024-04-05 16:12:29.595115: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    }
   ],
   "source": [
    "# Avoid tensorflow using cached embeddings\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "\n",
    "model = MODEL_CLASS(batch_size=batch_size, max_user=train_dataset.max_user(), max_item=train_dataset.max_item(), \n",
    "    dim_embed=50, l2_reg=0.001, opt='Adam', sess_config=None)\n",
    "sampler = PairwiseSampler(batch_size=batch_size, dataset=train_dataset, num_process=4)\n",
    "model_trainer = ImplicitModelTrainer(batch_size=batch_size, test_batch_size=test_batch_size,\n",
    "                                     train_dataset=train_dataset, model=model, sampler=sampler,\n",
    "                                     eval_save_prefix=OUTPUT_PATH + DATASET_NAME,\n",
    "                                     item_serving_size=500)\n",
    "auc_evaluator = AUC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Start training with FULL evaluation ==\n",
      "[Itr 100] Finished\n",
      "[Itr 200] Finished\n",
      "[Itr 300] Finished\n",
      "[Itr 400] Finished\n",
      "[Itr 500] Finished\n",
      "[Itr 600] Finished\n",
      "[Itr 700] Finished\n",
      "[Itr 800] Finished\n",
      "[Itr 900] Finished\n",
      "[Itr 1000] Finished\n",
      "INFO:tensorflow:./Output/cml-yahoo/yahoo-1000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 1000] loss: 1758.855740\n",
      "[Itr 1100] Finished\n",
      "[Itr 1200] Finished\n",
      "[Itr 1300] Finished\n",
      "[Itr 1400] Finished\n",
      "[Itr 1500] Finished\n",
      "[Itr 1600] Finished\n",
      "[Itr 1700] Finished\n",
      "[Itr 1800] Finished\n",
      "[Itr 1900] Finished\n",
      "[Itr 2000] Finished\n",
      "INFO:tensorflow:./Output/cml-yahoo/yahoo-2000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 2000] loss: 649.815192\n",
      "[Itr 2100] Finished\n",
      "[Itr 2200] Finished\n",
      "[Itr 2300] Finished\n",
      "[Itr 2400] Finished\n",
      "[Itr 2500] Finished\n",
      "[Itr 2600] Finished\n",
      "[Itr 2700] Finished\n",
      "[Itr 2800] Finished\n",
      "[Itr 2900] Finished\n",
      "[Itr 3000] Finished\n",
      "INFO:tensorflow:./Output/cml-yahoo/yahoo-3000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 3000] loss: 574.620435\n",
      "[Itr 3100] Finished\n",
      "[Itr 3200] Finished\n",
      "[Itr 3300] Finished\n",
      "[Itr 3400] Finished\n",
      "[Itr 3500] Finished\n",
      "[Itr 3600] Finished\n",
      "[Itr 3700] Finished\n",
      "[Itr 3800] Finished\n",
      "[Itr 3900] Finished\n",
      "[Itr 4000] Finished\n",
      "INFO:tensorflow:./Output/cml-yahoo/yahoo-4000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 4000] loss: 545.546925\n",
      "[Itr 4100] Finished\n",
      "[Itr 4200] Finished\n",
      "[Itr 4300] Finished\n",
      "[Itr 4400] Finished\n",
      "[Itr 4500] Finished\n",
      "[Itr 4600] Finished\n",
      "[Itr 4700] Finished\n",
      "[Itr 4800] Finished\n",
      "[Itr 4900] Finished\n",
      "[Itr 5000] Finished\n",
      "INFO:tensorflow:./Output/cml-yahoo/yahoo-5000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 5000] loss: 531.343761\n",
      "[Itr 5100] Finished\n",
      "[Itr 5200] Finished\n",
      "[Itr 5300] Finished\n",
      "[Itr 5400] Finished\n",
      "[Itr 5500] Finished\n",
      "[Itr 5600] Finished\n",
      "[Itr 5700] Finished\n",
      "[Itr 5800] Finished\n",
      "[Itr 5900] Finished\n",
      "[Itr 6000] Finished\n",
      "INFO:tensorflow:./Output/cml-yahoo/yahoo-6000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 6000] loss: 524.894955\n",
      "[Itr 6100] Finished\n",
      "[Itr 6200] Finished\n",
      "[Itr 6300] Finished\n",
      "[Itr 6400] Finished\n",
      "[Itr 6500] Finished\n",
      "[Itr 6600] Finished\n",
      "[Itr 6700] Finished\n",
      "[Itr 6800] Finished\n",
      "[Itr 6900] Finished\n",
      "[Itr 7000] Finished\n",
      "INFO:tensorflow:./Output/cml-yahoo/yahoo-7000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 7000] loss: 521.943432\n",
      "[Itr 7100] Finished\n",
      "[Itr 7200] Finished\n",
      "[Itr 7300] Finished\n",
      "[Itr 7400] Finished\n",
      "[Itr 7500] Finished\n",
      "[Itr 7600] Finished\n",
      "[Itr 7700] Finished\n",
      "[Itr 7800] Finished\n",
      "[Itr 7900] Finished\n",
      "[Itr 8000] Finished\n",
      "INFO:tensorflow:./Output/cml-yahoo/yahoo-8000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 8000] loss: 519.208444\n",
      "[Itr 8100] Finished\n",
      "[Itr 8200] Finished\n",
      "[Itr 8300] Finished\n",
      "[Itr 8400] Finished\n",
      "[Itr 8500] Finished\n",
      "[Itr 8600] Finished\n",
      "[Itr 8700] Finished\n",
      "[Itr 8800] Finished\n",
      "[Itr 8900] Finished\n",
      "[Itr 9000] Finished\n",
      "INFO:tensorflow:./Output/cml-yahoo/yahoo-9000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 9000] loss: 517.250359\n",
      "[Itr 9100] Finished\n",
      "[Itr 9200] Finished\n",
      "[Itr 9300] Finished\n",
      "[Itr 9400] Finished\n",
      "[Itr 9500] Finished\n",
      "[Itr 9600] Finished\n",
      "[Itr 9700] Finished\n",
      "[Itr 9800] Finished\n",
      "[Itr 9900] Finished\n",
      "[Itr 10000] Finished\n",
      "INFO:tensorflow:./Output/cml-yahoo/yahoo-10000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 10000] loss: 516.686373\n"
     ]
    }
   ],
   "source": [
    "model_trainer.train(num_itr=10001, display_itr=display_itr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:./Output/cml-yahoo/ is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "model.save(OUTPUT_PATH,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DEFINING FUNCTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eq(infilename, infilename_neg, trainfilename, gamma=-1.0, K=1):\n",
    "    infile = open(infilename, 'rb')\n",
    "    infile_neg = open(infilename_neg, 'rb')\n",
    "    P = pickle.load(infile)\n",
    "    infile.close()\n",
    "    P_neg = pickle.load(infile_neg)\n",
    "    infile_neg.close()\n",
    "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
    "    #\n",
    "    for theuser in P[\"users\"]:\n",
    "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "    #\n",
    "    Zui = dict()\n",
    "    Ni = dict()\n",
    "    # fill in dictionary Ni\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "    del trainset\n",
    "\n",
    "    # count #users with non-zero item frequencies\n",
    "    nonzero_user_count = 0\n",
    "    for theuser in P[\"users\"]:\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for pos_item in pos_items:\n",
    "            if pos_item in Ni:\n",
    "                nonzero_user_count += 1\n",
    "                break\n",
    "    # fill in dictionary Zui\n",
    "    for theuser in P[\"users\"]:\n",
    "        all_scores = np.array(P[\"results\"][theuser])\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for i, pos_item in enumerate(pos_items):\n",
    "            pos_score = pos_scores[i]\n",
    "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
    "    # calculate per-user scores\n",
    "    sum_user_auc = 0.0\n",
    "    sum_user_recall = 0.0\n",
    "\n",
    "    for theuser in P[\"users\"]:\n",
    "        numerator_auc = 0.0\n",
    "        numerator_recall = 0.0\n",
    "        denominator = 0.0\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            if theitem not in Ni:\n",
    "                continue\n",
    "            pui = np.power(Ni[theitem], (gamma + 1) / 2.0)\n",
    "\n",
    "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser])) / pui\n",
    "            # Calcolo il Recall a 1, vedi nota 6 paper\n",
    "            if Zui[(theuser, theitem)] < K:\n",
    "                numerator_recall += 1.0 / pui\n",
    "            denominator += 1 / pui\n",
    "                \n",
    "\n",
    "        if denominator > 0:\n",
    "            sum_user_auc += numerator_auc / denominator\n",
    "            sum_user_recall += numerator_recall / denominator \n",
    "\n",
    "    return {\n",
    "        \"auc\"       : sum_user_auc / nonzero_user_count,\n",
    "        \"recall\"    : sum_user_recall / nonzero_user_count\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aoa(infilename, infilename_neg, trainfilename, K=1):\n",
    "    infile = open(infilename, 'rb')\n",
    "    infile_neg = open(infilename_neg, 'rb')\n",
    "    P = pickle.load(infile)\n",
    "    infile.close()\n",
    "    P_neg = pickle.load(infile_neg)\n",
    "    infile_neg.close()\n",
    "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
    "    #\n",
    "    for theuser in P[\"users\"]:\n",
    "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "    #\n",
    "    Zui = dict()\n",
    "    Ni = dict()\n",
    "    # fill in dictionary Ni\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "    del trainset\n",
    "    # count #users with non-zero item frequencies\n",
    "    nonzero_user_count = 0\n",
    "    for theuser in P[\"users\"]:\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for pos_item in pos_items:\n",
    "            if pos_item in Ni:\n",
    "                nonzero_user_count += 1\n",
    "                break\n",
    "    # fill in dictionary Zui\n",
    "    for theuser in P[\"users\"]:\n",
    "        all_scores = np.array(P[\"results\"][theuser])\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for i, pos_item in enumerate(pos_items):\n",
    "            pos_score = pos_scores[i]\n",
    "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
    "    # calculate per-user scores\n",
    "    sum_user_auc = 0.0\n",
    "    sum_user_recall = 0.0\n",
    "    for theuser in P[\"users\"]:\n",
    "        numerator_auc = 0.0\n",
    "        numerator_recall = 0.0\n",
    "        denominator = 0.0\n",
    "\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            if theitem not in Ni:\n",
    "                continue\n",
    "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser]))\n",
    "            # Calcolo il Recall a 30, vedi nota 6 paper\n",
    "            if Zui[(theuser, theitem)] < K:\n",
    "                numerator_recall += 1.0\n",
    "            denominator += 1 \n",
    "\n",
    "        if denominator > 0:\n",
    "            sum_user_auc += numerator_auc / denominator\n",
    "            sum_user_recall += numerator_recall / denominator\n",
    "\n",
    "    return {\n",
    "        \"auc\"       : sum_user_auc / nonzero_user_count,\n",
    "        \"recall\"    : sum_user_recall / nonzero_user_count\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified(infilename, infilename_neg, trainfilename, gamma=1.0, K=30, partition=10):\n",
    "\n",
    "    # Read pickles\n",
    "    infile = open(infilename, 'rb')\n",
    "    infile_neg = open(infilename_neg, 'rb')\n",
    "    P = pickle.load(infile)\n",
    "    infile.close()\n",
    "    P_neg = pickle.load(infile_neg)\n",
    "    infile_neg.close()\n",
    "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
    "\n",
    "    # Merge P and P_neg\n",
    "    for theuser in P[\"users\"]:\n",
    "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "\n",
    "    Zui = dict()\n",
    "    Ni = dict()\n",
    "\n",
    "    # Compute frequencies of items in the training set\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "    del trainset\n",
    "\n",
    "    # Compute recommendations for each user\n",
    "    for theuser in P[\"users\"]:\n",
    "        all_scores = np.array(P[\"results\"][theuser])\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for i, pos_item in enumerate(pos_items):\n",
    "            pos_score = pos_scores[i]\n",
    "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
    "\n",
    "    pui = dict()\n",
    "    w = dict()\n",
    "\n",
    "    # Compute dictionary of propensity scores\n",
    "    for theuser in P[\"users\"]:\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            if theitem not in Ni:\n",
    "                continue\n",
    "            if theitem in pui:\n",
    "                continue\n",
    "            pui[theitem] = np.power(Ni[theitem], (gamma + 1) / 2.0)\n",
    "\n",
    "    # Take the list of items (not tuples) in pui sorted by value\n",
    "    items_sorted_by_value = sorted(pui, key=pui.get, reverse=True)\n",
    "\n",
    "    # Compute linspace between the pui[0] and pui[-1] \n",
    "\n",
    "    # Maybe try to split the logspace instead of the linspace?\n",
    "    # logspace = np.logspace(pui[items_sorted_by_value[0]], pui[items_sorted_by_value[-1]], partition+1)\n",
    "\n",
    "    linspace = np.linspace(pui[items_sorted_by_value[0]], pui[items_sorted_by_value[-1]], partition+1)\n",
    "   \n",
    "    # Compute dictionary w, that is, for each item, assigns the average of the puis in the partition it belongs to\n",
    "    i=0\n",
    "    j = 0\n",
    "    while i < len(items_sorted_by_value):\n",
    "                            \n",
    "        avg = 0\n",
    "        start = i\n",
    "        end = i\n",
    "    \n",
    "        while i < len(items_sorted_by_value) and pui[items_sorted_by_value[i]] >= linspace[j+1]:\n",
    "            avg += 1.0 / pui[items_sorted_by_value[i]]\n",
    "            end = i\n",
    "            i += 1\n",
    "        \n",
    "        # Is the average the only good choice? even with the log space split?\n",
    "        avg = avg / (end - start + 1)\n",
    "\n",
    "        for k in range(start, end+1):\n",
    "            w[items_sorted_by_value[k]] = avg\n",
    "\n",
    "\n",
    "        j += 1\n",
    "\n",
    "    nonzero_user_count = 0\n",
    "    sum_user_auc = 0.0\n",
    "    sum_user_recall = 0.0\n",
    "\n",
    "    # Compute score with AUC and compute Recall\n",
    "    for theuser in P[\"users\"]:\n",
    "        numerator_auc = 0.0\n",
    "        numerator_recall = 0.0\n",
    "        denominator = 0.0\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            # Skip items with null frequency\n",
    "            if  theitem not in Ni:\n",
    "                continue\n",
    "            # Add things to be summed for each item\n",
    "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser])) * w[theitem]\n",
    "            # Add things for recall\n",
    "            if Zui[(theuser, theitem)] < K:\n",
    "                numerator_recall += 1.0 * w[theitem] #spetta\n",
    "            # Increment denominator that the sum must be divided by \n",
    "            denominator += 1 / pui[theitem]\n",
    "\n",
    "\n",
    "        # If there was at least one item for the user, count the user and sum the results\n",
    "        if denominator > 0:\n",
    "            nonzero_user_count += 1\n",
    "            sum_user_auc += numerator_auc / denominator\n",
    "            sum_user_recall += numerator_recall / denominator \n",
    "\n",
    "    return {\n",
    "        \"auc\"       : sum_user_auc / nonzero_user_count, \n",
    "        \"recall\"    : sum_user_recall / nonzero_user_count\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_logspace(infilename, infilename_neg, trainfilename, gamma=1.0, K=30, partition=10):\n",
    "\n",
    "    # Read pickles\n",
    "    infile = open(infilename, 'rb')\n",
    "    infile_neg = open(infilename_neg, 'rb')\n",
    "    P = pickle.load(infile)\n",
    "    infile.close()\n",
    "    P_neg = pickle.load(infile_neg)\n",
    "    infile_neg.close()\n",
    "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
    "\n",
    "    # Merge P and P_neg\n",
    "    for theuser in P[\"users\"]:\n",
    "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "\n",
    "    Zui = dict()\n",
    "    Ni = dict()\n",
    "\n",
    "    # Compute frequencies of items in the training set\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "    del trainset\n",
    "\n",
    "    # Compute recommendations for each user\n",
    "    for theuser in P[\"users\"]:\n",
    "        all_scores = np.array(P[\"results\"][theuser])\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for i, pos_item in enumerate(pos_items):\n",
    "            pos_score = pos_scores[i]\n",
    "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
    "\n",
    "    pui = dict()\n",
    "    w = dict()\n",
    "\n",
    "    # Compute dictionary of propensity scores\n",
    "    for theuser in P[\"users\"]:\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            if theitem not in Ni:\n",
    "                continue\n",
    "            if theitem in pui:\n",
    "                continue\n",
    "            pui[theitem] = np.power(Ni[theitem], (gamma + 1) / 2.0)\n",
    "\n",
    "    # Take the list of items (not tuples) in pui sorted by value\n",
    "    items_sorted_by_value = sorted(pui, key=pui.get, reverse=True)\n",
    "\n",
    "    # Compute linspace between the pui[0] and pui[-1] \n",
    "\n",
    "    # Maybe try to split the logspace instead of the linspace?\n",
    "    logspace = np.logspace(pui[items_sorted_by_value[0]], pui[items_sorted_by_value[-1]], partition+1)\n",
    "   \n",
    "    # Compute dictionary w, that is, for each item, assigns the average of the puis in the partition it belongs to\n",
    "    i=0\n",
    "    j = 0\n",
    "    while i < len(items_sorted_by_value):\n",
    "                            \n",
    "        avg = 0\n",
    "        start = i\n",
    "        end = i\n",
    "    \n",
    "        while i < len(items_sorted_by_value) and pui[items_sorted_by_value[i]] >= logspace[j+1]:\n",
    "            avg += 1.0 / pui[items_sorted_by_value[i]]\n",
    "            end = i\n",
    "            i += 1\n",
    "        \n",
    "        # Is the average the only good choice? even with the log space split?\n",
    "        avg = avg / (end - start + 1)\n",
    "\n",
    "        for k in range(start, end+1):\n",
    "            w[items_sorted_by_value[k]] = avg\n",
    "\n",
    "\n",
    "        j += 1\n",
    "\n",
    "    nonzero_user_count = 0\n",
    "    sum_user_auc = 0.0\n",
    "    sum_user_recall = 0.0\n",
    "\n",
    "    # Compute score with AUC and compute Recall\n",
    "    for theuser in P[\"users\"]:\n",
    "        numerator_auc = 0.0\n",
    "        numerator_recall = 0.0\n",
    "        denominator = 0.0\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            # Skip items with null frequency\n",
    "            if  theitem not in Ni:\n",
    "                continue\n",
    "            # Add things to be summed for each item\n",
    "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser])) * w[theitem]\n",
    "            # Add things for recall\n",
    "            if Zui[(theuser, theitem)] < K:\n",
    "                numerator_recall += 1.0 * w[theitem] #spetta\n",
    "            # Increment denominator that the sum must be divided by \n",
    "            denominator += 1 / pui[theitem]\n",
    "\n",
    "\n",
    "        # If there was at least one item for the user, count the user and sum the results\n",
    "        if denominator > 0:\n",
    "            nonzero_user_count += 1\n",
    "            sum_user_auc += numerator_auc / denominator\n",
    "            sum_user_recall += numerator_recall / denominator \n",
    "\n",
    "    return {\n",
    "        \"auc\"       : sum_user_auc / nonzero_user_count, \n",
    "        \"recall\"    : sum_user_recall / nonzero_user_count\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_2(infilename, infilename_neg, trainfilename, gamma=1.0, K=30, partition=10):\n",
    "\n",
    "    # Read pickles\n",
    "    infile = open(infilename, 'rb')\n",
    "    infile_neg = open(infilename_neg, 'rb')\n",
    "    P = pickle.load(infile)\n",
    "    infile.close()\n",
    "    P_neg = pickle.load(infile_neg)\n",
    "    infile_neg.close()\n",
    "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
    "\n",
    "    # Merge P and P_neg\n",
    "    for theuser in P[\"users\"]:\n",
    "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "\n",
    "    Zui = dict()\n",
    "    Ni = dict()\n",
    "\n",
    "    # Compute frequencies of items in the training set\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "    del trainset\n",
    "\n",
    "    # Compute recommendations for each user\n",
    "    for theuser in P[\"users\"]:\n",
    "        all_scores = np.array(P[\"results\"][theuser])\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for i, pos_item in enumerate(pos_items):\n",
    "            pos_score = pos_scores[i]\n",
    "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
    "\n",
    "    pui = dict()\n",
    "    w = dict()\n",
    "\n",
    "    # Compute dictionary of propensity scores\n",
    "    for theuser in P[\"users\"]:\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            if theitem not in Ni:\n",
    "                continue\n",
    "            if theitem in pui:\n",
    "                continue\n",
    "            pui[theitem] = np.power(Ni[theitem], (gamma + 1) / 2.0)\n",
    "\n",
    "    # Take the list of items (not tuples) in pui sorted by value\n",
    "    items_sorted_by_value = sorted(pui, key=pui.get, reverse=True)\n",
    "\n",
    "    # Compute linspace between the 0 to len(item_sorted...)\n",
    "    linspace = np.linspace(0, len(items_sorted_by_value), partition+1)\n",
    "   \n",
    "    # Compute dictionary w, that is, for each item, assigns the average of the puis in the partition it belongs to\n",
    "    i=0\n",
    "    j = 0\n",
    "    while i < len(items_sorted_by_value):\n",
    "                            \n",
    "        avg = 0\n",
    "        start = i\n",
    "        end = i\n",
    "    \n",
    "        while i < len(items_sorted_by_value) and i < linspace[j+1]:\n",
    "            avg += 1.0 / pui[items_sorted_by_value[i]]\n",
    "            end = i\n",
    "            i += 1\n",
    "        \n",
    "        avg = avg / (end - start + 1)\n",
    "\n",
    "        for k in range(start, end+1):\n",
    "            w[items_sorted_by_value[k]] = avg\n",
    "\n",
    "\n",
    "        j += 1\n",
    "\n",
    "    nonzero_user_count = 0\n",
    "    sum_user_auc = 0.0\n",
    "    sum_user_recall = 0.0\n",
    "\n",
    "    # Compute score with AUC and compute Recall\n",
    "    for theuser in P[\"users\"]:\n",
    "        numerator_auc = 0.0\n",
    "        numerator_recall = 0.0\n",
    "        denominator = 0.0\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            # Skip items with null frequency\n",
    "            if  theitem not in Ni:\n",
    "                continue\n",
    "            # Add things to be summed for each item\n",
    "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser])) * w[theitem]\n",
    "            # Add things for recall\n",
    "            if Zui[(theuser, theitem)] < K:\n",
    "                numerator_recall += 1.0 * w[theitem] #spetta\n",
    "            # Increment denominator that the sum must be divided by \n",
    "            denominator += 1 / pui[theitem]\n",
    "\n",
    "\n",
    "        # If there was at least one item for the user, count the user and sum the results\n",
    "        if denominator > 0:\n",
    "            nonzero_user_count += 1\n",
    "            sum_user_auc += numerator_auc / denominator\n",
    "            sum_user_recall += numerator_recall / denominator \n",
    "\n",
    "    return {\n",
    "        \"auc\"       : sum_user_auc / nonzero_user_count, \n",
    "        \"recall\"    : sum_user_recall / nonzero_user_count\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0. ,  93.2, 186.4, 279.6, 372.8, 466. , 559.2, 652.4, 745.6,\n",
       "       838.8, 932. ])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linspace = np.linspace(0, 932, 10+1)\n",
    "linspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **EVALUATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_results = []\n",
    "recall_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = dict()\n",
    "raw_data['train_data'] = np.load(folder_name + \"training_arr.npy\")\n",
    "raw_data['test_data_pos_biased'] = np.load(folder_name + \"biased-test_arr_pos.npy\")\n",
    "raw_data['test_data_neg_biased'] = np.load(folder_name + \"biased-test_arr_neg.npy\")\n",
    "raw_data['test_data_pos_unbiased'] = np.load(folder_name + \"unbiased-test_arr_pos.npy\")\n",
    "raw_data['test_data_neg_unbiased'] = np.load(folder_name + \"unbiased-test_arr_neg.npy\")\n",
    "raw_data['max_user'] = 15401\n",
    "raw_data['max_item'] = 1001\n",
    "batch_size = 8000\n",
    "test_batch_size = 1000\n",
    "display_itr = 1000\n",
    "\n",
    "train_dataset = ImplicitDataset(raw_data['train_data'], raw_data['max_user'], raw_data['max_item'], name='Train')\n",
    "test_dataset_pos_biased = ImplicitDataset(raw_data['test_data_pos_biased'], raw_data['max_user'], raw_data['max_item'])\n",
    "test_dataset_neg_biased = ImplicitDataset(raw_data['test_data_neg_biased'], raw_data['max_user'], raw_data['max_item'])\n",
    "test_dataset_pos_unbiased = ImplicitDataset(raw_data['test_data_pos_unbiased'], raw_data['max_user'], raw_data['max_item'])\n",
    "test_dataset_neg_unbiased = ImplicitDataset(raw_data['test_data_neg_unbiased'], raw_data['max_user'], raw_data['max_item'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./Output/cml-yahoo/\n"
     ]
    }
   ],
   "source": [
    "#Code to avoid tf using cached embeddings\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "model = MODEL_CLASS(batch_size=batch_size, max_user=train_dataset.max_user(), max_item=train_dataset.max_item(),\n",
    "    dim_embed=50, l2_reg=0.001, opt='Adam', sess_config=None)\n",
    "sampler = PairwiseSampler(batch_size=batch_size, dataset=train_dataset, num_process=4)\n",
    "model_trainer = ImplicitModelTrainer(batch_size=batch_size, test_batch_size=test_batch_size,\n",
    "                                     train_dataset=train_dataset, model=model, sampler=sampler,\n",
    "                                     eval_save_prefix=OUTPUT_PATH + DATASET_NAME,\n",
    "                                     item_serving_size=500)\n",
    "auc_evaluator = AUC()\n",
    "\n",
    "model.load(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Subsampling negative items]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    }
   ],
   "source": [
    "model_trainer._eval_manager = ImplicitEvalManager(evaluators=[auc_evaluator])\n",
    "model_trainer._num_negatives = 200\n",
    "model_trainer._exclude_positives([train_dataset, test_dataset_pos_biased, test_dataset_neg_biased])\n",
    "model_trainer._sample_negatives(seed=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biased Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2070/2070 [00:00<00:00, 2638.04it/s]\n",
      "100%|| 2296/2296 [00:25<00:00, 90.50it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AUC': [0.5109661016949152,\n",
       "  0.5292881355932204,\n",
       "  0.5090939597315436,\n",
       "  0.501526845637584,\n",
       "  0.4819696969696969,\n",
       "  0.4916610169491526,\n",
       "  0.5248993288590603,\n",
       "  0.5493750000000001,\n",
       "  0.5049832214765101,\n",
       "  0.49050505050505055,\n",
       "  0.5272542372881357,\n",
       "  0.517179054054054,\n",
       "  0.47917085427135675,\n",
       "  0.49478333333333335,\n",
       "  0.5199131944444445,\n",
       "  0.4848154362416107,\n",
       "  0.5218729096989966,\n",
       "  0.4722278911564625,\n",
       "  0.49442953020134217,\n",
       "  0.5404208754208754,\n",
       "  0.5287959866220736,\n",
       "  0.5320034246575343,\n",
       "  0.5047826086956522,\n",
       "  0.45390939597315433,\n",
       "  0.4978595317725752,\n",
       "  0.4954401408450704,\n",
       "  0.5020962199312715,\n",
       "  0.4976588628762541,\n",
       "  0.4790816326530612,\n",
       "  0.5153030303030304,\n",
       "  0.47370805369127517,\n",
       "  0.4972558922558923,\n",
       "  0.4652256944444445,\n",
       "  0.5101505016722407,\n",
       "  0.48699664429530204,\n",
       "  0.5074328859060403,\n",
       "  0.5444147157190635,\n",
       "  0.4568027210884354,\n",
       "  0.4898154362416107,\n",
       "  0.5067060810810812,\n",
       "  0.489847972972973,\n",
       "  0.45332758620689656,\n",
       "  0.5434129692832764,\n",
       "  0.4737542087542087,\n",
       "  0.4798657718120805,\n",
       "  0.4924410774410774,\n",
       "  0.5263299663299663,\n",
       "  0.5200333333333333,\n",
       "  0.49263422818791947,\n",
       "  0.49168896321070227,\n",
       "  0.5082666666666666,\n",
       "  0.5018350168350169,\n",
       "  0.4948305084745763,\n",
       "  0.49906040268456375,\n",
       "  0.47642123287671234,\n",
       "  0.47313758389261745,\n",
       "  0.5066498316498317,\n",
       "  0.4925344827586207,\n",
       "  0.4679194630872483,\n",
       "  0.5570169491525424,\n",
       "  0.5049831649831651,\n",
       "  0.5127181208053692,\n",
       "  0.4568707482993198,\n",
       "  0.47725589225589227,\n",
       "  0.4564046822742475,\n",
       "  0.5140268456375839,\n",
       "  0.4596801346801346,\n",
       "  0.4773489932885906,\n",
       "  0.4777833333333333,\n",
       "  0.5191638795986623,\n",
       "  0.5070066889632108,\n",
       "  0.5201337792642141,\n",
       "  0.4476430976430976,\n",
       "  0.4956506849315068,\n",
       "  0.47944256756756765,\n",
       "  0.5032881355932203,\n",
       "  0.473238255033557,\n",
       "  0.49150000000000005,\n",
       "  0.4794295302013423,\n",
       "  0.49107744107744106,\n",
       "  0.48070707070707075,\n",
       "  0.47399659863945576,\n",
       "  0.497231543624161,\n",
       "  0.5066778523489933,\n",
       "  0.4721885521885522,\n",
       "  0.5018518518518519,\n",
       "  0.45028716216216214,\n",
       "  0.48785223367697594,\n",
       "  0.5622466216216215,\n",
       "  0.5374662162162163,\n",
       "  0.5017171717171717,\n",
       "  0.5312751677852349,\n",
       "  0.4378040540540541,\n",
       "  0.4985234899328859,\n",
       "  0.5226767676767677,\n",
       "  0.472208904109589,\n",
       "  0.4940771812080537,\n",
       "  0.5039966555183947,\n",
       "  0.48616554054054045,\n",
       "  0.5367114093959732,\n",
       "  0.4880912162162162,\n",
       "  0.5111301369863014,\n",
       "  0.5048469387755102,\n",
       "  0.5360664335664337,\n",
       "  0.5179666666666666,\n",
       "  0.5001525423728814,\n",
       "  0.5019565217391304,\n",
       "  0.49956375838926176,\n",
       "  0.5174500000000001,\n",
       "  0.5076767676767677,\n",
       "  0.4980872483221477,\n",
       "  0.500959595959596,\n",
       "  0.49498316498316497,\n",
       "  0.5013095238095238,\n",
       "  0.5176936026936027,\n",
       "  0.5222,\n",
       "  0.4857885906040268,\n",
       "  0.5034680134680134,\n",
       "  0.5095593220338983,\n",
       "  0.45565,\n",
       "  0.4536700336700337,\n",
       "  0.4724242424242424,\n",
       "  0.5778762541806021,\n",
       "  0.4734899328859059,\n",
       "  0.491271186440678,\n",
       "  0.5195959595959596,\n",
       "  0.5551683501683502,\n",
       "  0.5125084175084175,\n",
       "  0.5389093959731543,\n",
       "  0.5158557046979867,\n",
       "  0.48536912751677846,\n",
       "  0.4645189003436426,\n",
       "  0.5203220338983051,\n",
       "  0.5137282229965157,\n",
       "  0.5458724832214765,\n",
       "  0.4781208053691276,\n",
       "  0.49317567567567566,\n",
       "  0.5177516778523491,\n",
       "  0.4923322147651006,\n",
       "  0.4825510204081633,\n",
       "  0.539054054054054,\n",
       "  0.5439115646258504,\n",
       "  0.5164745762711864,\n",
       "  0.5289666666666667,\n",
       "  0.5225171232876712,\n",
       "  0.5209698996655518,\n",
       "  0.5103177257525083,\n",
       "  0.4957263513513514,\n",
       "  0.5204026845637584,\n",
       "  0.4665604026845637,\n",
       "  0.49838541666666664,\n",
       "  0.4910200668896321,\n",
       "  0.5101845637583894,\n",
       "  0.5052739726027398,\n",
       "  0.5018833333333333,\n",
       "  0.5540666666666667,\n",
       "  0.5049664429530201,\n",
       "  0.5073232323232324,\n",
       "  0.4905555555555555,\n",
       "  0.46396959459459464,\n",
       "  0.5139057239057239,\n",
       "  0.4953547297297297,\n",
       "  0.5026182432432431,\n",
       "  0.47001677852348994,\n",
       "  0.4788392857142857,\n",
       "  0.5427946127946127,\n",
       "  0.5220805369127517,\n",
       "  0.5026779661016949,\n",
       "  0.5240969899665552,\n",
       "  0.5176677852348993,\n",
       "  0.46689189189189184,\n",
       "  0.49613945578231294,\n",
       "  0.5117176870748299,\n",
       "  0.46021959459459455,\n",
       "  0.48986486486486486,\n",
       "  0.47894648829431435,\n",
       "  0.496241610738255,\n",
       "  0.5176006711409397,\n",
       "  0.5358892617449664,\n",
       "  0.48944816053511697,\n",
       "  0.43237288135593216,\n",
       "  0.4804882154882155,\n",
       "  0.5344237288135594,\n",
       "  0.5286868686868689,\n",
       "  0.4941275167785235,\n",
       "  0.4753678929765886,\n",
       "  0.5101006711409396,\n",
       "  0.4971140939597315,\n",
       "  0.4504666666666666,\n",
       "  0.4664833333333333,\n",
       "  0.5431649831649832,\n",
       "  0.5530602006688964,\n",
       "  0.4817171717171717,\n",
       "  0.5223389830508475,\n",
       "  0.4543150684931507,\n",
       "  0.5164478114478114,\n",
       "  0.4751706484641638,\n",
       "  0.5023154362416107,\n",
       "  0.4951170568561873,\n",
       "  0.5119256756756757,\n",
       "  0.5237751677852348,\n",
       "  0.5313758389261745,\n",
       "  0.5093288590604027,\n",
       "  0.48904999999999993,\n",
       "  0.4791016949152542,\n",
       "  0.4986531986531986,\n",
       "  0.5107432432432432,\n",
       "  0.5012457912457912,\n",
       "  0.48978333333333335,\n",
       "  0.5222297297297298,\n",
       "  0.47356418918918924,\n",
       "  0.4938795986622073,\n",
       "  0.46685618729096984,\n",
       "  0.4853367003367003,\n",
       "  0.5145959595959596,\n",
       "  0.4667725752508361,\n",
       "  0.5185353535353535,\n",
       "  0.5181986531986532,\n",
       "  0.5188006756756757,\n",
       "  0.4934228187919464,\n",
       "  0.4677027027027027,\n",
       "  0.4946822742474917,\n",
       "  0.49975,\n",
       "  0.5096153846153846,\n",
       "  0.5444295302013422,\n",
       "  0.5055067567567567,\n",
       "  0.4935738255033557,\n",
       "  0.4913422818791946,\n",
       "  0.47175170068027217,\n",
       "  0.4970875420875421,\n",
       "  0.4401535836177475,\n",
       "  0.4870469798657718,\n",
       "  0.4892642140468228,\n",
       "  0.4973322147651007,\n",
       "  0.5179933110367893,\n",
       "  0.45095637583892617,\n",
       "  0.5160200668896322,\n",
       "  0.48771477663230234,\n",
       "  0.5050838926174496,\n",
       "  0.4998989898989898,\n",
       "  0.48135906040268456,\n",
       "  0.5024666666666667,\n",
       "  0.5293166666666668,\n",
       "  0.5037080536912751,\n",
       "  0.5015666666666667,\n",
       "  0.5265886287625418,\n",
       "  0.46462068965517245,\n",
       "  0.4353187919463087,\n",
       "  0.45959731543624166,\n",
       "  0.5165319865319864,\n",
       "  0.527728813559322,\n",
       "  0.4926190476190477,\n",
       "  0.5093791946308726,\n",
       "  0.5129933110367894,\n",
       "  0.5393598615916955,\n",
       "  0.49455000000000005,\n",
       "  0.48034999999999994,\n",
       "  0.5007859531772576,\n",
       "  0.49170608108108116,\n",
       "  0.48617056856187296,\n",
       "  0.47741666666666666,\n",
       "  0.4841496598639456,\n",
       "  0.5082154882154882,\n",
       "  0.5019360269360269,\n",
       "  0.4963087248322148,\n",
       "  0.537248322147651,\n",
       "  0.4968499999999999,\n",
       "  0.5016442953020135,\n",
       "  0.5369152542372881,\n",
       "  0.5488255033557048,\n",
       "  0.4976262626262627,\n",
       "  0.520155172413793,\n",
       "  0.5243133802816902,\n",
       "  0.5057432432432432,\n",
       "  0.5133389830508475,\n",
       "  0.5207070707070707,\n",
       "  0.5408892617449664,\n",
       "  0.5138590604026846,\n",
       "  0.5134879725085911,\n",
       "  0.5461340206185568,\n",
       "  0.47641156462585044,\n",
       "  0.49281666666666657,\n",
       "  0.4903333333333333,\n",
       "  0.46590301003344486,\n",
       "  0.5062040133779264,\n",
       "  0.4474581939799332,\n",
       "  0.5322895622895623,\n",
       "  0.4594612794612794,\n",
       "  0.4927777777777778,\n",
       "  0.5158277027027027,\n",
       "  0.45357859531772576,\n",
       "  0.47474402730375426,\n",
       "  0.55445,\n",
       "  0.4709966216216217,\n",
       "  0.4549163879598663,\n",
       "  0.46262541806020063,\n",
       "  0.4805201342281879,\n",
       "  0.5011705685618729,\n",
       "  0.5106484641638225,\n",
       "  0.5451672240802675,\n",
       "  0.5105254237288136,\n",
       "  0.5309899328859061,\n",
       "  0.5008614864864864,\n",
       "  0.5159060402684564,\n",
       "  0.44608108108108113,\n",
       "  0.5092857142857143,\n",
       "  0.5313414634146342,\n",
       "  0.5196153846153847,\n",
       "  0.4771114864864865,\n",
       "  0.4644557823129251,\n",
       "  0.5061666666666667,\n",
       "  0.5096476510067114,\n",
       "  0.5387123745819398,\n",
       "  0.5113879598662207,\n",
       "  0.5077348993288591,\n",
       "  0.44233221476510065,\n",
       "  0.5256818181818181,\n",
       "  0.5019191919191919,\n",
       "  0.48141379310344834,\n",
       "  0.5008163265306123,\n",
       "  0.5169152542372881,\n",
       "  0.4909197324414716,\n",
       "  0.4733163265306122,\n",
       "  0.490200668896321,\n",
       "  0.5275084175084175,\n",
       "  0.49856418918918916,\n",
       "  0.5149999999999999,\n",
       "  0.504448160535117,\n",
       "  0.5531833333333332,\n",
       "  0.4904898648648649,\n",
       "  0.5440604026845638,\n",
       "  0.46867892976588627,\n",
       "  0.570204081632653,\n",
       "  0.5201010101010101,\n",
       "  0.5524829931972789,\n",
       "  0.5207388316151202,\n",
       "  0.4567234848484848,\n",
       "  0.5162962962962963,\n",
       "  0.5417056856187291,\n",
       "  0.4549158249158249,\n",
       "  0.5215593220338983,\n",
       "  0.4905650684931507,\n",
       "  0.4739393939393939,\n",
       "  0.43487730061349694,\n",
       "  0.47381270903010037,\n",
       "  0.4780201342281879,\n",
       "  0.4552013422818792,\n",
       "  0.46094999999999997,\n",
       "  0.521996644295302,\n",
       "  0.5177181208053692,\n",
       "  0.5315033783783784,\n",
       "  0.5200166666666667,\n",
       "  0.5075862068965517,\n",
       "  0.49560200668896315,\n",
       "  0.5088590604026845,\n",
       "  0.5455517241379311,\n",
       "  0.499047619047619,\n",
       "  0.4643749999999999,\n",
       "  0.5258862876254181,\n",
       "  0.47501666666666664,\n",
       "  0.491438127090301,\n",
       "  0.5501174496644295,\n",
       "  0.5312962962962963,\n",
       "  0.5370066889632107,\n",
       "  0.5229322033898306,\n",
       "  0.5115254237288136,\n",
       "  0.49665551839464883,\n",
       "  0.4744966442953019,\n",
       "  0.4859013605442177,\n",
       "  0.507190635451505,\n",
       "  0.4495791245791246,\n",
       "  0.49371237458193984,\n",
       "  0.5135666666666666,\n",
       "  0.5171043771043772,\n",
       "  0.4989765100671141,\n",
       "  0.5287205387205387,\n",
       "  0.5000334448160535,\n",
       "  0.5058361204013379,\n",
       "  0.4720547945205479,\n",
       "  0.5077424749163879,\n",
       "  0.5259010600706713,\n",
       "  0.5978277153558053,\n",
       "  0.5123569023569023,\n",
       "  0.519983164983165,\n",
       "  0.5382046979865771,\n",
       "  0.46749999999999997,\n",
       "  0.5191722972972973,\n",
       "  0.4877533783783784,\n",
       "  0.5128231292517007,\n",
       "  0.4808249158249158,\n",
       "  0.46064189189189186,\n",
       "  0.49979999999999997,\n",
       "  0.5156333333333333,\n",
       "  0.552320819112628,\n",
       "  0.46297658862876256,\n",
       "  0.49225752508361204,\n",
       "  0.5035472972972972,\n",
       "  0.49473244147157197,\n",
       "  0.5272297297297297,\n",
       "  0.5231605351170568,\n",
       "  0.5010810810810811,\n",
       "  0.49856890459363956,\n",
       "  0.523523489932886,\n",
       "  0.475,\n",
       "  0.5065824915824916,\n",
       "  0.5369127516778524,\n",
       "  0.5549331103678931,\n",
       "  0.5139632107023412,\n",
       "  0.4797474747474748,\n",
       "  0.4904391891891892,\n",
       "  0.4876101694915254,\n",
       "  0.4521525423728813,\n",
       "  0.44825423728813557,\n",
       "  0.4214882943143813,\n",
       "  0.5011166666666665,\n",
       "  0.5224745762711864,\n",
       "  0.5270469798657719,\n",
       "  0.4678793103448276,\n",
       "  0.5019063545150502,\n",
       "  0.5087288135593221,\n",
       "  0.4983390410958904,\n",
       "  0.5124652777777777,\n",
       "  0.5521717171717172,\n",
       "  0.5261241610738255,\n",
       "  0.4978885135135135,\n",
       "  0.5117056856187291,\n",
       "  0.5006375838926175,\n",
       "  0.49956375838926176,\n",
       "  0.484261744966443,\n",
       "  0.5312203389830509,\n",
       "  0.5246785714285714,\n",
       "  0.5172073578595318,\n",
       "  0.4977946127946128,\n",
       "  0.5443097643097643,\n",
       "  0.4625084745762712,\n",
       "  0.46952961672473864,\n",
       "  0.4740635451505017,\n",
       "  0.5499464285714286,\n",
       "  0.48744067796610163,\n",
       "  0.5246801346801346,\n",
       "  0.5280902777777777,\n",
       "  0.48073129251700675,\n",
       "  0.5353716216216216,\n",
       "  0.5036195286195286,\n",
       "  0.5019397993311037,\n",
       "  0.5024496644295302,\n",
       "  0.4448817567567568,\n",
       "  0.5364932885906041,\n",
       "  0.46441077441077444,\n",
       "  0.5287543252595156,\n",
       "  0.5156040268456376,\n",
       "  0.4629264214046823,\n",
       "  0.5067785234899328,\n",
       "  0.4881292517006802,\n",
       "  0.48868243243243237,\n",
       "  0.4998474576271186,\n",
       "  0.5027,\n",
       "  0.4564455782312925,\n",
       "  0.5147966101694915,\n",
       "  0.4772727272727273,\n",
       "  0.4966166666666666,\n",
       "  0.4795890410958904,\n",
       "  0.5060616438356165,\n",
       "  0.503993288590604,\n",
       "  0.4734782608695652,\n",
       "  0.5083557046979866,\n",
       "  0.49789830508474575,\n",
       "  0.5162794612794613,\n",
       "  0.46946127946127947,\n",
       "  0.5314020270270271,\n",
       "  0.5193288590604026,\n",
       "  0.4712765957446808,\n",
       "  0.5220817120622567,\n",
       "  0.5155685618729097,\n",
       "  0.47993265993266,\n",
       "  0.49248287671232877,\n",
       "  0.5335234899328859,\n",
       "  0.45606779661016955,\n",
       "  0.4780602006688963,\n",
       "  0.5319166666666667,\n",
       "  0.5098653198653199,\n",
       "  0.4903177257525084,\n",
       "  0.4533779264214047,\n",
       "  0.5554208754208754,\n",
       "  0.5048299319727891,\n",
       "  0.4890338983050847,\n",
       "  0.5182993197278911,\n",
       "  0.5077702702702703,\n",
       "  0.5415333333333333,\n",
       "  0.49053511705685615,\n",
       "  0.542608695652174,\n",
       "  0.4658390410958904,\n",
       "  0.48468013468013466,\n",
       "  0.5637751677852348,\n",
       "  0.5000501672240802,\n",
       "  0.48188775510204085,\n",
       "  0.49099999999999994,\n",
       "  0.503097643097643,\n",
       "  0.527056856187291,\n",
       "  0.49898333333333333,\n",
       "  0.48730769230769233,\n",
       "  0.5568666666666666,\n",
       "  0.4961655405405406,\n",
       "  0.5493959731543624,\n",
       "  0.5147658862876254,\n",
       "  0.5143120805369128,\n",
       "  0.5311447811447811,\n",
       "  0.4640443686006826,\n",
       "  0.4795317725752508,\n",
       "  0.5262583892617451,\n",
       "  0.4913299663299664,\n",
       "  0.531518771331058,\n",
       "  0.5098327759197324,\n",
       "  0.5329054054054054,\n",
       "  0.47441275167785235,\n",
       "  0.4845134228187919,\n",
       "  0.5018493150684932,\n",
       "  0.5122986577181209,\n",
       "  0.4893939393939394,\n",
       "  0.5057612456747405,\n",
       "  0.45603333333333335,\n",
       "  0.5173666666666666,\n",
       "  0.48623333333333335,\n",
       "  0.5072666666666666,\n",
       "  0.49417229729729734,\n",
       "  0.5222073578595318,\n",
       "  0.5217281879194631,\n",
       "  0.5086454849498327,\n",
       "  0.4895805369127517,\n",
       "  0.5103523489932886,\n",
       "  0.4938754325259516,\n",
       "  0.533469387755102,\n",
       "  0.48274999999999996,\n",
       "  0.4634863945578232,\n",
       "  0.49488215488215487,\n",
       "  0.4782214765100671,\n",
       "  0.5251006711409396,\n",
       "  0.4617013888888889,\n",
       "  0.4781569965870307,\n",
       "  0.48645973154362415,\n",
       "  0.4832094594594595,\n",
       "  0.5155536912751677,\n",
       "  0.46625838926174507,\n",
       "  0.5576262626262626,\n",
       "  0.5373154362416107,\n",
       "  0.4918644067796611,\n",
       "  0.49498327759197325,\n",
       "  0.4409152542372881,\n",
       "  0.4813210702341137,\n",
       "  0.5446822742474917,\n",
       "  0.5111538461538462,\n",
       "  0.45763513513513515,\n",
       "  0.4945134228187919,\n",
       "  0.5342307692307693,\n",
       "  0.4836912751677852,\n",
       "  0.5138383838383836,\n",
       "  0.4900337837837838,\n",
       "  0.48937710437710435,\n",
       "  0.45626262626262626,\n",
       "  0.4872013651877133,\n",
       "  0.44685810810810805,\n",
       "  0.48348639455782305,\n",
       "  0.5147474747474747,\n",
       "  0.46377516778523487,\n",
       "  0.47460481099656354,\n",
       "  0.5267056856187291,\n",
       "  0.543238255033557,\n",
       "  0.5291610738255034,\n",
       "  0.5033788395904437,\n",
       "  0.4781208053691276,\n",
       "  0.46591216216216214,\n",
       "  0.5219897959183674,\n",
       "  0.475,\n",
       "  0.46399328859060396,\n",
       "  0.4826360544217688,\n",
       "  0.5193120805369127,\n",
       "  0.45399317406143336,\n",
       "  0.4615798611111111,\n",
       "  0.47361486486486487,\n",
       "  0.5402901023890785,\n",
       "  0.4475166666666667,\n",
       "  0.5249331103678929,\n",
       "  0.5062709030100334,\n",
       "  0.5181772575250836,\n",
       "  0.49107142857142855,\n",
       "  0.5101689189189189,\n",
       "  0.4910367892976589,\n",
       "  0.45815878378378383,\n",
       "  0.4967558528428094,\n",
       "  0.5537959866220735,\n",
       "  0.4637074829931973,\n",
       "  0.48944444444444446,\n",
       "  0.48342905405405395,\n",
       "  0.5217730496453901,\n",
       "  0.5753355704697987,\n",
       "  0.546554054054054,\n",
       "  0.48697986577181207,\n",
       "  0.4861036789297658,\n",
       "  0.5329623287671232,\n",
       "  0.5099158249158249,\n",
       "  0.49228333333333335,\n",
       "  0.4781678082191781,\n",
       "  0.5022895622895622,\n",
       "  0.47001672240802667,\n",
       "  0.47735690235690237,\n",
       "  0.5030936454849498,\n",
       "  0.4983666666666666,\n",
       "  0.455738255033557,\n",
       "  0.4820302013422819,\n",
       "  0.5055743243243243,\n",
       "  0.5001515151515152,\n",
       "  0.5488127090301004,\n",
       "  0.5144500000000001,\n",
       "  0.4187458193979934,\n",
       "  0.5196822742474916,\n",
       "  0.5144816053511705,\n",
       "  0.5324916387959866,\n",
       "  0.5111241610738255,\n",
       "  0.4759183673469388,\n",
       "  0.5177224199288257,\n",
       "  0.4823232323232323,\n",
       "  0.5197979797979798,\n",
       "  0.5125838926174495,\n",
       "  0.49000000000000005,\n",
       "  0.5141610738255034,\n",
       "  0.5022408026755852,\n",
       "  0.4820677966101695,\n",
       "  0.5168394648829432,\n",
       "  0.5011,\n",
       "  0.5674832214765101,\n",
       "  0.5034731543624161,\n",
       "  0.5351013513513513,\n",
       "  0.5433670033670034,\n",
       "  0.4754697986577181,\n",
       "  0.463513986013986,\n",
       "  0.5327926421404683,\n",
       "  0.45906666666666657,\n",
       "  0.5415646258503402,\n",
       "  0.5234615384615385,\n",
       "  0.5135353535353536,\n",
       "  0.4954882154882156,\n",
       "  0.477054794520548,\n",
       "  0.49443143812709034,\n",
       "  0.5002931034482759,\n",
       "  0.455819397993311,\n",
       "  0.47819999999999996,\n",
       "  0.5108333333333334,\n",
       "  0.48874576271186443,\n",
       "  0.5053050847457627,\n",
       "  0.5254222972972972,\n",
       "  0.4906688963210702,\n",
       "  0.49644067796610175,\n",
       "  0.5003703703703705,\n",
       "  0.4722222222222222,\n",
       "  0.508733108108108,\n",
       "  0.5112157534246575,\n",
       "  0.48136363636363627,\n",
       "  0.510874125874126,\n",
       "  0.5566220735785954,\n",
       "  0.46098305084745755,\n",
       "  0.49305460750853236,\n",
       "  0.5058163265306123,\n",
       "  0.5162962962962963,\n",
       "  0.501493288590604,\n",
       "  0.5348809523809523,\n",
       "  0.4710102739726028,\n",
       "  0.5184121621621621,\n",
       "  0.47991638795986624,\n",
       "  0.4694576271186441,\n",
       "  0.5184848484848485,\n",
       "  0.4844,\n",
       "  0.4854931972789115,\n",
       "  0.5385185185185185,\n",
       "  0.47454081632653056,\n",
       "  0.43787625418060205,\n",
       "  0.4922408026755853,\n",
       "  0.5202006688963211,\n",
       "  0.49787414965986393,\n",
       "  0.5431250000000001,\n",
       "  0.4979765886287626,\n",
       "  0.4625503355704698,\n",
       "  0.4847333333333334,\n",
       "  0.5178020134228188,\n",
       "  0.5199149659863945,\n",
       "  0.5150501672240803,\n",
       "  0.5267398648648648,\n",
       "  0.5170735785953177,\n",
       "  0.5150865051903115,\n",
       "  0.5114597315436241,\n",
       "  0.5247818791946308,\n",
       "  0.49035000000000006,\n",
       "  0.49750000000000005,\n",
       "  0.5463299663299663,\n",
       "  0.5250666666666667,\n",
       "  0.4998653198653199,\n",
       "  0.5153209459459459,\n",
       "  0.45698333333333335,\n",
       "  0.5016833333333334,\n",
       "  0.5012040133779264,\n",
       "  0.5197297297297296,\n",
       "  0.5080204778156997,\n",
       "  0.4747651006711409,\n",
       "  0.5078135593220339,\n",
       "  0.4923809523809524,\n",
       "  0.5330068728522337,\n",
       "  0.47192567567567567,\n",
       "  0.47593645484949837,\n",
       "  0.5003231292517007,\n",
       "  0.5437328767123288,\n",
       "  0.47929530201342285,\n",
       "  0.5099664429530202,\n",
       "  0.4301654411764706,\n",
       "  0.4934615384615384,\n",
       "  0.49161616161616156,\n",
       "  0.48609427609427613,\n",
       "  0.4779598662207359,\n",
       "  0.47362711864406787,\n",
       "  0.5118918918918919,\n",
       "  0.47859999999999997,\n",
       "  0.4775,\n",
       "  0.44208754208754214,\n",
       "  0.49277966101694914,\n",
       "  0.5183676975945017,\n",
       "  0.4969397993311037,\n",
       "  0.544695945945946,\n",
       "  0.48472881355932207,\n",
       "  0.5025844594594595,\n",
       "  0.509,\n",
       "  0.5009661016949153,\n",
       "  0.4694691780821918,\n",
       "  0.49308510638297864,\n",
       "  0.45421708185053383,\n",
       "  0.43274137931034484,\n",
       "  0.5030872483221477,\n",
       "  0.5252006688963211,\n",
       "  0.5200682593856655,\n",
       "  0.5016220735785953,\n",
       "  0.5724406779661018,\n",
       "  0.5464237288135594,\n",
       "  0.4825084745762711,\n",
       "  0.5248154362416106,\n",
       "  0.538695652173913,\n",
       "  0.5143434343434344,\n",
       "  0.4795423728813559,\n",
       "  0.433,\n",
       "  0.49934782608695644,\n",
       "  0.4770973154362417,\n",
       "  0.5147118644067796,\n",
       "  0.504728813559322,\n",
       "  0.5058783783783785,\n",
       "  0.4668941979522185,\n",
       "  0.5104810996563575,\n",
       "  0.4889212328767123,\n",
       "  0.48301346801346795,\n",
       "  0.4593456375838926,\n",
       "  0.5284693877551021,\n",
       "  0.5035836177474403,\n",
       "  0.5307849829351535,\n",
       "  0.4420608108108108,\n",
       "  0.5223076923076924,\n",
       "  0.4924333333333334,\n",
       "  0.5176132404181185,\n",
       "  0.47517006802721085,\n",
       "  0.5338698630136985,\n",
       "  0.490267558528428,\n",
       "  0.4641864406779661,\n",
       "  0.4903691275167785,\n",
       "  0.48292682926829256,\n",
       "  0.48125,\n",
       "  0.5024381625441696,\n",
       "  0.49591872791519437,\n",
       "  0.5200508474576271,\n",
       "  0.5475250836120401,\n",
       "  0.4881772575250836,\n",
       "  0.5327027027027027,\n",
       "  0.5138795986622073,\n",
       "  0.544247491638796,\n",
       "  0.5495101351351351,\n",
       "  0.5038006756756757,\n",
       "  0.47850847457627127,\n",
       "  0.5258585858585859,\n",
       "  0.4752076124567473,\n",
       "  0.4764333333333334,\n",
       "  0.4373076923076923,\n",
       "  0.4857312925170068,\n",
       "  0.516896551724138,\n",
       "  0.4903872053872053,\n",
       "  0.4888474576271187,\n",
       "  0.5167845117845118,\n",
       "  0.5166053511705685,\n",
       "  0.5312207357859532,\n",
       "  0.541510067114094,\n",
       "  0.5063758389261747,\n",
       "  0.5144237288135594,\n",
       "  0.5091442953020134,\n",
       "  0.5476510067114094,\n",
       "  0.49669463087248317,\n",
       "  0.5102508361204013,\n",
       "  0.5154208754208753,\n",
       "  0.496060606060606,\n",
       "  0.5052525252525253,\n",
       "  0.5331740614334471,\n",
       "  0.5230602006688964,\n",
       "  0.5287959866220736,\n",
       "  0.5056140350877194,\n",
       "  0.5315540540540541,\n",
       "  0.48563868613138683,\n",
       "  0.4840301003344482,\n",
       "  0.4898657718120806,\n",
       "  0.4501505016722408,\n",
       "  0.496877133105802,\n",
       "  0.5426689189189189,\n",
       "  0.48200668896321064,\n",
       "  0.4899315068493151,\n",
       "  0.5200501672240803,\n",
       "  0.5338775510204081,\n",
       "  0.5210034602076126,\n",
       "  0.4970169491525424,\n",
       "  0.48386824324324323,\n",
       "  0.4702833333333334,\n",
       "  0.504054054054054,\n",
       "  0.4755033557046979,\n",
       "  0.5241471571906354,\n",
       "  0.5126599326599326,\n",
       "  0.5017785234899328,\n",
       "  0.46499999999999997,\n",
       "  0.48374161073825506,\n",
       "  0.4907849829351536,\n",
       "  0.48131756756756755,\n",
       "  0.4512289562289562,\n",
       "  0.4588294314381271,\n",
       "  0.5222558922558922,\n",
       "  0.4664237288135593,\n",
       "  0.5027090301003344,\n",
       "  0.4625349650349651,\n",
       "  0.48067567567567576,\n",
       "  0.4820973154362417,\n",
       "  0.4830333333333333,\n",
       "  0.4825506756756757,\n",
       "  0.4901515151515152,\n",
       "  0.5451851851851851,\n",
       "  0.5120234113712375,\n",
       "  0.49556856187290976,\n",
       "  0.5213636363636363,\n",
       "  0.49033670033670035,\n",
       "  0.4984915254237288,\n",
       "  0.5249493243243243,\n",
       "  0.4696333333333333,\n",
       "  0.4961948529411765,\n",
       "  0.5060906040268456,\n",
       "  0.4777591973244147,\n",
       "  0.5601864406779661,\n",
       "  0.4794612794612795,\n",
       "  0.48238333333333333,\n",
       "  0.5201182432432433,\n",
       "  0.48735593220338985,\n",
       "  0.5140666666666667,\n",
       "  0.4943120805369128,\n",
       "  0.4866053511705686,\n",
       "  0.5039273356401384,\n",
       "  0.5532068965517242,\n",
       "  0.5230769230769231,\n",
       "  0.4999498327759197,\n",
       "  0.5340909090909091,\n",
       "  0.4875171232876712,\n",
       "  0.5303691275167784,\n",
       "  0.5352881355932203,\n",
       "  0.4722033898305085,\n",
       "  0.5045719178082191,\n",
       "  0.4676677852348994,\n",
       "  0.4891666666666667,\n",
       "  0.5267627118644067,\n",
       "  0.4597474747474748,\n",
       "  0.5402910958904109,\n",
       "  0.5025833333333334,\n",
       "  0.4948154362416107,\n",
       "  0.5399310344827587,\n",
       "  0.5168074324324324,\n",
       "  0.46394557823129245,\n",
       "  0.51246644295302,\n",
       "  0.5323657718120804,\n",
       "  0.506426116838488,\n",
       "  0.5402210884353741,\n",
       "  0.4736363636363637,\n",
       "  0.5025083612040134,\n",
       "  0.5036195286195286,\n",
       "  0.47706081081081075,\n",
       "  0.47799331103678927,\n",
       "  0.5410380622837371,\n",
       "  0.5276271186440677,\n",
       "  0.4970833333333333,\n",
       "  0.4840969899665552,\n",
       "  0.5225084175084175,\n",
       "  0.4802721088435374,\n",
       "  0.4693050847457627,\n",
       "  0.4980204778156997,\n",
       "  0.4868791946308725,\n",
       "  0.5816326530612245,\n",
       "  0.5276858108108108,\n",
       "  0.5226883561643836,\n",
       "  0.4688356164383561,\n",
       "  0.5261443661971832,\n",
       "  0.48845,\n",
       "  0.5185304054054054,\n",
       "  0.48267676767676776,\n",
       "  0.5068900343642612,\n",
       "  0.5014166666666667,\n",
       "  0.5401333333333332,\n",
       "  0.5096621621621621,\n",
       "  0.49262626262626263,\n",
       "  0.49604729729729724,\n",
       "  0.549040404040404,\n",
       "  0.4901864406779661,\n",
       "  0.5179965156794425,\n",
       "  0.45417785234899327,\n",
       "  0.5049496644295302,\n",
       "  0.4959760273972602,\n",
       "  0.49206597222222215,\n",
       "  0.5015151515151515,\n",
       "  0.511986531986532,\n",
       "  0.5313926174496644,\n",
       "  0.5122972972972972,\n",
       "  0.4884060402684564,\n",
       "  0.5025254237288136,\n",
       "  0.5204266211604096,\n",
       "  0.5158585858585859,\n",
       "  0.4914166666666666,\n",
       "  0.46875,\n",
       "  0.4882033898305084,\n",
       "  0.4571380471380472,\n",
       "  0.47109929078014184,\n",
       "  0.4833783783783783,\n",
       "  0.5416666666666666,\n",
       "  0.5221999999999999,\n",
       "  0.5511705685618729,\n",
       "  0.4665268456375839,\n",
       "  0.4508833333333333,\n",
       "  0.4850513698630138,\n",
       "  0.531103678929766,\n",
       "  0.4762286689419795,\n",
       "  0.5590604026845638,\n",
       "  0.48545608108108107,\n",
       "  0.5304666666666668,\n",
       "  0.44399317406143346,\n",
       "  0.4542617449664429,\n",
       "  0.4348474576271186,\n",
       "  0.5207312925170068,\n",
       "  0.5082107023411372,\n",
       "  0.5141750841750842,\n",
       "  0.504040404040404,\n",
       "  0.4815436241610738,\n",
       "  0.5439261744966443,\n",
       "  0.506542372881356,\n",
       "  0.4991919191919192,\n",
       "  0.4520344827586207,\n",
       "  0.48625838926174497,\n",
       "  0.5224662162162161,\n",
       "  0.5166333333333334,\n",
       "  0.4515033783783784,\n",
       "  0.5105666666666667,\n",
       "  0.4666967509025271,\n",
       "  0.513754266211604,\n",
       "  0.4505762711864407,\n",
       "  0.4688720538720539,\n",
       "  0.4869833333333332,\n",
       "  0.5349331103678929,\n",
       "  0.49914141414141416,\n",
       "  0.5026182432432431,\n",
       "  0.48433898305084744,\n",
       "  0.4998817567567569,\n",
       "  0.515887372013652,\n",
       "  0.5294166666666668,\n",
       "  0.48438127090301,\n",
       "  0.5391077441077441,\n",
       "  0.4831649831649832,\n",
       "  0.4914189189189189,\n",
       "  0.5005892255892256,\n",
       "  0.5181438127090301,\n",
       "  0.5243425605536332,\n",
       "  0.4915436241610738,\n",
       "  0.5027441077441077,\n",
       "  0.4841304347826087,\n",
       "  0.5510104529616724,\n",
       "  0.4836824324324325,\n",
       "  0.4609228187919463,\n",
       "  0.5228082191780822,\n",
       "  0.5094217687074829,\n",
       "  0.5517676767676768,\n",
       "  0.5161904761904762,\n",
       "  0.4918367346938776,\n",
       "  0.5248657718120806,\n",
       "  0.5062331081081082,\n",
       "  0.47151202749140886,\n",
       "  0.44183050847457617,\n",
       "  0.44616554054054053,\n",
       "  0.5079591836734694,\n",
       "  0.4475085324232082,\n",
       "  0.5174666666666667,\n",
       "  0.5376182432432434,\n",
       "  0.47473154362416115,\n",
       "  ...]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_trainer._eval_save_prefix = OUTPUT_PREFIX + \"-test-pos-biased\"\n",
    "model_trainer._evaluate_partial(test_dataset_pos_biased)\n",
    "\n",
    "model_trainer._eval_save_prefix = OUTPUT_PREFIX +  \"-test-neg-biased\"\n",
    "model_trainer._evaluate_partial(test_dataset_neg_biased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unbiased Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2296/2296 [00:00<00:00, 2983.47it/s]\n",
      "100%|| 2296/2296 [00:01<00:00, 1650.04it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AUC': [0.5283333333333333,\n",
       "  0.5279999999999999,\n",
       "  0.4749999999999999,\n",
       "  0.5066666666666666,\n",
       "  0.3888888888888889,\n",
       "  0.4685714285714285,\n",
       "  0.3941666666666666,\n",
       "  0.5664285714285715,\n",
       "  0.46375,\n",
       "  0.37833333333333335,\n",
       "  0.48562500000000003,\n",
       "  0.4192857142857144,\n",
       "  0.5707142857142858,\n",
       "  0.396875,\n",
       "  0.468125,\n",
       "  0.5255555555555556,\n",
       "  0.5177777777777778,\n",
       "  0.575625,\n",
       "  0.5628571428571428,\n",
       "  0.5327777777777778,\n",
       "  0.504375,\n",
       "  0.49666666666666676,\n",
       "  0.41333333333333333,\n",
       "  0.5025,\n",
       "  0.63625,\n",
       "  0.325,\n",
       "  0.66,\n",
       "  0.48812500000000003,\n",
       "  0.386875,\n",
       "  0.5288888888888889,\n",
       "  0.2885714285714286,\n",
       "  0.5711111111111111,\n",
       "  0.445,\n",
       "  0.5122222222222222,\n",
       "  0.5216666666666667,\n",
       "  0.6599999999999999,\n",
       "  0.625,\n",
       "  0.3608333333333333,\n",
       "  0.5133333333333333,\n",
       "  0.3235714285714285,\n",
       "  0.54875,\n",
       "  0.41812499999999997,\n",
       "  0.6425,\n",
       "  0.59,\n",
       "  0.4172222222222222,\n",
       "  0.4475,\n",
       "  0.5928571428571429,\n",
       "  0.32916666666666666,\n",
       "  0.4683333333333333,\n",
       "  0.3683333333333334,\n",
       "  0.4316666666666667,\n",
       "  0.516,\n",
       "  0.38833333333333336,\n",
       "  0.4388888888888889,\n",
       "  0.345,\n",
       "  0.4714285714285715,\n",
       "  0.5962500000000001,\n",
       "  0.7124999999999999,\n",
       "  0.5472222222222222,\n",
       "  0.40199999999999997,\n",
       "  0.4972222222222222,\n",
       "  0.673888888888889,\n",
       "  0.38444444444444437,\n",
       "  0.5592857142857143,\n",
       "  0.43444444444444447,\n",
       "  0.6894444444444444,\n",
       "  0.44999999999999996,\n",
       "  0.5875,\n",
       "  0.405,\n",
       "  0.5108333333333333,\n",
       "  0.5749999999999998,\n",
       "  0.416875,\n",
       "  0.40611111111111114,\n",
       "  0.2833333333333334,\n",
       "  0.34500000000000003,\n",
       "  0.3855555555555556,\n",
       "  0.4938888888888888,\n",
       "  0.5327777777777778,\n",
       "  0.4855555555555556,\n",
       "  0.495625,\n",
       "  0.549375,\n",
       "  0.545,\n",
       "  0.395625,\n",
       "  0.40125,\n",
       "  0.5738888888888889,\n",
       "  0.4933333333333334,\n",
       "  0.4116666666666667,\n",
       "  0.49187499999999995,\n",
       "  0.6156250000000001,\n",
       "  0.4575,\n",
       "  0.5383333333333333,\n",
       "  0.5611111111111112,\n",
       "  0.430625,\n",
       "  0.47333333333333333,\n",
       "  0.5583333333333332,\n",
       "  0.40277777777777785,\n",
       "  0.37,\n",
       "  0.6566666666666667,\n",
       "  0.42888888888888893,\n",
       "  0.3921428571428572,\n",
       "  0.388125,\n",
       "  0.525,\n",
       "  0.44555555555555554,\n",
       "  0.4538888888888889,\n",
       "  0.5244444444444444,\n",
       "  0.6127777777777779,\n",
       "  0.6066666666666666,\n",
       "  0.35687500000000005,\n",
       "  0.6325,\n",
       "  0.6028571428571429,\n",
       "  0.44111111111111106,\n",
       "  0.46888888888888886,\n",
       "  0.4161111111111111,\n",
       "  0.353125,\n",
       "  0.470625,\n",
       "  0.47625,\n",
       "  0.4175,\n",
       "  0.44222222222222224,\n",
       "  0.548125,\n",
       "  0.2885714285714286,\n",
       "  0.3427777777777778,\n",
       "  0.601111111111111,\n",
       "  0.5883333333333334,\n",
       "  0.52125,\n",
       "  0.4844444444444444,\n",
       "  0.435,\n",
       "  0.4092857142857143,\n",
       "  0.5155555555555555,\n",
       "  0.38875,\n",
       "  0.3821428571428571,\n",
       "  0.671111111111111,\n",
       "  0.4625,\n",
       "  0.45611111111111113,\n",
       "  0.563125,\n",
       "  0.5449999999999999,\n",
       "  0.35166666666666674,\n",
       "  0.47777777777777775,\n",
       "  0.5075000000000001,\n",
       "  0.4466666666666667,\n",
       "  0.5750000000000001,\n",
       "  0.4811111111111111,\n",
       "  0.5527777777777777,\n",
       "  0.49399999999999994,\n",
       "  0.5711111111111111,\n",
       "  0.5011111111111111,\n",
       "  0.41125,\n",
       "  0.6077777777777779,\n",
       "  0.47166666666666657,\n",
       "  0.25375000000000003,\n",
       "  0.3838888888888889,\n",
       "  0.5883333333333334,\n",
       "  0.4355555555555556,\n",
       "  0.4725,\n",
       "  0.405625,\n",
       "  0.5038888888888889,\n",
       "  0.5107142857142858,\n",
       "  0.4361111111111111,\n",
       "  0.47,\n",
       "  0.364375,\n",
       "  0.4033333333333334,\n",
       "  0.21722222222222223,\n",
       "  0.4941666666666667,\n",
       "  0.42722222222222217,\n",
       "  0.4494444444444444,\n",
       "  0.4816666666666667,\n",
       "  0.558125,\n",
       "  0.5121428571428572,\n",
       "  0.18500000000000003,\n",
       "  0.5922222222222222,\n",
       "  0.26875,\n",
       "  0.5025000000000001,\n",
       "  0.5355555555555555,\n",
       "  0.379375,\n",
       "  0.28833333333333333,\n",
       "  0.6733333333333333,\n",
       "  0.42000000000000004,\n",
       "  0.35562499999999997,\n",
       "  0.555625,\n",
       "  0.49888888888888894,\n",
       "  0.5711111111111111,\n",
       "  0.480625,\n",
       "  0.619375,\n",
       "  0.4571428571428572,\n",
       "  0.595,\n",
       "  0.4714285714285714,\n",
       "  0.3138888888888889,\n",
       "  0.5433333333333333,\n",
       "  0.5650000000000001,\n",
       "  0.44555555555555554,\n",
       "  0.6277777777777778,\n",
       "  0.52,\n",
       "  0.4255555555555556,\n",
       "  0.38187499999999996,\n",
       "  0.4155555555555555,\n",
       "  0.3007142857142857,\n",
       "  0.36875,\n",
       "  0.455,\n",
       "  0.53,\n",
       "  0.3716666666666667,\n",
       "  0.779375,\n",
       "  0.4883333333333333,\n",
       "  0.5037499999999999,\n",
       "  0.42166666666666663,\n",
       "  0.6777777777777777,\n",
       "  0.508888888888889,\n",
       "  0.43571428571428567,\n",
       "  0.23333333333333336,\n",
       "  0.41222222222222216,\n",
       "  0.5588888888888889,\n",
       "  0.49749999999999994,\n",
       "  0.5822222222222222,\n",
       "  0.505,\n",
       "  0.360625,\n",
       "  0.548125,\n",
       "  0.4861111111111111,\n",
       "  0.474375,\n",
       "  0.5027777777777778,\n",
       "  0.4305555555555556,\n",
       "  0.4744444444444445,\n",
       "  0.43333333333333335,\n",
       "  0.395,\n",
       "  0.44055555555555553,\n",
       "  0.4294444444444444,\n",
       "  0.6061111111111113,\n",
       "  0.4511111111111112,\n",
       "  0.3922222222222222,\n",
       "  0.6183333333333334,\n",
       "  0.4861111111111111,\n",
       "  0.3464285714285714,\n",
       "  0.5543750000000001,\n",
       "  0.32222222222222224,\n",
       "  0.2955555555555555,\n",
       "  0.3955555555555555,\n",
       "  0.5544444444444444,\n",
       "  0.3888888888888889,\n",
       "  0.6233333333333334,\n",
       "  0.30833333333333335,\n",
       "  0.4028571428571429,\n",
       "  0.445,\n",
       "  0.5422222222222223,\n",
       "  0.43625,\n",
       "  0.3738888888888889,\n",
       "  0.7075,\n",
       "  0.37187500000000007,\n",
       "  0.45055555555555554,\n",
       "  0.7514285714285714,\n",
       "  0.47500000000000003,\n",
       "  0.3364285714285714,\n",
       "  0.5549999999999999,\n",
       "  0.55,\n",
       "  0.455625,\n",
       "  0.5611111111111111,\n",
       "  0.5188888888888888,\n",
       "  0.47777777777777775,\n",
       "  0.5305555555555554,\n",
       "  0.5994444444444444,\n",
       "  0.5316666666666667,\n",
       "  0.4283333333333334,\n",
       "  0.5385714285714285,\n",
       "  0.44333333333333336,\n",
       "  0.4055555555555555,\n",
       "  0.5164285714285713,\n",
       "  0.513125,\n",
       "  0.6044444444444443,\n",
       "  0.43999999999999995,\n",
       "  0.605,\n",
       "  0.3788888888888889,\n",
       "  0.4377777777777778,\n",
       "  0.5427777777777778,\n",
       "  0.4788888888888889,\n",
       "  0.423125,\n",
       "  0.3872222222222222,\n",
       "  0.3377777777777778,\n",
       "  0.4992857142857143,\n",
       "  0.4033333333333333,\n",
       "  0.4735714285714286,\n",
       "  0.691875,\n",
       "  0.6066666666666667,\n",
       "  0.47333333333333333,\n",
       "  0.6977777777777777,\n",
       "  0.645625,\n",
       "  0.4255555555555555,\n",
       "  0.41555555555555557,\n",
       "  0.40499999999999997,\n",
       "  0.39142857142857146,\n",
       "  0.4388888888888889,\n",
       "  0.528125,\n",
       "  0.485625,\n",
       "  0.49166666666666664,\n",
       "  0.35277777777777775,\n",
       "  0.37124999999999997,\n",
       "  0.5133333333333333,\n",
       "  0.46444444444444444,\n",
       "  0.3488888888888889,\n",
       "  0.585625,\n",
       "  0.4288888888888889,\n",
       "  0.333125,\n",
       "  0.7207142857142858,\n",
       "  0.6577777777777778,\n",
       "  0.506875,\n",
       "  0.43714285714285717,\n",
       "  0.505625,\n",
       "  0.41583333333333333,\n",
       "  0.7011111111111111,\n",
       "  0.5393749999999999,\n",
       "  0.5874999999999999,\n",
       "  0.45444444444444443,\n",
       "  0.6216666666666666,\n",
       "  0.42500000000000004,\n",
       "  0.4041666666666666,\n",
       "  0.5311111111111111,\n",
       "  0.5878571428571429,\n",
       "  0.551875,\n",
       "  0.54625,\n",
       "  0.3555555555555556,\n",
       "  0.32944444444444443,\n",
       "  0.34875,\n",
       "  0.5633333333333334,\n",
       "  0.4683333333333333,\n",
       "  0.37222222222222223,\n",
       "  0.5725,\n",
       "  0.4183333333333333,\n",
       "  0.31416666666666665,\n",
       "  0.47714285714285715,\n",
       "  0.5328571428571428,\n",
       "  0.42166666666666663,\n",
       "  0.4161111111111111,\n",
       "  0.445625,\n",
       "  0.69625,\n",
       "  0.4788888888888889,\n",
       "  0.4361111111111111,\n",
       "  0.42444444444444446,\n",
       "  0.6233333333333334,\n",
       "  0.6311111111111111,\n",
       "  0.68,\n",
       "  0.4699999999999999,\n",
       "  0.23900000000000002,\n",
       "  0.614375,\n",
       "  0.4655555555555555,\n",
       "  0.47388888888888897,\n",
       "  0.706875,\n",
       "  0.5787500000000001,\n",
       "  0.38357142857142856,\n",
       "  0.6558333333333334,\n",
       "  0.41888888888888887,\n",
       "  0.44642857142857145,\n",
       "  0.45722222222222225,\n",
       "  0.5555555555555556,\n",
       "  0.5441666666666668,\n",
       "  0.6372222222222221,\n",
       "  0.62125,\n",
       "  0.6188888888888888,\n",
       "  0.46416666666666667,\n",
       "  0.3038888888888889,\n",
       "  0.5238888888888888,\n",
       "  0.41625,\n",
       "  0.4166666666666667,\n",
       "  0.44166666666666665,\n",
       "  0.5061111111111111,\n",
       "  0.321875,\n",
       "  0.42562500000000003,\n",
       "  0.35944444444444446,\n",
       "  0.5359999999999999,\n",
       "  0.33055555555555555,\n",
       "  0.6455555555555557,\n",
       "  0.5662499999999999,\n",
       "  0.3238888888888889,\n",
       "  0.6366666666666666,\n",
       "  0.2871428571428572,\n",
       "  0.4321428571428571,\n",
       "  0.44055555555555553,\n",
       "  0.6077777777777776,\n",
       "  0.3894444444444444,\n",
       "  0.4872222222222223,\n",
       "  0.5521428571428572,\n",
       "  0.42,\n",
       "  0.3127777777777777,\n",
       "  0.438,\n",
       "  0.31611111111111106,\n",
       "  0.4905555555555556,\n",
       "  0.55,\n",
       "  0.48200000000000004,\n",
       "  0.5255555555555556,\n",
       "  0.495,\n",
       "  0.3661111111111111,\n",
       "  0.4916666666666667,\n",
       "  0.6377777777777777,\n",
       "  0.5355555555555556,\n",
       "  0.32333333333333336,\n",
       "  0.6775,\n",
       "  0.24166666666666664,\n",
       "  0.6183333333333333,\n",
       "  0.46625000000000005,\n",
       "  0.5285714285714286,\n",
       "  0.411875,\n",
       "  0.6227777777777778,\n",
       "  0.530625,\n",
       "  0.4116666666666666,\n",
       "  0.6599999999999999,\n",
       "  0.6166666666666667,\n",
       "  0.4278571428571428,\n",
       "  0.508,\n",
       "  0.44555555555555554,\n",
       "  0.29277777777777775,\n",
       "  0.3825,\n",
       "  0.5,\n",
       "  0.55,\n",
       "  0.473125,\n",
       "  0.4027777777777778,\n",
       "  0.561875,\n",
       "  0.483,\n",
       "  0.43062500000000004,\n",
       "  0.5514285714285714,\n",
       "  0.5127777777777778,\n",
       "  0.5792857142857143,\n",
       "  0.39222222222222225,\n",
       "  0.5822222222222222,\n",
       "  0.5433333333333334,\n",
       "  0.5311111111111111,\n",
       "  0.47777777777777775,\n",
       "  0.4971428571428572,\n",
       "  0.595,\n",
       "  0.5961111111111111,\n",
       "  0.39,\n",
       "  0.4577777777777778,\n",
       "  0.41071428571428564,\n",
       "  0.5266666666666667,\n",
       "  0.4066666666666666,\n",
       "  0.35666666666666663,\n",
       "  0.53,\n",
       "  0.38357142857142856,\n",
       "  0.4372222222222222,\n",
       "  0.44722222222222224,\n",
       "  0.558125,\n",
       "  0.4883333333333334,\n",
       "  0.34277777777777785,\n",
       "  0.5392857142857144,\n",
       "  0.5549999999999999,\n",
       "  0.3075,\n",
       "  0.5666666666666667,\n",
       "  0.395,\n",
       "  0.45357142857142857,\n",
       "  0.5975,\n",
       "  0.2866666666666667,\n",
       "  0.36916666666666664,\n",
       "  0.524375,\n",
       "  0.31,\n",
       "  0.4685714285714285,\n",
       "  0.610625,\n",
       "  0.4694444444444445,\n",
       "  0.45187499999999997,\n",
       "  0.4125,\n",
       "  0.46111111111111114,\n",
       "  0.461875,\n",
       "  0.429375,\n",
       "  0.3722222222222222,\n",
       "  0.52375,\n",
       "  0.47500000000000003,\n",
       "  0.4275,\n",
       "  0.36333333333333334,\n",
       "  0.445,\n",
       "  0.415,\n",
       "  0.4605555555555555,\n",
       "  0.4644444444444445,\n",
       "  0.48777777777777775,\n",
       "  0.4588888888888889,\n",
       "  0.47777777777777775,\n",
       "  0.613125,\n",
       "  0.48500000000000004,\n",
       "  0.41916666666666663,\n",
       "  0.48055555555555557,\n",
       "  0.42333333333333334,\n",
       "  0.5277777777777778,\n",
       "  0.36666666666666664,\n",
       "  0.39055555555555554,\n",
       "  0.47500000000000003,\n",
       "  0.49375,\n",
       "  0.4725,\n",
       "  0.3472222222222222,\n",
       "  0.5433333333333333,\n",
       "  0.5194444444444444,\n",
       "  0.55,\n",
       "  0.444375,\n",
       "  0.40750000000000003,\n",
       "  0.38125,\n",
       "  0.57375,\n",
       "  0.4205555555555555,\n",
       "  0.26333333333333336,\n",
       "  0.5455555555555556,\n",
       "  0.33399999999999996,\n",
       "  0.14333333333333334,\n",
       "  0.60375,\n",
       "  0.6733333333333333,\n",
       "  0.5872222222222222,\n",
       "  0.449375,\n",
       "  0.5385714285714286,\n",
       "  0.52,\n",
       "  0.3727777777777777,\n",
       "  0.42874999999999996,\n",
       "  0.5438888888888889,\n",
       "  0.5472222222222222,\n",
       "  0.580625,\n",
       "  0.6258333333333334,\n",
       "  0.32916666666666666,\n",
       "  0.45055555555555554,\n",
       "  0.6716666666666666,\n",
       "  0.3125,\n",
       "  0.31277777777777777,\n",
       "  0.5535714285714286,\n",
       "  0.61,\n",
       "  0.626875,\n",
       "  0.5738888888888889,\n",
       "  0.4583333333333333,\n",
       "  0.4607142857142857,\n",
       "  0.4422222222222223,\n",
       "  0.48277777777777775,\n",
       "  0.48888888888888893,\n",
       "  0.4516666666666667,\n",
       "  0.5983333333333334,\n",
       "  0.34687500000000004,\n",
       "  0.40499999999999997,\n",
       "  0.5144444444444445,\n",
       "  0.343125,\n",
       "  0.5633333333333334,\n",
       "  0.44555555555555554,\n",
       "  0.774375,\n",
       "  0.55375,\n",
       "  0.49,\n",
       "  0.5083333333333333,\n",
       "  0.49166666666666664,\n",
       "  0.245,\n",
       "  0.455,\n",
       "  0.5738888888888889,\n",
       "  0.45611111111111113,\n",
       "  0.6514285714285714,\n",
       "  0.5472222222222222,\n",
       "  0.589375,\n",
       "  0.4566666666666666,\n",
       "  0.5783333333333334,\n",
       "  0.5127777777777777,\n",
       "  0.376,\n",
       "  0.37444444444444447,\n",
       "  0.34833333333333333,\n",
       "  0.4278571428571429,\n",
       "  0.6116666666666668,\n",
       "  0.43277777777777776,\n",
       "  0.4077777777777778,\n",
       "  0.37222222222222223,\n",
       "  0.5622222222222222,\n",
       "  0.3535714285714286,\n",
       "  0.45562499999999995,\n",
       "  0.3335714285714286,\n",
       "  0.59,\n",
       "  0.4811111111111111,\n",
       "  0.008333333333333333,\n",
       "  0.565625,\n",
       "  0.34388888888888886,\n",
       "  0.33285714285714285,\n",
       "  0.3794444444444445,\n",
       "  0.5111111111111113,\n",
       "  0.45333333333333337,\n",
       "  0.4605555555555556,\n",
       "  0.6816666666666666,\n",
       "  0.45249999999999996,\n",
       "  0.42277777777777786,\n",
       "  0.5255555555555556,\n",
       "  0.46444444444444444,\n",
       "  0.5107142857142856,\n",
       "  0.6328571428571428,\n",
       "  0.605,\n",
       "  0.29400000000000004,\n",
       "  0.6394444444444444,\n",
       "  0.5577777777777777,\n",
       "  0.4794444444444444,\n",
       "  0.6900000000000001,\n",
       "  0.57375,\n",
       "  0.6075,\n",
       "  0.3633333333333333,\n",
       "  0.5372222222222223,\n",
       "  0.29750000000000004,\n",
       "  0.3661111111111111,\n",
       "  0.5127777777777778,\n",
       "  0.44625000000000004,\n",
       "  0.4161111111111111,\n",
       "  0.470625,\n",
       "  0.35944444444444446,\n",
       "  0.5035714285714287,\n",
       "  0.5327777777777778,\n",
       "  0.6533333333333333,\n",
       "  0.5311111111111111,\n",
       "  0.47500000000000003,\n",
       "  0.6566666666666667,\n",
       "  0.5850000000000001,\n",
       "  0.5325,\n",
       "  0.5533333333333333,\n",
       "  0.5314285714285715,\n",
       "  0.6366666666666667,\n",
       "  0.42999999999999994,\n",
       "  0.4422222222222223,\n",
       "  0.5874999999999999,\n",
       "  0.38722222222222225,\n",
       "  0.24214285714285716,\n",
       "  0.5322222222222223,\n",
       "  0.4314285714285714,\n",
       "  0.5194444444444445,\n",
       "  0.674375,\n",
       "  0.5422222222222223,\n",
       "  0.416875,\n",
       "  0.5422222222222222,\n",
       "  0.39899999999999997,\n",
       "  0.3811111111111111,\n",
       "  0.6556249999999999,\n",
       "  0.4238888888888888,\n",
       "  0.5355555555555556,\n",
       "  0.42277777777777775,\n",
       "  0.5750000000000001,\n",
       "  0.504375,\n",
       "  0.5938888888888889,\n",
       "  0.5211111111111111,\n",
       "  0.5214285714285715,\n",
       "  0.5349999999999999,\n",
       "  0.556875,\n",
       "  0.270625,\n",
       "  0.658125,\n",
       "  0.5388888888888889,\n",
       "  0.5900000000000001,\n",
       "  0.403125,\n",
       "  0.48,\n",
       "  0.3238888888888889,\n",
       "  0.5844444444444444,\n",
       "  0.5183333333333333,\n",
       "  0.36,\n",
       "  0.40555555555555556,\n",
       "  0.41625,\n",
       "  0.37833333333333335,\n",
       "  0.6114285714285714,\n",
       "  0.51,\n",
       "  0.3977777777777778,\n",
       "  0.578125,\n",
       "  0.43099999999999994,\n",
       "  0.45062499999999994,\n",
       "  0.5761111111111111,\n",
       "  0.3894444444444444,\n",
       "  0.4266666666666667,\n",
       "  0.2677777777777778,\n",
       "  0.43444444444444447,\n",
       "  0.41400000000000003,\n",
       "  0.4833333333333334,\n",
       "  0.51,\n",
       "  0.630625,\n",
       "  0.3966666666666667,\n",
       "  0.46125000000000005,\n",
       "  0.49187500000000006,\n",
       "  0.2275,\n",
       "  0.5105555555555555,\n",
       "  0.5555555555555556,\n",
       "  0.411875,\n",
       "  0.449375,\n",
       "  0.46222222222222226,\n",
       "  0.40750000000000003,\n",
       "  0.5977777777777777,\n",
       "  0.35428571428571437,\n",
       "  0.4178571428571428,\n",
       "  0.5161111111111111,\n",
       "  0.5466666666666666,\n",
       "  0.6275000000000001,\n",
       "  0.5871428571428572,\n",
       "  0.4388888888888889,\n",
       "  0.44166666666666665,\n",
       "  0.624375,\n",
       "  0.4527777777777777,\n",
       "  0.6094444444444445,\n",
       "  0.543888888888889,\n",
       "  0.44875,\n",
       "  0.41333333333333333,\n",
       "  0.3677777777777778,\n",
       "  0.34,\n",
       "  0.31,\n",
       "  0.5225,\n",
       "  0.6266666666666666,\n",
       "  0.3111111111111111,\n",
       "  0.45222222222222225,\n",
       "  0.375,\n",
       "  0.2621428571428571,\n",
       "  0.5977777777777777,\n",
       "  0.75,\n",
       "  0.49277777777777776,\n",
       "  0.426875,\n",
       "  0.36333333333333334,\n",
       "  0.40499999999999997,\n",
       "  0.5616666666666668,\n",
       "  0.345,\n",
       "  0.46928571428571425,\n",
       "  0.36333333333333334,\n",
       "  0.3561111111111111,\n",
       "  0.503125,\n",
       "  0.4527777777777778,\n",
       "  0.6961111111111111,\n",
       "  0.6166666666666667,\n",
       "  0.4633333333333333,\n",
       "  0.4175,\n",
       "  0.35125,\n",
       "  0.45,\n",
       "  0.40888888888888886,\n",
       "  0.3761111111111111,\n",
       "  0.51,\n",
       "  0.5127777777777779,\n",
       "  0.63125,\n",
       "  0.40625,\n",
       "  0.47277777777777774,\n",
       "  0.655,\n",
       "  0.4542857142857143,\n",
       "  0.5391666666666666,\n",
       "  0.37555555555555553,\n",
       "  0.468125,\n",
       "  0.28111111111111114,\n",
       "  0.5075,\n",
       "  0.586875,\n",
       "  0.43937499999999996,\n",
       "  0.49444444444444446,\n",
       "  0.505,\n",
       "  0.5888888888888889,\n",
       "  0.5466666666666666,\n",
       "  0.45444444444444443,\n",
       "  0.528125,\n",
       "  0.697857142857143,\n",
       "  0.6564285714285714,\n",
       "  0.365,\n",
       "  0.39875000000000005,\n",
       "  0.350625,\n",
       "  0.503125,\n",
       "  0.418125,\n",
       "  0.47857142857142854,\n",
       "  0.5405555555555556,\n",
       "  0.43571428571428567,\n",
       "  0.5828571428571429,\n",
       "  0.525625,\n",
       "  0.6061111111111113,\n",
       "  0.46142857142857147,\n",
       "  0.42111111111111116,\n",
       "  0.48,\n",
       "  0.3742857142857142,\n",
       "  0.44222222222222224,\n",
       "  0.277,\n",
       "  0.6716666666666666,\n",
       "  0.5661111111111111,\n",
       "  0.4600000000000001,\n",
       "  0.6127777777777778,\n",
       "  0.46777777777777785,\n",
       "  0.5233333333333333,\n",
       "  0.374375,\n",
       "  0.34714285714285714,\n",
       "  0.391875,\n",
       "  0.4605555555555556,\n",
       "  0.26111111111111107,\n",
       "  0.46687500000000004,\n",
       "  0.4721428571428571,\n",
       "  0.47500000000000003,\n",
       "  0.5378571428571429,\n",
       "  0.39444444444444443,\n",
       "  0.5511111111111111,\n",
       "  0.43111111111111106,\n",
       "  0.3638888888888889,\n",
       "  0.39125,\n",
       "  0.43,\n",
       "  0.45944444444444443,\n",
       "  0.5405555555555556,\n",
       "  0.4092857142857143,\n",
       "  0.5966666666666667,\n",
       "  0.5149999999999999,\n",
       "  0.375,\n",
       "  0.5127777777777777,\n",
       "  0.6225,\n",
       "  0.5575,\n",
       "  0.5692857142857144,\n",
       "  0.5564285714285714,\n",
       "  0.5461111111111111,\n",
       "  0.36714285714285716,\n",
       "  0.5125,\n",
       "  0.41111111111111115,\n",
       "  0.47562499999999996,\n",
       "  0.48666666666666664,\n",
       "  0.41444444444444445,\n",
       "  0.33944444444444444,\n",
       "  0.29999999999999993,\n",
       "  0.3775,\n",
       "  0.47777777777777775,\n",
       "  0.5061111111111111,\n",
       "  0.31875,\n",
       "  0.6422222222222221,\n",
       "  0.6833333333333333,\n",
       "  0.4266666666666667,\n",
       "  0.509375,\n",
       "  0.43555555555555553,\n",
       "  0.575,\n",
       "  0.38333333333333336,\n",
       "  0.550625,\n",
       "  0.5661111111111111,\n",
       "  0.43687499999999996,\n",
       "  0.48250000000000004,\n",
       "  0.48944444444444446,\n",
       "  0.46699999999999997,\n",
       "  0.5716666666666667,\n",
       "  0.4783333333333333,\n",
       "  0.4825,\n",
       "  0.459375,\n",
       "  0.35125,\n",
       "  0.289,\n",
       "  0.24333333333333332,\n",
       "  0.43222222222222223,\n",
       "  0.40833333333333327,\n",
       "  0.69625,\n",
       "  0.48000000000000004,\n",
       "  0.503888888888889,\n",
       "  0.41857142857142854,\n",
       "  0.46875,\n",
       "  0.6066666666666666,\n",
       "  0.5805555555555556,\n",
       "  0.5344444444444445,\n",
       "  0.33611111111111114,\n",
       "  0.48944444444444446,\n",
       "  0.5414285714285714,\n",
       "  0.3622222222222222,\n",
       "  0.4271428571428571,\n",
       "  0.49071428571428566,\n",
       "  0.265,\n",
       "  0.15214285714285716,\n",
       "  0.49874999999999997,\n",
       "  0.54,\n",
       "  0.38625000000000004,\n",
       "  0.3188888888888889,\n",
       "  0.4757142857142858,\n",
       "  0.535625,\n",
       "  0.5078571428571429,\n",
       "  0.4416666666666667,\n",
       "  0.45222222222222225,\n",
       "  0.418,\n",
       "  0.7194444444444446,\n",
       "  0.52125,\n",
       "  0.43062500000000004,\n",
       "  0.41625,\n",
       "  0.435625,\n",
       "  0.6088888888888889,\n",
       "  0.6116666666666668,\n",
       "  0.4588888888888889,\n",
       "  0.316,\n",
       "  0.35812499999999997,\n",
       "  0.49944444444444447,\n",
       "  0.4633333333333333,\n",
       "  0.655,\n",
       "  0.47928571428571426,\n",
       "  0.42071428571428565,\n",
       "  0.4875,\n",
       "  0.3572222222222222,\n",
       "  0.4655555555555555,\n",
       "  0.59875,\n",
       "  0.49555555555555547,\n",
       "  0.5266666666666667,\n",
       "  0.5505555555555556,\n",
       "  0.494375,\n",
       "  0.5183333333333332,\n",
       "  0.3333333333333333,\n",
       "  0.4675,\n",
       "  0.36444444444444446,\n",
       "  0.5950000000000001,\n",
       "  0.5325,\n",
       "  0.6377777777777778,\n",
       "  0.22375,\n",
       "  0.48375,\n",
       "  0.30857142857142855,\n",
       "  0.576875,\n",
       "  0.6825,\n",
       "  0.49666666666666665,\n",
       "  0.584375,\n",
       "  0.6421428571428571,\n",
       "  0.5907142857142856,\n",
       "  0.463125,\n",
       "  0.37785714285714295,\n",
       "  0.5066666666666667,\n",
       "  0.555,\n",
       "  0.648888888888889,\n",
       "  0.449,\n",
       "  0.4992857142857143,\n",
       "  0.5772222222222223,\n",
       "  0.559375,\n",
       "  0.5,\n",
       "  0.616111111111111,\n",
       "  0.52,\n",
       "  0.5772222222222223,\n",
       "  0.58,\n",
       "  0.6525,\n",
       "  0.43611111111111106,\n",
       "  0.41125,\n",
       "  0.41055555555555556,\n",
       "  0.5844444444444445,\n",
       "  0.3983333333333333,\n",
       "  0.4272222222222222,\n",
       "  0.5525,\n",
       "  0.3627777777777778,\n",
       "  0.3416666666666666,\n",
       "  0.3872222222222222,\n",
       "  0.37722222222222224,\n",
       "  0.4838888888888889,\n",
       "  0.5527777777777777,\n",
       "  0.32749999999999996,\n",
       "  0.3392857142857143,\n",
       "  0.42722222222222217,\n",
       "  0.5055555555555555,\n",
       "  0.6172222222222222,\n",
       "  0.4228571428571429,\n",
       "  0.36,\n",
       "  0.375,\n",
       "  0.4625,\n",
       "  0.4042857142857143,\n",
       "  0.3605555555555556,\n",
       "  0.49187499999999995,\n",
       "  0.3238888888888889,\n",
       "  0.5022222222222222,\n",
       "  0.62,\n",
       "  0.411875,\n",
       "  0.5177777777777778,\n",
       "  0.37916666666666665,\n",
       "  0.55,\n",
       "  0.42722222222222217,\n",
       "  0.39,\n",
       "  0.5342857142857144,\n",
       "  0.671111111111111,\n",
       "  0.4255555555555556,\n",
       "  0.533888888888889,\n",
       "  0.50875,\n",
       "  0.48062499999999997,\n",
       "  0.28250000000000003,\n",
       "  0.6594444444444445,\n",
       "  0.31777777777777777,\n",
       "  0.49888888888888894,\n",
       "  0.19499999999999998,\n",
       "  0.520625,\n",
       "  0.49416666666666664,\n",
       "  0.6011111111111112,\n",
       "  0.4192857142857144,\n",
       "  0.5261111111111111,\n",
       "  0.4605555555555556,\n",
       "  0.41055555555555556,\n",
       "  0.27499999999999997,\n",
       "  0.45687500000000003,\n",
       "  0.5466666666666667,\n",
       "  0.5927777777777776,\n",
       "  0.48444444444444446,\n",
       "  0.5283333333333333,\n",
       "  0.5638888888888889,\n",
       "  0.365,\n",
       "  0.385,\n",
       "  0.4588888888888889,\n",
       "  0.5605555555555556,\n",
       "  0.51625,\n",
       "  0.52,\n",
       "  0.3428571428571429,\n",
       "  0.4388888888888889,\n",
       "  0.380625,\n",
       "  0.548888888888889,\n",
       "  0.56,\n",
       "  0.57125,\n",
       "  0.375,\n",
       "  0.5527777777777777,\n",
       "  0.5359999999999999,\n",
       "  0.54,\n",
       "  0.311875,\n",
       "  0.5872222222222221,\n",
       "  0.5593750000000001,\n",
       "  0.43062500000000004,\n",
       "  0.4341666666666666,\n",
       "  0.5275,\n",
       "  0.6028571428571429,\n",
       "  0.5605555555555556,\n",
       "  0.513125,\n",
       "  0.5492857142857143,\n",
       "  0.503125,\n",
       "  0.4922222222222223,\n",
       "  0.35111111111111115,\n",
       "  0.5283333333333333,\n",
       "  0.4685714285714287,\n",
       "  0.45749999999999996,\n",
       "  0.441,\n",
       "  0.3711111111111111,\n",
       "  0.49214285714285716,\n",
       "  0.45333333333333337,\n",
       "  0.498125,\n",
       "  0.5272222222222223,\n",
       "  0.363,\n",
       "  0.374375,\n",
       "  0.5577777777777778,\n",
       "  0.4758333333333334,\n",
       "  0.57375,\n",
       "  0.547142857142857,\n",
       "  0.3733333333333333,\n",
       "  0.6842857142857143,\n",
       "  0.3916666666666666,\n",
       "  0.6585714285714286,\n",
       "  0.516875,\n",
       "  0.5733333333333335,\n",
       "  ...]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_trainer._eval_save_prefix = OUTPUT_PREFIX + \"-test-pos-unbiased\"\n",
    "model_trainer._evaluate_partial(test_dataset_pos_unbiased)\n",
    "\n",
    "model_trainer._eval_save_prefix = OUTPUT_PREFIX +  \"-test-neg-unbiased\"\n",
    "model_trainer._evaluate_partial(test_dataset_neg_unbiased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biased_results = dict()\n",
    "\n",
    "\n",
    "\n",
    "# biased_results[\"STRATIFIED_15\"] = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", gamma=1.5, K=30, partition=100)\n",
    "biased_results[\"AOA\"] = aoa(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", K=30)\n",
    "biased_results[\"UB_15\"] = eq(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", gamma=1.5, K=30)\n",
    "biased_results[\"UB_2\"] =  eq(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", gamma=2, K=30)\n",
    "biased_results[\"UB_25\"] =  eq(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", gamma=2.5, K=30)\n",
    "biased_results[\"UB_3\"] =  eq(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", gamma=3, K=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbiased_results = dict()\n",
    "\n",
    "# unbiased_results[\"STRATIFIED_15\"] = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", gamma=1.5, K=1, partition=100)\n",
    "unbiased_results[\"AOA\"] = aoa(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", K=1)\n",
    "unbiased_results[\"UB_15\"] = eq(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", gamma=1.5, K=1)\n",
    "unbiased_results[\"UB_2\"] =  eq(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", gamma=2, K=1)\n",
    "unbiased_results[\"UB_25\"] =  eq(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", gamma=2.5, K=1)\n",
    "unbiased_results[\"UB_3\"] =  eq(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", gamma=3, K=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_items = 932"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([558, 746, 931, 821, 380, 384, 696,  54, 415, 778,  85, 522,  31,\n",
       "       407, 767, 900, 610,  69, 128, 589, 886, 819, 738, 433, 534,  72,\n",
       "       607, 217,  93,  48, 468, 764, 168, 637, 898, 140, 238, 615, 430,\n",
       "       322, 429, 775, 735, 576, 396, 594, 478, 793, 786, 108, 121, 548,\n",
       "       759, 692, 364, 643, 544,  98, 229, 242, 491, 403, 333, 405, 473,\n",
       "       573, 711, 795, 492, 909, 184, 620, 236, 827, 716,  59, 349, 675,\n",
       "       334, 865, 710, 829, 768, 125, 925, 246, 515, 207, 126, 441, 166,\n",
       "       346, 852, 774, 595, 817, 922, 741, 890, 719,  83, 873, 874, 112,\n",
       "       352, 665, 434, 916, 555, 297, 831, 897, 171, 210, 582, 417, 398,\n",
       "       697, 612, 578, 910, 835, 118, 462, 379,  58, 359, 559, 338, 163,\n",
       "       122, 266,  78, 276, 885, 744, 191, 630, 745, 723, 496, 419, 560,\n",
       "       557, 878, 836, 820, 507, 851, 627, 661, 460,  94,   2,  39, 727,\n",
       "       459, 773, 699, 437, 583, 539, 667,  42, 130, 754, 825, 461,  35,\n",
       "       632, 791, 131, 899, 739, 703, 772, 158, 749, 161, 526, 516, 452,\n",
       "       731, 536, 300,  30, 743, 828, 180, 811, 862, 329, 664, 757, 647,\n",
       "       912, 412, 501, 891, 798, 310,  51, 532, 284, 475,  89, 476, 602,\n",
       "       488, 465, 344,   6, 375, 230, 725, 423, 175, 281,  18, 406, 704,\n",
       "       599, 299, 584, 453, 325, 666,  70, 585, 484, 193, 111, 199, 365,\n",
       "        65, 302, 603, 722,  80, 244, 117, 783, 115, 418, 896, 480, 736,\n",
       "       153, 358, 868, 170, 315, 178, 770, 262, 567,  10, 241,  82, 650,\n",
       "       799, 233, 721, 428, 295, 521, 860, 303, 265,  47, 903, 197, 319,\n",
       "        64, 335,  99, 633, 708, 390, 248, 392,  66, 689, 543, 211, 362,\n",
       "       354, 687, 693, 486, 149, 609, 648,  84, 553, 271,  96, 533, 498,\n",
       "       621])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums = np.arange(1, num_items+1)\n",
    "\n",
    "partitions = np.random.choice(nums, 300, replace=False)\n",
    "partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56f045d635f243bfb18e52e96f410c0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute biased and unbiased results with stratified for values of partition in (1,2*len(sorted_items))\n",
    "# and store biased and unbiased results such that abs(biased_results[key]['auc'] - unbiased_results[key]['auc']) + abs(biased_results[key]['recall'] - unbiased_results[key]['recall']) is minimized\n",
    "\n",
    "#This is the gamma used to compute the best partition\n",
    "gamma = 2\n",
    "\n",
    "key = \"STRATIFIED_\" + str(gamma).replace(\".\",\"\")\n",
    "\n",
    "unbiased_results[key] = dict()\n",
    "biased_results[key] = dict()\n",
    "best_partition=1\n",
    "\n",
    "#for p in tqdm(range(1, 2*num_items)):\n",
    "for p in tqdm(partitions):\n",
    "    temp_unbiased = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", gamma=gamma, K=1, partition=p)\n",
    "    temp_biased = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", gamma=gamma, K=30, partition=p)\n",
    "    if not unbiased_results[key]:\n",
    "        unbiased_results[key] = temp_unbiased\n",
    "    if not biased_results[key]:\n",
    "        biased_results[key] = temp_biased\n",
    "    elif abs(temp_biased['auc'] - temp_unbiased['auc']) + abs(temp_unbiased['recall'] - temp_biased['recall']) < abs(biased_results[key]['auc'] - unbiased_results[key]['auc']) + abs(biased_results[key]['recall'] - unbiased_results[key]['recall']):\n",
    "        biased_results[key]['auc'] = temp_biased['auc']\n",
    "        biased_results[key]['recall'] = temp_biased['recall']\n",
    "        unbiased_results[key]['auc'] = temp_unbiased['auc']\n",
    "        unbiased_results[key]['recall'] = temp_unbiased['recall']\n",
    "        best_partition = p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbiased_results[\"STRATIFIED_3\"] = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", gamma=3, K=1, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_3\"] = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", gamma=3, K=30, partition=best_partition)\n",
    "\n",
    "unbiased_results[\"STRATIFIED_2\"] = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", gamma=2, K=1, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_2\"] = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", gamma=2, K=30, partition=best_partition)\n",
    "\n",
    "unbiased_results[\"STRATIFIED_25\"] = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", gamma=2.5, K=1, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_25\"] = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", gamma=2.5, K=30, partition=best_partition)\n",
    "\n",
    "unbiased_results[\"STRATIFIED_15\"] = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", gamma=1.5, K=1, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_15\"] = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", gamma=1.5, K=30, partition=best_partition)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbiased_results[\"STRATIFIED_v2_3\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", gamma=3, K=1, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_v2_3\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", gamma=3, K=30, partition=best_partition)\n",
    "\n",
    "unbiased_results[\"STRATIFIED_v2_25\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", gamma=2.5, K=1, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_v2_25\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", gamma=2.5, K=30, partition=best_partition)\n",
    "\n",
    "unbiased_results[\"STRATIFIED_v2_2\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", gamma=2, K=1, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_v2_2\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", gamma=2, K=30, partition=best_partition)\n",
    "\n",
    "unbiased_results[\"STRATIFIED_v2_15\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", gamma=1.5, K=1, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_v2_15\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", gamma=1.5, K=30, partition=best_partition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "key, value = random.choice(list(biased_results.items()))\n",
    "rows = 2#len(list(value.keys()))\n",
    "columns = 13#len(list(biased_results.items()))\n",
    "results_array = np.zeros((rows,columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_results = dict()\n",
    "\n",
    "list_biased_res = list(biased_results.keys())\n",
    "\n",
    "for i in range(len(list_biased_res)):\n",
    "    key = list_biased_res[i]\n",
    "\n",
    "    for j in range(len(list(biased_results[key].keys()))):\n",
    "        key_2 = list(biased_results[key].keys())[j]\n",
    "\n",
    "        results_array[j][i] = abs(biased_results[key][key_2] - unbiased_results[key][key_2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_df = pd.DataFrame(columns=list(biased_results.keys()), data=results_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_values = list(biased_results[list(biased_results.keys())[0]].keys())\n",
    "mae_df.insert(0, \"metric\", metric_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **RESULTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>AOA</th>\n",
       "      <th>UB_15</th>\n",
       "      <th>UB_2</th>\n",
       "      <th>UB_25</th>\n",
       "      <th>UB_3</th>\n",
       "      <th>STRATIFIED_15</th>\n",
       "      <th>STRATIFIED_2</th>\n",
       "      <th>STRATIFIED_25</th>\n",
       "      <th>STRATIFIED_3</th>\n",
       "      <th>STRATIFIED_v2_3</th>\n",
       "      <th>STRATIFIED_v2_2</th>\n",
       "      <th>STRATIFIED_v2_25</th>\n",
       "      <th>STRATIFIED_v2_15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>auc</td>\n",
       "      <td>0.149395</td>\n",
       "      <td>0.122682</td>\n",
       "      <td>0.119498</td>\n",
       "      <td>0.117111</td>\n",
       "      <td>0.115319</td>\n",
       "      <td>0.119082</td>\n",
       "      <td>0.099087</td>\n",
       "      <td>0.037745</td>\n",
       "      <td>0.257602</td>\n",
       "      <td>0.115319</td>\n",
       "      <td>0.119498</td>\n",
       "      <td>0.117111</td>\n",
       "      <td>0.122682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>recall</td>\n",
       "      <td>0.379472</td>\n",
       "      <td>0.262428</td>\n",
       "      <td>0.251320</td>\n",
       "      <td>0.243180</td>\n",
       "      <td>0.237160</td>\n",
       "      <td>0.261659</td>\n",
       "      <td>0.257094</td>\n",
       "      <td>0.257288</td>\n",
       "      <td>0.221863</td>\n",
       "      <td>0.237160</td>\n",
       "      <td>0.251320</td>\n",
       "      <td>0.243180</td>\n",
       "      <td>0.262428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   metric       AOA     UB_15      UB_2     UB_25      UB_3  STRATIFIED_15  \\\n",
       "0     auc  0.149395  0.122682  0.119498  0.117111  0.115319       0.119082   \n",
       "1  recall  0.379472  0.262428  0.251320  0.243180  0.237160       0.261659   \n",
       "\n",
       "   STRATIFIED_2  STRATIFIED_25  STRATIFIED_3  STRATIFIED_v2_3  \\\n",
       "0      0.099087       0.037745      0.257602         0.115319   \n",
       "1      0.257094       0.257288      0.221863         0.237160   \n",
       "\n",
       "   STRATIFIED_v2_2  STRATIFIED_v2_25  STRATIFIED_v2_15  \n",
       "0         0.119498          0.117111          0.122682  \n",
       "1         0.251320          0.243180          0.262428  "
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RecSysEvaluation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
