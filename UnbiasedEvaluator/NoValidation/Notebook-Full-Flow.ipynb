{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **IMPORT LIBS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from openrec.tf1.legacy import ImplicitModelTrainer\n",
    "from openrec.tf1.legacy.utils.evaluators import ImplicitEvalManager\n",
    "from openrec.tf1.legacy.utils import ImplicitDataset\n",
    "from openrec.tf1.legacy.recommenders import CML, BPR, PMF\n",
    "from openrec.tf1.legacy.utils.evaluators import AUC\n",
    "from openrec.tf1.legacy.utils.samplers import PairwiseSampler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sps\n",
    "import os\n",
    "import pickle\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **GENERATE THE DATASET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2384795\n",
    "np.random.seed(seed=seed)\n",
    "\n",
    "folder_name = f\"../Dataset/\"\n",
    "\n",
    "if os.path.exists(folder_name) == False:\n",
    "    os.makedirs(folder_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>SongID</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>83</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>153</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>170</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>184</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>194</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  SongID  Rating\n",
       "0       1      14       5\n",
       "1       1      35       1\n",
       "2       1      46       1\n",
       "3       1      83       1\n",
       "4       1      93       1\n",
       "5       1      94       1\n",
       "6       1     153       5\n",
       "7       1     170       4\n",
       "8       1     184       5\n",
       "9       1     194       5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'yahoo_ymusic_v1/ydata-ymusic-rating-study-v1_0-train.txt'\n",
    "\n",
    "# Load the training set into a DataFrame\n",
    "df_train = pd.read_csv(folder_name+file_path, sep='\\t',names=[\"UserID\",\"SongID\",\"Rating\"], header=None)  # sep='\\t' for tab-separated values\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to implicit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"We treat items rated greater than or equal to 4 as relevant, and others as irrelevant, as suggested by prior literature.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>SongID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>ImplicitRating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>83</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>153</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>170</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>184</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>194</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  SongID  Rating  ImplicitRating\n",
       "0       1      14       5               1\n",
       "1       1      35       1               0\n",
       "2       1      46       1               0\n",
       "3       1      83       1               0\n",
       "4       1      93       1               0\n",
       "5       1      94       1               0\n",
       "6       1     153       5               1\n",
       "7       1     170       4               1\n",
       "8       1     184       5               1\n",
       "9       1     194       5               1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "POSITIVE_THRESHOLD = 4\n",
    "df_train['ImplicitRating'] = np.where(df_train['Rating'] >= POSITIVE_THRESHOLD, 1, 0)\n",
    "\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the number of users and items in the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The training set contains 300K ratings given by 15.4K users against 1K songs through natural interactions.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 15400)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_user = df_train[\"UserID\"].min()\n",
    "max_user = df_train[\"UserID\"].max()\n",
    "\n",
    "min_item = df_train[\"SongID\"].min()\n",
    "max_item = df_train[\"SongID\"].max()\n",
    "\n",
    "max_item, max_user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **GET UNBIASED TESTSET**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the unbiased testset and convert it to implicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>SongID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>ImplicitRating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>138</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>141</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>177</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>268</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>511</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>587</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>772</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>941</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  SongID  Rating  ImplicitRating\n",
       "0       1      49       1               0\n",
       "1       1     126       1               0\n",
       "2       1     138       1               0\n",
       "3       1     141       1               0\n",
       "4       1     177       1               0\n",
       "5       1     268       3               0\n",
       "6       1     511       1               0\n",
       "7       1     587       1               0\n",
       "8       1     772       5               1\n",
       "9       1     941       1               0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = '../Dataset/yahoo_ymusic_v1/ydata-ymusic-rating-study-v1_0-test.txt'\n",
    "df_test = pd.read_csv(file_path, sep='\\t',names=[\"UserID\",\"SongID\",\"Rating\"], header=None)  # sep='\\t' for tab-separated values\n",
    "df_test['ImplicitRating'] = np.where(df_test['Rating'] >= POSITIVE_THRESHOLD, 1, 0)\n",
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the number of users and items in the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The testing set is collected by asking a subset of 5.4K users to rate 10 randomly selected songs.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5400, 1000, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[\"UserID\"].max(), df_test[\"SongID\"].max(), int(df_test.shape[0]/df_test[\"UserID\"].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter unbiased testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"We filter the testing set by retaining users who have at least a relevant and an irrelevant song in the testing set and two relevant songs in the training set.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select users with at least an irrelevant song in the unbiased testset\n",
    "usersWithNegativeInteractionInTest = df_test[df_test[\"ImplicitRating\"] == 0][\"UserID\"].unique()\n",
    "\n",
    "# Select UserID of users with at least a relevant song in testset\n",
    "usersWithPositiveInteractionInTest = df_test[df_test[\"ImplicitRating\"] == 1][\"UserID\"].unique()\n",
    "\n",
    "# Select UserID of users with at least two relevant song in trainset\n",
    "usersWithTwoPositiveInteractions = df_train[df_train[\"ImplicitRating\"] == 1].groupby(\"UserID\").filter(lambda x: len(x) >= 2)['UserID'].unique()\n",
    "\n",
    "# Compute the intersection\n",
    "set1 = set(usersWithNegativeInteractionInTest)\n",
    "set2 = set(usersWithPositiveInteractionInTest)\n",
    "set3 = set(usersWithTwoPositiveInteractions)\n",
    "valid_users_testset = set1 & set2 & set3\n",
    "\n",
    "# Filter the testset\n",
    "df_test_filtered = df_test[df_test[\"UserID\"].isin(valid_users_testset)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"2296 users satisfy these requirements.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2296"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_users_testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape the unbiased test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the dataframe, for each row where ImplicitRating is 1, append [userID, itemID] to unbiased_pos_test_set\n",
    "# and for each row where ImplicitRating is 0, append [userID, itemID] to unbiased_neg_test_set\n",
    "\n",
    "unbiased_pos_test_set = df_test_filtered[df_test_filtered[\"ImplicitRating\"] == 1][[\"UserID\", \"SongID\"]].values\n",
    "unbiased_neg_test_set = df_test_filtered[df_test_filtered[\"ImplicitRating\"] == 0][[\"UserID\", \"SongID\"]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save unbiased test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbiased_pos_test_set_df = pd.DataFrame(unbiased_pos_test_set)\n",
    "unbiased_neg_test_set_df = pd.DataFrame(unbiased_neg_test_set)\n",
    "\n",
    "unbiased_pos_test_set_df.columns = [\"user_id\",\"item_id\"]\n",
    "unbiased_neg_test_set_df.columns = [\"user_id\",\"item_id\"]\n",
    "\n",
    "structured_data_pos_test_set_unbiased = unbiased_pos_test_set_df.to_records(index=False)\n",
    "structured_data_neg_test_set_unbiased = unbiased_neg_test_set_df.to_records(index=False)\n",
    "\n",
    "np.save(folder_name + \"unbiased-test_arr_pos.npy\", structured_data_pos_test_set_unbiased)\n",
    "np.save(folder_name + \"unbiased-test_arr_neg.npy\", structured_data_neg_test_set_unbiased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **GET BIASED TESTSET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>SongID</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>83</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>153</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>170</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>184</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>194</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  SongID  Rating\n",
       "0       1      14       5\n",
       "1       1      35       1\n",
       "2       1      46       1\n",
       "3       1      83       1\n",
       "4       1      93       1\n",
       "5       1      94       1\n",
       "6       1     153       5\n",
       "7       1     170       4\n",
       "8       1     184       5\n",
       "9       1     194       5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'yahoo_ymusic_v1/ydata-ymusic-rating-study-v1_0-train.txt'\n",
    "\n",
    "# Load the training set into a DataFrame\n",
    "df_train = pd.read_csv(folder_name+file_path, sep='\\t',names=[\"UserID\",\"SongID\",\"Rating\"], header=None)  # sep='\\t' for tab-separated values\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['ImplicitRating'] = np.where(df_train['Rating'] >= POSITIVE_THRESHOLD, 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[df_train[\"UserID\"].isin(valid_users_testset)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 58799 entries, 0 to 129178\n",
      "Data columns (total 4 columns):\n",
      " #   Column          Non-Null Count  Dtype\n",
      "---  ------          --------------  -----\n",
      " 0   UserID          58799 non-null  int64\n",
      " 1   SongID          58799 non-null  int64\n",
      " 2   Rating          58799 non-null  int64\n",
      " 3   ImplicitRating  58799 non-null  int64\n",
      "dtypes: int64(4)\n",
      "memory usage: 2.2 MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the biased test set and shape it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"We additionally held out a biased testing set (biased-testing) from the training set by randomly sampling 300 songs for each user.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompute, for each user, the list of songs with a relevant rating\n",
    "user_positive_ratings = df_train[df_train[\"ImplicitRating\"] == 1].groupby(\"UserID\")[\"SongID\"].apply(set)\n",
    "\n",
    "# Initialize the range of indexes for the items\n",
    "items_ids = np.arange(min_item, max_item + 1)\n",
    "# Set the number of songs for each user\n",
    "SONGS_FOR_BIASED_TEST = 300\n",
    "\n",
    "#IPOTESI MAN\n",
    "\n",
    "pos_test_set = []\n",
    "neg_test_set = []\n",
    "\n",
    "for user_id in valid_users_testset:\n",
    "    np.random.shuffle(items_ids)\n",
    "    test_items = set(items_ids[-SONGS_FOR_BIASED_TEST:])\n",
    "    pos_ids = user_positive_ratings.get(user_id, set()) & test_items\n",
    "\n",
    "    #set those to 0 so that they will no longer be used in training set\n",
    "    df_train.loc[(df_train['SongID'].isin(pos_ids)) & (df_train['UserID'] == user_id), 'ImplicitRating'] = 0\n",
    "\n",
    "    for id in test_items:\n",
    "        if id in pos_ids:\n",
    "            pos_test_set.append([user_id, id])\n",
    "        else:\n",
    "            neg_test_set.append([user_id, id])\n",
    "\n",
    "pos_test_set = np.array(pos_test_set)\n",
    "neg_test_set = np.array(neg_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the biased test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_test_set_df = pd.DataFrame(pos_test_set)\n",
    "neg_test_set_df = pd.DataFrame(neg_test_set)\n",
    "\n",
    "pos_test_set_df.columns = [\"user_id\",\"item_id\"]\n",
    "neg_test_set_df.columns = [\"user_id\",\"item_id\"]\n",
    "\n",
    "structured_data_pos_test_set = pos_test_set_df.to_records(index=False)\n",
    "structured_data_neg_test_set = neg_test_set_df.to_records(index=False)\n",
    "\n",
    "np.save(folder_name + \"biased-test_arr_pos.npy\", structured_data_pos_test_set)\n",
    "np.save(folder_name + \"biased-test_arr_neg.npy\", structured_data_neg_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **STORE TRAINSET**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take couples user-item filtering out the irrelevant ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only take the couples (user, item) with relevant rating\n",
    "new_df = df_train[df_train['ImplicitRating'] != 0]\n",
    "new_df = new_df.drop(columns=['Rating', 'ImplicitRating'])\n",
    "\n",
    "# Define a dictionary for renaming columns\n",
    "rename_dict = {\n",
    "    'UserID': 'user_id',\n",
    "    'SongID': 'item_id'\n",
    "}\n",
    "\n",
    "# Rename the columns\n",
    "new_df = new_df.rename(columns=rename_dict)\n",
    "\n",
    "# Convert the DataFrame to a structured array\n",
    "structured_data = new_df.to_records(index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = structured_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(folder_name + \"training_arr.npy\", train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MODEL CHOICE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = dict()\n",
    "raw_data['train_data'] = np.load(folder_name + \"training_arr.npy\")\n",
    "raw_data['max_user'] = 15401\n",
    "raw_data['max_item'] = 1001\n",
    "batch_size = 8000\n",
    "test_batch_size = 1000\n",
    "display_itr = 1000\n",
    "\n",
    "train_dataset = ImplicitDataset(raw_data['train_data'], raw_data['max_user'], raw_data['max_item'], name='Train')\n",
    "\n",
    "MODEL_CLASS = CML\n",
    "MODEL_PREFIX = \"cml\"\n",
    "DATASET_NAME = \"yahoo\"\n",
    "OUTPUT_FOLDER = \"./Output/\"\n",
    "OUTPUT_PATH = OUTPUT_FOLDER + MODEL_PREFIX + \"-\" + DATASET_NAME + \"/\"\n",
    "OUTPUT_PREFIX = str(OUTPUT_PATH) + str(MODEL_PREFIX) + \"-\" + str(DATASET_NAME)\n",
    "\n",
    "\n",
    "if os.path.exists(OUTPUT_PATH) == False:\n",
    "    os.makedirs(OUTPUT_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TRAIN THE MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:391: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:31: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:41: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:43: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:33: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/interactions/pairwise_eu_dist.py:71: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:596: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:75: The name tf.scatter_update is deprecated. Please use tf.compat.v1.scatter_update instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:144: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:365: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:148: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-29 12:36:16.773692: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
      "2024-03-29 12:36:16.780314: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 4192029999 Hz\n",
      "2024-03-29 12:36:16.781274: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x556a42af2ea0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2024-03-29 12:36:16.781290: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    }
   ],
   "source": [
    "# Avoid tensorflow using cached embeddings\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "\n",
    "model = MODEL_CLASS(batch_size=batch_size, max_user=train_dataset.max_user(), max_item=train_dataset.max_item(), \n",
    "    dim_embed=50, l2_reg=0.001, opt='Adam', sess_config=None)\n",
    "sampler = PairwiseSampler(batch_size=batch_size, dataset=train_dataset, num_process=4)\n",
    "model_trainer = ImplicitModelTrainer(batch_size=batch_size, test_batch_size=test_batch_size,\n",
    "                                     train_dataset=train_dataset, model=model, sampler=sampler,\n",
    "                                     eval_save_prefix=OUTPUT_PATH + DATASET_NAME,\n",
    "                                     item_serving_size=500)\n",
    "auc_evaluator = AUC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Start training with FULL evaluation ==\n",
      "[Itr 100] Finished\n",
      "[Itr 200] Finished\n",
      "[Itr 300] Finished\n",
      "[Itr 400] Finished\n",
      "[Itr 500] Finished\n",
      "[Itr 600] Finished\n",
      "[Itr 700] Finished\n",
      "[Itr 800] Finished\n",
      "[Itr 900] Finished\n",
      "[Itr 1000] Finished\n",
      "INFO:tensorflow:./Output/cml-yahoo/yahoo-1000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 1000] loss: 1768.235659\n",
      "[Itr 1100] Finished\n",
      "[Itr 1200] Finished\n",
      "[Itr 1300] Finished\n",
      "[Itr 1400] Finished\n",
      "[Itr 1500] Finished\n",
      "[Itr 1600] Finished\n",
      "[Itr 1700] Finished\n",
      "[Itr 1800] Finished\n",
      "[Itr 1900] Finished\n",
      "[Itr 2000] Finished\n",
      "INFO:tensorflow:./Output/cml-yahoo/yahoo-2000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 2000] loss: 649.306754\n",
      "[Itr 2100] Finished\n",
      "[Itr 2200] Finished\n",
      "[Itr 2300] Finished\n",
      "[Itr 2400] Finished\n",
      "[Itr 2500] Finished\n",
      "[Itr 2600] Finished\n",
      "[Itr 2700] Finished\n",
      "[Itr 2800] Finished\n",
      "[Itr 2900] Finished\n",
      "[Itr 3000] Finished\n",
      "INFO:tensorflow:./Output/cml-yahoo/yahoo-3000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 3000] loss: 574.229442\n",
      "[Itr 3100] Finished\n",
      "[Itr 3200] Finished\n",
      "[Itr 3300] Finished\n",
      "[Itr 3400] Finished\n",
      "[Itr 3500] Finished\n",
      "[Itr 3600] Finished\n",
      "[Itr 3700] Finished\n",
      "[Itr 3800] Finished\n",
      "[Itr 3900] Finished\n",
      "[Itr 4000] Finished\n",
      "INFO:tensorflow:./Output/cml-yahoo/yahoo-4000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 4000] loss: 546.507020\n",
      "[Itr 4100] Finished\n",
      "[Itr 4200] Finished\n",
      "[Itr 4300] Finished\n",
      "[Itr 4400] Finished\n",
      "[Itr 4500] Finished\n",
      "[Itr 4600] Finished\n",
      "[Itr 4700] Finished\n",
      "[Itr 4800] Finished\n",
      "[Itr 4900] Finished\n",
      "[Itr 5000] Finished\n",
      "INFO:tensorflow:./Output/cml-yahoo/yahoo-5000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 5000] loss: 532.541829\n",
      "[Itr 5100] Finished\n",
      "[Itr 5200] Finished\n",
      "[Itr 5300] Finished\n",
      "[Itr 5400] Finished\n",
      "[Itr 5500] Finished\n",
      "[Itr 5600] Finished\n",
      "[Itr 5700] Finished\n",
      "[Itr 5800] Finished\n",
      "[Itr 5900] Finished\n",
      "[Itr 6000] Finished\n",
      "INFO:tensorflow:./Output/cml-yahoo/yahoo-6000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 6000] loss: 525.123587\n",
      "[Itr 6100] Finished\n",
      "[Itr 6200] Finished\n",
      "[Itr 6300] Finished\n",
      "[Itr 6400] Finished\n",
      "[Itr 6500] Finished\n",
      "[Itr 6600] Finished\n",
      "[Itr 6700] Finished\n",
      "[Itr 6800] Finished\n",
      "[Itr 6900] Finished\n",
      "[Itr 7000] Finished\n",
      "INFO:tensorflow:./Output/cml-yahoo/yahoo-7000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 7000] loss: 520.953409\n",
      "[Itr 7100] Finished\n",
      "[Itr 7200] Finished\n",
      "[Itr 7300] Finished\n",
      "[Itr 7400] Finished\n",
      "[Itr 7500] Finished\n",
      "[Itr 7600] Finished\n",
      "[Itr 7700] Finished\n",
      "[Itr 7800] Finished\n",
      "[Itr 7900] Finished\n",
      "[Itr 8000] Finished\n",
      "INFO:tensorflow:./Output/cml-yahoo/yahoo-8000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 8000] loss: 519.029264\n",
      "[Itr 8100] Finished\n",
      "[Itr 8200] Finished\n",
      "[Itr 8300] Finished\n",
      "[Itr 8400] Finished\n",
      "[Itr 8500] Finished\n",
      "[Itr 8600] Finished\n",
      "[Itr 8700] Finished\n",
      "[Itr 8800] Finished\n",
      "[Itr 8900] Finished\n",
      "[Itr 9000] Finished\n",
      "INFO:tensorflow:./Output/cml-yahoo/yahoo-9000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 9000] loss: 516.913186\n",
      "[Itr 9100] Finished\n",
      "[Itr 9200] Finished\n",
      "[Itr 9300] Finished\n",
      "[Itr 9400] Finished\n",
      "[Itr 9500] Finished\n",
      "[Itr 9600] Finished\n",
      "[Itr 9700] Finished\n",
      "[Itr 9800] Finished\n",
      "[Itr 9900] Finished\n",
      "[Itr 10000] Finished\n",
      "INFO:tensorflow:./Output/cml-yahoo/yahoo-10000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 10000] loss: 515.887499\n"
     ]
    }
   ],
   "source": [
    "model_trainer.train(num_itr=10001, display_itr=display_itr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:./Output/cml-yahoo/ is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "model.save(OUTPUT_PATH,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DEFINING FUNCTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eq(infilename, infilename_neg, trainfilename, gamma=-1.0, K=1):\n",
    "    infile = open(infilename, 'rb')\n",
    "    infile_neg = open(infilename_neg, 'rb')\n",
    "    P = pickle.load(infile)\n",
    "    infile.close()\n",
    "    P_neg = pickle.load(infile_neg)\n",
    "    infile_neg.close()\n",
    "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
    "    #\n",
    "    for theuser in P[\"users\"]:\n",
    "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "    #\n",
    "    Zui = dict()\n",
    "    Ni = dict()\n",
    "    # fill in dictionary Ni\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "    del trainset\n",
    "\n",
    "    # count #users with non-zero item frequencies\n",
    "    nonzero_user_count = 0\n",
    "    for theuser in P[\"users\"]:\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for pos_item in pos_items:\n",
    "            if pos_item in Ni:\n",
    "                nonzero_user_count += 1\n",
    "                break\n",
    "    # fill in dictionary Zui\n",
    "    for theuser in P[\"users\"]:\n",
    "        all_scores = np.array(P[\"results\"][theuser])\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for i, pos_item in enumerate(pos_items):\n",
    "            pos_score = pos_scores[i]\n",
    "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
    "    # calculate per-user scores\n",
    "    sum_user_auc = 0.0\n",
    "    sum_user_recall = 0.0\n",
    "\n",
    "    for theuser in P[\"users\"]:\n",
    "        numerator_auc = 0.0\n",
    "        numerator_recall = 0.0\n",
    "        denominator = 0.0\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            if theitem not in Ni:\n",
    "                continue\n",
    "            pui = np.power(Ni[theitem], (gamma + 1) / 2.0)\n",
    "\n",
    "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser])) / pui\n",
    "            # Calcolo il Recall a 1, vedi nota 6 paper\n",
    "            if Zui[(theuser, theitem)] < K:\n",
    "                numerator_recall += 1.0 / pui\n",
    "            denominator += 1 / pui\n",
    "                \n",
    "\n",
    "        if denominator > 0:\n",
    "            sum_user_auc += numerator_auc / denominator\n",
    "            sum_user_recall += numerator_recall / denominator \n",
    "\n",
    "    return {\n",
    "        \"auc\"       : sum_user_auc / nonzero_user_count,\n",
    "        \"recall\"    : sum_user_recall / nonzero_user_count\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aoa(infilename, infilename_neg, trainfilename, K=1):\n",
    "    infile = open(infilename, 'rb')\n",
    "    infile_neg = open(infilename_neg, 'rb')\n",
    "    P = pickle.load(infile)\n",
    "    infile.close()\n",
    "    P_neg = pickle.load(infile_neg)\n",
    "    infile_neg.close()\n",
    "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
    "    #\n",
    "    for theuser in P[\"users\"]:\n",
    "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "    #\n",
    "    Zui = dict()\n",
    "    Ni = dict()\n",
    "    # fill in dictionary Ni\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "    del trainset\n",
    "    # count #users with non-zero item frequencies\n",
    "    nonzero_user_count = 0\n",
    "    for theuser in P[\"users\"]:\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for pos_item in pos_items:\n",
    "            if pos_item in Ni:\n",
    "                nonzero_user_count += 1\n",
    "                break\n",
    "    # fill in dictionary Zui\n",
    "    for theuser in P[\"users\"]:\n",
    "        all_scores = np.array(P[\"results\"][theuser])\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for i, pos_item in enumerate(pos_items):\n",
    "            pos_score = pos_scores[i]\n",
    "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
    "    # calculate per-user scores\n",
    "    sum_user_auc = 0.0\n",
    "    sum_user_recall = 0.0\n",
    "    for theuser in P[\"users\"]:\n",
    "        numerator_auc = 0.0\n",
    "        numerator_recall = 0.0\n",
    "        denominator = 0.0\n",
    "\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            if theitem not in Ni:\n",
    "                continue\n",
    "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser]))\n",
    "            # Calcolo il Recall a 30, vedi nota 6 paper\n",
    "            if Zui[(theuser, theitem)] < K:\n",
    "                numerator_recall += 1.0\n",
    "            denominator += 1 \n",
    "\n",
    "        if denominator > 0:\n",
    "            sum_user_auc += numerator_auc / denominator\n",
    "            sum_user_recall += numerator_recall / denominator\n",
    "\n",
    "    return {\n",
    "        \"auc\"       : sum_user_auc / nonzero_user_count,\n",
    "        \"recall\"    : sum_user_recall / nonzero_user_count\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified(infilename, infilename_neg, trainfilename, gamma=1.0, K=30, partition=10):\n",
    "\n",
    "    # Read pickles\n",
    "    infile = open(infilename, 'rb')\n",
    "    infile_neg = open(infilename_neg, 'rb')\n",
    "    P = pickle.load(infile)\n",
    "    infile.close()\n",
    "    P_neg = pickle.load(infile_neg)\n",
    "    infile_neg.close()\n",
    "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
    "\n",
    "    # Merge P and P_neg\n",
    "    for theuser in P[\"users\"]:\n",
    "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "\n",
    "    Zui = dict()\n",
    "    Ni = dict()\n",
    "\n",
    "    # Compute frequencies of items in the training set\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "    del trainset\n",
    "\n",
    "    # Compute recommendations for each user\n",
    "    for theuser in P[\"users\"]:\n",
    "        all_scores = np.array(P[\"results\"][theuser])\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for i, pos_item in enumerate(pos_items):\n",
    "            pos_score = pos_scores[i]\n",
    "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
    "\n",
    "    pui = dict()\n",
    "    w = dict()\n",
    "\n",
    "    # Compute dictionary of propensity scores\n",
    "    for theuser in P[\"users\"]:\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            if theitem not in Ni:\n",
    "                continue\n",
    "            if theitem in pui:\n",
    "                continue\n",
    "            pui[theitem] = np.power(Ni[theitem], (gamma + 1) / 2.0)\n",
    "\n",
    "    # Take the list of items (not tuples) in pui sorted by value\n",
    "    items_sorted_by_value = sorted(pui, key=pui.get, reverse=True)\n",
    "\n",
    "    # Compute linspace between the pui[0] and pui[-1] \n",
    "    linspace = np.linspace(pui[items_sorted_by_value[0]], pui[items_sorted_by_value[-1]], partition+1)\n",
    "   \n",
    "    # Compute dictionary w, that is, for each item, assigns the average of the puis in the partition it belongs to\n",
    "    i=0\n",
    "    j = 0\n",
    "    while i < len(items_sorted_by_value):\n",
    "                            \n",
    "        avg = 0\n",
    "        start = i\n",
    "        end = i\n",
    "    \n",
    "        while i < len(items_sorted_by_value) and pui[items_sorted_by_value[i]] >= linspace[j+1]:\n",
    "            avg += 1.0 / pui[items_sorted_by_value[i]]\n",
    "            end = i\n",
    "            i += 1\n",
    "        \n",
    "        avg = avg / (end - start + 1)\n",
    "\n",
    "        for k in range(start, end+1):\n",
    "            w[items_sorted_by_value[k]] = avg\n",
    "\n",
    "\n",
    "        j += 1\n",
    "\n",
    "    nonzero_user_count = 0\n",
    "    sum_user_auc = 0.0\n",
    "    sum_user_recall = 0.0\n",
    "\n",
    "    # Compute score with AUC and compute Recall\n",
    "    for theuser in P[\"users\"]:\n",
    "        numerator_auc = 0.0\n",
    "        numerator_recall = 0.0\n",
    "        denominator = 0.0\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            # Skip items with null frequency\n",
    "            if  theitem not in Ni:\n",
    "                continue\n",
    "            # Add things to be summed for each item\n",
    "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser])) * w[theitem]\n",
    "            # Add things for recall\n",
    "            if Zui[(theuser, theitem)] < K:\n",
    "                numerator_recall += 1.0 * w[theitem] #spetta\n",
    "            # Increment denominator that the sum must be divided by \n",
    "            denominator += 1 / pui[theitem]\n",
    "\n",
    "\n",
    "        # If there was at least one item for the user, count the user and sum the results\n",
    "        if denominator > 0:\n",
    "            nonzero_user_count += 1\n",
    "            sum_user_auc += numerator_auc / denominator\n",
    "            sum_user_recall += numerator_recall / denominator \n",
    "\n",
    "    return {\n",
    "        \"auc\"       : sum_user_auc / nonzero_user_count, \n",
    "        \"recall\"    : sum_user_recall / nonzero_user_count\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **EVALUATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_results = []\n",
    "recall_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = dict()\n",
    "raw_data['train_data'] = np.load(folder_name + \"training_arr.npy\")\n",
    "raw_data['test_data_pos_biased'] = np.load(folder_name + \"biased-test_arr_pos.npy\")\n",
    "raw_data['test_data_neg_biased'] = np.load(folder_name + \"biased-test_arr_neg.npy\")\n",
    "raw_data['test_data_pos_unbiased'] = np.load(folder_name + \"unbiased-test_arr_pos.npy\")\n",
    "raw_data['test_data_neg_unbiased'] = np.load(folder_name + \"unbiased-test_arr_neg.npy\")\n",
    "raw_data['max_user'] = 15401\n",
    "raw_data['max_item'] = 1001\n",
    "batch_size = 8000\n",
    "test_batch_size = 1000\n",
    "display_itr = 1000\n",
    "\n",
    "train_dataset = ImplicitDataset(raw_data['train_data'], raw_data['max_user'], raw_data['max_item'], name='Train')\n",
    "test_dataset_pos_biased = ImplicitDataset(raw_data['test_data_pos_biased'], raw_data['max_user'], raw_data['max_item'])\n",
    "test_dataset_neg_biased = ImplicitDataset(raw_data['test_data_neg_biased'], raw_data['max_user'], raw_data['max_item'])\n",
    "test_dataset_pos_unbiased = ImplicitDataset(raw_data['test_data_pos_unbiased'], raw_data['max_user'], raw_data['max_item'])\n",
    "test_dataset_neg_unbiased = ImplicitDataset(raw_data['test_data_neg_unbiased'], raw_data['max_user'], raw_data['max_item'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./Output/cml-yahoo/\n"
     ]
    }
   ],
   "source": [
    "#Code to avoid tf using cached embeddings\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "model = MODEL_CLASS(batch_size=batch_size, max_user=train_dataset.max_user(), max_item=train_dataset.max_item(),\n",
    "    dim_embed=50, l2_reg=0.001, opt='Adam', sess_config=None)\n",
    "sampler = PairwiseSampler(batch_size=batch_size, dataset=train_dataset, num_process=4)\n",
    "model_trainer = ImplicitModelTrainer(batch_size=batch_size, test_batch_size=test_batch_size,\n",
    "                                     train_dataset=train_dataset, model=model, sampler=sampler,\n",
    "                                     eval_save_prefix=OUTPUT_PATH + DATASET_NAME,\n",
    "                                     item_serving_size=500)\n",
    "auc_evaluator = AUC()\n",
    "\n",
    "model.load(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Subsampling negative items]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    }
   ],
   "source": [
    "model_trainer._eval_manager = ImplicitEvalManager(evaluators=[auc_evaluator])\n",
    "model_trainer._num_negatives = 200\n",
    "model_trainer._exclude_positives([train_dataset, test_dataset_pos_biased, test_dataset_neg_biased])\n",
    "model_trainer._sample_negatives(seed=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biased Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2070/2070 [00:00<00:00, 2641.07it/s]\n",
      "100%|| 2296/2296 [00:25<00:00, 90.86it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AUC': [0.5076271186440678,\n",
       "  0.506271186440678,\n",
       "  0.5027348993288591,\n",
       "  0.5019463087248321,\n",
       "  0.4898316498316499,\n",
       "  0.4743728813559322,\n",
       "  0.5206879194630873,\n",
       "  0.5511317567567567,\n",
       "  0.5093120805369128,\n",
       "  0.513047138047138,\n",
       "  0.5214915254237287,\n",
       "  0.5127702702702702,\n",
       "  0.4757286432160805,\n",
       "  0.5069833333333333,\n",
       "  0.5159548611111111,\n",
       "  0.48922818791946315,\n",
       "  0.5137959866220736,\n",
       "  0.4623639455782313,\n",
       "  0.4967281879194631,\n",
       "  0.5175252525252525,\n",
       "  0.5357859531772575,\n",
       "  0.527277397260274,\n",
       "  0.5184615384615384,\n",
       "  0.44310402684563766,\n",
       "  0.47668896321070237,\n",
       "  0.49315140845070427,\n",
       "  0.4975773195876289,\n",
       "  0.49876254180602003,\n",
       "  0.4879081632653061,\n",
       "  0.52513468013468,\n",
       "  0.4663087248322148,\n",
       "  0.5041582491582492,\n",
       "  0.47843749999999996,\n",
       "  0.5076421404682275,\n",
       "  0.4563590604026846,\n",
       "  0.5035906040268456,\n",
       "  0.514314381270903,\n",
       "  0.4532993197278911,\n",
       "  0.4963422818791946,\n",
       "  0.5022972972972973,\n",
       "  0.4911655405405405,\n",
       "  0.44620689655172413,\n",
       "  0.5423037542662117,\n",
       "  0.4843771043771044,\n",
       "  0.48109060402684567,\n",
       "  0.4922053872053872,\n",
       "  0.5363299663299663,\n",
       "  0.5220333333333333,\n",
       "  0.5165604026845637,\n",
       "  0.47110367892976596,\n",
       "  0.5164333333333334,\n",
       "  0.5068013468013468,\n",
       "  0.5170508474576271,\n",
       "  0.5110906040268457,\n",
       "  0.49261986301369864,\n",
       "  0.47692953020134227,\n",
       "  0.49713804713804716,\n",
       "  0.5102931034482759,\n",
       "  0.47560402684563763,\n",
       "  0.5486440677966102,\n",
       "  0.4869865319865321,\n",
       "  0.5291275167785235,\n",
       "  0.47397959183673477,\n",
       "  0.47484848484848496,\n",
       "  0.45488294314381267,\n",
       "  0.5097818791946308,\n",
       "  0.48223905723905724,\n",
       "  0.48357382550335565,\n",
       "  0.4768166666666667,\n",
       "  0.4938294314381271,\n",
       "  0.5054180602006689,\n",
       "  0.5468394648829431,\n",
       "  0.44439393939393945,\n",
       "  0.4728253424657534,\n",
       "  0.4858277027027027,\n",
       "  0.4874067796610169,\n",
       "  0.4520973154362416,\n",
       "  0.47363333333333324,\n",
       "  0.4958892617449664,\n",
       "  0.49526936026936036,\n",
       "  0.46882154882154886,\n",
       "  0.4850680272108844,\n",
       "  0.4998993288590605,\n",
       "  0.5058221476510066,\n",
       "  0.49957912457912457,\n",
       "  0.5088383838383839,\n",
       "  0.45092905405405403,\n",
       "  0.48039518900343636,\n",
       "  0.5542398648648649,\n",
       "  0.5273648648648649,\n",
       "  0.4898484848484849,\n",
       "  0.5180369127516778,\n",
       "  0.4514358108108108,\n",
       "  0.49401006711409406,\n",
       "  0.5306902356902358,\n",
       "  0.47113013698630135,\n",
       "  0.5115268456375839,\n",
       "  0.5207692307692308,\n",
       "  0.4795439189189189,\n",
       "  0.5406375838926175,\n",
       "  0.5004729729729729,\n",
       "  0.5191952054794521,\n",
       "  0.501734693877551,\n",
       "  0.5276223776223775,\n",
       "  0.49305,\n",
       "  0.48576271186440684,\n",
       "  0.49909698996655516,\n",
       "  0.48624161073825506,\n",
       "  0.5383,\n",
       "  0.5045286195286194,\n",
       "  0.48374161073825506,\n",
       "  0.4918518518518519,\n",
       "  0.4798821548821548,\n",
       "  0.4916666666666667,\n",
       "  0.5193939393939394,\n",
       "  0.52865,\n",
       "  0.4910738255033557,\n",
       "  0.49296296296296294,\n",
       "  0.5015932203389829,\n",
       "  0.4672166666666667,\n",
       "  0.45821548821548824,\n",
       "  0.4842255892255892,\n",
       "  0.5761036789297659,\n",
       "  0.5027181208053692,\n",
       "  0.4817627118644068,\n",
       "  0.5015993265993267,\n",
       "  0.5418350168350169,\n",
       "  0.5127609427609429,\n",
       "  0.5320637583892618,\n",
       "  0.5341778523489932,\n",
       "  0.4842785234899329,\n",
       "  0.4754639175257732,\n",
       "  0.49954237288135594,\n",
       "  0.5041114982578397,\n",
       "  0.5458221476510067,\n",
       "  0.49441275167785226,\n",
       "  0.4935472972972973,\n",
       "  0.5127684563758389,\n",
       "  0.4982550335570469,\n",
       "  0.4985034013605442,\n",
       "  0.5438344594594594,\n",
       "  0.5174319727891158,\n",
       "  0.5182881355932203,\n",
       "  0.5358833333333334,\n",
       "  0.5320890410958904,\n",
       "  0.5047157190635453,\n",
       "  0.49070234113712363,\n",
       "  0.493902027027027,\n",
       "  0.533489932885906,\n",
       "  0.47265100671140936,\n",
       "  0.4910590277777778,\n",
       "  0.4882107023411372,\n",
       "  0.530486577181208,\n",
       "  0.4883904109589042,\n",
       "  0.4888166666666667,\n",
       "  0.5738833333333334,\n",
       "  0.5039093959731543,\n",
       "  0.4776262626262627,\n",
       "  0.5259427609427609,\n",
       "  0.5018074324324325,\n",
       "  0.5179461279461279,\n",
       "  0.49135135135135133,\n",
       "  0.5141047297297298,\n",
       "  0.46872483221476513,\n",
       "  0.47900000000000004,\n",
       "  0.5550505050505051,\n",
       "  0.5087919463087248,\n",
       "  0.5046440677966101,\n",
       "  0.5099665551839465,\n",
       "  0.5334060402684563,\n",
       "  0.47332770270270275,\n",
       "  0.49343537414965993,\n",
       "  0.5223809523809523,\n",
       "  0.4565371621621621,\n",
       "  0.48866554054054057,\n",
       "  0.48416387959866214,\n",
       "  0.492751677852349,\n",
       "  0.5076006711409395,\n",
       "  0.5259228187919464,\n",
       "  0.4734782608695652,\n",
       "  0.4394237288135593,\n",
       "  0.4636195286195286,\n",
       "  0.5435762711864407,\n",
       "  0.5235521885521885,\n",
       "  0.5020805369127517,\n",
       "  0.4851003344481605,\n",
       "  0.5202516778523489,\n",
       "  0.505721476510067,\n",
       "  0.44405,\n",
       "  0.4611833333333333,\n",
       "  0.5255218855218856,\n",
       "  0.55061872909699,\n",
       "  0.4754713804713804,\n",
       "  0.5202033898305084,\n",
       "  0.4275342465753425,\n",
       "  0.5241414141414142,\n",
       "  0.5043856655290103,\n",
       "  0.5060234899328859,\n",
       "  0.526304347826087,\n",
       "  0.5221959459459459,\n",
       "  0.49696308724832217,\n",
       "  0.563255033557047,\n",
       "  0.5005369127516779,\n",
       "  0.497,\n",
       "  0.4874576271186441,\n",
       "  0.49920875420875416,\n",
       "  0.5063175675675676,\n",
       "  0.48488215488215486,\n",
       "  0.5053833333333334,\n",
       "  0.5181081081081081,\n",
       "  0.444391891891892,\n",
       "  0.4953344481605352,\n",
       "  0.4541973244147157,\n",
       "  0.5013636363636363,\n",
       "  0.5213804713804715,\n",
       "  0.4725919732441471,\n",
       "  0.5137878787878788,\n",
       "  0.5060437710437711,\n",
       "  0.5282770270270271,\n",
       "  0.5147315436241611,\n",
       "  0.4753885135135135,\n",
       "  0.48693979933110365,\n",
       "  0.47848333333333337,\n",
       "  0.5189297658862877,\n",
       "  0.5509731543624161,\n",
       "  0.510945945945946,\n",
       "  0.5001677852348992,\n",
       "  0.5128355704697987,\n",
       "  0.49712585034013607,\n",
       "  0.47035353535353536,\n",
       "  0.4696075085324232,\n",
       "  0.49290268456375835,\n",
       "  0.49234113712374583,\n",
       "  0.5020469798657718,\n",
       "  0.5077591973244147,\n",
       "  0.44394295302013426,\n",
       "  0.517675585284281,\n",
       "  0.4885910652920962,\n",
       "  0.4981879194630874,\n",
       "  0.5120707070707071,\n",
       "  0.5069966442953019,\n",
       "  0.49908333333333343,\n",
       "  0.5248666666666666,\n",
       "  0.5077852348993288,\n",
       "  0.48671666666666663,\n",
       "  0.5353678929765886,\n",
       "  0.46581034482758615,\n",
       "  0.437751677852349,\n",
       "  0.4601510067114094,\n",
       "  0.5206060606060606,\n",
       "  0.541915254237288,\n",
       "  0.4992307692307692,\n",
       "  0.5323825503355705,\n",
       "  0.48600334448160537,\n",
       "  0.5448615916955016,\n",
       "  0.4884166666666666,\n",
       "  0.45988333333333337,\n",
       "  0.5023076923076923,\n",
       "  0.48611486486486494,\n",
       "  0.4924414715719064,\n",
       "  0.48511666666666675,\n",
       "  0.47739795918367345,\n",
       "  0.4812626262626263,\n",
       "  0.49537037037037035,\n",
       "  0.5024496644295302,\n",
       "  0.5182550335570469,\n",
       "  0.5007666666666667,\n",
       "  0.5206711409395972,\n",
       "  0.5442542372881357,\n",
       "  0.55498322147651,\n",
       "  0.46892255892255885,\n",
       "  0.5190172413793103,\n",
       "  0.5147887323943662,\n",
       "  0.49442567567567564,\n",
       "  0.5055084745762712,\n",
       "  0.532020202020202,\n",
       "  0.5273825503355706,\n",
       "  0.49590604026845636,\n",
       "  0.5089518900343644,\n",
       "  0.5373711340206185,\n",
       "  0.4844557823129252,\n",
       "  0.47635,\n",
       "  0.4936166666666667,\n",
       "  0.4726086956521739,\n",
       "  0.4768729096989967,\n",
       "  0.46036789297658864,\n",
       "  0.5262962962962963,\n",
       "  0.46870370370370373,\n",
       "  0.4961111111111111,\n",
       "  0.5220945945945946,\n",
       "  0.4367391304347825,\n",
       "  0.46218430034129687,\n",
       "  0.5374833333333333,\n",
       "  0.4772297297297297,\n",
       "  0.462742474916388,\n",
       "  0.46520066889632106,\n",
       "  0.47291946308724836,\n",
       "  0.5103678929765886,\n",
       "  0.5053754266211604,\n",
       "  0.5444648829431439,\n",
       "  0.5210847457627119,\n",
       "  0.5326174496644295,\n",
       "  0.5131925675675676,\n",
       "  0.5150838926174497,\n",
       "  0.45162162162162167,\n",
       "  0.49336734693877543,\n",
       "  0.5255749128919861,\n",
       "  0.5224080267558529,\n",
       "  0.468125,\n",
       "  0.4673469387755102,\n",
       "  0.49898333333333333,\n",
       "  0.4936241610738255,\n",
       "  0.533695652173913,\n",
       "  0.5043311036789299,\n",
       "  0.5249496644295302,\n",
       "  0.44310402684563754,\n",
       "  0.5202972027972028,\n",
       "  0.4927104377104377,\n",
       "  0.49662068965517236,\n",
       "  0.536513605442177,\n",
       "  0.5211694915254237,\n",
       "  0.4838294314381272,\n",
       "  0.5031122448979592,\n",
       "  0.47904682274247495,\n",
       "  0.5169360269360269,\n",
       "  0.4974155405405405,\n",
       "  0.5282323232323232,\n",
       "  0.5019397993311037,\n",
       "  0.5554833333333332,\n",
       "  0.4799662162162162,\n",
       "  0.5137080536912751,\n",
       "  0.49344481605351176,\n",
       "  0.519608843537415,\n",
       "  0.5068855218855219,\n",
       "  0.5578571428571428,\n",
       "  0.5046735395189004,\n",
       "  0.4740530303030303,\n",
       "  0.5311279461279461,\n",
       "  0.5387458193979934,\n",
       "  0.4751683501683502,\n",
       "  0.5084237288135594,\n",
       "  0.46744863013698634,\n",
       "  0.47653198653198653,\n",
       "  0.4371165644171779,\n",
       "  0.4638795986622073,\n",
       "  0.46520134228187926,\n",
       "  0.453003355704698,\n",
       "  0.4753666666666666,\n",
       "  0.4955536912751678,\n",
       "  0.5354194630872483,\n",
       "  0.4967736486486487,\n",
       "  0.5182166666666667,\n",
       "  0.5181896551724139,\n",
       "  0.49073578595317724,\n",
       "  0.5031543624161073,\n",
       "  0.5483275862068965,\n",
       "  0.4995068027210884,\n",
       "  0.4602702702702703,\n",
       "  0.5174414715719063,\n",
       "  0.49323333333333336,\n",
       "  0.48314381270903,\n",
       "  0.5536744966442954,\n",
       "  0.5562626262626262,\n",
       "  0.511438127090301,\n",
       "  0.5341186440677966,\n",
       "  0.5168305084745762,\n",
       "  0.4772073578595318,\n",
       "  0.46803691275167786,\n",
       "  0.495204081632653,\n",
       "  0.5078595317725754,\n",
       "  0.47552188552188557,\n",
       "  0.5084448160535117,\n",
       "  0.5103833333333334,\n",
       "  0.5022727272727273,\n",
       "  0.5093456375838926,\n",
       "  0.5260269360269361,\n",
       "  0.495551839464883,\n",
       "  0.49566889632107025,\n",
       "  0.4940068493150685,\n",
       "  0.48608695652173917,\n",
       "  0.5257597173144877,\n",
       "  0.5926966292134831,\n",
       "  0.5234511784511785,\n",
       "  0.5228282828282829,\n",
       "  0.5230369127516777,\n",
       "  0.48944055944055953,\n",
       "  0.5136824324324325,\n",
       "  0.4750675675675676,\n",
       "  0.5194387755102041,\n",
       "  0.4881481481481482,\n",
       "  0.45538851351351356,\n",
       "  0.47146666666666665,\n",
       "  0.5055666666666667,\n",
       "  0.5654948805460751,\n",
       "  0.44749163879598663,\n",
       "  0.4805351170568562,\n",
       "  0.47978040540540534,\n",
       "  0.5071404682274248,\n",
       "  0.5146452702702703,\n",
       "  0.5254682274247492,\n",
       "  0.48224662162162163,\n",
       "  0.47351590106007063,\n",
       "  0.5245302013422819,\n",
       "  0.49415000000000003,\n",
       "  0.507053872053872,\n",
       "  0.5431879194630873,\n",
       "  0.5380936454849498,\n",
       "  0.5175585284280937,\n",
       "  0.47451178451178455,\n",
       "  0.5000844594594595,\n",
       "  0.49703389830508476,\n",
       "  0.4882881355932204,\n",
       "  0.4627627118644067,\n",
       "  0.44897993311036793,\n",
       "  0.5062333333333333,\n",
       "  0.5127118644067796,\n",
       "  0.5179194630872483,\n",
       "  0.4658448275862069,\n",
       "  0.49501672240802674,\n",
       "  0.49691525423728816,\n",
       "  0.5008904109589041,\n",
       "  0.5215277777777777,\n",
       "  0.5464141414141414,\n",
       "  0.5389597315436242,\n",
       "  0.5158952702702703,\n",
       "  0.5205351170568562,\n",
       "  0.49615771812080545,\n",
       "  0.5014093959731544,\n",
       "  0.4739597315436242,\n",
       "  0.5544576271186441,\n",
       "  0.5016250000000001,\n",
       "  0.5205183946488294,\n",
       "  0.5072053872053872,\n",
       "  0.5261784511784511,\n",
       "  0.4632372881355931,\n",
       "  0.4754181184668989,\n",
       "  0.46397993311036784,\n",
       "  0.5406428571428571,\n",
       "  0.4921694915254237,\n",
       "  0.513013468013468,\n",
       "  0.5122743055555556,\n",
       "  0.4956802721088435,\n",
       "  0.532347972972973,\n",
       "  0.49868686868686873,\n",
       "  0.49162207357859533,\n",
       "  0.5106375838926174,\n",
       "  0.45028716216216214,\n",
       "  0.5370805369127517,\n",
       "  0.47430976430976435,\n",
       "  0.517871972318339,\n",
       "  0.5130201342281879,\n",
       "  0.47769230769230764,\n",
       "  0.4867953020134228,\n",
       "  0.4879591836734694,\n",
       "  0.4943412162162162,\n",
       "  0.5086610169491524,\n",
       "  0.5013833333333333,\n",
       "  0.43209183673469387,\n",
       "  0.5229322033898305,\n",
       "  0.48452861952861953,\n",
       "  0.4782,\n",
       "  0.47866438356164376,\n",
       "  0.5090924657534246,\n",
       "  0.49439597315436234,\n",
       "  0.4788461538461538,\n",
       "  0.5026677852348994,\n",
       "  0.5216440677966101,\n",
       "  0.5158417508417508,\n",
       "  0.4734006734006735,\n",
       "  0.5341385135135135,\n",
       "  0.5281040268456376,\n",
       "  0.48404255319148937,\n",
       "  0.504124513618677,\n",
       "  0.5199163879598662,\n",
       "  0.4962794612794612,\n",
       "  0.4970034246575342,\n",
       "  0.5453355704697986,\n",
       "  0.4584915254237288,\n",
       "  0.49145484949832774,\n",
       "  0.5055,\n",
       "  0.5110269360269362,\n",
       "  0.49779264214046826,\n",
       "  0.45924749163879597,\n",
       "  0.5552525252525252,\n",
       "  0.5107823129251701,\n",
       "  0.4926271186440678,\n",
       "  0.5319557823129251,\n",
       "  0.4981925675675676,\n",
       "  0.5408833333333333,\n",
       "  0.49021739130434777,\n",
       "  0.5333779264214047,\n",
       "  0.48696917808219176,\n",
       "  0.4902020202020202,\n",
       "  0.5534731543624161,\n",
       "  0.4981605351170568,\n",
       "  0.4706292517006803,\n",
       "  0.4829,\n",
       "  0.49503367003367005,\n",
       "  0.5188294314381271,\n",
       "  0.5055166666666666,\n",
       "  0.48376254180602,\n",
       "  0.56075,\n",
       "  0.49559121621621627,\n",
       "  0.5263255033557048,\n",
       "  0.5120903010033444,\n",
       "  0.5120973154362417,\n",
       "  0.5180808080808081,\n",
       "  0.4860068259385665,\n",
       "  0.5151672240802675,\n",
       "  0.5158724832214765,\n",
       "  0.507929292929293,\n",
       "  0.5354778156996587,\n",
       "  0.513494983277592,\n",
       "  0.5105743243243243,\n",
       "  0.47998322147650996,\n",
       "  0.5080033557046979,\n",
       "  0.4941267123287671,\n",
       "  0.49793624161073824,\n",
       "  0.5016666666666667,\n",
       "  0.4907266435986159,\n",
       "  0.4870833333333333,\n",
       "  0.5150333333333333,\n",
       "  0.4963666666666667,\n",
       "  0.48810000000000003,\n",
       "  0.4948141891891892,\n",
       "  0.510066889632107,\n",
       "  0.5346308724832215,\n",
       "  0.4901672240802676,\n",
       "  0.4765771812080537,\n",
       "  0.5136241610738255,\n",
       "  0.4876643598615917,\n",
       "  0.547687074829932,\n",
       "  0.47011666666666674,\n",
       "  0.4552551020408163,\n",
       "  0.514040404040404,\n",
       "  0.4780201342281879,\n",
       "  0.5121812080536913,\n",
       "  0.47220486111111115,\n",
       "  0.45324232081911264,\n",
       "  0.5003859060402684,\n",
       "  0.49582770270270266,\n",
       "  0.48850671140939594,\n",
       "  0.48760067114093963,\n",
       "  0.5447138047138047,\n",
       "  0.5346812080536913,\n",
       "  0.509220338983051,\n",
       "  0.4920234113712375,\n",
       "  0.4599152542372882,\n",
       "  0.4614548494983278,\n",
       "  0.5505351170568561,\n",
       "  0.52061872909699,\n",
       "  0.44194256756756756,\n",
       "  0.4883053691275168,\n",
       "  0.5318896321070234,\n",
       "  0.48045302013422825,\n",
       "  0.4994276094276094,\n",
       "  0.45283783783783793,\n",
       "  0.49092592592592593,\n",
       "  0.4843771043771044,\n",
       "  0.48044368600682585,\n",
       "  0.45400337837837834,\n",
       "  0.5023129251700681,\n",
       "  0.4983333333333333,\n",
       "  0.46499999999999997,\n",
       "  0.47524054982817876,\n",
       "  0.5018729096989966,\n",
       "  0.5187751677852349,\n",
       "  0.5295805369127516,\n",
       "  0.4906313993174061,\n",
       "  0.48145973154362415,\n",
       "  0.46347972972972973,\n",
       "  0.5268027210884354,\n",
       "  0.46611111111111114,\n",
       "  0.4628355704697987,\n",
       "  0.45688775510204077,\n",
       "  0.521258389261745,\n",
       "  0.48873720136518767,\n",
       "  0.4809548611111112,\n",
       "  0.49527027027027026,\n",
       "  0.5544368600682593,\n",
       "  0.4540166666666666,\n",
       "  0.5316889632107025,\n",
       "  0.51561872909699,\n",
       "  0.48545150501672235,\n",
       "  0.4846938775510204,\n",
       "  0.5287331081081081,\n",
       "  0.48792642140468234,\n",
       "  0.4372297297297298,\n",
       "  0.5040468227424749,\n",
       "  0.550484949832776,\n",
       "  0.4794897959183673,\n",
       "  0.4896632996632997,\n",
       "  0.4876520270270269,\n",
       "  0.5138120567375886,\n",
       "  0.5556711409395974,\n",
       "  0.5555743243243243,\n",
       "  0.4765771812080538,\n",
       "  0.4800668896321071,\n",
       "  0.5337842465753425,\n",
       "  0.5054377104377105,\n",
       "  0.49235000000000007,\n",
       "  0.4828595890410959,\n",
       "  0.5287037037037037,\n",
       "  0.4611036789297659,\n",
       "  0.4676599326599327,\n",
       "  0.5039632107023412,\n",
       "  0.49310000000000004,\n",
       "  0.4440268456375839,\n",
       "  0.4861577181208054,\n",
       "  0.4931249999999999,\n",
       "  0.5061111111111112,\n",
       "  0.5170903010033444,\n",
       "  0.5258666666666666,\n",
       "  0.43732441471571903,\n",
       "  0.5091471571906354,\n",
       "  0.4788127090301003,\n",
       "  0.5157190635451505,\n",
       "  0.5133053691275169,\n",
       "  0.46930272108843524,\n",
       "  0.4937900355871886,\n",
       "  0.4918013468013468,\n",
       "  0.5224242424242425,\n",
       "  0.5283892617449664,\n",
       "  0.4955912162162162,\n",
       "  0.514010067114094,\n",
       "  0.49645484949832774,\n",
       "  0.4808983050847458,\n",
       "  0.4985284280936455,\n",
       "  0.4903333333333334,\n",
       "  0.5461912751677852,\n",
       "  0.5121308724832215,\n",
       "  0.5379054054054054,\n",
       "  0.5282491582491582,\n",
       "  0.47132550335570467,\n",
       "  0.4973776223776224,\n",
       "  0.5405183946488294,\n",
       "  0.47254999999999997,\n",
       "  0.5480612244897959,\n",
       "  0.5337792642140468,\n",
       "  0.5168181818181818,\n",
       "  0.520050505050505,\n",
       "  0.4663527397260274,\n",
       "  0.48961538461538456,\n",
       "  0.5105862068965517,\n",
       "  0.4655351170568562,\n",
       "  0.45909999999999995,\n",
       "  0.5044557823129252,\n",
       "  0.508728813559322,\n",
       "  0.49508474576271183,\n",
       "  0.5326858108108109,\n",
       "  0.46745819397993316,\n",
       "  0.49149152542372887,\n",
       "  0.5082828282828282,\n",
       "  0.5017845117845118,\n",
       "  0.4994087837837837,\n",
       "  0.49619863013698623,\n",
       "  0.4964814814814815,\n",
       "  0.5213986013986014,\n",
       "  0.5468729096989967,\n",
       "  0.4788644067796611,\n",
       "  0.5024402730375426,\n",
       "  0.5070068027210884,\n",
       "  0.5093771043771045,\n",
       "  0.5065268456375839,\n",
       "  0.5171428571428571,\n",
       "  0.4790239726027397,\n",
       "  0.5260810810810811,\n",
       "  0.4847324414715719,\n",
       "  0.49176271186440673,\n",
       "  0.5208754208754208,\n",
       "  0.4753833333333334,\n",
       "  0.4922448979591837,\n",
       "  0.5360606060606061,\n",
       "  0.47707482993197275,\n",
       "  0.45354515050167227,\n",
       "  0.5006856187290969,\n",
       "  0.5167391304347826,\n",
       "  0.4945238095238095,\n",
       "  0.5529560810810811,\n",
       "  0.5024916387959867,\n",
       "  0.45286912751677855,\n",
       "  0.48145,\n",
       "  0.519261744966443,\n",
       "  0.49700680272108844,\n",
       "  0.4956688963210702,\n",
       "  0.5179391891891892,\n",
       "  0.49620401337792636,\n",
       "  0.5150519031141868,\n",
       "  0.5223825503355706,\n",
       "  0.5231208053691275,\n",
       "  0.4878833333333334,\n",
       "  0.4966442953020134,\n",
       "  0.5433501683501684,\n",
       "  0.5207166666666667,\n",
       "  0.5120875420875421,\n",
       "  0.5236317567567568,\n",
       "  0.46768333333333334,\n",
       "  0.49776666666666664,\n",
       "  0.5158862876254181,\n",
       "  0.5149662162162162,\n",
       "  0.4866382252559726,\n",
       "  0.4671812080536913,\n",
       "  0.5033050847457627,\n",
       "  0.4793197278911565,\n",
       "  0.5327319587628866,\n",
       "  0.45461148648648647,\n",
       "  0.49704013377926426,\n",
       "  0.4977891156462585,\n",
       "  0.5254965753424657,\n",
       "  0.48501677852349,\n",
       "  0.49914429530201343,\n",
       "  0.4273897058823529,\n",
       "  0.5193311036789299,\n",
       "  0.505,\n",
       "  0.4926767676767677,\n",
       "  0.49801003344481604,\n",
       "  0.4646779661016949,\n",
       "  0.4980067567567568,\n",
       "  0.4563833333333334,\n",
       "  0.5056499999999999,\n",
       "  0.45644781144781144,\n",
       "  0.4816101694915254,\n",
       "  0.516254295532646,\n",
       "  0.49792642140468224,\n",
       "  0.5356418918918919,\n",
       "  0.49703389830508476,\n",
       "  0.4955912162162162,\n",
       "  0.5053166666666667,\n",
       "  0.5068983050847458,\n",
       "  0.4921575342465754,\n",
       "  0.4939539007092199,\n",
       "  0.46382562277580064,\n",
       "  0.4158965517241379,\n",
       "  0.5041107382550336,\n",
       "  0.513494983277592,\n",
       "  0.5208703071672355,\n",
       "  0.5140301003344482,\n",
       "  0.5756610169491525,\n",
       "  0.5333728813559322,\n",
       "  0.47862711864406776,\n",
       "  0.5247651006711409,\n",
       "  0.5383946488294314,\n",
       "  0.4994949494949495,\n",
       "  0.47777966101694913,\n",
       "  0.42675,\n",
       "  0.4982608695652175,\n",
       "  0.48753355704697987,\n",
       "  0.5115593220338983,\n",
       "  0.5201525423728813,\n",
       "  0.49631756756756756,\n",
       "  0.47962457337883957,\n",
       "  0.5204639175257733,\n",
       "  0.49506849315068496,\n",
       "  0.47942760942760937,\n",
       "  0.44,\n",
       "  0.5241836734693877,\n",
       "  0.49308873720136526,\n",
       "  0.5218771331058021,\n",
       "  0.4539864864864865,\n",
       "  0.5110869565217391,\n",
       "  0.5038666666666668,\n",
       "  0.5307317073170732,\n",
       "  0.47023809523809523,\n",
       "  0.5208904109589042,\n",
       "  0.4856354515050167,\n",
       "  0.4503898305084746,\n",
       "  0.5039765100671141,\n",
       "  0.490418118466899,\n",
       "  0.46808333333333335,\n",
       "  0.5002650176678446,\n",
       "  0.5002120141342756,\n",
       "  0.5095762711864407,\n",
       "  0.5188963210702341,\n",
       "  0.47744147157190636,\n",
       "  0.5081418918918917,\n",
       "  0.5206187290969899,\n",
       "  0.5359030100334449,\n",
       "  0.5592736486486487,\n",
       "  0.5088851351351351,\n",
       "  0.4804915254237288,\n",
       "  0.5049326599326599,\n",
       "  0.48692041522491347,\n",
       "  0.48675,\n",
       "  0.4572408026755853,\n",
       "  0.4832142857142857,\n",
       "  0.5230344827586207,\n",
       "  0.48481481481481487,\n",
       "  0.48788135593220344,\n",
       "  0.5057575757575757,\n",
       "  0.4912374581939799,\n",
       "  0.5345986622073579,\n",
       "  0.5299161073825503,\n",
       "  0.4962080536912752,\n",
       "  0.519,\n",
       "  0.4949161073825504,\n",
       "  0.5281375838926174,\n",
       "  0.5154194630872483,\n",
       "  0.5013210702341137,\n",
       "  0.5284343434343434,\n",
       "  0.48616161616161613,\n",
       "  0.5308417508417509,\n",
       "  0.5467235494880546,\n",
       "  0.52938127090301,\n",
       "  0.5224247491638796,\n",
       "  0.5250175438596492,\n",
       "  0.5100844594594595,\n",
       "  0.45744525547445253,\n",
       "  0.48142140468227423,\n",
       "  0.49738255033557044,\n",
       "  0.4727591973244148,\n",
       "  0.5143856655290103,\n",
       "  0.5377871621621622,\n",
       "  0.46627090301003343,\n",
       "  0.4909760273972603,\n",
       "  0.5097993311036789,\n",
       "  0.5371428571428571,\n",
       "  0.5177162629757786,\n",
       "  0.5119661016949153,\n",
       "  0.492652027027027,\n",
       "  0.47279999999999994,\n",
       "  0.5141722972972973,\n",
       "  0.49882550335570464,\n",
       "  0.5153344481605352,\n",
       "  0.4980471380471381,\n",
       "  0.49607382550335566,\n",
       "  0.4561486486486487,\n",
       "  0.49038590604026844,\n",
       "  0.49583617747440273,\n",
       "  0.5084966216216216,\n",
       "  0.4533501683501683,\n",
       "  0.45207357859531777,\n",
       "  0.5150505050505051,\n",
       "  0.4911694915254238,\n",
       "  0.5139297658862877,\n",
       "  0.4657517482517482,\n",
       "  0.466722972972973,\n",
       "  0.5051677852348994,\n",
       "  0.4731,\n",
       "  0.49121621621621625,\n",
       "  0.5116161616161615,\n",
       "  0.5197474747474747,\n",
       "  0.5095819397993311,\n",
       "  0.47538461538461535,\n",
       "  0.5280471380471381,\n",
       "  0.5022053872053871,\n",
       "  0.5061186440677966,\n",
       "  0.5374493243243242,\n",
       "  0.49171666666666664,\n",
       "  0.4795955882352941,\n",
       "  0.48731543624161083,\n",
       "  0.47020066889632117,\n",
       "  0.5150847457627118,\n",
       "  0.46826599326599333,\n",
       "  0.46885,\n",
       "  0.5163175675675676,\n",
       "  0.4848644067796611,\n",
       "  0.51725,\n",
       "  0.4882885906040268,\n",
       "  0.49859531772575244,\n",
       "  0.5064878892733564,\n",
       "  0.5302413793103449,\n",
       "  0.5290969899665552,\n",
       "  0.5063879598662208,\n",
       "  0.5362457912457912,\n",
       "  0.4745205479452055,\n",
       "  0.5263087248322147,\n",
       "  0.5522542372881356,\n",
       "  0.47606779661016946,\n",
       "  0.5123287671232878,\n",
       "  0.47726510067114103,\n",
       "  0.47725000000000006,\n",
       "  0.524949152542373,\n",
       "  0.45072390572390575,\n",
       "  0.5340924657534246,\n",
       "  0.4870666666666667,\n",
       "  0.49290268456375835,\n",
       "  0.5112413793103447,\n",
       "  0.5207094594594595,\n",
       "  0.46209183673469384,\n",
       "  0.5121979865771812,\n",
       "  0.5131543624161073,\n",
       "  0.49266323024054987,\n",
       "  0.5456462585034014,\n",
       "  0.48080808080808085,\n",
       "  0.49220735785953185,\n",
       "  0.4856397306397307,\n",
       "  0.48423986486486487,\n",
       "  0.48784280936454855,\n",
       "  0.520726643598616,\n",
       "  0.5160169491525425,\n",
       "  0.48255,\n",
       "  0.511438127090301,\n",
       "  0.5047979797979798,\n",
       "  0.4949319727891156,\n",
       "  0.47162711864406776,\n",
       "  0.5155460750853242,\n",
       "  0.4694127516778523,\n",
       "  0.564234693877551,\n",
       "  0.5409290540540541,\n",
       "  0.4995719178082192,\n",
       "  0.46542808219178083,\n",
       "  0.5295422535211267,\n",
       "  0.5226666666666667,\n",
       "  0.5117567567567568,\n",
       "  0.49696969696969695,\n",
       "  0.5242439862542956,\n",
       "  0.49915000000000004,\n",
       "  0.54615,\n",
       "  0.5038851351351351,\n",
       "  0.48304713804713817,\n",
       "  0.5066216216216216,\n",
       "  0.5255892255892256,\n",
       "  0.5160508474576272,\n",
       "  0.5171602787456446,\n",
       "  0.4726845637583892,\n",
       "  0.49140939597315436,\n",
       "  0.5034931506849315,\n",
       "  0.47746527777777786,\n",
       "  0.5041077441077441,\n",
       "  0.5178956228956229,\n",
       "  0.5297651006711409,\n",
       "  0.5292736486486487,\n",
       "  0.4828859060402684,\n",
       "  0.5046101694915255,\n",
       "  0.532320819112628,\n",
       "  0.5007407407407407,\n",
       "  0.48721666666666663,\n",
       "  0.508013698630137,\n",
       "  0.4976440677966102,\n",
       "  0.46294612794612794,\n",
       "  0.4701418439716312,\n",
       "  0.4969256756756757,\n",
       "  0.5214166666666668,\n",
       "  0.49326666666666663,\n",
       "  0.5439130434782609,\n",
       "  0.46895973154362414,\n",
       "  0.46071666666666666,\n",
       "  0.48734589041095894,\n",
       "  0.5109866220735785,\n",
       "  0.49819112627986345,\n",
       "  0.5586073825503356,\n",
       "  0.5038851351351351,\n",
       "  0.5066333333333334,\n",
       "  0.45697952218430027,\n",
       "  0.43330536912751677,\n",
       "  0.4402033898305085,\n",
       "  0.5014285714285714,\n",
       "  0.5135785953177258,\n",
       "  0.5142929292929294,\n",
       "  0.49037037037037035,\n",
       "  0.490268456375839,\n",
       "  0.5471308724832216,\n",
       "  0.5056101694915254,\n",
       "  0.5027946127946128,\n",
       "  0.45212068965517244,\n",
       "  0.47290268456375845,\n",
       "  0.5505574324324324,\n",
       "  0.50155,\n",
       "  0.4639020270270271,\n",
       "  0.5115166666666666,\n",
       "  0.47454873646209395,\n",
       "  0.4965529010238908,\n",
       "  0.4587966101694915,\n",
       "  0.4821043771043771,\n",
       "  0.45563333333333333,\n",
       "  0.5149498327759198,\n",
       "  0.49917508417508416,\n",
       "  0.5030405405405405,\n",
       "  0.49635593220338986,\n",
       "  0.4904054054054054,\n",
       "  0.5172184300341297,\n",
       "  0.5238666666666667,\n",
       "  0.48515050167224083,\n",
       "  0.5297138047138047,\n",
       "  0.4695117845117845,\n",
       "  0.48329391891891893,\n",
       "  0.5135858585858587,\n",
       "  0.5175250836120402,\n",
       "  0.5257093425605536,\n",
       "  0.48020134228187916,\n",
       "  0.48067340067340075,\n",
       "  0.4697324414715719,\n",
       "  0.5636933797909407,\n",
       "  0.4988344594594595,\n",
       "  0.4595469798657718,\n",
       "  0.526541095890411,\n",
       "  0.5094047619047618,\n",
       "  0.5327272727272727,\n",
       "  0.5119897959183672,\n",
       "  0.5102210884353742,\n",
       "  0.5310906040268456,\n",
       "  0.5104898648648648,\n",
       "  0.44072164948453607,\n",
       "  0.4645084745762713,\n",
       "  0.4676520270270271,\n",
       "  0.5365816326530612,\n",
       "  0.4442491467576792,\n",
       "  0.5365333333333333,\n",
       "  0.5410641891891892,\n",
       "  0.5275671140939597,\n",
       "  ...]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_trainer._eval_save_prefix = OUTPUT_PREFIX + \"-test-pos-biased\"\n",
    "model_trainer._evaluate_partial(test_dataset_pos_biased)\n",
    "\n",
    "model_trainer._eval_save_prefix = OUTPUT_PREFIX +  \"-test-neg-biased\"\n",
    "model_trainer._evaluate_partial(test_dataset_neg_biased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unbiased Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2296/2296 [00:00<00:00, 3048.96it/s]\n",
      "100%|| 2296/2296 [00:01<00:00, 1701.89it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AUC': [0.5327777777777778,\n",
       "  0.5640000000000001,\n",
       "  0.5308333333333333,\n",
       "  0.5066666666666666,\n",
       "  0.4288888888888888,\n",
       "  0.44214285714285717,\n",
       "  0.32749999999999996,\n",
       "  0.6371428571428572,\n",
       "  0.35,\n",
       "  0.48055555555555557,\n",
       "  0.44375,\n",
       "  0.4535714285714286,\n",
       "  0.5471428571428572,\n",
       "  0.50125,\n",
       "  0.48,\n",
       "  0.5177777777777777,\n",
       "  0.5094444444444445,\n",
       "  0.536875,\n",
       "  0.5285714285714286,\n",
       "  0.5299999999999999,\n",
       "  0.460625,\n",
       "  0.47,\n",
       "  0.4027777777777778,\n",
       "  0.49124999999999996,\n",
       "  0.67875,\n",
       "  0.30125,\n",
       "  0.64,\n",
       "  0.541875,\n",
       "  0.3725,\n",
       "  0.625,\n",
       "  0.2735714285714285,\n",
       "  0.5538888888888889,\n",
       "  0.4783333333333333,\n",
       "  0.49222222222222217,\n",
       "  0.4511111111111111,\n",
       "  0.62,\n",
       "  0.608125,\n",
       "  0.31666666666666665,\n",
       "  0.5133333333333333,\n",
       "  0.3807142857142857,\n",
       "  0.5381250000000001,\n",
       "  0.424375,\n",
       "  0.580625,\n",
       "  0.603,\n",
       "  0.39222222222222225,\n",
       "  0.48125000000000007,\n",
       "  0.6128571428571429,\n",
       "  0.33499999999999996,\n",
       "  0.5572222222222222,\n",
       "  0.44999999999999996,\n",
       "  0.4677777777777778,\n",
       "  0.5609999999999999,\n",
       "  0.4491666666666667,\n",
       "  0.42500000000000004,\n",
       "  0.3888888888888889,\n",
       "  0.5107142857142858,\n",
       "  0.6331249999999999,\n",
       "  0.6875,\n",
       "  0.6044444444444443,\n",
       "  0.385,\n",
       "  0.5438888888888889,\n",
       "  0.7166666666666666,\n",
       "  0.4683333333333333,\n",
       "  0.5364285714285714,\n",
       "  0.4222222222222222,\n",
       "  0.71,\n",
       "  0.37777777777777777,\n",
       "  0.575,\n",
       "  0.4266666666666667,\n",
       "  0.48333333333333334,\n",
       "  0.5500000000000002,\n",
       "  0.48874999999999996,\n",
       "  0.435,\n",
       "  0.24555555555555555,\n",
       "  0.32199999999999995,\n",
       "  0.4083333333333334,\n",
       "  0.4333333333333333,\n",
       "  0.55,\n",
       "  0.5316666666666667,\n",
       "  0.5425,\n",
       "  0.474375,\n",
       "  0.5472222222222222,\n",
       "  0.3225,\n",
       "  0.409375,\n",
       "  0.5661111111111112,\n",
       "  0.5066666666666666,\n",
       "  0.40111111111111114,\n",
       "  0.41562499999999997,\n",
       "  0.6012500000000001,\n",
       "  0.47375,\n",
       "  0.536111111111111,\n",
       "  0.5366666666666666,\n",
       "  0.44,\n",
       "  0.44777777777777783,\n",
       "  0.6925,\n",
       "  0.4494444444444444,\n",
       "  0.4144444444444445,\n",
       "  0.675,\n",
       "  0.39111111111111113,\n",
       "  0.42428571428571427,\n",
       "  0.51125,\n",
       "  0.3955555555555556,\n",
       "  0.43555555555555553,\n",
       "  0.45499999999999996,\n",
       "  0.5794444444444444,\n",
       "  0.5850000000000001,\n",
       "  0.6383333333333333,\n",
       "  0.26125,\n",
       "  0.71875,\n",
       "  0.5435714285714286,\n",
       "  0.4066666666666667,\n",
       "  0.39888888888888885,\n",
       "  0.31444444444444447,\n",
       "  0.40499999999999997,\n",
       "  0.49687499999999996,\n",
       "  0.56625,\n",
       "  0.35624999999999996,\n",
       "  0.5944444444444446,\n",
       "  0.58125,\n",
       "  0.35714285714285715,\n",
       "  0.36111111111111116,\n",
       "  0.5338888888888889,\n",
       "  0.5022222222222221,\n",
       "  0.56125,\n",
       "  0.4972222222222222,\n",
       "  0.4772222222222222,\n",
       "  0.4142857142857143,\n",
       "  0.4527777777777778,\n",
       "  0.386875,\n",
       "  0.2785714285714286,\n",
       "  0.7311111111111112,\n",
       "  0.62125,\n",
       "  0.4127777777777778,\n",
       "  0.505,\n",
       "  0.47125,\n",
       "  0.3866666666666667,\n",
       "  0.46333333333333343,\n",
       "  0.5137499999999999,\n",
       "  0.4138888888888889,\n",
       "  0.5855555555555556,\n",
       "  0.5605555555555556,\n",
       "  0.5344444444444444,\n",
       "  0.44899999999999995,\n",
       "  0.5927777777777777,\n",
       "  0.5255555555555556,\n",
       "  0.37249999999999994,\n",
       "  0.5772222222222223,\n",
       "  0.4072222222222222,\n",
       "  0.330625,\n",
       "  0.35777777777777775,\n",
       "  0.6000000000000001,\n",
       "  0.4444444444444444,\n",
       "  0.445,\n",
       "  0.46687500000000004,\n",
       "  0.5594444444444444,\n",
       "  0.4835714285714286,\n",
       "  0.4055555555555556,\n",
       "  0.3744444444444444,\n",
       "  0.49124999999999996,\n",
       "  0.4072222222222222,\n",
       "  0.27055555555555555,\n",
       "  0.46249999999999997,\n",
       "  0.49888888888888894,\n",
       "  0.4072222222222222,\n",
       "  0.4305555555555556,\n",
       "  0.6043750000000001,\n",
       "  0.4614285714285714,\n",
       "  0.18500000000000003,\n",
       "  0.535,\n",
       "  0.341875,\n",
       "  0.4391666666666667,\n",
       "  0.46388888888888896,\n",
       "  0.42125,\n",
       "  0.3405555555555555,\n",
       "  0.6488888888888888,\n",
       "  0.4471428571428571,\n",
       "  0.37375,\n",
       "  0.5175,\n",
       "  0.49888888888888894,\n",
       "  0.5627777777777777,\n",
       "  0.44062500000000004,\n",
       "  0.5543750000000001,\n",
       "  0.5028571428571429,\n",
       "  0.6614285714285714,\n",
       "  0.4785714285714286,\n",
       "  0.4066666666666667,\n",
       "  0.5394444444444445,\n",
       "  0.54125,\n",
       "  0.45611111111111113,\n",
       "  0.6233333333333333,\n",
       "  0.5587500000000001,\n",
       "  0.40444444444444444,\n",
       "  0.356875,\n",
       "  0.4077777777777778,\n",
       "  0.36714285714285727,\n",
       "  0.43500000000000005,\n",
       "  0.4258333333333333,\n",
       "  0.46875,\n",
       "  0.37,\n",
       "  0.72,\n",
       "  0.4666666666666667,\n",
       "  0.481875,\n",
       "  0.4072222222222222,\n",
       "  0.6666666666666665,\n",
       "  0.5066666666666666,\n",
       "  0.4371428571428572,\n",
       "  0.24249999999999997,\n",
       "  0.4861111111111111,\n",
       "  0.51,\n",
       "  0.385,\n",
       "  0.4672222222222222,\n",
       "  0.5333333333333334,\n",
       "  0.41124999999999995,\n",
       "  0.5525,\n",
       "  0.5727777777777778,\n",
       "  0.491875,\n",
       "  0.41444444444444445,\n",
       "  0.4644444444444445,\n",
       "  0.5655555555555556,\n",
       "  0.48000000000000004,\n",
       "  0.405,\n",
       "  0.42444444444444446,\n",
       "  0.44999999999999996,\n",
       "  0.6411111111111112,\n",
       "  0.5266666666666666,\n",
       "  0.3983333333333333,\n",
       "  0.6483333333333334,\n",
       "  0.5327777777777778,\n",
       "  0.3764285714285714,\n",
       "  0.553125,\n",
       "  0.30944444444444447,\n",
       "  0.2961111111111111,\n",
       "  0.4438888888888889,\n",
       "  0.5411111111111111,\n",
       "  0.3566666666666667,\n",
       "  0.5950000000000001,\n",
       "  0.3211111111111112,\n",
       "  0.335,\n",
       "  0.54875,\n",
       "  0.5716666666666667,\n",
       "  0.41250000000000003,\n",
       "  0.3983333333333334,\n",
       "  0.8666666666666667,\n",
       "  0.4,\n",
       "  0.4772222222222222,\n",
       "  0.7078571428571429,\n",
       "  0.515,\n",
       "  0.31857142857142856,\n",
       "  0.675625,\n",
       "  0.518,\n",
       "  0.425,\n",
       "  0.5766666666666667,\n",
       "  0.583888888888889,\n",
       "  0.45333333333333337,\n",
       "  0.5822222222222222,\n",
       "  0.6322222222222222,\n",
       "  0.49222222222222217,\n",
       "  0.4233333333333334,\n",
       "  0.5028571428571429,\n",
       "  0.32916666666666666,\n",
       "  0.4,\n",
       "  0.5042857142857143,\n",
       "  0.44875,\n",
       "  0.4855555555555556,\n",
       "  0.49611111111111117,\n",
       "  0.5527777777777777,\n",
       "  0.44833333333333336,\n",
       "  0.4155555555555555,\n",
       "  0.5188888888888888,\n",
       "  0.42388888888888887,\n",
       "  0.26125,\n",
       "  0.3472222222222222,\n",
       "  0.3227777777777778,\n",
       "  0.4621428571428571,\n",
       "  0.46611111111111103,\n",
       "  0.41071428571428575,\n",
       "  0.6325000000000001,\n",
       "  0.6572222222222223,\n",
       "  0.48277777777777775,\n",
       "  0.6677777777777778,\n",
       "  0.650625,\n",
       "  0.39666666666666667,\n",
       "  0.48944444444444446,\n",
       "  0.47562499999999996,\n",
       "  0.3371428571428572,\n",
       "  0.44833333333333336,\n",
       "  0.515,\n",
       "  0.36874999999999997,\n",
       "  0.49,\n",
       "  0.37722222222222224,\n",
       "  0.403125,\n",
       "  0.4672222222222222,\n",
       "  0.44055555555555553,\n",
       "  0.3788888888888889,\n",
       "  0.5425,\n",
       "  0.4494444444444444,\n",
       "  0.359375,\n",
       "  0.6771428571428572,\n",
       "  0.6988888888888888,\n",
       "  0.649375,\n",
       "  0.4142857142857143,\n",
       "  0.49999999999999994,\n",
       "  0.4816666666666667,\n",
       "  0.7366666666666667,\n",
       "  0.45375,\n",
       "  0.5625,\n",
       "  0.42944444444444446,\n",
       "  0.5544444444444445,\n",
       "  0.46625000000000005,\n",
       "  0.38499999999999995,\n",
       "  0.5022222222222222,\n",
       "  0.6257142857142858,\n",
       "  0.58875,\n",
       "  0.561875,\n",
       "  0.33555555555555555,\n",
       "  0.31555555555555553,\n",
       "  0.39875000000000005,\n",
       "  0.6166666666666667,\n",
       "  0.5222222222222223,\n",
       "  0.39888888888888885,\n",
       "  0.589375,\n",
       "  0.385,\n",
       "  0.39166666666666666,\n",
       "  0.505,\n",
       "  0.535,\n",
       "  0.34833333333333333,\n",
       "  0.4311111111111111,\n",
       "  0.45562500000000006,\n",
       "  0.708125,\n",
       "  0.3411111111111111,\n",
       "  0.4011111111111111,\n",
       "  0.5466666666666666,\n",
       "  0.44999999999999996,\n",
       "  0.5855555555555555,\n",
       "  0.6622222222222223,\n",
       "  0.5114285714285715,\n",
       "  0.377,\n",
       "  0.556875,\n",
       "  0.4566666666666666,\n",
       "  0.38333333333333336,\n",
       "  0.6131249999999999,\n",
       "  0.58375,\n",
       "  0.4014285714285714,\n",
       "  0.6550000000000001,\n",
       "  0.33055555555555555,\n",
       "  0.4585714285714286,\n",
       "  0.3938888888888889,\n",
       "  0.548888888888889,\n",
       "  0.5499999999999999,\n",
       "  0.6044444444444445,\n",
       "  0.621875,\n",
       "  0.5816666666666666,\n",
       "  0.5341666666666667,\n",
       "  0.30666666666666664,\n",
       "  0.6016666666666668,\n",
       "  0.45625000000000004,\n",
       "  0.3983333333333334,\n",
       "  0.44055555555555553,\n",
       "  0.5288888888888889,\n",
       "  0.41062499999999996,\n",
       "  0.38625,\n",
       "  0.4972222222222222,\n",
       "  0.728,\n",
       "  0.3477777777777778,\n",
       "  0.6144444444444445,\n",
       "  0.4975,\n",
       "  0.32999999999999996,\n",
       "  0.6272222222222221,\n",
       "  0.4064285714285714,\n",
       "  0.445,\n",
       "  0.45611111111111113,\n",
       "  0.5549999999999999,\n",
       "  0.47111111111111115,\n",
       "  0.5544444444444445,\n",
       "  0.5078571428571429,\n",
       "  0.34624999999999995,\n",
       "  0.44388888888888883,\n",
       "  0.413,\n",
       "  0.30000000000000004,\n",
       "  0.48000000000000004,\n",
       "  0.604375,\n",
       "  0.48,\n",
       "  0.5961111111111111,\n",
       "  0.4844444444444444,\n",
       "  0.4822222222222222,\n",
       "  0.4658333333333333,\n",
       "  0.5055555555555556,\n",
       "  0.4955555555555555,\n",
       "  0.28,\n",
       "  0.705625,\n",
       "  0.3011111111111111,\n",
       "  0.5911111111111111,\n",
       "  0.516875,\n",
       "  0.5557142857142857,\n",
       "  0.465,\n",
       "  0.6611111111111111,\n",
       "  0.43437499999999996,\n",
       "  0.3711111111111111,\n",
       "  0.6166666666666667,\n",
       "  0.6472222222222223,\n",
       "  0.46928571428571425,\n",
       "  0.628,\n",
       "  0.43777777777777777,\n",
       "  0.3427777777777778,\n",
       "  0.365,\n",
       "  0.5466666666666666,\n",
       "  0.569375,\n",
       "  0.445625,\n",
       "  0.4799999999999999,\n",
       "  0.6481250000000001,\n",
       "  0.454,\n",
       "  0.369375,\n",
       "  0.4992857142857143,\n",
       "  0.5216666666666667,\n",
       "  0.5642857142857143,\n",
       "  0.3655555555555556,\n",
       "  0.5872222222222222,\n",
       "  0.4211111111111111,\n",
       "  0.5694444444444444,\n",
       "  0.45888888888888896,\n",
       "  0.5457142857142857,\n",
       "  0.6733333333333333,\n",
       "  0.54,\n",
       "  0.38055555555555554,\n",
       "  0.47611111111111115,\n",
       "  0.3992857142857143,\n",
       "  0.6033333333333333,\n",
       "  0.38666666666666666,\n",
       "  0.40444444444444444,\n",
       "  0.6407142857142858,\n",
       "  0.39,\n",
       "  0.4755555555555555,\n",
       "  0.45611111111111113,\n",
       "  0.5850000000000001,\n",
       "  0.48055555555555557,\n",
       "  0.3755555555555556,\n",
       "  0.5057142857142857,\n",
       "  0.5641666666666666,\n",
       "  0.370625,\n",
       "  0.545,\n",
       "  0.42500000000000004,\n",
       "  0.48928571428571427,\n",
       "  0.516875,\n",
       "  0.32416666666666666,\n",
       "  0.3458333333333334,\n",
       "  0.43937499999999996,\n",
       "  0.35083333333333333,\n",
       "  0.5264285714285714,\n",
       "  0.606875,\n",
       "  0.4677777777777778,\n",
       "  0.5025,\n",
       "  0.46625,\n",
       "  0.41055555555555556,\n",
       "  0.5549999999999999,\n",
       "  0.420625,\n",
       "  0.4444444444444444,\n",
       "  0.40125,\n",
       "  0.5033333333333334,\n",
       "  0.49812500000000004,\n",
       "  0.4816666666666667,\n",
       "  0.47062499999999996,\n",
       "  0.43999999999999995,\n",
       "  0.4666666666666667,\n",
       "  0.42888888888888893,\n",
       "  0.435,\n",
       "  0.4694444444444444,\n",
       "  0.4766666666666667,\n",
       "  0.6268750000000001,\n",
       "  0.47277777777777774,\n",
       "  0.43416666666666665,\n",
       "  0.5205555555555557,\n",
       "  0.4116666666666666,\n",
       "  0.515,\n",
       "  0.31777777777777777,\n",
       "  0.4883333333333334,\n",
       "  0.4714285714285715,\n",
       "  0.4412499999999999,\n",
       "  0.44937499999999997,\n",
       "  0.45444444444444443,\n",
       "  0.5238888888888888,\n",
       "  0.5349999999999999,\n",
       "  0.5188888888888888,\n",
       "  0.50625,\n",
       "  0.466875,\n",
       "  0.40187500000000004,\n",
       "  0.55875,\n",
       "  0.5255555555555556,\n",
       "  0.19555555555555554,\n",
       "  0.5472222222222222,\n",
       "  0.327,\n",
       "  0.13833333333333334,\n",
       "  0.71,\n",
       "  0.6408333333333333,\n",
       "  0.6111111111111112,\n",
       "  0.408125,\n",
       "  0.4642857142857143,\n",
       "  0.454375,\n",
       "  0.3461111111111111,\n",
       "  0.40375,\n",
       "  0.5444444444444444,\n",
       "  0.5811111111111111,\n",
       "  0.5125,\n",
       "  0.6625,\n",
       "  0.22166666666666668,\n",
       "  0.34388888888888886,\n",
       "  0.5888888888888889,\n",
       "  0.328125,\n",
       "  0.3838888888888889,\n",
       "  0.5185714285714286,\n",
       "  0.5077777777777778,\n",
       "  0.6581250000000001,\n",
       "  0.5144444444444445,\n",
       "  0.4275,\n",
       "  0.3471428571428571,\n",
       "  0.40555555555555556,\n",
       "  0.5094444444444444,\n",
       "  0.4722222222222221,\n",
       "  0.32055555555555554,\n",
       "  0.6549999999999999,\n",
       "  0.33687500000000004,\n",
       "  0.39,\n",
       "  0.5,\n",
       "  0.398125,\n",
       "  0.5016666666666667,\n",
       "  0.4327777777777777,\n",
       "  0.749375,\n",
       "  0.498125,\n",
       "  0.526875,\n",
       "  0.5144444444444445,\n",
       "  0.5294444444444445,\n",
       "  0.25666666666666665,\n",
       "  0.43,\n",
       "  0.5116666666666667,\n",
       "  0.4527777777777778,\n",
       "  0.65,\n",
       "  0.5233333333333332,\n",
       "  0.54,\n",
       "  0.3733333333333333,\n",
       "  0.5222222222222223,\n",
       "  0.533888888888889,\n",
       "  0.477,\n",
       "  0.39555555555555555,\n",
       "  0.3383333333333334,\n",
       "  0.39571428571428574,\n",
       "  0.5988888888888888,\n",
       "  0.4111111111111111,\n",
       "  0.325,\n",
       "  0.41000000000000003,\n",
       "  0.5983333333333333,\n",
       "  0.37785714285714284,\n",
       "  0.548125,\n",
       "  0.39785714285714285,\n",
       "  0.580625,\n",
       "  0.4799999999999999,\n",
       "  0.0033333333333333335,\n",
       "  0.588125,\n",
       "  0.31,\n",
       "  0.34285714285714286,\n",
       "  0.3733333333333333,\n",
       "  0.5022222222222222,\n",
       "  0.5399999999999999,\n",
       "  0.4855555555555556,\n",
       "  0.7177777777777777,\n",
       "  0.4383333333333333,\n",
       "  0.42500000000000004,\n",
       "  0.538888888888889,\n",
       "  0.4488888888888889,\n",
       "  0.45,\n",
       "  0.6364285714285713,\n",
       "  0.604,\n",
       "  0.28300000000000003,\n",
       "  0.565,\n",
       "  0.5633333333333335,\n",
       "  0.4694444444444444,\n",
       "  0.6435714285714286,\n",
       "  0.554375,\n",
       "  0.58375,\n",
       "  0.3766666666666667,\n",
       "  0.5950000000000001,\n",
       "  0.4166666666666667,\n",
       "  0.3683333333333334,\n",
       "  0.4422222222222223,\n",
       "  0.49562500000000004,\n",
       "  0.4305555555555556,\n",
       "  0.4775,\n",
       "  0.38222222222222224,\n",
       "  0.495,\n",
       "  0.47111111111111115,\n",
       "  0.5455555555555556,\n",
       "  0.5688888888888889,\n",
       "  0.4488888888888889,\n",
       "  0.5977777777777777,\n",
       "  0.5611111111111112,\n",
       "  0.521875,\n",
       "  0.5100000000000001,\n",
       "  0.6071428571428571,\n",
       "  0.5977777777777777,\n",
       "  0.34312499999999996,\n",
       "  0.3855555555555556,\n",
       "  0.5393749999999999,\n",
       "  0.45222222222222225,\n",
       "  0.32571428571428573,\n",
       "  0.49166666666666664,\n",
       "  0.39142857142857146,\n",
       "  0.4861111111111111,\n",
       "  0.63625,\n",
       "  0.4766666666666667,\n",
       "  0.429375,\n",
       "  0.5205555555555557,\n",
       "  0.397,\n",
       "  0.3527777777777778,\n",
       "  0.535,\n",
       "  0.48444444444444446,\n",
       "  0.4883333333333334,\n",
       "  0.4294444444444444,\n",
       "  0.535,\n",
       "  0.49562500000000004,\n",
       "  0.5744444444444444,\n",
       "  0.4705555555555555,\n",
       "  0.3535714285714286,\n",
       "  0.49785714285714283,\n",
       "  0.54375,\n",
       "  0.31999999999999995,\n",
       "  0.6275,\n",
       "  0.5583333333333332,\n",
       "  0.5405555555555555,\n",
       "  0.41437500000000005,\n",
       "  0.38250000000000006,\n",
       "  0.21944444444444444,\n",
       "  0.5983333333333333,\n",
       "  0.4438888888888889,\n",
       "  0.344375,\n",
       "  0.4027777777777778,\n",
       "  0.4025,\n",
       "  0.47,\n",
       "  0.6007142857142858,\n",
       "  0.47333333333333333,\n",
       "  0.4766666666666667,\n",
       "  0.463125,\n",
       "  0.442,\n",
       "  0.46125000000000005,\n",
       "  0.5827777777777778,\n",
       "  0.41888888888888887,\n",
       "  0.40222222222222226,\n",
       "  0.37555555555555553,\n",
       "  0.38111111111111107,\n",
       "  0.398,\n",
       "  0.42000000000000004,\n",
       "  0.5237499999999999,\n",
       "  0.618125,\n",
       "  0.39222222222222225,\n",
       "  0.519375,\n",
       "  0.481875,\n",
       "  0.3675,\n",
       "  0.45999999999999996,\n",
       "  0.5705555555555555,\n",
       "  0.435625,\n",
       "  0.41000000000000003,\n",
       "  0.5688888888888889,\n",
       "  0.315,\n",
       "  0.5977777777777776,\n",
       "  0.3078571428571429,\n",
       "  0.4321428571428571,\n",
       "  0.4694444444444444,\n",
       "  0.495,\n",
       "  0.510625,\n",
       "  0.5764285714285714,\n",
       "  0.4772222222222222,\n",
       "  0.4377777777777777,\n",
       "  0.676875,\n",
       "  0.5133333333333332,\n",
       "  0.5566666666666666,\n",
       "  0.4294444444444444,\n",
       "  0.43000000000000005,\n",
       "  0.3688888888888888,\n",
       "  0.39555555555555555,\n",
       "  0.32,\n",
       "  0.3416666666666667,\n",
       "  0.465625,\n",
       "  0.6416666666666666,\n",
       "  0.3661111111111111,\n",
       "  0.3805555555555556,\n",
       "  0.42000000000000004,\n",
       "  0.225,\n",
       "  0.6566666666666666,\n",
       "  0.7050000000000001,\n",
       "  0.36611111111111116,\n",
       "  0.47125000000000006,\n",
       "  0.3938888888888889,\n",
       "  0.4188888888888889,\n",
       "  0.5422222222222222,\n",
       "  0.4061111111111111,\n",
       "  0.4435714285714286,\n",
       "  0.4427777777777778,\n",
       "  0.3577777777777778,\n",
       "  0.47562499999999996,\n",
       "  0.5327777777777778,\n",
       "  0.7238888888888888,\n",
       "  0.6355555555555557,\n",
       "  0.44833333333333336,\n",
       "  0.43625,\n",
       "  0.2125,\n",
       "  0.471,\n",
       "  0.4361111111111111,\n",
       "  0.42999999999999994,\n",
       "  0.48062499999999997,\n",
       "  0.5161111111111112,\n",
       "  0.648125,\n",
       "  0.41624999999999995,\n",
       "  0.41777777777777775,\n",
       "  0.6268750000000001,\n",
       "  0.41857142857142854,\n",
       "  0.6233333333333334,\n",
       "  0.38333333333333336,\n",
       "  0.42625,\n",
       "  0.35,\n",
       "  0.504375,\n",
       "  0.375625,\n",
       "  0.345625,\n",
       "  0.48000000000000004,\n",
       "  0.5812499999999999,\n",
       "  0.5900000000000001,\n",
       "  0.5744444444444444,\n",
       "  0.5105555555555555,\n",
       "  0.509375,\n",
       "  0.6985714285714285,\n",
       "  0.7050000000000001,\n",
       "  0.41166666666666674,\n",
       "  0.389375,\n",
       "  0.33625,\n",
       "  0.495,\n",
       "  0.37375,\n",
       "  0.3807142857142857,\n",
       "  0.5661111111111111,\n",
       "  0.39357142857142857,\n",
       "  0.5757142857142857,\n",
       "  0.486875,\n",
       "  0.6394444444444445,\n",
       "  0.48857142857142855,\n",
       "  0.5172222222222222,\n",
       "  0.4914285714285715,\n",
       "  0.4014285714285714,\n",
       "  0.4744444444444445,\n",
       "  0.354,\n",
       "  0.555,\n",
       "  0.5188888888888888,\n",
       "  0.5116666666666667,\n",
       "  0.6855555555555556,\n",
       "  0.4561111111111111,\n",
       "  0.5838888888888889,\n",
       "  0.32375000000000004,\n",
       "  0.3842857142857143,\n",
       "  0.374375,\n",
       "  0.46611111111111114,\n",
       "  0.275,\n",
       "  0.43062500000000004,\n",
       "  0.47214285714285714,\n",
       "  0.4083333333333334,\n",
       "  0.5485714285714285,\n",
       "  0.5127777777777778,\n",
       "  0.5577777777777778,\n",
       "  0.37333333333333335,\n",
       "  0.39888888888888885,\n",
       "  0.45249999999999996,\n",
       "  0.41388888888888886,\n",
       "  0.4755555555555555,\n",
       "  0.5144444444444445,\n",
       "  0.3114285714285714,\n",
       "  0.49944444444444447,\n",
       "  0.39214285714285707,\n",
       "  0.32875,\n",
       "  0.5333333333333332,\n",
       "  0.6591666666666666,\n",
       "  0.49,\n",
       "  0.4885714285714286,\n",
       "  0.4428571428571429,\n",
       "  0.49666666666666676,\n",
       "  0.35642857142857143,\n",
       "  0.45562500000000006,\n",
       "  0.5511111111111112,\n",
       "  0.420625,\n",
       "  0.4833333333333334,\n",
       "  0.38222222222222224,\n",
       "  0.31333333333333335,\n",
       "  0.24437499999999998,\n",
       "  0.290625,\n",
       "  0.4766666666666668,\n",
       "  0.48555555555555546,\n",
       "  0.27125,\n",
       "  0.5111111111111111,\n",
       "  0.7261111111111112,\n",
       "  0.495,\n",
       "  0.406875,\n",
       "  0.4172222222222222,\n",
       "  0.541875,\n",
       "  0.4033333333333333,\n",
       "  0.48875,\n",
       "  0.5811111111111111,\n",
       "  0.418125,\n",
       "  0.37187499999999996,\n",
       "  0.5261111111111112,\n",
       "  0.33799999999999997,\n",
       "  0.616111111111111,\n",
       "  0.4883333333333334,\n",
       "  0.4425,\n",
       "  0.503125,\n",
       "  0.338125,\n",
       "  0.41900000000000004,\n",
       "  0.3194444444444444,\n",
       "  0.4361111111111111,\n",
       "  0.32749999999999996,\n",
       "  0.61625,\n",
       "  0.4478571428571429,\n",
       "  0.5444444444444445,\n",
       "  0.4078571428571428,\n",
       "  0.5125,\n",
       "  0.47666666666666657,\n",
       "  0.5850000000000001,\n",
       "  0.5094444444444444,\n",
       "  0.36444444444444446,\n",
       "  0.37666666666666665,\n",
       "  0.5057142857142857,\n",
       "  0.375,\n",
       "  0.4035714285714286,\n",
       "  0.5664285714285714,\n",
       "  0.3016666666666667,\n",
       "  0.2492857142857143,\n",
       "  0.47374999999999995,\n",
       "  0.5311111111111111,\n",
       "  0.348125,\n",
       "  0.40777777777777774,\n",
       "  0.4957142857142857,\n",
       "  0.528125,\n",
       "  0.47214285714285714,\n",
       "  0.47277777777777774,\n",
       "  0.46166666666666667,\n",
       "  0.5030000000000001,\n",
       "  0.6938888888888888,\n",
       "  0.5706249999999999,\n",
       "  0.533125,\n",
       "  0.4525,\n",
       "  0.4643750000000001,\n",
       "  0.6172222222222222,\n",
       "  0.5783333333333334,\n",
       "  0.46611111111111114,\n",
       "  0.398,\n",
       "  0.3275,\n",
       "  0.4561111111111111,\n",
       "  0.4872222222222223,\n",
       "  0.6268750000000001,\n",
       "  0.5142857142857142,\n",
       "  0.25285714285714284,\n",
       "  0.44749999999999995,\n",
       "  0.3888888888888889,\n",
       "  0.45611111111111113,\n",
       "  0.59625,\n",
       "  0.6805555555555556,\n",
       "  0.4799999999999999,\n",
       "  0.5105555555555555,\n",
       "  0.516875,\n",
       "  0.5716666666666668,\n",
       "  0.3983333333333334,\n",
       "  0.458125,\n",
       "  0.3488888888888889,\n",
       "  0.7091666666666666,\n",
       "  0.5237499999999999,\n",
       "  0.5744444444444444,\n",
       "  0.3125,\n",
       "  0.5800000000000001,\n",
       "  0.27785714285714286,\n",
       "  0.535625,\n",
       "  0.7125,\n",
       "  0.49666666666666676,\n",
       "  0.5931249999999999,\n",
       "  0.6621428571428571,\n",
       "  0.5957142857142858,\n",
       "  0.48125000000000007,\n",
       "  0.3314285714285714,\n",
       "  0.4933333333333334,\n",
       "  0.509375,\n",
       "  0.581111111111111,\n",
       "  0.398,\n",
       "  0.5078571428571429,\n",
       "  0.6088888888888888,\n",
       "  0.5675,\n",
       "  0.28500000000000003,\n",
       "  0.5672222222222223,\n",
       "  0.548888888888889,\n",
       "  0.5816666666666667,\n",
       "  0.5533333333333332,\n",
       "  0.665625,\n",
       "  0.3711111111111111,\n",
       "  0.42,\n",
       "  0.3794444444444445,\n",
       "  0.5922222222222221,\n",
       "  0.4122222222222222,\n",
       "  0.44222222222222224,\n",
       "  0.4525,\n",
       "  0.40611111111111114,\n",
       "  0.32944444444444443,\n",
       "  0.2938888888888889,\n",
       "  0.43333333333333335,\n",
       "  0.536111111111111,\n",
       "  0.5033333333333334,\n",
       "  0.25416666666666665,\n",
       "  0.3164285714285714,\n",
       "  0.45444444444444443,\n",
       "  0.565,\n",
       "  0.5761111111111111,\n",
       "  0.3314285714285714,\n",
       "  0.39875,\n",
       "  0.36312500000000003,\n",
       "  0.51375,\n",
       "  0.2742857142857143,\n",
       "  0.34888888888888886,\n",
       "  0.43562500000000004,\n",
       "  0.3527777777777778,\n",
       "  0.4388888888888889,\n",
       "  0.5638888888888888,\n",
       "  0.48000000000000004,\n",
       "  0.4122222222222222,\n",
       "  0.32,\n",
       "  0.5327777777777778,\n",
       "  0.4233333333333333,\n",
       "  0.35277777777777775,\n",
       "  0.557142857142857,\n",
       "  0.6505555555555556,\n",
       "  0.4855555555555556,\n",
       "  0.4905555555555556,\n",
       "  0.48625,\n",
       "  0.6156250000000001,\n",
       "  0.40375000000000005,\n",
       "  0.5983333333333334,\n",
       "  0.4255555555555555,\n",
       "  0.4266666666666667,\n",
       "  0.19722222222222224,\n",
       "  0.53375,\n",
       "  0.5216666666666666,\n",
       "  0.5766666666666667,\n",
       "  0.4785714285714286,\n",
       "  0.5433333333333332,\n",
       "  0.5449999999999999,\n",
       "  0.42111111111111105,\n",
       "  0.2942857142857143,\n",
       "  0.40249999999999997,\n",
       "  0.4766666666666666,\n",
       "  0.5783333333333334,\n",
       "  0.6083333333333333,\n",
       "  0.505,\n",
       "  0.47777777777777775,\n",
       "  0.37916666666666665,\n",
       "  0.35333333333333333,\n",
       "  0.413888888888889,\n",
       "  0.5933333333333333,\n",
       "  0.49875,\n",
       "  0.5688888888888889,\n",
       "  0.26857142857142857,\n",
       "  0.4438888888888889,\n",
       "  0.42937499999999995,\n",
       "  0.5888888888888888,\n",
       "  0.55375,\n",
       "  0.594375,\n",
       "  0.389375,\n",
       "  0.4927777777777778,\n",
       "  0.375,\n",
       "  0.5371428571428571,\n",
       "  0.33875,\n",
       "  0.47555555555555556,\n",
       "  0.6575,\n",
       "  0.445625,\n",
       "  0.44,\n",
       "  0.3466666666666667,\n",
       "  0.5964285714285714,\n",
       "  0.5822222222222222,\n",
       "  0.510625,\n",
       "  0.59,\n",
       "  0.609375,\n",
       "  0.5377777777777778,\n",
       "  0.3472222222222222,\n",
       "  0.5577777777777778,\n",
       "  0.48214285714285715,\n",
       "  0.44437499999999996,\n",
       "  0.404,\n",
       "  0.3544444444444445,\n",
       "  0.4749999999999999,\n",
       "  0.48277777777777775,\n",
       "  0.4774999999999999,\n",
       "  0.5650000000000001,\n",
       "  0.5109999999999999,\n",
       "  0.38875,\n",
       "  0.5722222222222223,\n",
       "  0.48249999999999993,\n",
       "  0.448125,\n",
       "  0.48071428571428576,\n",
       "  0.3266666666666666,\n",
       "  0.7478571428571429,\n",
       "  0.33222222222222225,\n",
       "  0.6121428571428572,\n",
       "  0.566875,\n",
       "  0.6755555555555557,\n",
       "  ...]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_trainer._eval_save_prefix = OUTPUT_PREFIX + \"-test-pos-unbiased\"\n",
    "model_trainer._evaluate_partial(test_dataset_pos_unbiased)\n",
    "\n",
    "model_trainer._eval_save_prefix = OUTPUT_PREFIX +  \"-test-neg-unbiased\"\n",
    "model_trainer._evaluate_partial(test_dataset_neg_unbiased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "biased_results = dict()\n",
    "\n",
    "\n",
    "\n",
    "# biased_results[\"STRATIFIED_15\"] = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", gamma=1.5, K=30, partition=100)\n",
    "biased_results[\"AOA\"] = aoa(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", K=30)\n",
    "biased_results[\"UB_15\"] = eq(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", gamma=1.5, K=30)\n",
    "biased_results[\"UB_2\"] =  eq(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", gamma=2, K=30)\n",
    "biased_results[\"UB_25\"] =  eq(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", gamma=2.5, K=30)\n",
    "biased_results[\"UB_3\"] =  eq(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", gamma=3, K=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbiased_results = dict()\n",
    "\n",
    "# unbiased_results[\"STRATIFIED_15\"] = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", gamma=1.5, K=1, partition=100)\n",
    "unbiased_results[\"AOA\"] = aoa(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", K=1)\n",
    "unbiased_results[\"UB_15\"] = eq(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", gamma=1.5, K=1)\n",
    "unbiased_results[\"UB_2\"] =  eq(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", gamma=2, K=1)\n",
    "unbiased_results[\"UB_25\"] =  eq(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", gamma=2.5, K=1)\n",
    "unbiased_results[\"UB_3\"] =  eq(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", gamma=3, K=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b97054ce9994b55a13fd0b3f092462e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1863 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute biased and unbiased results with stratified for values of partition in (1,2*len(sorted_items))\n",
    "# and store biased and unbiased results such that abs(biased_results[key]['auc'] - unbiased_results[key]['auc']) + abs(biased_results[key]['recall'] - unbiased_results[key]['recall']) is minimized\n",
    "num_items = 932\n",
    "\n",
    "unbiased_results[\"STRATIFIED_15\"] = dict()\n",
    "biased_results[\"STRATIFIED_15\"] = dict()\n",
    "best_partition=1\n",
    "\n",
    "for p in tqdm(range(1, 2*num_items)):\n",
    "    temp_unbiased = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", gamma=1.5, K=1, partition=p)\n",
    "    temp_biased = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", gamma=1.5, K=30, partition=p)\n",
    "    if not unbiased_results[\"STRATIFIED_15\"]:\n",
    "        unbiased_results[\"STRATIFIED_15\"] = temp_unbiased\n",
    "    if not biased_results[\"STRATIFIED_15\"]:\n",
    "        biased_results[\"STRATIFIED_15\"] = temp_biased\n",
    "    elif abs(temp_biased['auc'] - temp_unbiased['auc']) + abs(temp_unbiased['recall'] - temp_biased['recall']) < abs(biased_results[\"STRATIFIED_15\"]['auc'] - unbiased_results[\"STRATIFIED_15\"]['auc']) + abs(biased_results[\"STRATIFIED_15\"]['recall'] - unbiased_results[\"STRATIFIED_15\"]['recall']):\n",
    "        biased_results[\"STRATIFIED_15\"]['auc'] = temp_biased['auc']\n",
    "        biased_results[\"STRATIFIED_15\"]['recall'] = temp_biased['recall']\n",
    "        unbiased_results[\"STRATIFIED_15\"]['auc'] = temp_unbiased['auc']\n",
    "        unbiased_results[\"STRATIFIED_15\"]['recall'] = temp_unbiased['recall']\n",
    "        best_partition = p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbiased_results[\"STRATIFIED_2\"] = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", gamma=2, K=1, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_2\"] = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", gamma=2, K=30, partition=best_partition)\n",
    "\n",
    "unbiased_results[\"STRATIFIED_25\"] = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", gamma=2.5, K=1, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_25\"] = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", gamma=2.5, K=30, partition=best_partition)\n",
    "\n",
    "unbiased_results[\"STRATIFIED_3\"] = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", gamma=3, K=1, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_3\"] = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", gamma=3, K=30, partition=best_partition)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "key, value = random.choice(list(biased_results.items()))\n",
    "rows = 2#len(list(value.keys()))\n",
    "columns = 9#len(list(biased_results.items()))\n",
    "results_array = np.zeros((rows,columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_results = dict()\n",
    "\n",
    "list_biased_res = list(biased_results.keys())\n",
    "\n",
    "for i in range(len(list_biased_res)):\n",
    "    key = list_biased_res[i]\n",
    "\n",
    "    for j in range(len(list(biased_results[key].keys()))):\n",
    "        key_2 = list(biased_results[key].keys())[j]\n",
    "\n",
    "        results_array[j][i] = abs(biased_results[key][key_2] - unbiased_results[key][key_2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_df = pd.DataFrame(columns=list(biased_results.keys()), data=results_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_values = list(biased_results[list(biased_results.keys())[0]].keys())\n",
    "mae_df.insert(0, \"metric\", metric_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **RESULTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>AOA</th>\n",
       "      <th>UB_15</th>\n",
       "      <th>UB_2</th>\n",
       "      <th>UB_25</th>\n",
       "      <th>UB_3</th>\n",
       "      <th>STRATIFIED_15</th>\n",
       "      <th>STRATIFIED_2</th>\n",
       "      <th>STRATIFIED_25</th>\n",
       "      <th>STRATIFIED_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>auc</td>\n",
       "      <td>0.152750</td>\n",
       "      <td>0.125714</td>\n",
       "      <td>0.122354</td>\n",
       "      <td>0.119810</td>\n",
       "      <td>0.117876</td>\n",
       "      <td>0.022406</td>\n",
       "      <td>0.014835</td>\n",
       "      <td>0.369804</td>\n",
       "      <td>0.208499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>recall</td>\n",
       "      <td>0.378857</td>\n",
       "      <td>0.258728</td>\n",
       "      <td>0.247177</td>\n",
       "      <td>0.238659</td>\n",
       "      <td>0.232314</td>\n",
       "      <td>0.271626</td>\n",
       "      <td>0.398290</td>\n",
       "      <td>0.616611</td>\n",
       "      <td>2.394251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   metric       AOA     UB_15      UB_2     UB_25      UB_3  STRATIFIED_15  \\\n",
       "0     auc  0.152750  0.125714  0.122354  0.119810  0.117876       0.022406   \n",
       "1  recall  0.378857  0.258728  0.247177  0.238659  0.232314       0.271626   \n",
       "\n",
       "   STRATIFIED_2  STRATIFIED_25  STRATIFIED_3  \n",
       "0      0.014835       0.369804      0.208499  \n",
       "1      0.398290       0.616611      2.394251  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RecSysEvaluation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
