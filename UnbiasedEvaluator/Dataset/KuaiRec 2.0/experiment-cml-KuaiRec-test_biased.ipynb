{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **SETUP**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjYyDBmbcEL9"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nUxhCz5MkDZk"
      },
      "outputs": [],
      "source": [
        "from __future__ import division\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os\n",
        "from openrec.tf1.legacy import ImplicitModelTrainer\n",
        "from openrec.tf1.legacy.utils import ImplicitDataset\n",
        "from openrec.tf1.legacy.utils.evaluators import ImplicitEvalManager\n",
        "from openrec.tf1.legacy.recommenders import CML\n",
        "from openrec.tf1.legacy.utils.evaluators import AUC\n",
        "from openrec.tf1.legacy.utils.samplers import PairwiseSampler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "auc_results = []\n",
        "recall_results = []\n",
        "\n",
        "seed = 76424236\n",
        "np.random.seed(seed=seed)\n",
        "\n",
        "folder_name = f\"./generated_data/\"\n",
        "\n",
        "if os.path.exists(folder_name) == False:\n",
        "    os.makedirs(folder_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "g0C--vI7lUe2"
      },
      "outputs": [],
      "source": [
        "raw_data = dict()\n",
        "raw_data['train_data'] = np.load(folder_name + \"training_arr.npy\")\n",
        "raw_data['test_data_pos'] = np.load(folder_name + \"biased-test_arr_pos.npy\")\n",
        "raw_data['test_data_neg'] = np.load(folder_name + \"biased-test_arr_neg.npy\")\n",
        "raw_data['max_user'] = 7177\n",
        "raw_data['max_item'] = 10729\n",
        "batch_size = 8000\n",
        "test_batch_size = 1000\n",
        "display_itr = 1000\n",
        "\n",
        "train_dataset = ImplicitDataset(raw_data['train_data'], raw_data['max_user'], raw_data['max_item'], name='Train')\n",
        "test_dataset_pos = ImplicitDataset(raw_data['test_data_pos'], raw_data['max_user'], raw_data['max_item'])\n",
        "test_dataset_neg = ImplicitDataset(raw_data['test_data_neg'], raw_data['max_user'], raw_data['max_item'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c61lBOIqcawA"
      },
      "source": [
        "## Define Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:391: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:31: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:41: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:43: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:33: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/interactions/pairwise_eu_dist.py:71: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:596: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:75: The name tf.scatter_update is deprecated. Please use tf.compat.v1.scatter_update instead.\n",
            "\n",
            "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:144: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:365: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:148: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-04-24 12:50:28.829024: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
            "2024-04-24 12:50:28.832397: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 4192050000 Hz\n",
            "2024-04-24 12:50:28.833157: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x556c446ac1c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2024-04-24 12:50:28.833172: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./generated_data/cml-KuaiRec\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf                     # Code to avoid tf using cached embeddings\n",
        "tf.compat.v1.reset_default_graph()\n",
        "\n",
        "cml_model = CML(batch_size=batch_size, max_user=train_dataset.max_user(), max_item=train_dataset.max_item(),\n",
        "    dim_embed=50, l2_reg=0.001, opt='Adam', sess_config=None)\n",
        "sampler = PairwiseSampler(batch_size=batch_size, dataset=train_dataset, num_process=4)\n",
        "model_trainer = ImplicitModelTrainer(batch_size=batch_size, test_batch_size=test_batch_size,\n",
        "                                     train_dataset=train_dataset, model=cml_model, sampler=sampler,\n",
        "                                     eval_save_prefix=folder_name+\"KuaiRec\",\n",
        "                                     item_serving_size=500)\n",
        "auc_evaluator = AUC()\n",
        "\n",
        "cml_model.load(folder_name+\"cml-KuaiRec\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **DEFINE FUNCTION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def eq(infilename, infilename_neg, trainfilename, gamma=1.0, K=30):\n",
        "\n",
        "    # Load the pickles\n",
        "    infile = open(infilename, 'rb')\n",
        "    infile_neg = open(infilename_neg, 'rb')\n",
        "    P = pickle.load(infile)\n",
        "    infile.close()\n",
        "    P_neg = pickle.load(infile_neg)\n",
        "    infile_neg.close()\n",
        "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
        "    \n",
        "    # Merge the two dictionaries\n",
        "    for theuser in P[\"users\"]:\n",
        "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
        "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
        "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
        "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
        "    \n",
        "    Zui = dict()\n",
        "    Ni = dict()\n",
        "\n",
        "    # Compute the frequency of each item in the training set\n",
        "    trainset = np.load(trainfilename)\n",
        "    for i in trainset['item_id']:\n",
        "        if i in Ni:\n",
        "            Ni[i] += 1\n",
        "        else:\n",
        "            Ni[i] = 1\n",
        "    del trainset\n",
        "\n",
        "    # Count the number of users with at least one positive item in the test set\n",
        "    # (not the smartest way)\n",
        "    nonzero_user_count = 0\n",
        "    for theuser in P[\"users\"]:\n",
        "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
        "        for pos_item in pos_items:\n",
        "            if pos_item in Ni:\n",
        "                nonzero_user_count += 1\n",
        "                break\n",
        "\n",
        "    # Compute the recommendations\n",
        "    for theuser in P[\"users\"]:\n",
        "        all_scores = np.array(P[\"results\"][theuser])\n",
        "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
        "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
        "        for i, pos_item in enumerate(pos_items):\n",
        "            pos_score = pos_scores[i]\n",
        "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
        "\n",
        "    # Compute the scores using AUC and compute the recall\n",
        "    sum_user_auc = 0.0\n",
        "    sum_user_recall = 0.0\n",
        "    for theuser in P[\"users\"]:\n",
        "        numerator_auc = 0.0\n",
        "        numerator_recall = 0.0\n",
        "        denominator = 0.0\n",
        "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
        "            if theitem not in Ni:\n",
        "                continue\n",
        "            pui = np.power(Ni[theitem], (gamma + 1) / 2.0)\n",
        "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser])) / pui\n",
        "            if Zui[(theuser, theitem)] < K:\n",
        "                numerator_recall += 1.0 / pui\n",
        "            denominator += 1 / pui\n",
        "        if denominator > 0:\n",
        "            sum_user_auc += numerator_auc / denominator\n",
        "            sum_user_recall += numerator_recall / denominator\n",
        "\n",
        "    return {\n",
        "        \"auc\"       : sum_user_auc / nonzero_user_count,\n",
        "        \"recall\"    : sum_user_recall / nonzero_user_count\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def aoa(infilename, infilename_neg, trainfilename, K=1):\n",
        "\n",
        "    # Load pickles\n",
        "    infile = open(infilename, 'rb')\n",
        "    infile_neg = open(infilename_neg, 'rb')\n",
        "    P = pickle.load(infile)\n",
        "    infile.close()\n",
        "    P_neg = pickle.load(infile_neg)\n",
        "    infile_neg.close()\n",
        "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
        "    \n",
        "    # Merge the two dictionaries\n",
        "    for theuser in P[\"users\"]:\n",
        "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
        "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
        "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
        "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
        "        \n",
        "    Zui = dict()\n",
        "    Ni = dict()\n",
        "    \n",
        "    # Count item frequencies in the training set\n",
        "    trainset = np.load(trainfilename)\n",
        "    for i in trainset['item_id']:\n",
        "        if i in Ni:\n",
        "            Ni[i] += 1\n",
        "        else:\n",
        "            Ni[i] = 1\n",
        "    del trainset\n",
        "    \n",
        "    # Count the number of users with at least one positive item in the test set \n",
        "    # (not the smartest way)\n",
        "    nonzero_user_count = 0\n",
        "    for theuser in P[\"users\"]:\n",
        "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
        "        for pos_item in pos_items:\n",
        "            if pos_item in Ni:\n",
        "                nonzero_user_count += 1\n",
        "                break\n",
        "\n",
        "    # Compute the recommendations for each user\n",
        "    for theuser in P[\"users\"]:\n",
        "        all_scores = np.array(P[\"results\"][theuser])\n",
        "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
        "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
        "        for i, pos_item in enumerate(pos_items):\n",
        "            pos_score = pos_scores[i]\n",
        "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
        "            \n",
        "    sum_user_auc = 0.0\n",
        "    sum_user_recall = 0.0\n",
        "\n",
        "    # Compute the scores using AUC and compute the recall\n",
        "    for theuser in P[\"users\"]:\n",
        "        numerator_auc = 0.0\n",
        "        numerator_recall = 0.0\n",
        "        denominator = 0.0\n",
        "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
        "            if theitem not in Ni:\n",
        "                continue\n",
        "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser]))\n",
        "            if Zui[(theuser, theitem)] < K:\n",
        "                numerator_recall += 1.0\n",
        "            denominator += 1 \n",
        "        if denominator > 0:\n",
        "            sum_user_auc += numerator_auc / denominator\n",
        "            sum_user_recall += numerator_recall / denominator\n",
        "\n",
        "    return {\n",
        "        \"auc\"       : sum_user_auc / nonzero_user_count,\n",
        "        \"recall\"    : sum_user_recall / nonzero_user_count\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **TEST**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTBMf1BPcvgL"
      },
      "source": [
        "## Generate Raw Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Subsampling negative items]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5907/5907 [00:02<00:00, 2878.50it/s]\n",
            "100%|██████████| 7176/7176 [01:17<00:00, 92.40it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'AUC': [0.556010101010101,\n",
              "  0.4992398648648649,\n",
              "  0.5087919463087248,\n",
              "  0.544421768707483,\n",
              "  0.5247324414715718,\n",
              "  0.5150841750841751,\n",
              "  0.4965886287625418,\n",
              "  0.48907407407407405,\n",
              "  0.504679054054054,\n",
              "  0.4912671232876712,\n",
              "  0.5544107744107744,\n",
              "  0.5182881355932205,\n",
              "  0.5299496644295303,\n",
              "  0.44734375000000004,\n",
              "  0.4981939799331104,\n",
              "  0.5011655405405404,\n",
              "  0.5152173913043478,\n",
              "  0.4899826989619377,\n",
              "  0.4904347826086956,\n",
              "  0.47169999999999995,\n",
              "  0.4901677852348993,\n",
              "  0.47605,\n",
              "  0.49714765100671143,\n",
              "  0.5306354515050168,\n",
              "  0.4827333333333333,\n",
              "  0.48431438127090304,\n",
              "  0.5405872483221478,\n",
              "  0.48506711409395975,\n",
              "  0.5055442176870748,\n",
              "  0.5033217993079585,\n",
              "  0.4584459459459459,\n",
              "  0.48657094594594597,\n",
              "  0.5135102739726027,\n",
              "  0.438758389261745,\n",
              "  0.5206688963210703,\n",
              "  0.5291864406779662,\n",
              "  0.5186666666666667,\n",
              "  0.4763166666666666,\n",
              "  0.5052033898305085,\n",
              "  0.4668896321070235,\n",
              "  0.5358862876254181,\n",
              "  0.49731418918918924,\n",
              "  0.4961148648648647,\n",
              "  0.5188793103448276,\n",
              "  0.5227027027027027,\n",
              "  0.5480936454849498,\n",
              "  0.4971833333333333,\n",
              "  0.5243898305084745,\n",
              "  0.4809827586206897,\n",
              "  0.5484731543624161,\n",
              "  0.4756529209621993,\n",
              "  0.51105,\n",
              "  0.5395904436860068,\n",
              "  0.5207482993197279,\n",
              "  0.5448833333333334,\n",
              "  0.48510000000000003,\n",
              "  0.48771666666666663,\n",
              "  0.502104377104377,\n",
              "  0.5112626262626263,\n",
              "  0.506571906354515,\n",
              "  0.5144368600682594,\n",
              "  0.4890202702702703,\n",
              "  0.5249161073825503,\n",
              "  0.5166949152542373,\n",
              "  0.45215000000000005,\n",
              "  0.46108695652173914,\n",
              "  0.47908026755852845,\n",
              "  0.5013344594594595,\n",
              "  0.48753355704697987,\n",
              "  0.4779697986577181,\n",
              "  0.5029264214046822,\n",
              "  0.5256060606060606,\n",
              "  0.467901023890785,\n",
              "  0.5062374581939799,\n",
              "  0.5027348993288591,\n",
              "  0.5310666666666667,\n",
              "  0.4937162162162162,\n",
              "  0.5084797297297297,\n",
              "  0.5172108843537415,\n",
              "  0.46708754208754216,\n",
              "  0.47719798657718115,\n",
              "  0.5075752508361204,\n",
              "  0.4856802721088436,\n",
              "  0.48701666666666665,\n",
              "  0.5285641891891891,\n",
              "  0.47368243243243235,\n",
              "  0.46841137123745824,\n",
              "  0.5056856187290969,\n",
              "  0.5022558922558923,\n",
              "  0.4862244897959183,\n",
              "  0.44265993265993264,\n",
              "  0.47586206896551725,\n",
              "  0.5099661016949153,\n",
              "  0.5364383561643836,\n",
              "  0.5043434343434343,\n",
              "  0.5117785234899328,\n",
              "  0.47208333333333335,\n",
              "  0.5424074074074074,\n",
              "  0.5026333333333333,\n",
              "  0.5528813559322033,\n",
              "  0.45160472972972976,\n",
              "  0.5162925170068028,\n",
              "  0.48211666666666664,\n",
              "  0.5078282828282827,\n",
              "  0.4896979865771812,\n",
              "  0.4715635738831615,\n",
              "  0.4971,\n",
              "  0.49335616438356167,\n",
              "  0.48244147157190637,\n",
              "  0.5239862542955327,\n",
              "  0.5279461279461279,\n",
              "  0.5074740484429067,\n",
              "  0.5032323232323231,\n",
              "  0.49052013422818797,\n",
              "  0.4944006849315069,\n",
              "  0.5176430976430977,\n",
              "  0.47811036789297656,\n",
              "  0.46744755244755243,\n",
              "  0.515929054054054,\n",
              "  0.5034899328859062,\n",
              "  0.48218333333333324,\n",
              "  0.5312798634812287,\n",
              "  0.5040136054421769,\n",
              "  0.47404761904761905,\n",
              "  0.4823693379790941,\n",
              "  0.4997818791946308,\n",
              "  0.507010135135135,\n",
              "  0.47100000000000003,\n",
              "  0.4798310810810811,\n",
              "  0.4845791245791246,\n",
              "  0.47126262626262627,\n",
              "  0.5080333333333334,\n",
              "  0.5366101694915255,\n",
              "  0.4681925675675676,\n",
              "  0.5244368600682594,\n",
              "  0.49996632996633,\n",
              "  0.4837166666666667,\n",
              "  0.49807692307692314,\n",
              "  0.5162121212121212,\n",
              "  0.47689597315436244,\n",
              "  0.5341973244147157,\n",
              "  0.47313333333333335,\n",
              "  0.5015833333333334,\n",
              "  0.5133050847457628,\n",
              "  0.4835234899328859,\n",
              "  0.5227891156462585,\n",
              "  0.48796979865771817,\n",
              "  0.4965635738831615,\n",
              "  0.45149484536082474,\n",
              "  0.4412541806020067,\n",
              "  0.44030303030303036,\n",
              "  0.48058724832214755,\n",
              "  0.49968120805369126,\n",
              "  0.49493311036789295,\n",
              "  0.46982935153583616,\n",
              "  0.47202341137123743,\n",
              "  0.5230333333333335,\n",
              "  0.503026755852843,\n",
              "  0.4742346938775511,\n",
              "  0.49656040268456386,\n",
              "  0.4752972027972028,\n",
              "  0.5145205479452055,\n",
              "  0.4732068965517242,\n",
              "  0.5111447811447811,\n",
              "  0.4713917525773195,\n",
              "  0.4653666666666667,\n",
              "  0.5241137123745818,\n",
              "  0.4931605351170568,\n",
              "  0.5106972789115646,\n",
              "  0.4855050505050505,\n",
              "  0.4871979865771812,\n",
              "  0.4699658703071672,\n",
              "  0.5105685618729096,\n",
              "  0.4712080536912752,\n",
              "  0.5350836120401338,\n",
              "  0.46615771812080536,\n",
              "  0.4677591973244148,\n",
              "  0.5063636363636363,\n",
              "  0.441505016722408,\n",
              "  0.48012195121951223,\n",
              "  0.5337966101694914,\n",
              "  0.5153859060402683,\n",
              "  0.5291414141414141,\n",
              "  0.4992474916387959,\n",
              "  0.5420302013422817,\n",
              "  0.5541333333333334,\n",
              "  0.505520134228188,\n",
              "  0.5070973154362416,\n",
              "  0.537764505119454,\n",
              "  0.4639261744966443,\n",
              "  0.5344128113879004,\n",
              "  0.5118791946308725,\n",
              "  0.4421739130434783,\n",
              "  0.5103198653198653,\n",
              "  0.49202422145328717,\n",
              "  0.5077759197324414,\n",
              "  0.47919732441471574,\n",
              "  0.5240784982935154,\n",
              "  0.5124659863945579,\n",
              "  0.4954347826086956,\n",
              "  0.561402027027027,\n",
              "  0.45224662162162166,\n",
              "  0.5186655405405406,\n",
              "  0.49247474747474745,\n",
              "  0.5583444816053511,\n",
              "  0.5071644295302014,\n",
              "  0.5046476510067115,\n",
              "  0.500542372881356,\n",
              "  0.5065033783783783,\n",
              "  0.5227591973244148,\n",
              "  0.5248801369863013,\n",
              "  0.5178082191780822,\n",
              "  0.5447796610169492,\n",
              "  0.5077027027027028,\n",
              "  0.543013468013468,\n",
              "  0.4917517006802721,\n",
              "  0.48296979865771816,\n",
              "  0.5062286689419795,\n",
              "  0.490993265993266,\n",
              "  0.51125,\n",
              "  0.4171621621621621,\n",
              "  0.5072297297297297,\n",
              "  0.48328282828282826,\n",
              "  0.4663879598662207,\n",
              "  0.5171308724832214,\n",
              "  0.47634146341463407,\n",
              "  0.5280333333333334,\n",
              "  0.46829431438127084,\n",
              "  0.5400836120401338,\n",
              "  0.4930666666666666,\n",
              "  0.5370578231292517,\n",
              "  0.48466555183946497,\n",
              "  0.4900505050505051,\n",
              "  0.4524829931972789,\n",
              "  0.5113210702341137,\n",
              "  0.4629598662207358,\n",
              "  0.5239625850340136,\n",
              "  0.49217391304347824,\n",
              "  0.5207939189189189,\n",
              "  0.47376689189189186,\n",
              "  0.51965,\n",
              "  0.5148833333333334,\n",
              "  0.5210402684563759,\n",
              "  0.48612794612794613,\n",
              "  0.4698989898989899,\n",
              "  0.5291638795986622,\n",
              "  0.5187837837837838,\n",
              "  0.5301515151515152,\n",
              "  0.5018666666666667,\n",
              "  0.4674000000000001,\n",
              "  0.4899328859060403,\n",
              "  0.5129421768707483,\n",
              "  0.5476541095890411,\n",
              "  0.47150337837837836,\n",
              "  0.5075838926174496,\n",
              "  0.47396959459459465,\n",
              "  0.4808131487889274,\n",
              "  0.5331772575250837,\n",
              "  0.4838552188552188,\n",
              "  0.48084175084175085,\n",
              "  0.4980068728522337,\n",
              "  0.5127499999999999,\n",
              "  0.44738255033557045,\n",
              "  0.4767281879194631,\n",
              "  0.5078187919463087,\n",
              "  0.5195302013422819,\n",
              "  0.47464999999999996,\n",
              "  0.45336148648648644,\n",
              "  0.5248825503355704,\n",
              "  0.4643120805369128,\n",
              "  0.4959215017064847,\n",
              "  0.5175335570469799,\n",
              "  0.4422390572390573,\n",
              "  0.5373993288590604,\n",
              "  0.5191609589041097,\n",
              "  0.5650000000000001,\n",
              "  0.4944481605351171,\n",
              "  0.4729560810810811,\n",
              "  0.49724489795918364,\n",
              "  0.48501666666666665,\n",
              "  0.5329391891891891,\n",
              "  0.4996812080536912,\n",
              "  0.4787248322147651,\n",
              "  0.484057239057239,\n",
              "  0.5007118644067796,\n",
              "  0.49324999999999997,\n",
              "  0.5111224489795918,\n",
              "  0.4892881355932204,\n",
              "  0.44286689419795217,\n",
              "  0.4793311036789297,\n",
              "  0.4789093959731543,\n",
              "  0.48815254237288136,\n",
              "  0.548020134228188,\n",
              "  0.5188590604026846,\n",
              "  0.5074833333333334,\n",
              "  0.4983501683501683,\n",
              "  0.49444067796610175,\n",
              "  0.5059666666666667,\n",
              "  0.5201333333333334,\n",
              "  0.474198606271777,\n",
              "  0.5361815068493151,\n",
              "  0.5081166666666667,\n",
              "  0.48878472222222213,\n",
              "  0.49475862068965515,\n",
              "  0.504178082191781,\n",
              "  0.5283161512027491,\n",
              "  0.5053559322033898,\n",
              "  0.5200671140939597,\n",
              "  0.49028333333333335,\n",
              "  0.48535234899328855,\n",
              "  0.4911525423728813,\n",
              "  0.5390033783783783,\n",
              "  0.47198996655518394,\n",
              "  0.48791808873720127,\n",
              "  0.5215719063545151,\n",
              "  0.47916387959866213,\n",
              "  0.5098154362416107,\n",
              "  0.5159333333333335,\n",
              "  0.5191722972972973,\n",
              "  0.5054833333333334,\n",
              "  0.5703166666666667,\n",
              "  0.44375850340136047,\n",
              "  0.5056711409395973,\n",
              "  0.5267056856187291,\n",
              "  0.4667676767676768,\n",
              "  0.48207770270270267,\n",
              "  0.4903050847457628,\n",
              "  0.5120066889632107,\n",
              "  0.5041333333333333,\n",
              "  0.5041166666666667,\n",
              "  0.5109556313993174,\n",
              "  0.5196328671328672,\n",
              "  0.5284113712374582,\n",
              "  0.49670068027210873,\n",
              "  0.5280236486486487,\n",
              "  0.5049000000000001,\n",
              "  0.48666095890410954,\n",
              "  0.5162414965986394,\n",
              "  0.5151379310344827,\n",
              "  0.44739655172413795,\n",
              "  0.49292642140468224,\n",
              "  0.4456779661016949,\n",
              "  0.5277815699658702,\n",
              "  0.49900673400673407,\n",
              "  0.507996632996633,\n",
              "  0.5067128027681661,\n",
              "  0.46911666666666674,\n",
              "  0.4718556701030928,\n",
              "  0.5363131313131313,\n",
              "  0.4884615384615385,\n",
              "  0.5026845637583893,\n",
              "  0.5274570446735395,\n",
              "  0.5001010101010102,\n",
              "  0.4991408934707904,\n",
              "  0.4848166666666666,\n",
              "  0.4808862876254181,\n",
              "  0.5188795986622073,\n",
              "  0.4921812080536913,\n",
              "  0.500221843003413,\n",
              "  0.489847972972973,\n",
              "  0.49125423728813555,\n",
              "  0.5384797297297297,\n",
              "  0.4766666666666666,\n",
              "  0.4889965397923875,\n",
              "  0.4967558528428094,\n",
              "  0.49601351351351347,\n",
              "  0.47361204013377933,\n",
              "  0.49770270270270267,\n",
              "  0.5110166666666667,\n",
              "  0.49309764309764303,\n",
              "  0.5357382550335571,\n",
              "  0.4849113475177305,\n",
              "  0.49958053691275167,\n",
              "  0.5034899328859062,\n",
              "  0.5055852842809365,\n",
              "  0.5301877133105801,\n",
              "  0.4955892255892256,\n",
              "  0.5102730375426621,\n",
              "  0.5126833333333334,\n",
              "  0.4874916387959866,\n",
              "  0.5146598639455783,\n",
              "  0.5037288135593221,\n",
              "  0.5212837837837838,\n",
              "  0.4853275862068966,\n",
              "  0.5000677966101694,\n",
              "  0.46375,\n",
              "  0.5304026845637584,\n",
              "  0.47801694915254234,\n",
              "  0.49391228070175436,\n",
              "  0.5243979933110368,\n",
              "  0.483160535117057,\n",
              "  0.4750671140939597,\n",
              "  0.5350847457627117,\n",
              "  0.47215,\n",
              "  0.5226510067114094,\n",
              "  0.5020805369127517,\n",
              "  0.45988215488215484,\n",
              "  0.5268999999999999,\n",
              "  0.4914237288135594,\n",
              "  0.4883164983164983,\n",
              "  0.5093898305084745,\n",
              "  0.4674570446735396,\n",
              "  0.5105685618729097,\n",
              "  0.4845819397993312,\n",
              "  0.5430952380952382,\n",
              "  0.5224324324324325,\n",
              "  0.47610714285714284,\n",
              "  0.5239597315436242,\n",
              "  0.4964646464646465,\n",
              "  0.5475254237288135,\n",
              "  0.474103448275862,\n",
              "  0.4971283783783783,\n",
              "  0.46885906040268466,\n",
              "  0.4734731543624161,\n",
              "  0.5035016835016836,\n",
              "  0.4890646258503401,\n",
              "  0.4921598639455782,\n",
              "  0.5054931972789116,\n",
              "  0.45831615120274916,\n",
              "  0.5294444444444445,\n",
              "  0.531952861952862,\n",
              "  0.5107525083612041,\n",
              "  0.5227118644067796,\n",
              "  0.504,\n",
              "  0.49894648829431437,\n",
              "  0.5262289562289563,\n",
              "  0.5083838383838383,\n",
              "  0.4606655290102389,\n",
              "  0.48626262626262623,\n",
              "  0.5188795986622073,\n",
              "  0.4770066889632107,\n",
              "  0.5192006802721087,\n",
              "  0.5297260273972603,\n",
              "  0.48658862876254183,\n",
              "  0.5148489932885906,\n",
              "  0.5187966101694916,\n",
              "  0.47910774410774415,\n",
              "  0.510989932885906,\n",
              "  0.5271070234113712,\n",
              "  0.4966053511705686,\n",
              "  0.45933110367892976,\n",
              "  0.47981034482758617,\n",
              "  0.5235353535353535,\n",
              "  0.5025844594594594,\n",
              "  0.53685,\n",
              "  0.5344127516778523,\n",
              "  0.5262286689419795,\n",
              "  0.5109395973154363,\n",
              "  0.47035593220338984,\n",
              "  0.5043918918918919,\n",
              "  0.5147972972972973,\n",
              "  0.45038590604026846,\n",
              "  0.5066333333333334,\n",
              "  0.47716442953020133,\n",
              "  0.43526845637583883,\n",
              "  0.5282828282828284,\n",
              "  0.5224916387959866,\n",
              "  0.4826254180602007,\n",
              "  0.46548494983277594,\n",
              "  0.4977013422818792,\n",
              "  0.46765886287625413,\n",
              "  0.5114333333333334,\n",
              "  0.5124496644295301,\n",
              "  0.5045166666666666,\n",
              "  0.5106020066889633,\n",
              "  0.5201020408163264,\n",
              "  0.501761744966443,\n",
              "  0.5041245791245791,\n",
              "  0.5145945945945946,\n",
              "  0.5058277027027028,\n",
              "  0.5099152542372882,\n",
              "  0.4608277027027028,\n",
              "  0.5065500000000001,\n",
              "  0.44741554054054056,\n",
              "  0.5161036789297658,\n",
              "  0.5024500000000001,\n",
              "  0.4913636363636364,\n",
              "  0.47951505016722407,\n",
              "  0.4793367346938776,\n",
              "  0.5132823129251701,\n",
              "  0.5343456375838925,\n",
              "  0.4928474576271186,\n",
              "  0.541060606060606,\n",
              "  0.4753010033444816,\n",
              "  0.498494983277592,\n",
              "  0.4870034843205574,\n",
              "  0.4839455782312924,\n",
              "  0.4873693379790941,\n",
              "  0.5049,\n",
              "  0.4548090277777778,\n",
              "  0.5463468013468014,\n",
              "  0.5010033444816053,\n",
              "  0.5591919191919192,\n",
              "  0.5135000000000001,\n",
              "  0.48033557046979863,\n",
              "  0.507281879194631,\n",
              "  0.4806610169491526,\n",
              "  0.4934827586206896,\n",
              "  0.46744966442953023,\n",
              "  0.45448333333333335,\n",
              "  0.46580985915492945,\n",
              "  0.46901006711409393,\n",
              "  0.5047324414715719,\n",
              "  0.45669550173010387,\n",
              "  0.47698333333333326,\n",
              "  0.48836120401337796,\n",
              "  0.506233108108108,\n",
              "  0.4589335664335664,\n",
              "  0.5177181208053692,\n",
              "  0.5201166666666667,\n",
              "  0.5555685618729097,\n",
              "  0.5429765886287625,\n",
              "  0.49897993311036787,\n",
              "  0.5002188552188552,\n",
              "  0.4587123745819397,\n",
              "  0.47561016949152546,\n",
              "  0.4551923076923076,\n",
              "  0.5011241610738255,\n",
              "  0.4858417508417509,\n",
              "  0.48466555183946486,\n",
              "  0.5554515050167224,\n",
              "  0.4708528428093646,\n",
              "  0.4726333333333333,\n",
              "  0.5042760942760943,\n",
              "  0.5091946308724832,\n",
              "  0.5100682593856655,\n",
              "  0.48848639455782306,\n",
              "  0.5191414141414141,\n",
              "  0.5273913043478261,\n",
              "  0.48370689655172416,\n",
              "  0.4681016949152543,\n",
              "  0.5686120401337792,\n",
              "  0.5153833333333334,\n",
              "  0.4661409395973155,\n",
              "  0.52925,\n",
              "  0.4554166666666667,\n",
              "  0.5554833333333332,\n",
              "  0.5242166666666667,\n",
              "  0.51561872909699,\n",
              "  0.48023890784982937,\n",
              "  0.5375000000000001,\n",
              "  0.4837290969899665,\n",
              "  0.5119932432432432,\n",
              "  0.5225083612040134,\n",
              "  0.5097448979591837,\n",
              "  0.4754515050167224,\n",
              "  0.4973711340206186,\n",
              "  0.5160234899328859,\n",
              "  0.49288333333333334,\n",
              "  0.4878020134228188,\n",
              "  0.4817114093959732,\n",
              "  0.49335016835016826,\n",
              "  0.5171739130434783,\n",
              "  0.49862876254180594,\n",
              "  0.4606101694915254,\n",
              "  0.48484848484848486,\n",
              "  0.49378892733564006,\n",
              "  0.4791275167785234,\n",
              "  0.49856666666666666,\n",
              "  0.4934406779661017,\n",
              "  0.4812080536912752,\n",
              "  0.4845469798657718,\n",
              "  0.4804180602006689,\n",
              "  0.5493333333333333,\n",
              "  0.49098993288590603,\n",
              "  0.4965166666666666,\n",
              "  0.49345,\n",
              "  0.4765853658536586,\n",
              "  0.5099328859060402,\n",
              "  0.521151202749141,\n",
              "  0.4725250836120401,\n",
              "  0.4977013422818792,\n",
              "  0.5228428093645485,\n",
              "  0.5033501683501683,\n",
              "  0.4492809364548495,\n",
              "  0.5164527027027027,\n",
              "  0.4963389830508475,\n",
              "  0.4997147651006712,\n",
              "  0.4822073578595317,\n",
              "  0.4983053691275168,\n",
              "  0.49328333333333335,\n",
              "  0.4853938356164384,\n",
              "  0.4987290969899666,\n",
              "  0.4675259515570935,\n",
              "  0.4877609427609428,\n",
              "  0.4899496644295302,\n",
              "  0.4759197324414716,\n",
              "  0.50795,\n",
              "  0.526952861952862,\n",
              "  0.50785,\n",
              "  0.5021694915254237,\n",
              "  0.5130333333333333,\n",
              "  0.5240604026845637,\n",
              "  0.482972972972973,\n",
              "  0.528888888888889,\n",
              "  0.5012962962962962,\n",
              "  0.553915254237288,\n",
              "  0.5044666666666667,\n",
              "  0.473561872909699,\n",
              "  0.4712116040955631,\n",
              "  0.5314897260273972,\n",
              "  0.47383050847457625,\n",
              "  0.4663833333333334,\n",
              "  0.5142760942760943,\n",
              "  0.45812080536912747,\n",
              "  0.5096166666666666,\n",
              "  0.49396193771626296,\n",
              "  0.4969256756756757,\n",
              "  0.47541806020066885,\n",
              "  0.49105,\n",
              "  0.47633445945945946,\n",
              "  0.4740986394557823,\n",
              "  0.53645,\n",
              "  0.5213636363636364,\n",
              "  0.5401505016722408,\n",
              "  0.5191694915254238,\n",
              "  0.4748666666666667,\n",
              "  0.4963590604026845,\n",
              "  0.47966442953020133,\n",
              "  0.5187758620689655,\n",
              "  0.5005308219178082,\n",
              "  0.49920338983050844,\n",
              "  0.502593220338983,\n",
              "  0.48057432432432434,\n",
              "  0.47558922558922556,\n",
              "  0.5036301369863013,\n",
              "  0.5040635451505018,\n",
              "  0.4947157190635451,\n",
              "  0.5403691275167786,\n",
              "  0.4913590604026845,\n",
              "  0.5020973154362416,\n",
              "  0.4648135593220339,\n",
              "  0.5058500000000001,\n",
              "  0.5114383561643836,\n",
              "  0.4983166666666667,\n",
              "  0.4964864864864864,\n",
              "  0.5006397306397308,\n",
              "  0.5028368794326241,\n",
              "  0.5117892976588629,\n",
              "  0.4694983277591973,\n",
              "  0.47000000000000003,\n",
              "  0.49117845117845116,\n",
              "  0.48247491638795986,\n",
              "  0.4884020618556701,\n",
              "  0.5,\n",
              "  0.45326599326599326,\n",
              "  0.5079421768707483,\n",
              "  0.5667224080267558,\n",
              "  0.5133164983164983,\n",
              "  0.5015593220338983,\n",
              "  0.4645286195286195,\n",
              "  0.4565292096219932,\n",
              "  0.4942687074829932,\n",
              "  0.5065033783783784,\n",
              "  0.5108591065292096,\n",
              "  0.5174581939799331,\n",
              "  0.553494983277592,\n",
              "  0.48772575250836125,\n",
              "  0.49843537414965994,\n",
              "  0.5113590604026845,\n",
              "  0.5368103448275863,\n",
              "  0.5005593220338983,\n",
              "  0.5013758389261744,\n",
              "  0.48153333333333337,\n",
              "  0.50105,\n",
              "  0.5581772575250836,\n",
              "  0.4744630872483221,\n",
              "  0.509527027027027,\n",
              "  0.5076421404682274,\n",
              "  0.5228911564625849,\n",
              "  0.4988682432432433,\n",
              "  0.5335544217687075,\n",
              "  0.536505016722408,\n",
              "  0.4780267558528428,\n",
              "  0.4948141891891891,\n",
              "  0.5195150501672241,\n",
              "  0.47570175438596485,\n",
              "  0.5264214046822743,\n",
              "  0.5434628378378379,\n",
              "  0.49491525423728816,\n",
              "  0.48625423728813555,\n",
              "  0.49724080267558535,\n",
              "  0.4911499999999999,\n",
              "  0.4754666666666667,\n",
              "  0.521864406779661,\n",
              "  0.536476510067114,\n",
              "  0.519795918367347,\n",
              "  0.49286666666666673,\n",
              "  0.5105685618729097,\n",
              "  0.4958666666666666,\n",
              "  0.5171571906354515,\n",
              "  0.5273825503355704,\n",
              "  0.5064548494983278,\n",
              "  0.5071790540540541,\n",
              "  0.5107966101694915,\n",
              "  0.49538590604026844,\n",
              "  0.48138047138047135,\n",
              "  0.49080536912751677,\n",
              "  0.5136585365853659,\n",
              "  0.5295484949832776,\n",
              "  0.5457550335570469,\n",
              "  0.5403754266211604,\n",
              "  0.530456081081081,\n",
              "  0.4921043771043772,\n",
              "  0.483775167785235,\n",
              "  0.4896644295302014,\n",
              "  0.4832274247491638,\n",
              "  0.48484745762711867,\n",
              "  0.5490709459459459,\n",
              "  0.5003535353535353,\n",
              "  0.5129661016949152,\n",
              "  0.49219798657718117,\n",
              "  0.5378327645051195,\n",
              "  0.4779729729729729,\n",
              "  0.47411564625850333,\n",
              "  0.4940572390572391,\n",
              "  0.49364864864864866,\n",
              "  0.4597118644067797,\n",
              "  0.5134060402684564,\n",
              "  0.530959595959596,\n",
              "  0.47491582491582496,\n",
              "  0.5435738255033558,\n",
              "  0.5170307167235495,\n",
              "  0.4791833333333333,\n",
              "  0.4808892617449665,\n",
              "  0.4864814814814815,\n",
              "  0.4998825503355705,\n",
              "  0.479244966442953,\n",
              "  0.488494983277592,\n",
              "  0.5027833333333334,\n",
              "  0.48364406779661023,\n",
              "  0.48166107382550344,\n",
              "  0.5112876254180602,\n",
              "  0.45554054054054055,\n",
              "  0.47943877551020403,\n",
              "  0.5497666666666667,\n",
              "  0.47780201342281875,\n",
              "  0.5080887372013652,\n",
              "  0.5002195945945946,\n",
              "  0.4717845117845117,\n",
              "  0.5295833333333333,\n",
              "  0.5096271186440677,\n",
              "  0.5192033898305084,\n",
              "  0.49757679180887376,\n",
              "  0.4624324324324324,\n",
              "  0.5498,\n",
              "  0.5120034246575342,\n",
              "  0.5147826086956522,\n",
              "  0.4791610738255033,\n",
              "  0.43626666666666664,\n",
              "  0.5093097643097644,\n",
              "  0.528561872909699,\n",
              "  0.5084333333333333,\n",
              "  0.5262080536912752,\n",
              "  0.4996644295302013,\n",
              "  0.4997306397306398,\n",
              "  0.5102356902356902,\n",
              "  0.488010033444816,\n",
              "  0.4599498327759198,\n",
              "  0.5294027303754266,\n",
              "  0.46868600682593853,\n",
              "  0.4799999999999999,\n",
              "  0.4964007092198582,\n",
              "  0.5128523489932886,\n",
              "  0.48702341137123745,\n",
              "  0.4916833333333333,\n",
              "  0.5046812080536913,\n",
              "  0.49868333333333337,\n",
              "  0.5452833333333332,\n",
              "  0.4733670033670034,\n",
              "  0.48444816053511713,\n",
              "  0.49326666666666674,\n",
              "  0.5385016835016835,\n",
              "  0.5054222972972974,\n",
              "  0.5032833333333333,\n",
              "  0.4812080536912752,\n",
              "  0.483843537414966,\n",
              "  0.5091778523489934,\n",
              "  0.4835953177257524,\n",
              "  0.48661016949152547,\n",
              "  0.5041724137931035,\n",
              "  0.5485284280936454,\n",
              "  0.4680267558528428,\n",
              "  0.45452218430034125,\n",
              "  0.5297466216216217,\n",
              "  0.5171833333333333,\n",
              "  0.4950511945392492,\n",
              "  0.44728187919463086,\n",
              "  0.5091414141414141,\n",
              "  0.5442976588628763,\n",
              "  0.5729863481228669,\n",
              "  0.5119666666666667,\n",
              "  0.4817736486486487,\n",
              "  0.472809364548495,\n",
              "  0.5383444816053512,\n",
              "  0.5324074074074074,\n",
              "  0.46912751677852355,\n",
              "  0.4935254237288136,\n",
              "  0.5114189189189189,\n",
              "  0.48821799307958474,\n",
              "  0.5465886287625419,\n",
              "  0.5151020408163265,\n",
              "  0.46121621621621617,\n",
              "  0.5015100671140941,\n",
              "  0.47798657718120807,\n",
              "  0.49603040540540544,\n",
              "  0.4659060402684563,\n",
              "  0.5025850340136054,\n",
              "  0.5086454849498327,\n",
              "  0.5056587837837838,\n",
              "  0.5120302013422818,\n",
              "  0.5378619528619529,\n",
              "  0.5187755102040815,\n",
              "  0.4978619528619529,\n",
              "  0.49146666666666666,\n",
              "  0.4538758389261745,\n",
              "  0.49535000000000007,\n",
              "  0.5270401337792642,\n",
              "  0.495,\n",
              "  0.4571833333333332,\n",
              "  0.4876610169491526,\n",
              "  0.469406779661017,\n",
              "  0.4840133779264214,\n",
              "  0.46015151515151514,\n",
              "  0.4847000000000001,\n",
              "  0.4749,\n",
              "  0.4476936026936027,\n",
              "  0.49145,\n",
              "  0.5100501672240803,\n",
              "  0.4730677966101695,\n",
              "  0.5082926829268293,\n",
              "  0.5887457627118644,\n",
              "  0.5025862068965518,\n",
              "  0.4950501672240802,\n",
              "  0.5075257731958763,\n",
              "  0.43354515050167225,\n",
              "  0.5239554794520548,\n",
              "  0.4851258992805756,\n",
              "  0.4760833333333333,\n",
              "  0.5179333333333334,\n",
              "  0.4761833333333333,\n",
              "  0.5164166666666667,\n",
              "  0.48981605351170565,\n",
              "  0.4942666666666667,\n",
              "  0.5380267558528429,\n",
              "  0.49445,\n",
              "  0.5383783783783783,\n",
              "  0.5077090301003344,\n",
              "  0.4877797202797203,\n",
              "  0.515,\n",
              "  0.510953177257525,\n",
              "  0.521271186440678,\n",
              "  0.5086409395973154,\n",
              "  0.48265202702702703,\n",
              "  0.5052666666666666,\n",
              "  0.48343959731543623,\n",
              "  0.5291919191919192,\n",
              "  0.5160570469798658,\n",
              "  0.5099833333333333,\n",
              "  0.49021812080536914,\n",
              "  0.4639166666666667,\n",
              "  0.47996655518394643,\n",
              "  0.5205387205387206,\n",
              "  0.47298986486486483,\n",
              "  0.45908304498269903,\n",
              "  0.5280833333333333,\n",
              "  0.49921404682274245,\n",
              "  0.4523244147157191,\n",
              "  0.5190604026845638,\n",
              "  0.5098154362416107,\n",
              "  0.5531879194630872,\n",
              "  0.5012116040955631,\n",
              "  0.50625,\n",
              "  0.47340604026845634,\n",
              "  0.4691551724137931,\n",
              "  0.5532166666666668,\n",
              "  0.5075000000000001,\n",
              "  0.52675,\n",
              "  0.4808862876254181,\n",
              "  0.5401700680272109,\n",
              "  0.5022833333333333,\n",
              "  0.55839590443686,\n",
              "  0.523227424749164,\n",
              "  0.4781166666666667,\n",
              "  0.510750853242321,\n",
              "  0.4535666666666666,\n",
              "  0.5095302013422819,\n",
              "  0.46248310810810805,\n",
              "  0.51421768707483,\n",
              "  0.47234006734006745,\n",
              "  0.4855333333333333,\n",
              "  0.49566666666666664,\n",
              "  0.42006711409395975,\n",
              "  0.48276666666666673,\n",
              "  0.47734797297297293,\n",
              "  0.51421768707483,\n",
              "  0.48856187290969905,\n",
              "  0.48972972972972967,\n",
              "  0.5220068027210885,\n",
              "  0.509910071942446,\n",
              "  0.4900333333333334,\n",
              "  0.4967508417508417,\n",
              "  0.48713286713286713,\n",
              "  0.5455872483221476,\n",
              "  0.5350168350168351,\n",
              "  0.5131270903010033,\n",
              "  0.5073489932885906,\n",
              "  0.5148154362416106,\n",
              "  0.5192281879194631,\n",
              "  0.5053355704697987,\n",
              "  0.5236026936026936,\n",
              "  0.4955183946488295,\n",
              "  0.4997306397306398,\n",
              "  0.5349826388888889,\n",
              "  0.4940436241610738,\n",
              "  0.47802013422818795,\n",
              "  0.5471644295302014,\n",
              "  0.4952380952380953,\n",
              "  0.48969899665551847,\n",
              "  0.4697306397306397,\n",
              "  0.5497000000000001,\n",
              "  0.4835690235690236,\n",
              "  0.4714141414141414,\n",
              "  0.5570875420875421,\n",
              "  0.5027380952380952,\n",
              "  0.5420735785953177,\n",
              "  0.4777854671280276,\n",
              "  0.5086499999999999,\n",
              "  0.51247491638796,\n",
              "  0.5127609427609429,\n",
              "  0.5172108843537415,\n",
              "  0.4877090301003345,\n",
              "  0.5220401337792642,\n",
              "  0.4905913978494624,\n",
              "  0.52660409556314,\n",
              "  0.48903010033444827,\n",
              "  0.4696801346801347,\n",
              "  0.5106166666666667,\n",
              "  0.48862244897959184,\n",
              "  0.4847324414715719,\n",
              "  0.4757070707070707,\n",
              "  0.5220608108108108,\n",
              "  0.4838666666666667,\n",
              "  0.5234448160535117,\n",
              "  0.493428093645485,\n",
              "  0.4942499999999999,\n",
              "  0.48988333333333334,\n",
              "  0.4949128919860627,\n",
              "  0.5175252525252525,\n",
              "  0.5010535117056856,\n",
              "  0.5047297297297296,\n",
              "  0.5143000000000001,\n",
              "  0.5085762711864407,\n",
              "  0.47899665551839476,\n",
              "  0.45981605351170574,\n",
              "  0.5227397260273973,\n",
              "  0.4887589928057554,\n",
              "  0.5083161512027492,\n",
              "  0.4930574324324324,\n",
              "  0.47832214765100667,\n",
              "  0.5581740614334472,\n",
              "  0.5104515050167224,\n",
              "  0.4913166666666666,\n",
              "  0.5166053511705686,\n",
              "  0.5256610169491526,\n",
              "  0.5179898648648649,\n",
              "  0.4938851351351351,\n",
              "  0.5089518900343643,\n",
              "  0.4895959595959596,\n",
              "  0.5101186440677966,\n",
              "  0.5112709030100334,\n",
              "  0.5386000000000001,\n",
              "  0.4738333333333333,\n",
              "  0.5038175675675676,\n",
              "  0.5211577181208054,\n",
              "  0.5309861591695502,\n",
              "  0.5185135135135135,\n",
              "  0.4679194630872483,\n",
              "  0.535989932885906,\n",
              "  0.49213333333333337,\n",
              "  0.49325423728813567,\n",
              "  0.5043333333333334,\n",
              "  0.4852203389830509,\n",
              "  0.511969696969697,\n",
              "  0.4582659932659933,\n",
              "  0.5077609427609427,\n",
              "  0.5024166666666667,\n",
              "  0.4623458904109589,\n",
              "  0.4966949152542373,\n",
              "  0.5377796610169491,\n",
              "  0.49615254237288137,\n",
              "  0.4587710437710438,\n",
              "  0.48048333333333326,\n",
              "  0.5247166666666667,\n",
              "  0.5343624161073826,\n",
              "  0.5016275167785236,\n",
              "  0.44718644067796604,\n",
              "  0.4682441471571906,\n",
              "  0.5514013840830451,\n",
              "  0.5364432989690723,\n",
              "  ...]}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_trainer._eval_manager = ImplicitEvalManager(evaluators=[auc_evaluator])\n",
        "model_trainer._num_negatives = 200\n",
        "model_trainer._exclude_positives([train_dataset, test_dataset_pos, test_dataset_neg])\n",
        "model_trainer._sample_negatives(seed=10)\n",
        "\n",
        "model_trainer._eval_save_prefix = folder_name+\"cml-KuaiRec-test-pos-biased\"\n",
        "model_trainer._evaluate_partial(test_dataset_pos)\n",
        "\n",
        "model_trainer._eval_save_prefix = folder_name+\"cml-KuaiRec-test-neg-biased\"\n",
        "model_trainer._evaluate_partial(test_dataset_neg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'auc': 0.8519140061717886, 'recall': 0.5378264210310162}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eq(folder_name+\"cml-KuaiRec-test-pos-biased_evaluate_partial.pickle\", folder_name+\"cml-KuaiRec-test-neg-biased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", gamma=1.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'auc': 0.8469662794391772, 'recall': 0.5216021101398857}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eq(folder_name+\"cml-KuaiRec-test-pos-biased_evaluate_partial.pickle\", folder_name+\"cml-KuaiRec-test-neg-biased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", gamma=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'auc': 0.8406755337419534, 'recall': 0.4996077803018351}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eq(folder_name+\"cml-KuaiRec-test-pos-biased_evaluate_partial.pickle\", folder_name+\"cml-KuaiRec-test-neg-biased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", gamma=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'auc': 0.9148412723461649, 'recall': 0.11669449955279332}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "aoa(folder_name+\"cml-KuaiRec-test-pos-biased_evaluate_partial.pickle\", folder_name+\"cml-KuaiRec-test-neg-biased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "RecSysEvaluation",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
