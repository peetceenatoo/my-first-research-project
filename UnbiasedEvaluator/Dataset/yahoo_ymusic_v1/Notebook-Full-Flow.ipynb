{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **IMPORT LIBS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from openrec.tf1.legacy import ImplicitModelTrainer\n",
    "from openrec.tf1.legacy.utils.evaluators import ImplicitEvalManager\n",
    "from openrec.tf1.legacy.utils import ImplicitDataset\n",
    "from openrec.tf1.legacy.recommenders import CML, BPR, PMF\n",
    "from openrec.tf1.legacy.utils.evaluators import AUC\n",
    "from openrec.tf1.legacy.utils.samplers import PairwiseSampler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sps\n",
    "import os\n",
    "import pickle\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **GENERATE THE DATASET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2384795\n",
    "np.random.seed(seed=seed)\n",
    "\n",
    "output_name = f\"./generated_data/\"\n",
    "folder_name = f\"./original_files/\"\n",
    "\n",
    "if os.path.exists(output_name) == False:\n",
    "    os.makedirs(output_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>SongID</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>83</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>153</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>170</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>184</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>194</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  SongID  Rating\n",
       "0       1      14       5\n",
       "1       1      35       1\n",
       "2       1      46       1\n",
       "3       1      83       1\n",
       "4       1      93       1\n",
       "5       1      94       1\n",
       "6       1     153       5\n",
       "7       1     170       4\n",
       "8       1     184       5\n",
       "9       1     194       5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'ydata-ymusic-rating-study-v1_0-train.txt'\n",
    "\n",
    "# Load the training set into a DataFrame\n",
    "df_train = pd.read_csv(folder_name+file_path, sep='\\t',names=[\"UserID\",\"SongID\",\"Rating\"], header=None)  # sep='\\t' for tab-separated values\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to implicit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"We treat items rated greater than or equal to 4 as relevant, and others as irrelevant, as suggested by prior literature.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>SongID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>ImplicitRating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>83</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>153</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>170</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>184</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>194</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  SongID  Rating  ImplicitRating\n",
       "0       1      14       5               1\n",
       "1       1      35       1               0\n",
       "2       1      46       1               0\n",
       "3       1      83       1               0\n",
       "4       1      93       1               0\n",
       "5       1      94       1               0\n",
       "6       1     153       5               1\n",
       "7       1     170       4               1\n",
       "8       1     184       5               1\n",
       "9       1     194       5               1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "POSITIVE_THRESHOLD = 4\n",
    "df_train['ImplicitRating'] = np.where(df_train['Rating'] >= POSITIVE_THRESHOLD, 1, 0)\n",
    "\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the number of users and items in the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The training set contains 300K ratings given by 15.4K users against 1K songs through natural interactions.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 15400)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_user = df_train[\"UserID\"].min()\n",
    "max_user = df_train[\"UserID\"].max()\n",
    "\n",
    "min_item = df_train[\"SongID\"].min()\n",
    "max_item = df_train[\"SongID\"].max()\n",
    "\n",
    "max_item, max_user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **GET UNBIASED TESTSET**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the unbiased testset and convert it to implicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>SongID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>ImplicitRating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>138</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>141</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>177</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>268</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>511</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>587</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>772</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>941</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  SongID  Rating  ImplicitRating\n",
       "0       1      49       1               0\n",
       "1       1     126       1               0\n",
       "2       1     138       1               0\n",
       "3       1     141       1               0\n",
       "4       1     177       1               0\n",
       "5       1     268       3               0\n",
       "6       1     511       1               0\n",
       "7       1     587       1               0\n",
       "8       1     772       5               1\n",
       "9       1     941       1               0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = folder_name + 'ydata-ymusic-rating-study-v1_0-test.txt'\n",
    "df_test = pd.read_csv(file_path, sep='\\t',names=[\"UserID\",\"SongID\",\"Rating\"], header=None)  # sep='\\t' for tab-separated values\n",
    "df_test['ImplicitRating'] = np.where(df_test['Rating'] >= POSITIVE_THRESHOLD, 1, 0)\n",
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the number of users and items in the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The testing set is collected by asking a subset of 5.4K users to rate 10 randomly selected songs.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5400, 1000, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[\"UserID\"].max(), df_test[\"SongID\"].max(), int(df_test.shape[0]/df_test[\"UserID\"].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter unbiased testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"We filter the testing set by retaining users who have at least a relevant and an irrelevant song in the testing set and two relevant songs in the training set.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select users with at least an irrelevant song in the unbiased testset\n",
    "usersWithNegativeInteractionInTest = df_test[df_test[\"ImplicitRating\"] == 0][\"UserID\"].unique()\n",
    "\n",
    "# Select UserID of users with at least a relevant song in testset\n",
    "usersWithPositiveInteractionInTest = df_test[df_test[\"ImplicitRating\"] == 1][\"UserID\"].unique()\n",
    "\n",
    "# Select UserID of users with at least two relevant song in trainset\n",
    "usersWithTwoPositiveInteractions = df_train[df_train[\"ImplicitRating\"] == 1].groupby(\"UserID\").filter(lambda x: len(x) >= 2)['UserID'].unique()\n",
    "\n",
    "# Compute the intersection\n",
    "set1 = set(usersWithNegativeInteractionInTest)\n",
    "set2 = set(usersWithPositiveInteractionInTest)\n",
    "set3 = set(usersWithTwoPositiveInteractions)\n",
    "valid_users_testset = set1 & set2 & set3\n",
    "\n",
    "# Filter the testset\n",
    "df_test_filtered = df_test[df_test[\"UserID\"].isin(valid_users_testset)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"2296 users satisfy these requirements.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2296"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_users_testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape the unbiased test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the dataframe, for each row where ImplicitRating is 1, append [userID, itemID] to unbiased_pos_test_set\n",
    "# and for each row where ImplicitRating is 0, append [userID, itemID] to unbiased_neg_test_set\n",
    "\n",
    "unbiased_pos_test_set = df_test_filtered[df_test_filtered[\"ImplicitRating\"] == 1][[\"UserID\", \"SongID\"]].values\n",
    "unbiased_neg_test_set = df_test_filtered[df_test_filtered[\"ImplicitRating\"] == 0][[\"UserID\", \"SongID\"]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save unbiased test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbiased_pos_test_set_df = pd.DataFrame(unbiased_pos_test_set)\n",
    "unbiased_neg_test_set_df = pd.DataFrame(unbiased_neg_test_set)\n",
    "\n",
    "unbiased_pos_test_set_df.columns = [\"user_id\",\"item_id\"]\n",
    "unbiased_neg_test_set_df.columns = [\"user_id\",\"item_id\"]\n",
    "\n",
    "structured_data_pos_test_set_unbiased = unbiased_pos_test_set_df.to_records(index=False)\n",
    "structured_data_neg_test_set_unbiased = unbiased_neg_test_set_df.to_records(index=False)\n",
    "\n",
    "np.save(output_name + \"unbiased-test_arr_pos.npy\", structured_data_pos_test_set_unbiased)\n",
    "np.save(output_name + \"unbiased-test_arr_neg.npy\", structured_data_neg_test_set_unbiased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **GET BIASED TESTSET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>SongID</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>83</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>153</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>170</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>184</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>194</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  SongID  Rating\n",
       "0       1      14       5\n",
       "1       1      35       1\n",
       "2       1      46       1\n",
       "3       1      83       1\n",
       "4       1      93       1\n",
       "5       1      94       1\n",
       "6       1     153       5\n",
       "7       1     170       4\n",
       "8       1     184       5\n",
       "9       1     194       5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'ydata-ymusic-rating-study-v1_0-train.txt'\n",
    "\n",
    "# Load the training set into a DataFrame\n",
    "df_train = pd.read_csv(folder_name+file_path, sep='\\t',names=[\"UserID\",\"SongID\",\"Rating\"], header=None)  # sep='\\t' for tab-separated values\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['ImplicitRating'] = np.where(df_train['Rating'] >= POSITIVE_THRESHOLD, 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[df_train[\"UserID\"].isin(valid_users_testset)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 58799 entries, 0 to 129178\n",
      "Data columns (total 4 columns):\n",
      " #   Column          Non-Null Count  Dtype\n",
      "---  ------          --------------  -----\n",
      " 0   UserID          58799 non-null  int64\n",
      " 1   SongID          58799 non-null  int64\n",
      " 2   Rating          58799 non-null  int64\n",
      " 3   ImplicitRating  58799 non-null  int64\n",
      "dtypes: int64(4)\n",
      "memory usage: 2.2 MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the biased test set and shape it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"We additionally held out a biased testing set (biased-testing) from the training set by randomly sampling 300 songs for each user.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompute, for each user, the list of songs with a relevant rating\n",
    "user_positive_ratings = df_train[df_train[\"ImplicitRating\"] == 1].groupby(\"UserID\")[\"SongID\"].apply(set)\n",
    "\n",
    "# Initialize the range of indexes for the items\n",
    "items_ids = np.arange(min_item, max_item + 1)\n",
    "# Set the number of songs for each user\n",
    "SONGS_FOR_BIASED_TEST = 300\n",
    "\n",
    "#IPOTESI MAN\n",
    "\n",
    "pos_test_set = []\n",
    "neg_test_set = []\n",
    "\n",
    "for user_id in valid_users_testset:\n",
    "    np.random.shuffle(items_ids)\n",
    "    test_items = set(items_ids[-SONGS_FOR_BIASED_TEST:])\n",
    "    pos_ids = user_positive_ratings.get(user_id, set()) & test_items\n",
    "\n",
    "    #set those to 0 so that they will no longer be used in training set\n",
    "    df_train.loc[(df_train['SongID'].isin(pos_ids)) & (df_train['UserID'] == user_id), 'ImplicitRating'] = 0\n",
    "\n",
    "    for id in test_items:\n",
    "        if id in pos_ids:\n",
    "            pos_test_set.append([user_id, id])\n",
    "        else:\n",
    "            neg_test_set.append([user_id, id])\n",
    "\n",
    "pos_test_set = np.array(pos_test_set)\n",
    "neg_test_set = np.array(neg_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the biased test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_test_set_df = pd.DataFrame(pos_test_set)\n",
    "neg_test_set_df = pd.DataFrame(neg_test_set)\n",
    "\n",
    "pos_test_set_df.columns = [\"user_id\",\"item_id\"]\n",
    "neg_test_set_df.columns = [\"user_id\",\"item_id\"]\n",
    "\n",
    "structured_data_pos_test_set = pos_test_set_df.to_records(index=False)\n",
    "structured_data_neg_test_set = neg_test_set_df.to_records(index=False)\n",
    "\n",
    "np.save(output_name + \"biased-test_arr_pos.npy\", structured_data_pos_test_set)\n",
    "np.save(output_name + \"biased-test_arr_neg.npy\", structured_data_neg_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **STORE TRAINSET**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take couples user-item filtering out the irrelevant ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only take the couples (user, item) with relevant rating\n",
    "new_df = df_train[df_train['ImplicitRating'] != 0]\n",
    "new_df = new_df.drop(columns=['Rating', 'ImplicitRating'])\n",
    "\n",
    "# Define a dictionary for renaming columns\n",
    "rename_dict = {\n",
    "    'UserID': 'user_id',\n",
    "    'SongID': 'item_id'\n",
    "}\n",
    "\n",
    "# Rename the columns\n",
    "new_df = new_df.rename(columns=rename_dict)\n",
    "\n",
    "# Convert the DataFrame to a structured array\n",
    "structured_data = new_df.to_records(index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = structured_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(output_name + \"training_arr.npy\", train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MODEL CHOICE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = dict()\n",
    "raw_data['train_data'] = np.load(output_name + \"training_arr.npy\")\n",
    "raw_data['max_user'] = 15401\n",
    "raw_data['max_item'] = 1001\n",
    "batch_size = 8000\n",
    "test_batch_size = 1000\n",
    "display_itr = 1000\n",
    "\n",
    "train_dataset = ImplicitDataset(raw_data['train_data'], raw_data['max_user'], raw_data['max_item'], name='Train')\n",
    "\n",
    "MODEL_CLASS = CML\n",
    "MODEL_PREFIX = \"cml\"\n",
    "DATASET_NAME = \"yahoo\"\n",
    "OUTPUT_FOLDER = output_name\n",
    "OUTPUT_PATH = OUTPUT_FOLDER + MODEL_PREFIX + \"-\" + DATASET_NAME + \"/\"\n",
    "OUTPUT_PREFIX = str(OUTPUT_PATH) + str(MODEL_PREFIX) + \"-\" + str(DATASET_NAME)\n",
    "\n",
    "\n",
    "if os.path.exists(OUTPUT_PATH) == False:\n",
    "    os.makedirs(OUTPUT_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TRAIN THE MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:391: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:31: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:41: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:43: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:33: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/interactions/pairwise_eu_dist.py:71: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:596: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:75: The name tf.scatter_update is deprecated. Please use tf.compat.v1.scatter_update instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:144: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:365: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:148: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-24 10:22:51.057341: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
      "2024-04-24 10:22:51.059675: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 4192050000 Hz\n",
      "2024-04-24 10:22:51.060090: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x558f3d981ea0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2024-04-24 10:22:51.060105: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    }
   ],
   "source": [
    "# Avoid tensorflow using cached embeddings\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "\n",
    "model = MODEL_CLASS(batch_size=batch_size, max_user=train_dataset.max_user(), max_item=train_dataset.max_item(), \n",
    "    dim_embed=50, l2_reg=0.001, opt='Adam', sess_config=None)\n",
    "sampler = PairwiseSampler(batch_size=batch_size, dataset=train_dataset, num_process=4)\n",
    "model_trainer = ImplicitModelTrainer(batch_size=batch_size, test_batch_size=test_batch_size,\n",
    "                                     train_dataset=train_dataset, model=model, sampler=sampler,\n",
    "                                     eval_save_prefix=OUTPUT_PATH + DATASET_NAME,\n",
    "                                     item_serving_size=500)\n",
    "auc_evaluator = AUC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Start training with FULL evaluation ==\n",
      "[Itr 100] Finished\n",
      "[Itr 200] Finished\n",
      "[Itr 300] Finished\n",
      "[Itr 400] Finished\n",
      "[Itr 500] Finished\n",
      "[Itr 600] Finished\n",
      "[Itr 700] Finished\n",
      "[Itr 800] Finished\n",
      "[Itr 900] Finished\n",
      "[Itr 1000] Finished\n",
      "INFO:tensorflow:./generated_data/cml-yahoo/yahoo-1000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 1000] loss: 1753.353313\n",
      "[Itr 1100] Finished\n",
      "[Itr 1200] Finished\n",
      "[Itr 1300] Finished\n",
      "[Itr 1400] Finished\n",
      "[Itr 1500] Finished\n",
      "[Itr 1600] Finished\n",
      "[Itr 1700] Finished\n",
      "[Itr 1800] Finished\n",
      "[Itr 1900] Finished\n",
      "[Itr 2000] Finished\n",
      "INFO:tensorflow:./generated_data/cml-yahoo/yahoo-2000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 2000] loss: 649.723550\n",
      "[Itr 2100] Finished\n",
      "[Itr 2200] Finished\n",
      "[Itr 2300] Finished\n",
      "[Itr 2400] Finished\n",
      "[Itr 2500] Finished\n",
      "[Itr 2600] Finished\n",
      "[Itr 2700] Finished\n",
      "[Itr 2800] Finished\n",
      "[Itr 2900] Finished\n",
      "[Itr 3000] Finished\n",
      "INFO:tensorflow:./generated_data/cml-yahoo/yahoo-3000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 3000] loss: 575.896943\n",
      "[Itr 3100] Finished\n",
      "[Itr 3200] Finished\n",
      "[Itr 3300] Finished\n",
      "[Itr 3400] Finished\n",
      "[Itr 3500] Finished\n",
      "[Itr 3600] Finished\n",
      "[Itr 3700] Finished\n",
      "[Itr 3800] Finished\n",
      "[Itr 3900] Finished\n",
      "[Itr 4000] Finished\n",
      "INFO:tensorflow:./generated_data/cml-yahoo/yahoo-4000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 4000] loss: 547.989919\n",
      "[Itr 4100] Finished\n",
      "[Itr 4200] Finished\n",
      "[Itr 4300] Finished\n",
      "[Itr 4400] Finished\n",
      "[Itr 4500] Finished\n",
      "[Itr 4600] Finished\n",
      "[Itr 4700] Finished\n",
      "[Itr 4800] Finished\n",
      "[Itr 4900] Finished\n",
      "[Itr 5000] Finished\n",
      "INFO:tensorflow:./generated_data/cml-yahoo/yahoo-5000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 5000] loss: 534.783710\n",
      "[Itr 5100] Finished\n",
      "[Itr 5200] Finished\n",
      "[Itr 5300] Finished\n",
      "[Itr 5400] Finished\n",
      "[Itr 5500] Finished\n",
      "[Itr 5600] Finished\n",
      "[Itr 5700] Finished\n",
      "[Itr 5800] Finished\n",
      "[Itr 5900] Finished\n",
      "[Itr 6000] Finished\n",
      "INFO:tensorflow:./generated_data/cml-yahoo/yahoo-6000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 6000] loss: 527.181572\n",
      "[Itr 6100] Finished\n",
      "[Itr 6200] Finished\n",
      "[Itr 6300] Finished\n",
      "[Itr 6400] Finished\n",
      "[Itr 6500] Finished\n",
      "[Itr 6600] Finished\n",
      "[Itr 6700] Finished\n",
      "[Itr 6800] Finished\n",
      "[Itr 6900] Finished\n",
      "[Itr 7000] Finished\n",
      "INFO:tensorflow:./generated_data/cml-yahoo/yahoo-7000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 7000] loss: 524.655328\n",
      "[Itr 7100] Finished\n",
      "[Itr 7200] Finished\n",
      "[Itr 7300] Finished\n",
      "[Itr 7400] Finished\n",
      "[Itr 7500] Finished\n",
      "[Itr 7600] Finished\n",
      "[Itr 7700] Finished\n",
      "[Itr 7800] Finished\n",
      "[Itr 7900] Finished\n",
      "[Itr 8000] Finished\n",
      "INFO:tensorflow:./generated_data/cml-yahoo/yahoo-8000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 8000] loss: 519.769809\n",
      "[Itr 8100] Finished\n",
      "[Itr 8200] Finished\n",
      "[Itr 8300] Finished\n",
      "[Itr 8400] Finished\n",
      "[Itr 8500] Finished\n",
      "[Itr 8600] Finished\n",
      "[Itr 8700] Finished\n",
      "[Itr 8800] Finished\n",
      "[Itr 8900] Finished\n",
      "[Itr 9000] Finished\n",
      "INFO:tensorflow:./generated_data/cml-yahoo/yahoo-9000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 9000] loss: 519.040299\n",
      "[Itr 9100] Finished\n",
      "[Itr 9200] Finished\n",
      "[Itr 9300] Finished\n",
      "[Itr 9400] Finished\n",
      "[Itr 9500] Finished\n",
      "[Itr 9600] Finished\n",
      "[Itr 9700] Finished\n",
      "[Itr 9800] Finished\n",
      "[Itr 9900] Finished\n",
      "[Itr 10000] Finished\n",
      "INFO:tensorflow:./generated_data/cml-yahoo/yahoo-10000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 10000] loss: 517.383656\n"
     ]
    }
   ],
   "source": [
    "model_trainer.train(num_itr=10001, display_itr=display_itr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:./generated_data/cml-yahoo/ is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "model.save(OUTPUT_PATH,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DEFINING FUNCTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eq(infilename, infilename_neg, trainfilename, gamma=-1.0, K=1):\n",
    "    infile = open(infilename, 'rb')\n",
    "    infile_neg = open(infilename_neg, 'rb')\n",
    "    P = pickle.load(infile)\n",
    "    infile.close()\n",
    "    P_neg = pickle.load(infile_neg)\n",
    "    infile_neg.close()\n",
    "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
    "    #\n",
    "    for theuser in P[\"users\"]:\n",
    "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "    #\n",
    "    Zui = dict()\n",
    "    Ni = dict()\n",
    "    # fill in dictionary Ni\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "    del trainset\n",
    "\n",
    "    # count #users with non-zero item frequencies\n",
    "    nonzero_user_count = 0\n",
    "    for theuser in P[\"users\"]:\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for pos_item in pos_items:\n",
    "            if pos_item in Ni:\n",
    "                nonzero_user_count += 1\n",
    "                break\n",
    "    # fill in dictionary Zui\n",
    "    for theuser in P[\"users\"]:\n",
    "        all_scores = np.array(P[\"results\"][theuser])\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for i, pos_item in enumerate(pos_items):\n",
    "            pos_score = pos_scores[i]\n",
    "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
    "    # calculate per-user scores\n",
    "    sum_user_auc = 0.0\n",
    "    sum_user_recall = 0.0\n",
    "\n",
    "    for theuser in P[\"users\"]:\n",
    "        numerator_auc = 0.0\n",
    "        numerator_recall = 0.0\n",
    "        denominator = 0.0\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            if theitem not in Ni:\n",
    "                continue\n",
    "            pui = np.power(Ni[theitem], (gamma + 1) / 2.0)\n",
    "\n",
    "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser])) / pui\n",
    "            # Calcolo il Recall a 1, vedi nota 6 paper\n",
    "            if Zui[(theuser, theitem)] < K:\n",
    "                numerator_recall += 1.0 / pui\n",
    "            denominator += 1 / pui\n",
    "                \n",
    "\n",
    "        if denominator > 0:\n",
    "            sum_user_auc += numerator_auc / denominator\n",
    "            sum_user_recall += numerator_recall / denominator \n",
    "\n",
    "    return {\n",
    "        \"auc\"       : sum_user_auc / nonzero_user_count,\n",
    "        \"recall\"    : sum_user_recall / nonzero_user_count\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aoa(infilename, infilename_neg, trainfilename, K=1):\n",
    "    infile = open(infilename, 'rb')\n",
    "    infile_neg = open(infilename_neg, 'rb')\n",
    "    P = pickle.load(infile)\n",
    "    infile.close()\n",
    "    P_neg = pickle.load(infile_neg)\n",
    "    infile_neg.close()\n",
    "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
    "    #\n",
    "    for theuser in P[\"users\"]:\n",
    "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "    #\n",
    "    Zui = dict()\n",
    "    Ni = dict()\n",
    "    # fill in dictionary Ni\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "    del trainset\n",
    "    # count #users with non-zero item frequencies\n",
    "    nonzero_user_count = 0\n",
    "    for theuser in P[\"users\"]:\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for pos_item in pos_items:\n",
    "            if pos_item in Ni:\n",
    "                nonzero_user_count += 1\n",
    "                break\n",
    "    # fill in dictionary Zui\n",
    "    for theuser in P[\"users\"]:\n",
    "        all_scores = np.array(P[\"results\"][theuser])\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for i, pos_item in enumerate(pos_items):\n",
    "            pos_score = pos_scores[i]\n",
    "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
    "    # calculate per-user scores\n",
    "    sum_user_auc = 0.0\n",
    "    sum_user_recall = 0.0\n",
    "    for theuser in P[\"users\"]:\n",
    "        numerator_auc = 0.0\n",
    "        numerator_recall = 0.0\n",
    "        denominator = 0.0\n",
    "\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            if theitem not in Ni:\n",
    "                continue\n",
    "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser]))\n",
    "            # Calcolo il Recall a 30, vedi nota 6 paper\n",
    "            if Zui[(theuser, theitem)] < K:\n",
    "                numerator_recall += 1.0\n",
    "            denominator += 1 \n",
    "\n",
    "        if denominator > 0:\n",
    "            sum_user_auc += numerator_auc / denominator\n",
    "            sum_user_recall += numerator_recall / denominator\n",
    "\n",
    "    return {\n",
    "        \"auc\"       : sum_user_auc / nonzero_user_count,\n",
    "        \"recall\"    : sum_user_recall / nonzero_user_count\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified(infilename, infilename_neg, trainfilename, gamma=1.0, K=30, partition=10):\n",
    "\n",
    "    # Read pickles\n",
    "    infile = open(infilename, 'rb')\n",
    "    infile_neg = open(infilename_neg, 'rb')\n",
    "    P = pickle.load(infile)\n",
    "    infile.close()\n",
    "    P_neg = pickle.load(infile_neg)\n",
    "    infile_neg.close()\n",
    "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
    "\n",
    "    # Merge P and P_neg\n",
    "    for theuser in P[\"users\"]:\n",
    "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "\n",
    "    Zui = dict()\n",
    "    Ni = dict()\n",
    "\n",
    "    # Compute frequencies of items in the training set\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "    del trainset\n",
    "\n",
    "    # Compute recommendations for each user\n",
    "    for theuser in P[\"users\"]:\n",
    "        all_scores = np.array(P[\"results\"][theuser])\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for i, pos_item in enumerate(pos_items):\n",
    "            pos_score = pos_scores[i]\n",
    "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
    "\n",
    "    pui = dict()\n",
    "    w = dict()\n",
    "\n",
    "    # Compute dictionary of propensity scores\n",
    "    for theuser in P[\"users\"]:\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            if theitem not in Ni:\n",
    "                continue\n",
    "            if theitem in pui:\n",
    "                continue\n",
    "            pui[theitem] = np.power(Ni[theitem], (gamma + 1) / 2.0)\n",
    "\n",
    "    # Take the list of items (not tuples) in pui sorted by value\n",
    "    items_sorted_by_value = sorted(pui, key=pui.get, reverse=True)\n",
    "\n",
    "    # Compute linspace between the pui[0] and pui[-1] \n",
    "\n",
    "    # Maybe try to split the logspace instead of the linspace?\n",
    "    # logspace = np.logspace(pui[items_sorted_by_value[0]], pui[items_sorted_by_value[-1]], partition+1)\n",
    "\n",
    "    linspace = np.linspace(pui[items_sorted_by_value[0]], pui[items_sorted_by_value[-1]], partition+1)\n",
    "   \n",
    "    # Compute dictionary w, that is, for each item, assigns the average of the puis in the partition it belongs to\n",
    "    i=0\n",
    "    j = 0\n",
    "    while i < len(items_sorted_by_value):\n",
    "                            \n",
    "        avg = 0\n",
    "        start = i\n",
    "        end = i\n",
    "    \n",
    "        while i < len(items_sorted_by_value) and pui[items_sorted_by_value[i]] >= linspace[j+1]:\n",
    "            avg += 1.0 / pui[items_sorted_by_value[i]]\n",
    "            end = i\n",
    "            i += 1\n",
    "        \n",
    "        # Is the average the only good choice? even with the log space split?\n",
    "        avg = avg / (end - start + 1)\n",
    "\n",
    "        for k in range(start, end+1):\n",
    "            w[items_sorted_by_value[k]] = avg\n",
    "\n",
    "\n",
    "        j += 1\n",
    "\n",
    "    nonzero_user_count = 0\n",
    "    sum_user_auc = 0.0\n",
    "    sum_user_recall = 0.0\n",
    "\n",
    "    # Compute score with AUC and compute Recall\n",
    "    for theuser in P[\"users\"]:\n",
    "        numerator_auc = 0.0\n",
    "        numerator_recall = 0.0\n",
    "        denominator = 0.0\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            # Skip items with null frequency\n",
    "            if  theitem not in Ni:\n",
    "                continue\n",
    "            # Add things to be summed for each item\n",
    "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser])) * w[theitem]\n",
    "            # Add things for recall\n",
    "            if Zui[(theuser, theitem)] < K:\n",
    "                numerator_recall += 1.0 * w[theitem] #spetta\n",
    "            # Increment denominator that the sum must be divided by \n",
    "            denominator += 1 / pui[theitem]\n",
    "\n",
    "\n",
    "        # If there was at least one item for the user, count the user and sum the results\n",
    "        if denominator > 0:\n",
    "            nonzero_user_count += 1\n",
    "            sum_user_auc += numerator_auc / denominator\n",
    "            sum_user_recall += numerator_recall / denominator \n",
    "\n",
    "    return {\n",
    "        \"auc\"       : sum_user_auc / nonzero_user_count, \n",
    "        \"recall\"    : sum_user_recall / nonzero_user_count\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_logspace(infilename, infilename_neg, trainfilename, gamma=1.0, K=30, partition=10):\n",
    "\n",
    "    # Read pickles\n",
    "    infile = open(infilename, 'rb')\n",
    "    infile_neg = open(infilename_neg, 'rb')\n",
    "    P = pickle.load(infile)\n",
    "    infile.close()\n",
    "    P_neg = pickle.load(infile_neg)\n",
    "    infile_neg.close()\n",
    "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
    "\n",
    "    # Merge P and P_neg\n",
    "    for theuser in P[\"users\"]:\n",
    "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "\n",
    "    Zui = dict()\n",
    "    Ni = dict()\n",
    "\n",
    "    # Compute frequencies of items in the training set\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "    del trainset\n",
    "\n",
    "    # Compute recommendations for each user\n",
    "    for theuser in P[\"users\"]:\n",
    "        all_scores = np.array(P[\"results\"][theuser])\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for i, pos_item in enumerate(pos_items):\n",
    "            pos_score = pos_scores[i]\n",
    "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
    "\n",
    "    pui = dict()\n",
    "    w = dict()\n",
    "\n",
    "    # Compute dictionary of propensity scores\n",
    "    for theuser in P[\"users\"]:\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            if theitem not in Ni:\n",
    "                continue\n",
    "            if theitem in pui:\n",
    "                continue\n",
    "            pui[theitem] = np.power(Ni[theitem], (gamma + 1) / 2.0)\n",
    "\n",
    "    # Take the list of items (not tuples) in pui sorted by value\n",
    "    items_sorted_by_value = sorted(pui, key=pui.get, reverse=True)\n",
    "\n",
    "    # Compute linspace between the pui[0] and pui[-1] \n",
    "\n",
    "    # Maybe try to split the logspace instead of the linspace?\n",
    "    logspace = np.logspace(pui[items_sorted_by_value[0]], pui[items_sorted_by_value[-1]], partition+1)\n",
    "   \n",
    "    # Compute dictionary w, that is, for each item, assigns the average of the puis in the partition it belongs to\n",
    "    i=0\n",
    "    j = 0\n",
    "    while i < len(items_sorted_by_value):\n",
    "                            \n",
    "        avg = 0\n",
    "        start = i\n",
    "        end = i\n",
    "    \n",
    "        while i < len(items_sorted_by_value) and pui[items_sorted_by_value[i]] >= logspace[j+1]:\n",
    "            avg += 1.0 / pui[items_sorted_by_value[i]]\n",
    "            end = i\n",
    "            i += 1\n",
    "        \n",
    "        # Is the average the only good choice? even with the log space split?\n",
    "        avg = avg / (end - start + 1)\n",
    "\n",
    "        for k in range(start, end+1):\n",
    "            w[items_sorted_by_value[k]] = avg\n",
    "\n",
    "\n",
    "        j += 1\n",
    "\n",
    "    nonzero_user_count = 0\n",
    "    sum_user_auc = 0.0\n",
    "    sum_user_recall = 0.0\n",
    "\n",
    "    # Compute score with AUC and compute Recall\n",
    "    for theuser in P[\"users\"]:\n",
    "        numerator_auc = 0.0\n",
    "        numerator_recall = 0.0\n",
    "        denominator = 0.0\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            # Skip items with null frequency\n",
    "            if  theitem not in Ni:\n",
    "                continue\n",
    "            # Add things to be summed for each item\n",
    "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser])) * w[theitem]\n",
    "            # Add things for recall\n",
    "            if Zui[(theuser, theitem)] < K:\n",
    "                numerator_recall += 1.0 * w[theitem] #spetta\n",
    "            # Increment denominator that the sum must be divided by \n",
    "            denominator += 1 / pui[theitem]\n",
    "\n",
    "\n",
    "        # If there was at least one item for the user, count the user and sum the results\n",
    "        if denominator > 0:\n",
    "            nonzero_user_count += 1\n",
    "            sum_user_auc += numerator_auc / denominator\n",
    "            sum_user_recall += numerator_recall / denominator \n",
    "\n",
    "    return {\n",
    "        \"auc\"       : sum_user_auc / nonzero_user_count, \n",
    "        \"recall\"    : sum_user_recall / nonzero_user_count\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_2(infilename, infilename_neg, trainfilename, gamma=1.0, K=30, partition=10):\n",
    "\n",
    "    # Read pickles\n",
    "    infile = open(infilename, 'rb')\n",
    "    infile_neg = open(infilename_neg, 'rb')\n",
    "    P = pickle.load(infile)\n",
    "    infile.close()\n",
    "    P_neg = pickle.load(infile_neg)\n",
    "    infile_neg.close()\n",
    "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
    "\n",
    "    # Merge P and P_neg\n",
    "    for theuser in P[\"users\"]:\n",
    "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "\n",
    "    Zui = dict()\n",
    "    Ni = dict()\n",
    "\n",
    "    # Compute frequencies of items in the training set\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "    del trainset\n",
    "\n",
    "    # Compute recommendations for each user\n",
    "    for theuser in P[\"users\"]:\n",
    "        all_scores = np.array(P[\"results\"][theuser])\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for i, pos_item in enumerate(pos_items):\n",
    "            pos_score = pos_scores[i]\n",
    "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
    "\n",
    "    pui = dict()\n",
    "    w = dict()\n",
    "\n",
    "    # Compute dictionary of propensity scores\n",
    "    for theuser in P[\"users\"]:\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            if theitem not in Ni:\n",
    "                continue\n",
    "            if theitem in pui:\n",
    "                continue\n",
    "            pui[theitem] = np.power(Ni[theitem], (gamma + 1) / 2.0)\n",
    "\n",
    "    # Take the list of items (not tuples) in pui sorted by value\n",
    "    items_sorted_by_value = sorted(pui, key=pui.get, reverse=True)\n",
    "\n",
    "    # Compute linspace between the 0 to len(item_sorted...)\n",
    "    linspace = np.linspace(0, len(items_sorted_by_value), partition+1)\n",
    "   \n",
    "    # Compute dictionary w, that is, for each item, assigns the average of the puis in the partition it belongs to\n",
    "    i=0\n",
    "    j = 0\n",
    "    while i < len(items_sorted_by_value):\n",
    "                            \n",
    "        avg = 0\n",
    "        start = i\n",
    "        end = i\n",
    "    \n",
    "        while i < len(items_sorted_by_value) and i < linspace[j+1]:\n",
    "            avg += 1.0 / pui[items_sorted_by_value[i]]\n",
    "            end = i\n",
    "            i += 1\n",
    "        \n",
    "        avg = avg / (end - start + 1)\n",
    "\n",
    "        for k in range(start, end+1):\n",
    "            w[items_sorted_by_value[k]] = avg\n",
    "\n",
    "\n",
    "        j += 1\n",
    "\n",
    "    nonzero_user_count = 0\n",
    "    sum_user_auc = 0.0\n",
    "    sum_user_recall = 0.0\n",
    "\n",
    "    # Compute score with AUC and compute Recall\n",
    "    for theuser in P[\"users\"]:\n",
    "        numerator_auc = 0.0\n",
    "        numerator_recall = 0.0\n",
    "        denominator = 0.0\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            # Skip items with null frequency\n",
    "            if  theitem not in Ni:\n",
    "                continue\n",
    "            # Add things to be summed for each item\n",
    "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser])) * w[theitem]\n",
    "            # Add things for recall\n",
    "            if Zui[(theuser, theitem)] < K:\n",
    "                numerator_recall += 1.0 * w[theitem] #spetta\n",
    "            # Increment denominator that the sum must be divided by \n",
    "            denominator += 1 / pui[theitem]\n",
    "\n",
    "\n",
    "        # If there was at least one item for the user, count the user and sum the results\n",
    "        if denominator > 0:\n",
    "            nonzero_user_count += 1\n",
    "            sum_user_auc += numerator_auc / denominator\n",
    "            sum_user_recall += numerator_recall / denominator \n",
    "\n",
    "    return {\n",
    "        \"auc\"       : sum_user_auc / nonzero_user_count, \n",
    "        \"recall\"    : sum_user_recall / nonzero_user_count\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0. ,  93.2, 186.4, 279.6, 372.8, 466. , 559.2, 652.4, 745.6,\n",
       "       838.8, 932. ])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linspace = np.linspace(0, 932, 10+1)\n",
    "linspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **EVALUATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_results = []\n",
    "recall_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = dict()\n",
    "raw_data['train_data'] = np.load(output_name + \"training_arr.npy\")\n",
    "raw_data['test_data_pos_biased'] = np.load(output_name + \"biased-test_arr_pos.npy\")\n",
    "raw_data['test_data_neg_biased'] = np.load(output_name + \"biased-test_arr_neg.npy\")\n",
    "raw_data['test_data_pos_unbiased'] = np.load(output_name + \"unbiased-test_arr_pos.npy\")\n",
    "raw_data['test_data_neg_unbiased'] = np.load(output_name + \"unbiased-test_arr_neg.npy\")\n",
    "raw_data['max_user'] = 15401\n",
    "raw_data['max_item'] = 1001\n",
    "batch_size = 8000\n",
    "test_batch_size = 1000\n",
    "display_itr = 1000\n",
    "\n",
    "train_dataset = ImplicitDataset(raw_data['train_data'], raw_data['max_user'], raw_data['max_item'], name='Train')\n",
    "test_dataset_pos_biased = ImplicitDataset(raw_data['test_data_pos_biased'], raw_data['max_user'], raw_data['max_item'])\n",
    "test_dataset_neg_biased = ImplicitDataset(raw_data['test_data_neg_biased'], raw_data['max_user'], raw_data['max_item'])\n",
    "test_dataset_pos_unbiased = ImplicitDataset(raw_data['test_data_pos_unbiased'], raw_data['max_user'], raw_data['max_item'])\n",
    "test_dataset_neg_unbiased = ImplicitDataset(raw_data['test_data_neg_unbiased'], raw_data['max_user'], raw_data['max_item'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./generated_data/cml-yahoo/\n"
     ]
    }
   ],
   "source": [
    "#Code to avoid tf using cached embeddings\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "model = MODEL_CLASS(batch_size=batch_size, max_user=train_dataset.max_user(), max_item=train_dataset.max_item(),\n",
    "    dim_embed=50, l2_reg=0.001, opt='Adam', sess_config=None)\n",
    "sampler = PairwiseSampler(batch_size=batch_size, dataset=train_dataset, num_process=4)\n",
    "model_trainer = ImplicitModelTrainer(batch_size=batch_size, test_batch_size=test_batch_size,\n",
    "                                     train_dataset=train_dataset, model=model, sampler=sampler,\n",
    "                                     eval_save_prefix=OUTPUT_PATH + DATASET_NAME,\n",
    "                                     item_serving_size=500)\n",
    "auc_evaluator = AUC()\n",
    "\n",
    "model.load(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Subsampling negative items]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    }
   ],
   "source": [
    "model_trainer._eval_manager = ImplicitEvalManager(evaluators=[auc_evaluator])\n",
    "model_trainer._num_negatives = 200\n",
    "model_trainer._exclude_positives([train_dataset, test_dataset_pos_biased, test_dataset_neg_biased])\n",
    "model_trainer._sample_negatives(seed=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biased Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2070/2070 [00:00<00:00, 2571.41it/s]\n",
      "100%|| 2296/2296 [00:25<00:00, 89.88it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AUC': [0.5001016949152542,\n",
       "  0.5165593220338983,\n",
       "  0.4894295302013423,\n",
       "  0.4948489932885905,\n",
       "  0.48419191919191923,\n",
       "  0.5032881355932203,\n",
       "  0.5239765100671141,\n",
       "  0.5461655405405406,\n",
       "  0.5020637583892618,\n",
       "  0.49090909090909096,\n",
       "  0.5317457627118645,\n",
       "  0.5058108108108107,\n",
       "  0.4687939698492462,\n",
       "  0.49053333333333343,\n",
       "  0.5306770833333333,\n",
       "  0.4921644295302014,\n",
       "  0.5262040133779264,\n",
       "  0.4735374149659864,\n",
       "  0.4898322147651007,\n",
       "  0.5394949494949495,\n",
       "  0.5338963210702341,\n",
       "  0.542654109589041,\n",
       "  0.5241471571906354,\n",
       "  0.4566778523489933,\n",
       "  0.4867892976588629,\n",
       "  0.4684330985915493,\n",
       "  0.49034364261168384,\n",
       "  0.4967725752508361,\n",
       "  0.4759863945578231,\n",
       "  0.5187710437710438,\n",
       "  0.47192953020134226,\n",
       "  0.4892929292929293,\n",
       "  0.5018923611111111,\n",
       "  0.5276923076923076,\n",
       "  0.48802013422818796,\n",
       "  0.4958389261744966,\n",
       "  0.535819397993311,\n",
       "  0.4594217687074831,\n",
       "  0.4924496644295302,\n",
       "  0.5055236486486486,\n",
       "  0.489054054054054,\n",
       "  0.44205172413793103,\n",
       "  0.5381740614334471,\n",
       "  0.4919696969696969,\n",
       "  0.4736073825503355,\n",
       "  0.485959595959596,\n",
       "  0.5325589225589226,\n",
       "  0.5338666666666666,\n",
       "  0.5155201342281879,\n",
       "  0.5027591973244147,\n",
       "  0.5171,\n",
       "  0.5068350168350169,\n",
       "  0.5109661016949152,\n",
       "  0.5068791946308725,\n",
       "  0.48551369863013705,\n",
       "  0.4887080536912751,\n",
       "  0.4762289562289562,\n",
       "  0.5197758620689655,\n",
       "  0.448238255033557,\n",
       "  0.5529830508474576,\n",
       "  0.5031313131313132,\n",
       "  0.49031879194630873,\n",
       "  0.47443877551020414,\n",
       "  0.45898989898989895,\n",
       "  0.48277591973244144,\n",
       "  0.5119127516778523,\n",
       "  0.4713636363636364,\n",
       "  0.4886744966442952,\n",
       "  0.47100000000000003,\n",
       "  0.48961538461538456,\n",
       "  0.49608695652173906,\n",
       "  0.5125585284280937,\n",
       "  0.4341077441077441,\n",
       "  0.5049143835616439,\n",
       "  0.4888344594594594,\n",
       "  0.47061016949152534,\n",
       "  0.45498322147651,\n",
       "  0.48369999999999996,\n",
       "  0.4895637583892618,\n",
       "  0.5101178451178451,\n",
       "  0.46863636363636363,\n",
       "  0.48634353741496594,\n",
       "  0.5006208053691276,\n",
       "  0.5033221476510067,\n",
       "  0.4899326599326599,\n",
       "  0.5045791245791246,\n",
       "  0.44474662162162165,\n",
       "  0.5099312714776632,\n",
       "  0.5442398648648649,\n",
       "  0.5418074324324325,\n",
       "  0.4955050505050505,\n",
       "  0.500234899328859,\n",
       "  0.43998310810810815,\n",
       "  0.5090771812080537,\n",
       "  0.5294949494949495,\n",
       "  0.47943493150684935,\n",
       "  0.5081879194630873,\n",
       "  0.5158361204013377,\n",
       "  0.4865202702702702,\n",
       "  0.5319630872483221,\n",
       "  0.4812162162162162,\n",
       "  0.5048458904109588,\n",
       "  0.5032482993197278,\n",
       "  0.5322377622377622,\n",
       "  0.49074999999999996,\n",
       "  0.5036271186440677,\n",
       "  0.5147157190635453,\n",
       "  0.4943456375838926,\n",
       "  0.5213333333333332,\n",
       "  0.49316498316498314,\n",
       "  0.49013422818791946,\n",
       "  0.5125084175084175,\n",
       "  0.4981144781144781,\n",
       "  0.5025510204081632,\n",
       "  0.502053872053872,\n",
       "  0.52195,\n",
       "  0.4698657718120805,\n",
       "  0.4979461279461279,\n",
       "  0.5190508474576271,\n",
       "  0.46873333333333334,\n",
       "  0.4555555555555556,\n",
       "  0.4838047138047138,\n",
       "  0.5578093645484948,\n",
       "  0.5071979865771812,\n",
       "  0.5,\n",
       "  0.5109764309764311,\n",
       "  0.5547474747474748,\n",
       "  0.50506734006734,\n",
       "  0.5148825503355705,\n",
       "  0.509496644295302,\n",
       "  0.4824832214765102,\n",
       "  0.4773539518900344,\n",
       "  0.5174915254237288,\n",
       "  0.4924912891986062,\n",
       "  0.5612919463087248,\n",
       "  0.5071979865771812,\n",
       "  0.5007432432432432,\n",
       "  0.5168959731543624,\n",
       "  0.48724832214765096,\n",
       "  0.4882142857142856,\n",
       "  0.5419763513513514,\n",
       "  0.523656462585034,\n",
       "  0.5373559322033897,\n",
       "  0.51645,\n",
       "  0.5117465753424658,\n",
       "  0.5052675585284281,\n",
       "  0.4936120401337792,\n",
       "  0.4955743243243243,\n",
       "  0.5380536912751677,\n",
       "  0.4771308724832215,\n",
       "  0.48607638888888893,\n",
       "  0.4873913043478261,\n",
       "  0.5027181208053692,\n",
       "  0.4850684931506849,\n",
       "  0.5018333333333334,\n",
       "  0.52635,\n",
       "  0.5029362416107382,\n",
       "  0.5059932659932661,\n",
       "  0.5101178451178451,\n",
       "  0.49087837837837844,\n",
       "  0.5139562289562289,\n",
       "  0.49665540540540537,\n",
       "  0.523108108108108,\n",
       "  0.4707885906040269,\n",
       "  0.4955178571428572,\n",
       "  0.5551515151515152,\n",
       "  0.5146308724832215,\n",
       "  0.47271186440677965,\n",
       "  0.5139297658862877,\n",
       "  0.5234731543624161,\n",
       "  0.467347972972973,\n",
       "  0.47171768707483,\n",
       "  0.5158333333333335,\n",
       "  0.48124999999999996,\n",
       "  0.46731418918918916,\n",
       "  0.5007357859531772,\n",
       "  0.4805201342281879,\n",
       "  0.49538590604026844,\n",
       "  0.531493288590604,\n",
       "  0.47951505016722407,\n",
       "  0.42820338983050843,\n",
       "  0.48538720538720537,\n",
       "  0.5396779661016949,\n",
       "  0.5307575757575758,\n",
       "  0.5006208053691276,\n",
       "  0.4810367892976588,\n",
       "  0.5037751677852349,\n",
       "  0.5070134228187919,\n",
       "  0.45815,\n",
       "  0.45428333333333343,\n",
       "  0.522087542087542,\n",
       "  0.550886287625418,\n",
       "  0.45072390572390575,\n",
       "  0.5317796610169492,\n",
       "  0.4388698630136986,\n",
       "  0.5262962962962963,\n",
       "  0.4921672354948807,\n",
       "  0.4918624161073825,\n",
       "  0.5165384615384615,\n",
       "  0.5201013513513513,\n",
       "  0.510503355704698,\n",
       "  0.5446308724832214,\n",
       "  0.5140604026845638,\n",
       "  0.5203333333333333,\n",
       "  0.48013559322033905,\n",
       "  0.5081986531986532,\n",
       "  0.5107601351351352,\n",
       "  0.49188552188552187,\n",
       "  0.4814833333333333,\n",
       "  0.524831081081081,\n",
       "  0.4730405405405407,\n",
       "  0.5113545150501672,\n",
       "  0.4561371237458194,\n",
       "  0.4733838383838384,\n",
       "  0.5113804713804714,\n",
       "  0.45687290969899674,\n",
       "  0.49016835016835014,\n",
       "  0.4939393939393939,\n",
       "  0.5094594594594595,\n",
       "  0.5045973154362416,\n",
       "  0.49099662162162155,\n",
       "  0.5022909698996655,\n",
       "  0.4851666666666667,\n",
       "  0.5063545150501673,\n",
       "  0.5358389261744967,\n",
       "  0.5010135135135135,\n",
       "  0.5151006711409396,\n",
       "  0.49355704697986574,\n",
       "  0.4928911564625849,\n",
       "  0.4845622895622895,\n",
       "  0.47798634812286694,\n",
       "  0.49580536912751677,\n",
       "  0.5089464882943143,\n",
       "  0.49260067114093964,\n",
       "  0.5088127090301003,\n",
       "  0.433003355704698,\n",
       "  0.5228929765886288,\n",
       "  0.49706185567010325,\n",
       "  0.4874664429530201,\n",
       "  0.49968013468013467,\n",
       "  0.5092617449664429,\n",
       "  0.5033000000000001,\n",
       "  0.4836166666666666,\n",
       "  0.5218288590604027,\n",
       "  0.5139166666666667,\n",
       "  0.5422742474916389,\n",
       "  0.4621379310344828,\n",
       "  0.4548154362416108,\n",
       "  0.4622315436241611,\n",
       "  0.5210437710437711,\n",
       "  0.5362203389830509,\n",
       "  0.5022710622710623,\n",
       "  0.5058557046979866,\n",
       "  0.49172240802675576,\n",
       "  0.5291176470588236,\n",
       "  0.4970166666666666,\n",
       "  0.4723,\n",
       "  0.5046822742474916,\n",
       "  0.5063344594594594,\n",
       "  0.4747826086956522,\n",
       "  0.4731166666666667,\n",
       "  0.46937074829931974,\n",
       "  0.4774242424242425,\n",
       "  0.4922390572390572,\n",
       "  0.4947147651006711,\n",
       "  0.526728187919463,\n",
       "  0.5167666666666668,\n",
       "  0.5050503355704697,\n",
       "  0.5457966101694915,\n",
       "  0.5458724832214765,\n",
       "  0.4864478114478114,\n",
       "  0.4980689655172414,\n",
       "  0.5334330985915493,\n",
       "  0.5082432432432432,\n",
       "  0.5188474576271187,\n",
       "  0.5225420875420875,\n",
       "  0.5240604026845637,\n",
       "  0.4933389261744966,\n",
       "  0.5008247422680413,\n",
       "  0.541786941580756,\n",
       "  0.47491496598639454,\n",
       "  0.47424999999999995,\n",
       "  0.5266000000000001,\n",
       "  0.4566053511705685,\n",
       "  0.4923411371237458,\n",
       "  0.459113712374582,\n",
       "  0.5041245791245792,\n",
       "  0.4637878787878788,\n",
       "  0.4812962962962963,\n",
       "  0.5262162162162163,\n",
       "  0.4395484949832775,\n",
       "  0.47245733788395905,\n",
       "  0.5502999999999999,\n",
       "  0.4679222972972973,\n",
       "  0.4441304347826088,\n",
       "  0.45068561872909696,\n",
       "  0.47414429530201346,\n",
       "  0.4942140468227424,\n",
       "  0.49991467576791815,\n",
       "  0.5327759197324415,\n",
       "  0.5237118644067796,\n",
       "  0.5317281879194631,\n",
       "  0.5092905405405406,\n",
       "  0.5117953020134228,\n",
       "  0.46157094594594594,\n",
       "  0.5131292517006802,\n",
       "  0.5283101045296167,\n",
       "  0.5337123745819398,\n",
       "  0.4682094594594596,\n",
       "  0.46431972789115644,\n",
       "  0.4771666666666667,\n",
       "  0.5244463087248323,\n",
       "  0.5285284280936455,\n",
       "  0.511170568561873,\n",
       "  0.5169127516778524,\n",
       "  0.43932885906040275,\n",
       "  0.5308041958041958,\n",
       "  0.49957912457912457,\n",
       "  0.4705,\n",
       "  0.5254081632653061,\n",
       "  0.5312372881355932,\n",
       "  0.5057692307692307,\n",
       "  0.4787074829931973,\n",
       "  0.4662040133779264,\n",
       "  0.5296296296296297,\n",
       "  0.5199831081081082,\n",
       "  0.515976430976431,\n",
       "  0.49817725752508357,\n",
       "  0.5282,\n",
       "  0.5051182432432432,\n",
       "  0.5060906040268456,\n",
       "  0.4694314381270904,\n",
       "  0.5514115646258504,\n",
       "  0.5182996632996633,\n",
       "  0.5439285714285714,\n",
       "  0.49539518900343643,\n",
       "  0.47005681818181816,\n",
       "  0.5163468013468013,\n",
       "  0.5301505016722408,\n",
       "  0.488047138047138,\n",
       "  0.5091186440677966,\n",
       "  0.48248287671232876,\n",
       "  0.4392929292929292,\n",
       "  0.4389877300613497,\n",
       "  0.4816722408026756,\n",
       "  0.45894295302013416,\n",
       "  0.47000000000000003,\n",
       "  0.4829833333333334,\n",
       "  0.5038087248322147,\n",
       "  0.4985906040268456,\n",
       "  0.5145439189189189,\n",
       "  0.5223333333333334,\n",
       "  0.5252758620689657,\n",
       "  0.486438127090301,\n",
       "  0.5352181208053691,\n",
       "  0.5421724137931035,\n",
       "  0.49513605442176867,\n",
       "  0.460777027027027,\n",
       "  0.4951337792642141,\n",
       "  0.48903333333333326,\n",
       "  0.4791806020066891,\n",
       "  0.5666610738255033,\n",
       "  0.5712626262626262,\n",
       "  0.5280602006688963,\n",
       "  0.5189152542372881,\n",
       "  0.49267796610169484,\n",
       "  0.47812709030100325,\n",
       "  0.46974832214765094,\n",
       "  0.5101020408163265,\n",
       "  0.49568561872909694,\n",
       "  0.48469696969696974,\n",
       "  0.4986454849498328,\n",
       "  0.5180833333333333,\n",
       "  0.5187037037037038,\n",
       "  0.5075838926174496,\n",
       "  0.5300000000000001,\n",
       "  0.5003846153846154,\n",
       "  0.5113545150501673,\n",
       "  0.4954623287671233,\n",
       "  0.48498327759197324,\n",
       "  0.5169964664310954,\n",
       "  0.5935767790262172,\n",
       "  0.5218518518518518,\n",
       "  0.5192929292929294,\n",
       "  0.5037919463087248,\n",
       "  0.4798776223776224,\n",
       "  0.516418918918919,\n",
       "  0.4609966216216216,\n",
       "  0.4950680272108844,\n",
       "  0.49154882154882157,\n",
       "  0.46929054054054054,\n",
       "  0.49593333333333334,\n",
       "  0.5298666666666667,\n",
       "  0.5435324232081911,\n",
       "  0.4531605351170569,\n",
       "  0.48725752508361203,\n",
       "  0.46449324324324326,\n",
       "  0.506371237458194,\n",
       "  0.5258614864864865,\n",
       "  0.5129096989966555,\n",
       "  0.4913851351351351,\n",
       "  0.4947173144876325,\n",
       "  0.5273825503355706,\n",
       "  0.5008333333333334,\n",
       "  0.4888215488215488,\n",
       "  0.5324328859060403,\n",
       "  0.5454347826086957,\n",
       "  0.5193979933110368,\n",
       "  0.48297979797979795,\n",
       "  0.48912162162162165,\n",
       "  0.4808983050847458,\n",
       "  0.46389830508474567,\n",
       "  0.4841864406779661,\n",
       "  0.4260367892976588,\n",
       "  0.5250333333333334,\n",
       "  0.49789830508474575,\n",
       "  0.5166107382550336,\n",
       "  0.44399999999999995,\n",
       "  0.5143311036789299,\n",
       "  0.49584745762711857,\n",
       "  0.4925856164383561,\n",
       "  0.5067534722222222,\n",
       "  0.5613299663299663,\n",
       "  0.5366778523489932,\n",
       "  0.502331081081081,\n",
       "  0.4974581939799331,\n",
       "  0.49466442953020134,\n",
       "  0.5046644295302014,\n",
       "  0.4838087248322148,\n",
       "  0.561084745762712,\n",
       "  0.5131249999999999,\n",
       "  0.5188461538461538,\n",
       "  0.49949494949494955,\n",
       "  0.5341919191919191,\n",
       "  0.482949152542373,\n",
       "  0.47102787456445994,\n",
       "  0.4692474916387959,\n",
       "  0.5366964285714287,\n",
       "  0.5078813559322034,\n",
       "  0.5153198653198654,\n",
       "  0.5152777777777778,\n",
       "  0.49465986394557826,\n",
       "  0.537027027027027,\n",
       "  0.5040572390572391,\n",
       "  0.49326086956521736,\n",
       "  0.5284899328859061,\n",
       "  0.4557094594594594,\n",
       "  0.5371979865771812,\n",
       "  0.4636195286195286,\n",
       "  0.5208131487889273,\n",
       "  0.5020302013422819,\n",
       "  0.47638795986622073,\n",
       "  0.5009899328859061,\n",
       "  0.4865986394557824,\n",
       "  0.5165033783783783,\n",
       "  0.49510169491525424,\n",
       "  0.4861833333333334,\n",
       "  0.4526360544217687,\n",
       "  0.5269661016949154,\n",
       "  0.47313131313131307,\n",
       "  0.49324999999999997,\n",
       "  0.46924657534246567,\n",
       "  0.4954965753424658,\n",
       "  0.5049161073825503,\n",
       "  0.4646321070234114,\n",
       "  0.5069295302013422,\n",
       "  0.5194237288135594,\n",
       "  0.5058922558922558,\n",
       "  0.4685690235690235,\n",
       "  0.5261148648648649,\n",
       "  0.5158221476510066,\n",
       "  0.47939716312056735,\n",
       "  0.5126653696498055,\n",
       "  0.49275919732441464,\n",
       "  0.5013468013468013,\n",
       "  0.5035445205479452,\n",
       "  0.5409395973154362,\n",
       "  0.4398305084745763,\n",
       "  0.48645484949832773,\n",
       "  0.5123,\n",
       "  0.5384175084175085,\n",
       "  0.49909698996655527,\n",
       "  0.4575418060200669,\n",
       "  0.552996632996633,\n",
       "  0.5365816326530612,\n",
       "  0.5060677966101695,\n",
       "  0.5365816326530612,\n",
       "  0.49351351351351347,\n",
       "  0.5357333333333333,\n",
       "  0.48683946488294316,\n",
       "  0.5561705685618729,\n",
       "  0.4966780821917807,\n",
       "  0.5064983164983166,\n",
       "  0.5437248322147652,\n",
       "  0.4908528428093645,\n",
       "  0.4562074829931973,\n",
       "  0.5084666666666667,\n",
       "  0.516010101010101,\n",
       "  0.5265217391304348,\n",
       "  0.48686666666666667,\n",
       "  0.47630434782608705,\n",
       "  0.5537666666666666,\n",
       "  0.508614864864865,\n",
       "  0.5375503355704698,\n",
       "  0.49842809364548496,\n",
       "  0.5180536912751678,\n",
       "  0.5271548821548823,\n",
       "  0.46706484641638224,\n",
       "  0.481371237458194,\n",
       "  0.5110402684563758,\n",
       "  0.509141414141414,\n",
       "  0.5396075085324232,\n",
       "  0.5146488294314381,\n",
       "  0.4929222972972974,\n",
       "  0.5027181208053692,\n",
       "  0.5074664429530202,\n",
       "  0.5165410958904109,\n",
       "  0.48347315436241606,\n",
       "  0.48897306397306395,\n",
       "  0.5107785467128028,\n",
       "  0.48495000000000005,\n",
       "  0.5268833333333334,\n",
       "  0.5019000000000001,\n",
       "  0.5056,\n",
       "  0.48462837837837835,\n",
       "  0.5252842809364549,\n",
       "  0.5137080536912751,\n",
       "  0.4800668896321071,\n",
       "  0.4930536912751678,\n",
       "  0.5018288590604028,\n",
       "  0.4920069204152249,\n",
       "  0.5307312925170068,\n",
       "  0.46516666666666673,\n",
       "  0.4552380952380953,\n",
       "  0.4961279461279461,\n",
       "  0.47820469798657717,\n",
       "  0.5136241610738255,\n",
       "  0.4594791666666666,\n",
       "  0.4849829351535836,\n",
       "  0.5023322147651006,\n",
       "  0.4928378378378378,\n",
       "  0.5070637583892618,\n",
       "  0.4890939597315436,\n",
       "  0.5511447811447812,\n",
       "  0.5195469798657718,\n",
       "  0.49620338983050843,\n",
       "  0.5160702341137124,\n",
       "  0.4818305084745762,\n",
       "  0.4672240802675585,\n",
       "  0.540819397993311,\n",
       "  0.5332608695652173,\n",
       "  0.43545608108108114,\n",
       "  0.49151006711409395,\n",
       "  0.5410702341137125,\n",
       "  0.48028523489932884,\n",
       "  0.5108249158249158,\n",
       "  0.48006756756756763,\n",
       "  0.4845791245791246,\n",
       "  0.4812289562289562,\n",
       "  0.484914675767918,\n",
       "  0.4446114864864866,\n",
       "  0.46447278911564627,\n",
       "  0.502962962962963,\n",
       "  0.4617953020134228,\n",
       "  0.48857388316151207,\n",
       "  0.5103511705685618,\n",
       "  0.540755033557047,\n",
       "  0.5345637583892617,\n",
       "  0.48597269624573386,\n",
       "  0.5039765100671141,\n",
       "  0.4866891891891892,\n",
       "  0.5237755102040816,\n",
       "  0.48141414141414146,\n",
       "  0.4903691275167785,\n",
       "  0.49448979591836734,\n",
       "  0.5145302013422818,\n",
       "  0.4770307167235494,\n",
       "  0.481875,\n",
       "  0.48244932432432436,\n",
       "  0.5356313993174061,\n",
       "  0.47373333333333334,\n",
       "  0.5490635451505017,\n",
       "  0.5033779264214047,\n",
       "  0.49700668896321076,\n",
       "  0.4966836734693878,\n",
       "  0.5222297297297297,\n",
       "  0.48289297658862873,\n",
       "  0.4265033783783784,\n",
       "  0.5202842809364548,\n",
       "  0.5461036789297659,\n",
       "  0.4969047619047618,\n",
       "  0.49521885521885517,\n",
       "  0.47648648648648656,\n",
       "  0.5067553191489361,\n",
       "  0.5605536912751677,\n",
       "  0.5567567567567568,\n",
       "  0.47481543624161077,\n",
       "  0.4854347826086957,\n",
       "  0.5359417808219178,\n",
       "  0.5084511784511785,\n",
       "  0.4965500000000001,\n",
       "  0.4933732876712329,\n",
       "  0.500925925925926,\n",
       "  0.46841137123745824,\n",
       "  0.4798653198653199,\n",
       "  0.4896488294314381,\n",
       "  0.5077166666666666,\n",
       "  0.4422483221476511,\n",
       "  0.5013255033557047,\n",
       "  0.4917905405405405,\n",
       "  0.5074074074074074,\n",
       "  0.5404347826086956,\n",
       "  0.5158333333333334,\n",
       "  0.45115384615384607,\n",
       "  0.5123411371237458,\n",
       "  0.522675585284281,\n",
       "  0.5161371237458193,\n",
       "  0.524228187919463,\n",
       "  0.4821598639455783,\n",
       "  0.5115836298932385,\n",
       "  0.4748316498316498,\n",
       "  0.5336195286195285,\n",
       "  0.5090939597315436,\n",
       "  0.49146959459459466,\n",
       "  0.5183892617449665,\n",
       "  0.501170568561873,\n",
       "  0.49293220338983057,\n",
       "  0.5197491638795987,\n",
       "  0.5014166666666667,\n",
       "  0.566023489932886,\n",
       "  0.4893288590604027,\n",
       "  0.5336317567567567,\n",
       "  0.5152356902356903,\n",
       "  0.48119127516778515,\n",
       "  0.4948076923076923,\n",
       "  0.5215384615384615,\n",
       "  0.47906666666666675,\n",
       "  0.536343537414966,\n",
       "  0.5235785953177258,\n",
       "  0.5074242424242423,\n",
       "  0.49077441077441075,\n",
       "  0.4738013698630138,\n",
       "  0.49234113712374583,\n",
       "  0.49815517241379303,\n",
       "  0.48222408026755853,\n",
       "  0.4648999999999999,\n",
       "  0.509608843537415,\n",
       "  0.5111864406779661,\n",
       "  0.5063728813559322,\n",
       "  0.5404054054054054,\n",
       "  0.4635618729096991,\n",
       "  0.5016101694915255,\n",
       "  0.501986531986532,\n",
       "  0.48663299663299653,\n",
       "  0.5137331081081081,\n",
       "  0.510736301369863,\n",
       "  0.5,\n",
       "  0.5086713286713287,\n",
       "  0.5688127090301003,\n",
       "  0.45755932203389826,\n",
       "  0.5215529010238907,\n",
       "  0.5224489795918368,\n",
       "  0.5088720538720538,\n",
       "  0.49718120805369137,\n",
       "  0.5320408163265307,\n",
       "  0.48434931506849316,\n",
       "  0.5076520270270269,\n",
       "  0.49533444816053507,\n",
       "  0.4809322033898305,\n",
       "  0.507996632996633,\n",
       "  0.4732666666666666,\n",
       "  0.4957993197278912,\n",
       "  0.5316498316498317,\n",
       "  0.4642176870748299,\n",
       "  0.4778428093645485,\n",
       "  0.504180602006689,\n",
       "  0.5365217391304349,\n",
       "  0.4700850340136054,\n",
       "  0.5337668918918919,\n",
       "  0.4945317725752509,\n",
       "  0.4786241610738255,\n",
       "  0.5240166666666666,\n",
       "  0.49843959731543624,\n",
       "  0.4991156462585034,\n",
       "  0.49180602006688967,\n",
       "  0.5044594594594595,\n",
       "  0.5061872909698997,\n",
       "  0.512560553633218,\n",
       "  0.5126510067114093,\n",
       "  0.524278523489933,\n",
       "  0.4920833333333333,\n",
       "  0.5074832214765101,\n",
       "  0.5305723905723905,\n",
       "  0.5097166666666667,\n",
       "  0.5036026936026936,\n",
       "  0.5126182432432433,\n",
       "  0.4476833333333334,\n",
       "  0.5138999999999999,\n",
       "  0.5001839464882943,\n",
       "  0.5301351351351351,\n",
       "  0.5050170648464164,\n",
       "  0.48973154362416105,\n",
       "  0.5261694915254238,\n",
       "  0.471734693877551,\n",
       "  0.5289690721649485,\n",
       "  0.4502702702702703,\n",
       "  0.4877424749163879,\n",
       "  0.4916156462585035,\n",
       "  0.5530136986301369,\n",
       "  0.5032046979865771,\n",
       "  0.5023322147651006,\n",
       "  0.41766544117647053,\n",
       "  0.510752508361204,\n",
       "  0.5138047138047138,\n",
       "  0.4965488215488215,\n",
       "  0.49058528428093645,\n",
       "  0.4711525423728814,\n",
       "  0.4749662162162162,\n",
       "  0.4687333333333333,\n",
       "  0.5045666666666667,\n",
       "  0.4573232323232323,\n",
       "  0.5019322033898305,\n",
       "  0.5053780068728523,\n",
       "  0.49854515050167225,\n",
       "  0.5315709459459459,\n",
       "  0.4995254237288136,\n",
       "  0.5064695945945946,\n",
       "  0.5060333333333333,\n",
       "  0.4941864406779662,\n",
       "  0.4917636986301371,\n",
       "  0.4828546099290781,\n",
       "  0.4676690391459076,\n",
       "  0.43868965517241376,\n",
       "  0.49651006711409407,\n",
       "  0.5005351170568562,\n",
       "  0.5157679180887372,\n",
       "  0.5194648829431437,\n",
       "  0.5611864406779662,\n",
       "  0.5269322033898305,\n",
       "  0.4905254237288135,\n",
       "  0.5096979865771811,\n",
       "  0.5250501672240803,\n",
       "  0.5199494949494949,\n",
       "  0.46742372881355926,\n",
       "  0.4290166666666666,\n",
       "  0.4875083612040133,\n",
       "  0.4658221476510067,\n",
       "  0.5365593220338984,\n",
       "  0.48913559322033906,\n",
       "  0.5071114864864865,\n",
       "  0.4730034129692833,\n",
       "  0.509106529209622,\n",
       "  0.5094349315068493,\n",
       "  0.5028787878787879,\n",
       "  0.43765100671140944,\n",
       "  0.5171428571428572,\n",
       "  0.5113139931740615,\n",
       "  0.5101023890784984,\n",
       "  0.45310810810810814,\n",
       "  0.5261705685618728,\n",
       "  0.5092166666666667,\n",
       "  0.5260278745644599,\n",
       "  0.46875850340136055,\n",
       "  0.5249657534246576,\n",
       "  0.48608695652173906,\n",
       "  0.4592372881355932,\n",
       "  0.5003523489932885,\n",
       "  0.4807142857142857,\n",
       "  0.47143333333333337,\n",
       "  0.5048409893992933,\n",
       "  0.49321554770318016,\n",
       "  0.49313559322033906,\n",
       "  0.5310702341137123,\n",
       "  0.5057692307692307,\n",
       "  0.5330574324324324,\n",
       "  0.5108695652173914,\n",
       "  0.5254347826086957,\n",
       "  0.5457263513513514,\n",
       "  0.4951013513513514,\n",
       "  0.4885084745762712,\n",
       "  0.4973569023569024,\n",
       "  0.494515570934256,\n",
       "  0.4884166666666666,\n",
       "  0.4513377926421404,\n",
       "  0.4764795918367346,\n",
       "  0.5163103448275863,\n",
       "  0.5022222222222222,\n",
       "  0.49154237288135594,\n",
       "  0.5084343434343435,\n",
       "  0.495886287625418,\n",
       "  0.5337123745819399,\n",
       "  0.5397986577181209,\n",
       "  0.5027181208053692,\n",
       "  0.5138305084745762,\n",
       "  0.49228187919463084,\n",
       "  0.5331040268456375,\n",
       "  0.5049496644295303,\n",
       "  0.5085953177257525,\n",
       "  0.5132323232323233,\n",
       "  0.48020202020202024,\n",
       "  0.530892255892256,\n",
       "  0.5353924914675768,\n",
       "  0.5152341137123746,\n",
       "  0.5300668896321069,\n",
       "  0.525298245614035,\n",
       "  0.5064864864864865,\n",
       "  0.46478102189781023,\n",
       "  0.4810869565217392,\n",
       "  0.4724664429530202,\n",
       "  0.46687290969899664,\n",
       "  0.5223549488054607,\n",
       "  0.5569763513513514,\n",
       "  0.45973244147157183,\n",
       "  0.4964554794520548,\n",
       "  0.4907692307692308,\n",
       "  0.5285204081632654,\n",
       "  0.5119723183391003,\n",
       "  0.49211864406779665,\n",
       "  0.4863344594594595,\n",
       "  0.4860166666666667,\n",
       "  0.5244763513513514,\n",
       "  0.49528523489932885,\n",
       "  0.5420066889632107,\n",
       "  0.4939225589225589,\n",
       "  0.5086577181208053,\n",
       "  0.4842398648648648,\n",
       "  0.4840268456375839,\n",
       "  0.5058873720136519,\n",
       "  0.4783445945945947,\n",
       "  0.44801346801346803,\n",
       "  0.44837792642140467,\n",
       "  0.5130471380471381,\n",
       "  0.49777966101694915,\n",
       "  0.4990468227424749,\n",
       "  0.4728671328671329,\n",
       "  0.5041216216216217,\n",
       "  0.4998993288590605,\n",
       "  0.47586666666666666,\n",
       "  0.4979729729729731,\n",
       "  0.49946127946127955,\n",
       "  0.5218855218855218,\n",
       "  0.5127424749163879,\n",
       "  0.4676923076923077,\n",
       "  0.5367171717171717,\n",
       "  0.5010606060606061,\n",
       "  0.5066440677966102,\n",
       "  0.5371283783783785,\n",
       "  0.47758333333333325,\n",
       "  0.4958639705882353,\n",
       "  0.484781879194631,\n",
       "  0.47043478260869576,\n",
       "  0.5368813559322033,\n",
       "  0.4702188552188552,\n",
       "  0.48298333333333326,\n",
       "  0.5553885135135136,\n",
       "  0.496135593220339,\n",
       "  0.49673333333333336,\n",
       "  0.5057046979865772,\n",
       "  0.48458193979933106,\n",
       "  0.5052941176470588,\n",
       "  0.5487758620689654,\n",
       "  0.5122240802675585,\n",
       "  0.5041304347826088,\n",
       "  0.5198148148148147,\n",
       "  0.4831678082191781,\n",
       "  0.5357214765100671,\n",
       "  0.5503389830508474,\n",
       "  0.48005084745762716,\n",
       "  0.5102568493150685,\n",
       "  0.48446308724832216,\n",
       "  0.47168333333333334,\n",
       "  0.5368813559322033,\n",
       "  0.4712121212121212,\n",
       "  0.5353253424657534,\n",
       "  0.4846666666666667,\n",
       "  0.4826174496644295,\n",
       "  0.5192931034482758,\n",
       "  0.5195608108108108,\n",
       "  0.45753401360544216,\n",
       "  0.5299161073825503,\n",
       "  0.5252516778523489,\n",
       "  0.48448453608247416,\n",
       "  0.5287074829931973,\n",
       "  0.4637373737373737,\n",
       "  0.4902675585284281,\n",
       "  0.49750841750841757,\n",
       "  0.477483108108108,\n",
       "  0.4743645484949832,\n",
       "  0.5309342560553633,\n",
       "  0.5264576271186441,\n",
       "  0.48545,\n",
       "  0.5011036789297658,\n",
       "  0.5028282828282828,\n",
       "  0.5220408163265307,\n",
       "  0.46530508474576265,\n",
       "  0.5214846416382253,\n",
       "  0.4583892617449665,\n",
       "  0.5623639455782313,\n",
       "  0.519679054054054,\n",
       "  0.5005479452054793,\n",
       "  0.4601883561643836,\n",
       "  0.5251408450704225,\n",
       "  0.5054166666666666,\n",
       "  0.5317229729729729,\n",
       "  0.4982828282828283,\n",
       "  0.49994845360824736,\n",
       "  0.4677666666666667,\n",
       "  0.5196833333333334,\n",
       "  0.5041722972972973,\n",
       "  0.4842424242424242,\n",
       "  0.5093749999999999,\n",
       "  0.5436868686868687,\n",
       "  0.49766101694915255,\n",
       "  0.5078397212543554,\n",
       "  0.44562080536912757,\n",
       "  0.4859228187919463,\n",
       "  0.5027054794520548,\n",
       "  0.47453124999999996,\n",
       "  0.4981649831649831,\n",
       "  0.49530303030303025,\n",
       "  0.5136577181208053,\n",
       "  0.5349155405405407,\n",
       "  0.46437919463087246,\n",
       "  0.5144745762711864,\n",
       "  0.5410409556313993,\n",
       "  0.5231649831649832,\n",
       "  0.5027166666666667,\n",
       "  0.49750000000000005,\n",
       "  0.4869830508474577,\n",
       "  0.4444107744107744,\n",
       "  0.4768262411347518,\n",
       "  0.4864864864864865,\n",
       "  0.53125,\n",
       "  0.50075,\n",
       "  0.5441973244147157,\n",
       "  0.46786912751677856,\n",
       "  0.46598333333333336,\n",
       "  0.4927739726027397,\n",
       "  0.5097993311036789,\n",
       "  0.4746587030716723,\n",
       "  0.5677013422818792,\n",
       "  0.48048986486486484,\n",
       "  0.5118833333333334,\n",
       "  0.444778156996587,\n",
       "  0.4664261744966443,\n",
       "  0.45666101694915257,\n",
       "  0.504234693877551,\n",
       "  0.5165886287625419,\n",
       "  0.526952861952862,\n",
       "  0.5142424242424243,\n",
       "  0.48820469798657723,\n",
       "  0.5271979865771812,\n",
       "  0.4926610169491526,\n",
       "  0.5046801346801347,\n",
       "  0.44189655172413794,\n",
       "  0.4773825503355704,\n",
       "  0.5188513513513513,\n",
       "  0.48875,\n",
       "  0.4501689189189189,\n",
       "  0.5039166666666667,\n",
       "  0.4681768953068592,\n",
       "  0.4873549488054608,\n",
       "  0.4550677966101695,\n",
       "  0.4578956228956229,\n",
       "  0.49031666666666657,\n",
       "  0.5085117056856188,\n",
       "  0.4961447811447812,\n",
       "  0.5213006756756757,\n",
       "  0.49671186440677967,\n",
       "  0.49000000000000005,\n",
       "  0.5099317406143344,\n",
       "  0.5182166666666667,\n",
       "  0.5010535117056856,\n",
       "  0.5229461279461279,\n",
       "  0.4713636363636364,\n",
       "  0.4829054054054054,\n",
       "  0.5051178451178452,\n",
       "  0.5367725752508361,\n",
       "  0.5304498269896193,\n",
       "  0.47807046979865775,\n",
       "  0.5061616161616161,\n",
       "  0.4709197324414716,\n",
       "  0.5248606271777003,\n",
       "  0.45701013513513505,\n",
       "  0.4669295302013422,\n",
       "  0.5207876712328767,\n",
       "  0.49409863945578225,\n",
       "  0.5606228956228956,\n",
       "  0.517704081632653,\n",
       "  0.5168197278911565,\n",
       "  0.5240268456375838,\n",
       "  0.4992229729729729,\n",
       "  0.44876288659793817,\n",
       "  0.45230508474576275,\n",
       "  0.4668412162162162,\n",
       "  0.5312414965986395,\n",
       "  0.468344709897611,\n",
       "  0.5219666666666667,\n",
       "  0.5415202702702704,\n",
       "  0.4977348993288591,\n",
       "  ...]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_trainer._eval_save_prefix = OUTPUT_PREFIX + \"-test-pos-biased\"\n",
    "model_trainer._evaluate_partial(test_dataset_pos_biased)\n",
    "\n",
    "model_trainer._eval_save_prefix = OUTPUT_PREFIX +  \"-test-neg-biased\"\n",
    "model_trainer._evaluate_partial(test_dataset_neg_biased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unbiased Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2296/2296 [00:00<00:00, 2935.76it/s]\n",
      "100%|| 2296/2296 [00:01<00:00, 1628.80it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AUC': [0.6005555555555556,\n",
       "  0.5660000000000001,\n",
       "  0.5375,\n",
       "  0.49166666666666664,\n",
       "  0.42666666666666664,\n",
       "  0.47785714285714287,\n",
       "  0.39999999999999997,\n",
       "  0.5535714285714286,\n",
       "  0.433125,\n",
       "  0.41111111111111115,\n",
       "  0.483125,\n",
       "  0.4457142857142857,\n",
       "  0.625,\n",
       "  0.47000000000000003,\n",
       "  0.484375,\n",
       "  0.5105555555555555,\n",
       "  0.4705555555555556,\n",
       "  0.5475,\n",
       "  0.5307142857142857,\n",
       "  0.5405555555555556,\n",
       "  0.588125,\n",
       "  0.4391666666666667,\n",
       "  0.35944444444444446,\n",
       "  0.499375,\n",
       "  0.7525,\n",
       "  0.318125,\n",
       "  0.555,\n",
       "  0.494375,\n",
       "  0.37750000000000006,\n",
       "  0.5472222222222222,\n",
       "  0.3007142857142857,\n",
       "  0.49222222222222217,\n",
       "  0.43777777777777777,\n",
       "  0.508888888888889,\n",
       "  0.4338888888888889,\n",
       "  0.6085714285714285,\n",
       "  0.7025,\n",
       "  0.4475,\n",
       "  0.4811111111111111,\n",
       "  0.4007142857142857,\n",
       "  0.543125,\n",
       "  0.39625,\n",
       "  0.599375,\n",
       "  0.5879999999999999,\n",
       "  0.41666666666666663,\n",
       "  0.474375,\n",
       "  0.5614285714285714,\n",
       "  0.27416666666666667,\n",
       "  0.59,\n",
       "  0.445,\n",
       "  0.4716666666666667,\n",
       "  0.571,\n",
       "  0.4116666666666666,\n",
       "  0.4511111111111111,\n",
       "  0.4033333333333333,\n",
       "  0.5535714285714285,\n",
       "  0.604375,\n",
       "  0.755,\n",
       "  0.5105555555555555,\n",
       "  0.554,\n",
       "  0.5594444444444444,\n",
       "  0.6833333333333333,\n",
       "  0.45999999999999996,\n",
       "  0.5057142857142857,\n",
       "  0.41611111111111104,\n",
       "  0.6983333333333334,\n",
       "  0.4311111111111111,\n",
       "  0.61,\n",
       "  0.3522222222222222,\n",
       "  0.4375,\n",
       "  0.6177777777777779,\n",
       "  0.43062500000000004,\n",
       "  0.41500000000000004,\n",
       "  0.2505555555555555,\n",
       "  0.317,\n",
       "  0.4033333333333333,\n",
       "  0.4288888888888889,\n",
       "  0.5127777777777777,\n",
       "  0.4555555555555556,\n",
       "  0.525625,\n",
       "  0.45875,\n",
       "  0.6133333333333333,\n",
       "  0.308125,\n",
       "  0.425,\n",
       "  0.6177777777777779,\n",
       "  0.4955555555555555,\n",
       "  0.3761111111111111,\n",
       "  0.4725,\n",
       "  0.50125,\n",
       "  0.4575,\n",
       "  0.5411111111111111,\n",
       "  0.5338888888888889,\n",
       "  0.44,\n",
       "  0.52,\n",
       "  0.5066666666666667,\n",
       "  0.44999999999999996,\n",
       "  0.41944444444444445,\n",
       "  0.671111111111111,\n",
       "  0.42944444444444446,\n",
       "  0.4707142857142857,\n",
       "  0.495,\n",
       "  0.41444444444444445,\n",
       "  0.48888888888888893,\n",
       "  0.5122222222222222,\n",
       "  0.5938888888888889,\n",
       "  0.5327777777777778,\n",
       "  0.5391666666666666,\n",
       "  0.321875,\n",
       "  0.625625,\n",
       "  0.4957142857142857,\n",
       "  0.5161111111111111,\n",
       "  0.4822222222222222,\n",
       "  0.39666666666666667,\n",
       "  0.3325,\n",
       "  0.41312499999999996,\n",
       "  0.544375,\n",
       "  0.389375,\n",
       "  0.4983333333333333,\n",
       "  0.531875,\n",
       "  0.34142857142857136,\n",
       "  0.3994444444444445,\n",
       "  0.6138888888888889,\n",
       "  0.49166666666666664,\n",
       "  0.544375,\n",
       "  0.47611111111111115,\n",
       "  0.5261111111111112,\n",
       "  0.5178571428571429,\n",
       "  0.4577777777777778,\n",
       "  0.433125,\n",
       "  0.42071428571428576,\n",
       "  0.7044444444444444,\n",
       "  0.52125,\n",
       "  0.43277777777777776,\n",
       "  0.59625,\n",
       "  0.48062499999999997,\n",
       "  0.33055555555555555,\n",
       "  0.4533333333333334,\n",
       "  0.585625,\n",
       "  0.3938888888888889,\n",
       "  0.5972222222222222,\n",
       "  0.5994444444444444,\n",
       "  0.5766666666666667,\n",
       "  0.41200000000000003,\n",
       "  0.5244444444444445,\n",
       "  0.5033333333333334,\n",
       "  0.38375,\n",
       "  0.5583333333333333,\n",
       "  0.41000000000000003,\n",
       "  0.353125,\n",
       "  0.4083333333333333,\n",
       "  0.5538888888888889,\n",
       "  0.4516666666666666,\n",
       "  0.48687499999999995,\n",
       "  0.43,\n",
       "  0.5311111111111111,\n",
       "  0.495,\n",
       "  0.4122222222222222,\n",
       "  0.34,\n",
       "  0.459375,\n",
       "  0.4488888888888888,\n",
       "  0.23000000000000004,\n",
       "  0.5466666666666667,\n",
       "  0.5288888888888889,\n",
       "  0.46611111111111114,\n",
       "  0.5355555555555557,\n",
       "  0.61625,\n",
       "  0.4264285714285715,\n",
       "  0.11000000000000001,\n",
       "  0.5311111111111111,\n",
       "  0.299375,\n",
       "  0.5208333333333334,\n",
       "  0.4972222222222222,\n",
       "  0.41500000000000004,\n",
       "  0.30000000000000004,\n",
       "  0.6605555555555556,\n",
       "  0.43714285714285717,\n",
       "  0.375,\n",
       "  0.486875,\n",
       "  0.4933333333333333,\n",
       "  0.5244444444444444,\n",
       "  0.51,\n",
       "  0.610625,\n",
       "  0.5585714285714286,\n",
       "  0.585,\n",
       "  0.44428571428571423,\n",
       "  0.3188888888888889,\n",
       "  0.573888888888889,\n",
       "  0.5287499999999999,\n",
       "  0.5172222222222221,\n",
       "  0.6372222222222222,\n",
       "  0.380625,\n",
       "  0.42666666666666664,\n",
       "  0.330625,\n",
       "  0.4111111111111111,\n",
       "  0.28071428571428575,\n",
       "  0.57,\n",
       "  0.4475,\n",
       "  0.42125,\n",
       "  0.32944444444444443,\n",
       "  0.755625,\n",
       "  0.4466666666666666,\n",
       "  0.5131249999999999,\n",
       "  0.4655555555555556,\n",
       "  0.6783333333333333,\n",
       "  0.4255555555555556,\n",
       "  0.46214285714285713,\n",
       "  0.18666666666666668,\n",
       "  0.5272222222222223,\n",
       "  0.4511111111111111,\n",
       "  0.365,\n",
       "  0.5705555555555556,\n",
       "  0.48888888888888893,\n",
       "  0.42875,\n",
       "  0.58125,\n",
       "  0.4822222222222222,\n",
       "  0.48875,\n",
       "  0.385,\n",
       "  0.44166666666666665,\n",
       "  0.46166666666666667,\n",
       "  0.5711111111111111,\n",
       "  0.39888888888888885,\n",
       "  0.4705555555555556,\n",
       "  0.4811111111111111,\n",
       "  0.6238888888888888,\n",
       "  0.5172222222222222,\n",
       "  0.32,\n",
       "  0.6455555555555555,\n",
       "  0.4605555555555555,\n",
       "  0.3092857142857143,\n",
       "  0.484375,\n",
       "  0.30777777777777776,\n",
       "  0.2561111111111111,\n",
       "  0.48888888888888893,\n",
       "  0.5866666666666666,\n",
       "  0.3822222222222222,\n",
       "  0.5950000000000001,\n",
       "  0.30000000000000004,\n",
       "  0.34928571428571425,\n",
       "  0.44875,\n",
       "  0.5522222222222222,\n",
       "  0.574375,\n",
       "  0.42,\n",
       "  0.8041666666666666,\n",
       "  0.405625,\n",
       "  0.4694444444444444,\n",
       "  0.7164285714285714,\n",
       "  0.4733333333333334,\n",
       "  0.27714285714285714,\n",
       "  0.5575,\n",
       "  0.542,\n",
       "  0.49624999999999997,\n",
       "  0.5483333333333333,\n",
       "  0.5322222222222223,\n",
       "  0.4427777777777778,\n",
       "  0.5116666666666667,\n",
       "  0.6261111111111112,\n",
       "  0.5172222222222222,\n",
       "  0.42833333333333334,\n",
       "  0.54,\n",
       "  0.3533333333333333,\n",
       "  0.39,\n",
       "  0.4978571428571429,\n",
       "  0.51125,\n",
       "  0.5027777777777778,\n",
       "  0.44611111111111107,\n",
       "  0.5666666666666668,\n",
       "  0.47555555555555556,\n",
       "  0.38666666666666666,\n",
       "  0.5844444444444445,\n",
       "  0.47388888888888886,\n",
       "  0.344375,\n",
       "  0.3566666666666667,\n",
       "  0.34055555555555556,\n",
       "  0.5228571428571429,\n",
       "  0.4327777777777777,\n",
       "  0.37714285714285717,\n",
       "  0.604375,\n",
       "  0.6822222222222223,\n",
       "  0.4655555555555555,\n",
       "  0.6666666666666666,\n",
       "  0.601875,\n",
       "  0.49666666666666665,\n",
       "  0.46111111111111114,\n",
       "  0.37875000000000003,\n",
       "  0.4621428571428572,\n",
       "  0.4255555555555556,\n",
       "  0.446875,\n",
       "  0.479375,\n",
       "  0.4261111111111111,\n",
       "  0.43333333333333335,\n",
       "  0.37625,\n",
       "  0.5299999999999999,\n",
       "  0.46499999999999986,\n",
       "  0.4244444444444445,\n",
       "  0.568125,\n",
       "  0.42111111111111116,\n",
       "  0.23625,\n",
       "  0.7050000000000001,\n",
       "  0.6961111111111111,\n",
       "  0.56125,\n",
       "  0.40285714285714286,\n",
       "  0.645,\n",
       "  0.42166666666666663,\n",
       "  0.6488888888888888,\n",
       "  0.47562499999999996,\n",
       "  0.586875,\n",
       "  0.47777777777777786,\n",
       "  0.6138888888888889,\n",
       "  0.47125,\n",
       "  0.39749999999999996,\n",
       "  0.4822222222222222,\n",
       "  0.6221428571428572,\n",
       "  0.62375,\n",
       "  0.608125,\n",
       "  0.2816666666666667,\n",
       "  0.30722222222222223,\n",
       "  0.33875,\n",
       "  0.5816666666666667,\n",
       "  0.4883333333333333,\n",
       "  0.41833333333333333,\n",
       "  0.63375,\n",
       "  0.44722222222222224,\n",
       "  0.38916666666666666,\n",
       "  0.5164285714285715,\n",
       "  0.5221428571428571,\n",
       "  0.4583333333333333,\n",
       "  0.3844444444444444,\n",
       "  0.44125000000000003,\n",
       "  0.641875,\n",
       "  0.4361111111111111,\n",
       "  0.3527777777777778,\n",
       "  0.3977777777777778,\n",
       "  0.6233333333333333,\n",
       "  0.5883333333333334,\n",
       "  0.6394444444444445,\n",
       "  0.48928571428571427,\n",
       "  0.265,\n",
       "  0.553125,\n",
       "  0.39222222222222225,\n",
       "  0.4172222222222222,\n",
       "  0.6368750000000001,\n",
       "  0.538125,\n",
       "  0.3142857142857142,\n",
       "  0.6825,\n",
       "  0.4361111111111111,\n",
       "  0.4892857142857143,\n",
       "  0.4855555555555556,\n",
       "  0.5172222222222221,\n",
       "  0.5441666666666666,\n",
       "  0.5722222222222222,\n",
       "  0.66125,\n",
       "  0.49277777777777776,\n",
       "  0.48333333333333334,\n",
       "  0.31166666666666665,\n",
       "  0.5311111111111111,\n",
       "  0.46499999999999997,\n",
       "  0.40611111111111114,\n",
       "  0.41388888888888886,\n",
       "  0.47611111111111115,\n",
       "  0.30062500000000003,\n",
       "  0.35375,\n",
       "  0.4305555555555556,\n",
       "  0.7,\n",
       "  0.37833333333333335,\n",
       "  0.6088888888888889,\n",
       "  0.479375,\n",
       "  0.35777777777777775,\n",
       "  0.6327777777777777,\n",
       "  0.36714285714285716,\n",
       "  0.41,\n",
       "  0.3733333333333333,\n",
       "  0.5633333333333334,\n",
       "  0.48055555555555557,\n",
       "  0.5338888888888889,\n",
       "  0.40285714285714286,\n",
       "  0.484375,\n",
       "  0.44222222222222224,\n",
       "  0.46399999999999997,\n",
       "  0.32611111111111113,\n",
       "  0.45222222222222225,\n",
       "  0.57375,\n",
       "  0.473,\n",
       "  0.6549999999999999,\n",
       "  0.4577777777777778,\n",
       "  0.46222222222222226,\n",
       "  0.43166666666666664,\n",
       "  0.6094444444444443,\n",
       "  0.5677777777777777,\n",
       "  0.26333333333333336,\n",
       "  0.688125,\n",
       "  0.33944444444444444,\n",
       "  0.5755555555555555,\n",
       "  0.5362500000000001,\n",
       "  0.5771428571428572,\n",
       "  0.3575,\n",
       "  0.6611111111111111,\n",
       "  0.531875,\n",
       "  0.34,\n",
       "  0.653888888888889,\n",
       "  0.6405555555555555,\n",
       "  0.43,\n",
       "  0.505,\n",
       "  0.45499999999999996,\n",
       "  0.38388888888888884,\n",
       "  0.48562500000000003,\n",
       "  0.5672222222222223,\n",
       "  0.5231250000000001,\n",
       "  0.529375,\n",
       "  0.39722222222222225,\n",
       "  0.5718749999999999,\n",
       "  0.40800000000000003,\n",
       "  0.39375000000000004,\n",
       "  0.49071428571428577,\n",
       "  0.513888888888889,\n",
       "  0.5728571428571428,\n",
       "  0.4511111111111112,\n",
       "  0.5605555555555556,\n",
       "  0.4466666666666667,\n",
       "  0.5872222222222222,\n",
       "  0.47,\n",
       "  0.5792857142857143,\n",
       "  0.5891666666666667,\n",
       "  0.5294444444444444,\n",
       "  0.4305555555555556,\n",
       "  0.4561111111111111,\n",
       "  0.4185714285714286,\n",
       "  0.5341666666666667,\n",
       "  0.3955555555555556,\n",
       "  0.37000000000000005,\n",
       "  0.6328571428571429,\n",
       "  0.41571428571428576,\n",
       "  0.47388888888888897,\n",
       "  0.3922222222222222,\n",
       "  0.5218750000000001,\n",
       "  0.47888888888888886,\n",
       "  0.34611111111111115,\n",
       "  0.46714285714285714,\n",
       "  0.48999999999999994,\n",
       "  0.428125,\n",
       "  0.48888888888888893,\n",
       "  0.3266666666666666,\n",
       "  0.49071428571428577,\n",
       "  0.600625,\n",
       "  0.30666666666666664,\n",
       "  0.28750000000000003,\n",
       "  0.46125,\n",
       "  0.27166666666666667,\n",
       "  0.4364285714285714,\n",
       "  0.663125,\n",
       "  0.49222222222222217,\n",
       "  0.553125,\n",
       "  0.50875,\n",
       "  0.39944444444444444,\n",
       "  0.49750000000000005,\n",
       "  0.45625,\n",
       "  0.38611111111111107,\n",
       "  0.54875,\n",
       "  0.5116666666666667,\n",
       "  0.43562500000000004,\n",
       "  0.5744444444444444,\n",
       "  0.5287499999999999,\n",
       "  0.40750000000000003,\n",
       "  0.44166666666666665,\n",
       "  0.4355555555555556,\n",
       "  0.4811111111111111,\n",
       "  0.4077777777777778,\n",
       "  0.40611111111111114,\n",
       "  0.569375,\n",
       "  0.5277777777777778,\n",
       "  0.445,\n",
       "  0.5627777777777778,\n",
       "  0.41083333333333333,\n",
       "  0.5216666666666666,\n",
       "  0.3611111111111111,\n",
       "  0.48666666666666664,\n",
       "  0.5221428571428571,\n",
       "  0.446875,\n",
       "  0.400625,\n",
       "  0.44166666666666665,\n",
       "  0.5388888888888889,\n",
       "  0.5044444444444445,\n",
       "  0.5322222222222223,\n",
       "  0.5900000000000001,\n",
       "  0.37625,\n",
       "  0.348125,\n",
       "  0.59625,\n",
       "  0.5461111111111111,\n",
       "  0.2016666666666667,\n",
       "  0.5733333333333334,\n",
       "  0.441,\n",
       "  0.27666666666666667,\n",
       "  0.5650000000000001,\n",
       "  0.5566666666666666,\n",
       "  0.5661111111111112,\n",
       "  0.5025,\n",
       "  0.33999999999999997,\n",
       "  0.48500000000000004,\n",
       "  0.3438888888888889,\n",
       "  0.424375,\n",
       "  0.5422222222222222,\n",
       "  0.5633333333333332,\n",
       "  0.533125,\n",
       "  0.6724999999999999,\n",
       "  0.27166666666666667,\n",
       "  0.30833333333333335,\n",
       "  0.6566666666666667,\n",
       "  0.370625,\n",
       "  0.32499999999999996,\n",
       "  0.5228571428571428,\n",
       "  0.5666666666666665,\n",
       "  0.6012500000000001,\n",
       "  0.57,\n",
       "  0.385,\n",
       "  0.3757142857142858,\n",
       "  0.4905555555555556,\n",
       "  0.4272222222222222,\n",
       "  0.56,\n",
       "  0.3794444444444445,\n",
       "  0.6116666666666667,\n",
       "  0.395,\n",
       "  0.44214285714285717,\n",
       "  0.5783333333333334,\n",
       "  0.364375,\n",
       "  0.5738888888888889,\n",
       "  0.4255555555555556,\n",
       "  0.7268749999999999,\n",
       "  0.49,\n",
       "  0.543125,\n",
       "  0.5627777777777778,\n",
       "  0.6088888888888889,\n",
       "  0.235,\n",
       "  0.51125,\n",
       "  0.5127777777777778,\n",
       "  0.44555555555555554,\n",
       "  0.682142857142857,\n",
       "  0.4938888888888888,\n",
       "  0.5275,\n",
       "  0.3925,\n",
       "  0.5488888888888889,\n",
       "  0.5438888888888889,\n",
       "  0.41600000000000004,\n",
       "  0.4627777777777778,\n",
       "  0.4466666666666667,\n",
       "  0.46785714285714286,\n",
       "  0.6733333333333333,\n",
       "  0.5016666666666667,\n",
       "  0.4494444444444444,\n",
       "  0.43166666666666664,\n",
       "  0.5511111111111111,\n",
       "  0.3085714285714286,\n",
       "  0.45249999999999996,\n",
       "  0.3307142857142857,\n",
       "  0.5575,\n",
       "  0.48222222222222233,\n",
       "  0.08333333333333333,\n",
       "  0.421875,\n",
       "  0.4077777777777778,\n",
       "  0.4535714285714286,\n",
       "  0.4161111111111111,\n",
       "  0.5222222222222223,\n",
       "  0.4833333333333334,\n",
       "  0.4766666666666667,\n",
       "  0.6155555555555555,\n",
       "  0.4091666666666667,\n",
       "  0.42166666666666663,\n",
       "  0.5244444444444445,\n",
       "  0.5038888888888889,\n",
       "  0.3942857142857143,\n",
       "  0.6685714285714287,\n",
       "  0.63,\n",
       "  0.353,\n",
       "  0.5549999999999999,\n",
       "  0.5566666666666666,\n",
       "  0.49888888888888894,\n",
       "  0.6578571428571429,\n",
       "  0.614375,\n",
       "  0.5862499999999999,\n",
       "  0.3841666666666666,\n",
       "  0.6188888888888889,\n",
       "  0.27666666666666667,\n",
       "  0.375,\n",
       "  0.415,\n",
       "  0.50875,\n",
       "  0.4855555555555556,\n",
       "  0.41874999999999996,\n",
       "  0.4411111111111111,\n",
       "  0.4864285714285715,\n",
       "  0.5466666666666666,\n",
       "  0.6094444444444445,\n",
       "  0.6055555555555555,\n",
       "  0.4905555555555556,\n",
       "  0.69,\n",
       "  0.5766666666666668,\n",
       "  0.576875,\n",
       "  0.43333333333333335,\n",
       "  0.6242857142857143,\n",
       "  0.6183333333333333,\n",
       "  0.34187500000000004,\n",
       "  0.4366666666666667,\n",
       "  0.58125,\n",
       "  0.5072222222222222,\n",
       "  0.26785714285714285,\n",
       "  0.45999999999999996,\n",
       "  0.47857142857142854,\n",
       "  0.4466666666666667,\n",
       "  0.58875,\n",
       "  0.5744444444444444,\n",
       "  0.44562500000000005,\n",
       "  0.6083333333333334,\n",
       "  0.4069999999999999,\n",
       "  0.41055555555555556,\n",
       "  0.605625,\n",
       "  0.39555555555555555,\n",
       "  0.4322222222222222,\n",
       "  0.48499999999999993,\n",
       "  0.5894444444444445,\n",
       "  0.548125,\n",
       "  0.5833333333333335,\n",
       "  0.44166666666666665,\n",
       "  0.3585714285714286,\n",
       "  0.5121428571428572,\n",
       "  0.5425,\n",
       "  0.32875,\n",
       "  0.621875,\n",
       "  0.5322222222222224,\n",
       "  0.5622222222222222,\n",
       "  0.448125,\n",
       "  0.380625,\n",
       "  0.2911111111111111,\n",
       "  0.59,\n",
       "  0.4427777777777778,\n",
       "  0.29312499999999997,\n",
       "  0.3466666666666667,\n",
       "  0.5575,\n",
       "  0.22,\n",
       "  0.63,\n",
       "  0.4088888888888889,\n",
       "  0.445,\n",
       "  0.505,\n",
       "  0.495,\n",
       "  0.495625,\n",
       "  0.5961111111111111,\n",
       "  0.41500000000000004,\n",
       "  0.4172222222222222,\n",
       "  0.37666666666666665,\n",
       "  0.4538888888888889,\n",
       "  0.318,\n",
       "  0.4883333333333334,\n",
       "  0.5800000000000001,\n",
       "  0.6087499999999999,\n",
       "  0.44833333333333336,\n",
       "  0.465625,\n",
       "  0.49624999999999997,\n",
       "  0.2916666666666667,\n",
       "  0.49888888888888894,\n",
       "  0.5733333333333334,\n",
       "  0.36562500000000003,\n",
       "  0.355625,\n",
       "  0.5122222222222222,\n",
       "  0.2241666666666667,\n",
       "  0.5377777777777778,\n",
       "  0.3935714285714286,\n",
       "  0.4228571428571429,\n",
       "  0.5133333333333334,\n",
       "  0.5227777777777778,\n",
       "  0.54375,\n",
       "  0.6585714285714285,\n",
       "  0.4772222222222222,\n",
       "  0.3611111111111111,\n",
       "  0.598125,\n",
       "  0.4388888888888889,\n",
       "  0.5911111111111111,\n",
       "  0.4972222222222222,\n",
       "  0.35562499999999997,\n",
       "  0.335,\n",
       "  0.4766666666666667,\n",
       "  0.38749999999999996,\n",
       "  0.3338888888888889,\n",
       "  0.47187499999999993,\n",
       "  0.5038888888888889,\n",
       "  0.3927777777777777,\n",
       "  0.44277777777777777,\n",
       "  0.45999999999999996,\n",
       "  0.31928571428571434,\n",
       "  0.5577777777777778,\n",
       "  0.8200000000000001,\n",
       "  0.365,\n",
       "  0.36812500000000004,\n",
       "  0.335,\n",
       "  0.43555555555555553,\n",
       "  0.5227777777777778,\n",
       "  0.3238888888888889,\n",
       "  0.47714285714285715,\n",
       "  0.3561111111111111,\n",
       "  0.3172222222222222,\n",
       "  0.568125,\n",
       "  0.4972222222222222,\n",
       "  0.6583333333333333,\n",
       "  0.6805555555555557,\n",
       "  0.38555555555555554,\n",
       "  0.399375,\n",
       "  0.285625,\n",
       "  0.473,\n",
       "  0.4238888888888889,\n",
       "  0.48944444444444446,\n",
       "  0.49500000000000005,\n",
       "  0.5383333333333333,\n",
       "  0.5231250000000001,\n",
       "  0.38875000000000004,\n",
       "  0.48888888888888893,\n",
       "  0.643125,\n",
       "  0.3064285714285714,\n",
       "  0.6749999999999999,\n",
       "  0.34444444444444444,\n",
       "  0.465625,\n",
       "  0.30833333333333335,\n",
       "  0.49750000000000005,\n",
       "  0.5062500000000001,\n",
       "  0.38749999999999996,\n",
       "  0.48666666666666664,\n",
       "  0.4937500000000001,\n",
       "  0.48888888888888893,\n",
       "  0.5194444444444444,\n",
       "  0.5005555555555556,\n",
       "  0.48750000000000004,\n",
       "  0.555,\n",
       "  0.7464285714285713,\n",
       "  0.34055555555555556,\n",
       "  0.36687499999999995,\n",
       "  0.20625000000000002,\n",
       "  0.48875,\n",
       "  0.411875,\n",
       "  0.4571428571428572,\n",
       "  0.573888888888889,\n",
       "  0.38571428571428573,\n",
       "  0.4621428571428572,\n",
       "  0.6074999999999999,\n",
       "  0.6511111111111111,\n",
       "  0.46,\n",
       "  0.5155555555555555,\n",
       "  0.4864285714285714,\n",
       "  0.4414285714285714,\n",
       "  0.48277777777777786,\n",
       "  0.31,\n",
       "  0.6505555555555556,\n",
       "  0.515,\n",
       "  0.43722222222222223,\n",
       "  0.7149999999999999,\n",
       "  0.3661111111111111,\n",
       "  0.5833333333333334,\n",
       "  0.395,\n",
       "  0.32142857142857145,\n",
       "  0.41125,\n",
       "  0.48277777777777775,\n",
       "  0.26611111111111113,\n",
       "  0.444375,\n",
       "  0.5292857142857142,\n",
       "  0.4266666666666667,\n",
       "  0.44357142857142856,\n",
       "  0.4772222222222222,\n",
       "  0.5744444444444444,\n",
       "  0.4577777777777777,\n",
       "  0.32277777777777783,\n",
       "  0.42750000000000005,\n",
       "  0.4,\n",
       "  0.4538888888888889,\n",
       "  0.56,\n",
       "  0.3228571428571429,\n",
       "  0.5966666666666667,\n",
       "  0.40285714285714286,\n",
       "  0.338125,\n",
       "  0.61,\n",
       "  0.6750000000000002,\n",
       "  0.473125,\n",
       "  0.5485714285714286,\n",
       "  0.49071428571428577,\n",
       "  0.4716666666666667,\n",
       "  0.42142857142857143,\n",
       "  0.53125,\n",
       "  0.4844444444444444,\n",
       "  0.37124999999999997,\n",
       "  0.47555555555555556,\n",
       "  0.42777777777777776,\n",
       "  0.3838888888888889,\n",
       "  0.435,\n",
       "  0.35874999999999996,\n",
       "  0.46388888888888885,\n",
       "  0.495,\n",
       "  0.265625,\n",
       "  0.4655555555555556,\n",
       "  0.6572222222222224,\n",
       "  0.5255555555555556,\n",
       "  0.49749999999999994,\n",
       "  0.44722222222222224,\n",
       "  0.5531249999999999,\n",
       "  0.4222222222222222,\n",
       "  0.55125,\n",
       "  0.555,\n",
       "  0.445,\n",
       "  0.46375,\n",
       "  0.5211111111111112,\n",
       "  0.426,\n",
       "  0.5766666666666667,\n",
       "  0.4566666666666666,\n",
       "  0.56125,\n",
       "  0.496875,\n",
       "  0.32875,\n",
       "  0.36300000000000004,\n",
       "  0.2927777777777778,\n",
       "  0.43833333333333335,\n",
       "  0.355,\n",
       "  0.723125,\n",
       "  0.4707142857142857,\n",
       "  0.48000000000000004,\n",
       "  0.4085714285714287,\n",
       "  0.42000000000000004,\n",
       "  0.5541666666666666,\n",
       "  0.5994444444444444,\n",
       "  0.5538888888888889,\n",
       "  0.3855555555555556,\n",
       "  0.4816666666666667,\n",
       "  0.48142857142857143,\n",
       "  0.37444444444444447,\n",
       "  0.3835714285714285,\n",
       "  0.5271428571428572,\n",
       "  0.30083333333333334,\n",
       "  0.15857142857142859,\n",
       "  0.54625,\n",
       "  0.5672222222222223,\n",
       "  0.451875,\n",
       "  0.35111111111111115,\n",
       "  0.5828571428571429,\n",
       "  0.6012500000000001,\n",
       "  0.43000000000000005,\n",
       "  0.5372222222222223,\n",
       "  0.49944444444444447,\n",
       "  0.413,\n",
       "  0.7427777777777779,\n",
       "  0.526875,\n",
       "  0.488125,\n",
       "  0.42500000000000004,\n",
       "  0.4275,\n",
       "  0.5055555555555555,\n",
       "  0.5861111111111111,\n",
       "  0.45444444444444443,\n",
       "  0.292,\n",
       "  0.31875,\n",
       "  0.4683333333333333,\n",
       "  0.45944444444444443,\n",
       "  0.6756249999999999,\n",
       "  0.4307142857142857,\n",
       "  0.35928571428571426,\n",
       "  0.5075,\n",
       "  0.4138888888888889,\n",
       "  0.42388888888888887,\n",
       "  0.5650000000000001,\n",
       "  0.5094444444444445,\n",
       "  0.49444444444444446,\n",
       "  0.6755555555555556,\n",
       "  0.4925,\n",
       "  0.5261111111111111,\n",
       "  0.3441666666666667,\n",
       "  0.49875,\n",
       "  0.3716666666666667,\n",
       "  0.6083333333333333,\n",
       "  0.5287499999999999,\n",
       "  0.633888888888889,\n",
       "  0.27375,\n",
       "  0.55625,\n",
       "  0.27071428571428574,\n",
       "  0.5175000000000001,\n",
       "  0.6158333333333333,\n",
       "  0.49277777777777776,\n",
       "  0.606875,\n",
       "  0.5857142857142856,\n",
       "  0.6528571428571429,\n",
       "  0.48250000000000004,\n",
       "  0.33999999999999997,\n",
       "  0.445,\n",
       "  0.611875,\n",
       "  0.6377777777777778,\n",
       "  0.43,\n",
       "  0.6078571428571429,\n",
       "  0.5561111111111111,\n",
       "  0.5475000000000001,\n",
       "  0.40875,\n",
       "  0.5755555555555556,\n",
       "  0.49222222222222217,\n",
       "  0.555,\n",
       "  0.5427777777777778,\n",
       "  0.555625,\n",
       "  0.3455555555555556,\n",
       "  0.46875,\n",
       "  0.47777777777777786,\n",
       "  0.5816666666666667,\n",
       "  0.44833333333333336,\n",
       "  0.43333333333333335,\n",
       "  0.47124999999999995,\n",
       "  0.3566666666666667,\n",
       "  0.3938888888888889,\n",
       "  0.39111111111111113,\n",
       "  0.49555555555555564,\n",
       "  0.535,\n",
       "  0.6005555555555555,\n",
       "  0.40666666666666673,\n",
       "  0.32,\n",
       "  0.3855555555555556,\n",
       "  0.49833333333333335,\n",
       "  0.6288888888888889,\n",
       "  0.4742857142857143,\n",
       "  0.4375,\n",
       "  0.295625,\n",
       "  0.515625,\n",
       "  0.27999999999999997,\n",
       "  0.33666666666666667,\n",
       "  0.451875,\n",
       "  0.35777777777777775,\n",
       "  0.5072222222222221,\n",
       "  0.5688888888888888,\n",
       "  0.4125,\n",
       "  0.4861111111111111,\n",
       "  0.405,\n",
       "  0.6027777777777777,\n",
       "  0.4705555555555555,\n",
       "  0.4066666666666666,\n",
       "  0.5335714285714286,\n",
       "  0.6127777777777778,\n",
       "  0.45611111111111113,\n",
       "  0.44388888888888883,\n",
       "  0.47687500000000005,\n",
       "  0.52,\n",
       "  0.296875,\n",
       "  0.6566666666666667,\n",
       "  0.4705555555555556,\n",
       "  0.5216666666666667,\n",
       "  0.19333333333333333,\n",
       "  0.496875,\n",
       "  0.4624999999999999,\n",
       "  0.6344444444444445,\n",
       "  0.43999999999999995,\n",
       "  0.5816666666666667,\n",
       "  0.4794444444444445,\n",
       "  0.4972222222222222,\n",
       "  0.2721428571428572,\n",
       "  0.41625,\n",
       "  0.5191666666666667,\n",
       "  0.6222222222222222,\n",
       "  0.5122222222222222,\n",
       "  0.49,\n",
       "  0.5477777777777778,\n",
       "  0.3408333333333333,\n",
       "  0.41111111111111115,\n",
       "  0.41722222222222227,\n",
       "  0.5377777777777778,\n",
       "  0.5175,\n",
       "  0.5505555555555556,\n",
       "  0.25857142857142856,\n",
       "  0.4277777777777778,\n",
       "  0.375625,\n",
       "  0.6277777777777778,\n",
       "  0.5325,\n",
       "  0.55875,\n",
       "  0.34562499999999996,\n",
       "  0.5411111111111111,\n",
       "  0.4640000000000001,\n",
       "  0.49285714285714294,\n",
       "  0.30562500000000004,\n",
       "  0.5611111111111111,\n",
       "  0.6031249999999999,\n",
       "  0.48625,\n",
       "  0.44583333333333336,\n",
       "  0.49833333333333335,\n",
       "  0.6842857142857143,\n",
       "  0.5477777777777777,\n",
       "  0.496875,\n",
       "  0.49000000000000005,\n",
       "  0.570625,\n",
       "  0.4905555555555556,\n",
       "  0.37722222222222224,\n",
       "  0.5427777777777778,\n",
       "  0.47071428571428564,\n",
       "  0.42625,\n",
       "  0.42800000000000005,\n",
       "  0.3755555555555556,\n",
       "  0.485,\n",
       "  0.4911111111111111,\n",
       "  0.5075000000000001,\n",
       "  0.6466666666666667,\n",
       "  0.41600000000000004,\n",
       "  0.400625,\n",
       "  0.546111111111111,\n",
       "  0.5125000000000001,\n",
       "  0.496875,\n",
       "  0.5757142857142857,\n",
       "  0.3844444444444444,\n",
       "  0.7214285714285714,\n",
       "  0.4011111111111111,\n",
       "  0.6657142857142857,\n",
       "  0.395625,\n",
       "  0.5872222222222223,\n",
       "  ...]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_trainer._eval_save_prefix = OUTPUT_PREFIX + \"-test-pos-unbiased\"\n",
    "model_trainer._evaluate_partial(test_dataset_pos_unbiased)\n",
    "\n",
    "model_trainer._eval_save_prefix = OUTPUT_PREFIX +  \"-test-neg-unbiased\"\n",
    "model_trainer._evaluate_partial(test_dataset_neg_unbiased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "biased_results = dict()\n",
    "\n",
    "\n",
    "\n",
    "# biased_results[\"STRATIFIED_15\"] = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=1.5, K=30, partition=100)\n",
    "biased_results[\"AOA\"] = aoa(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", K=30)\n",
    "biased_results[\"UB_15\"] = eq(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=1.5, K=30)\n",
    "biased_results[\"UB_2\"] =  eq(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2, K=30)\n",
    "biased_results[\"UB_25\"] =  eq(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2.5, K=30)\n",
    "biased_results[\"UB_3\"] =  eq(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=3, K=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbiased_results = dict()\n",
    "\n",
    "# unbiased_results[\"STRATIFIED_15\"] = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=1.5, K=1, partition=100)\n",
    "unbiased_results[\"AOA\"] = aoa(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", K=1)\n",
    "unbiased_results[\"UB_15\"] = eq(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=1.5, K=1)\n",
    "unbiased_results[\"UB_2\"] =  eq(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2, K=1)\n",
    "unbiased_results[\"UB_25\"] =  eq(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2.5, K=1)\n",
    "unbiased_results[\"UB_3\"] =  eq(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=3, K=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_items = 932"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([690, 683, 656, 541, 490, 485, 654, 858, 880,  51, 353, 805, 689,\n",
       "        40, 548, 681, 393, 433, 780, 168, 122, 146, 600, 865, 564, 379,\n",
       "         2, 286, 499, 809, 470, 457, 910, 610, 321, 297, 666, 200, 444,\n",
       "       363, 505, 421, 834, 354, 917, 520, 833, 238, 604, 694, 546, 411,\n",
       "       686, 357, 117,  18, 108, 540, 530, 295, 177, 484, 273, 611,  62,\n",
       "       251, 892, 478, 324, 753, 920,  26, 300, 730, 440, 156, 680, 644,\n",
       "       722, 732, 822, 463, 637, 601, 378, 662, 606, 578, 521, 244, 225,\n",
       "        38, 333, 659, 270, 172,  24,  55, 932, 571, 848, 513, 284, 804,\n",
       "        20, 483, 128, 302,  11, 341, 428, 742, 390, 905, 206, 854, 752,\n",
       "       141,   5,  19,  22, 798, 303,   3, 422, 215, 298, 538, 467, 639,\n",
       "       915, 278, 911, 700, 266,  33, 893, 167, 370, 814, 356, 725, 870,\n",
       "       472, 309, 676, 197, 492, 228, 898, 866, 192, 557, 265, 692, 460,\n",
       "       118, 373, 664, 360, 607, 526,  43, 625, 152, 340, 294, 886,  96,\n",
       "       188,  30, 817, 925, 747, 299, 450, 684, 731, 672, 618, 142, 271,\n",
       "       928, 178, 721, 719, 648, 885, 332, 166, 764, 189, 653, 838, 758,\n",
       "       458, 227, 590, 667,  93, 455, 272, 703, 190,  67,  97, 895, 560,\n",
       "       646, 554, 868, 346, 785, 778, 835, 549, 429, 533, 289, 896,  15,\n",
       "       770, 296, 113, 230, 883, 771, 395, 799, 629,  46, 729, 786, 383,\n",
       "        63,   9, 784, 155, 318, 751, 173,  87, 763, 434, 612, 650, 220,\n",
       "       818, 512, 675, 231, 749, 329, 714,  12, 774, 878, 890, 768, 640,\n",
       "       776, 389, 456, 718, 120, 638, 276, 831, 919, 511, 581, 417, 737,\n",
       "       793, 615, 476, 567, 824, 149, 236, 191, 445, 437, 246, 503, 420,\n",
       "       642,  89, 529,  68, 361, 399,  86, 573, 823, 580,  77,  47, 400,\n",
       "       129])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums = np.arange(1, num_items+1)\n",
    "\n",
    "partitions = np.random.choice(nums, 300, replace=False)\n",
    "partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea2846814f4b40e9ae6ae2698ff93b57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute biased and unbiased results with stratified for values of partition in (1,2*len(sorted_items))\n",
    "# and store biased and unbiased results such that abs(biased_results[key]['auc'] - unbiased_results[key]['auc']) + abs(biased_results[key]['recall'] - unbiased_results[key]['recall']) is minimized\n",
    "\n",
    "#This is the gamma used to compute the best partition\n",
    "gamma = 2\n",
    "\n",
    "key = \"STRATIFIED_\" + str(gamma).replace(\".\",\"\")\n",
    "\n",
    "unbiased_results[key] = dict()\n",
    "biased_results[key] = dict()\n",
    "best_partition=1\n",
    "\n",
    "#for p in tqdm(range(1, 2*num_items)):\n",
    "for p in tqdm(partitions):\n",
    "    temp_unbiased = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=gamma, K=1, partition=p)\n",
    "    temp_biased = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=gamma, K=30, partition=p)\n",
    "    if not unbiased_results[key]:\n",
    "        unbiased_results[key] = temp_unbiased\n",
    "    if not biased_results[key]:\n",
    "        biased_results[key] = temp_biased\n",
    "    elif abs(temp_biased['auc'] - temp_unbiased['auc']) + abs(temp_unbiased['recall'] - temp_biased['recall']) < abs(biased_results[key]['auc'] - unbiased_results[key]['auc']) + abs(biased_results[key]['recall'] - unbiased_results[key]['recall']):\n",
    "        biased_results[key]['auc'] = temp_biased['auc']\n",
    "        biased_results[key]['recall'] = temp_biased['recall']\n",
    "        unbiased_results[key]['auc'] = temp_unbiased['auc']\n",
    "        unbiased_results[key]['recall'] = temp_unbiased['recall']\n",
    "        best_partition = p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbiased_results[\"STRATIFIED_15\"] = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=1.5, K=1, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_15\"] = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=1.5, K=30, partition=best_partition)\n",
    "\n",
    "unbiased_results[\"STRATIFIED_2\"] = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2, K=1, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_2\"] = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2, K=30, partition=best_partition)\n",
    "\n",
    "unbiased_results[\"STRATIFIED_25\"] = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2.5, K=1, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_25\"] = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2.5, K=30, partition=best_partition)\n",
    "\n",
    "unbiased_results[\"STRATIFIED_3\"] = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=3, K=1, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_3\"] = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=3, K=30, partition=best_partition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbiased_results[\"STRATIFIED_v2_15\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=1.5, K=1, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_v2_15\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=1.5, K=30, partition=best_partition)\n",
    "\n",
    "unbiased_results[\"STRATIFIED_v2_2\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2, K=1, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_v2_2\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2, K=30, partition=best_partition)\n",
    "\n",
    "unbiased_results[\"STRATIFIED_v2_25\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2.5, K=1, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_v2_25\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2.5, K=30, partition=best_partition)\n",
    "\n",
    "unbiased_results[\"STRATIFIED_v2_3\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=3, K=1, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_v2_3\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=3, K=30, partition=best_partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "key, value = random.choice(list(biased_results.items()))\n",
    "rows = 2#len(list(value.keys()))\n",
    "columns = 13#len(list(biased_results.items()))\n",
    "results_array = np.zeros((rows,columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_results = dict()\n",
    "\n",
    "list_biased_res = list(biased_results.keys())\n",
    "\n",
    "for i in range(len(list_biased_res)):\n",
    "    key = list_biased_res[i]\n",
    "\n",
    "    for j in range(len(list(biased_results[key].keys()))):\n",
    "        key_2 = list(biased_results[key].keys())[j]\n",
    "\n",
    "        results_array[j][i] = abs(biased_results[key][key_2] - unbiased_results[key][key_2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_df = pd.DataFrame(columns=list(biased_results.keys()), data=results_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_values = list(biased_results[list(biased_results.keys())[0]].keys())\n",
    "mae_df.insert(0, \"metric\", metric_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **RESULTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>AOA</th>\n",
       "      <th>UB_15</th>\n",
       "      <th>UB_2</th>\n",
       "      <th>UB_25</th>\n",
       "      <th>UB_3</th>\n",
       "      <th>STRATIFIED_15</th>\n",
       "      <th>STRATIFIED_2</th>\n",
       "      <th>STRATIFIED_25</th>\n",
       "      <th>STRATIFIED_3</th>\n",
       "      <th>STRATIFIED_v2_15</th>\n",
       "      <th>STRATIFIED_v2_2</th>\n",
       "      <th>STRATIFIED_v2_25</th>\n",
       "      <th>STRATIFIED_v2_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>auc</td>\n",
       "      <td>0.151122</td>\n",
       "      <td>0.124741</td>\n",
       "      <td>0.121437</td>\n",
       "      <td>0.118931</td>\n",
       "      <td>0.117027</td>\n",
       "      <td>0.112245</td>\n",
       "      <td>0.006199</td>\n",
       "      <td>0.148877</td>\n",
       "      <td>0.439170</td>\n",
       "      <td>0.127152</td>\n",
       "      <td>0.124270</td>\n",
       "      <td>0.122205</td>\n",
       "      <td>0.120769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>recall</td>\n",
       "      <td>0.374820</td>\n",
       "      <td>0.255265</td>\n",
       "      <td>0.243422</td>\n",
       "      <td>0.234609</td>\n",
       "      <td>0.227997</td>\n",
       "      <td>0.256658</td>\n",
       "      <td>0.242037</td>\n",
       "      <td>0.305144</td>\n",
       "      <td>0.687337</td>\n",
       "      <td>0.257185</td>\n",
       "      <td>0.245721</td>\n",
       "      <td>0.237326</td>\n",
       "      <td>0.231176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   metric       AOA     UB_15      UB_2     UB_25      UB_3  STRATIFIED_15  \\\n",
       "0     auc  0.151122  0.124741  0.121437  0.118931  0.117027       0.112245   \n",
       "1  recall  0.374820  0.255265  0.243422  0.234609  0.227997       0.256658   \n",
       "\n",
       "   STRATIFIED_2  STRATIFIED_25  STRATIFIED_3  STRATIFIED_v2_15  \\\n",
       "0      0.006199       0.148877      0.439170          0.127152   \n",
       "1      0.242037       0.305144      0.687337          0.257185   \n",
       "\n",
       "   STRATIFIED_v2_2  STRATIFIED_v2_25  STRATIFIED_v2_3  \n",
       "0         0.124270          0.122205         0.120769  \n",
       "1         0.245721          0.237326         0.231176  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RecSysEvaluation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
