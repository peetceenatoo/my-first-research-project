{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some notes: \n",
    "- i saw that some user, item tuples of the random test set are present in the training set, is this ok?\n",
    "- is subsampling of 200 items ok?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy.random as npr\n",
    "from scipy import sparse, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from openrec.tf1.legacy import ImplicitModelTrainer\n",
    "from openrec.tf1.legacy.utils.evaluators import ImplicitEvalManager\n",
    "from openrec.tf1.legacy.utils import ImplicitDataset\n",
    "from openrec.tf1.legacy.recommenders import CML, BPR, PMF\n",
    "from openrec.tf1.legacy.utils.evaluators import AUC\n",
    "from openrec.tf1.legacy.utils.samplers import PairwiseSampler\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "seed = 2384795\n",
    "np.random.seed(seed=seed)\n",
    "\n",
    "# Preparing folder for output data\n",
    "output_name = f\"./generated_data/\"\n",
    "if os.path.exists(output_name) == False:\n",
    "    os.makedirs(output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './original_files/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(os.path.join(DATA_DIR, 'train.ascii'), sep=\" \", header=None, engine=\"python\")\n",
    "test_data = pd.read_csv(os.path.join(DATA_DIR, 'test.ascii'), sep=\" \", header=None, engine=\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_vd_data = pd.DataFrame({\"userId\": sparse.coo_matrix(raw_data).row,                            \"songId\": sparse.coo_matrix(raw_data).col,                           \"rating\": sparse.coo_matrix(raw_data).data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.DataFrame({\"userId\": sparse.coo_matrix(test_data).row,                            \"songId\": sparse.coo_matrix(test_data).col,                           \"rating\": sparse.coo_matrix(test_data).data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_proportion_old(data, uid, test_prop=0.5, random_seed=0):\n",
    "    data_grouped_by_user = data.groupby(uid)\n",
    "    tr_list, te_list = list(), list()\n",
    "\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    for u, (_, group) in enumerate(data_grouped_by_user):\n",
    "        n_items_u = len(group)\n",
    "\n",
    "        if n_items_u >= 5:\n",
    "            idx = np.zeros(n_items_u, dtype='bool')\n",
    "            idx[np.random.choice(n_items_u, size=int(test_prop * n_items_u), replace=False).astype('int64')] = True\n",
    "\n",
    "            tr_list.append(group[np.logical_not(idx)])\n",
    "            te_list.append(group[idx])\n",
    "        else:\n",
    "            tr_list.append(group)\n",
    "\n",
    "        if u % 5000 == 0:\n",
    "            print(\"%d users sampled\" % u)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    data_tr = pd.concat(tr_list)\n",
    "    data_te = pd.concat(te_list)\n",
    "    \n",
    "    return data_tr, data_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_proportion(data, random_seed=0):\n",
    "\n",
    "    df_train = data\n",
    "\n",
    "    # Create a test df\n",
    "    df_test = pd.DataFrame(columns=data.columns)\n",
    "\n",
    "    # Precompute, for each user, the list of songs with a relevant rating\n",
    "    user_positive_ratings = data[data[\"rating\"] == 1].groupby(\"user_id\")[\"item_id\"].apply(set)\n",
    "    \n",
    "    min_item, max_item = data['item_id'].min(), data['item_id'].max()\n",
    "\n",
    "    # Initialize the range of indexes for the items\n",
    "    items_ids = np.arange(min_item, max_item + 1)\n",
    "\n",
    "    # Set the number of songs for each user\n",
    "    SONGS_FOR_BIASED_TEST = 90\n",
    "\n",
    "    users = set(data[\"user_id\"].unique())\n",
    "\n",
    "    # Extract the biased test set\n",
    "    for user_id in users:\n",
    "\n",
    "        # Get SONGS_FOR_BIASED_TEST items\n",
    "        np.random.shuffle(items_ids)\n",
    "        test_items = set(items_ids[-SONGS_FOR_BIASED_TEST:])\n",
    "\n",
    "        # Get which are positive\n",
    "        pos_ids = user_positive_ratings.get(user_id, set()) & test_items\n",
    "\n",
    "        # Get which are negative but in test_items\n",
    "        neg_ids = test_items - pos_ids\n",
    "\n",
    "        # Set the positive ones to 0 in the training set (extract)\n",
    "        df_train.loc[(df_train['item_id'].isin(pos_ids)) & (df_train['user_id'] == user_id), 'rating'] = 0\n",
    "\n",
    "        # now add them in the test set\n",
    "        # add to df_test the rows made of [user_id, pos_ids, 1] and [user_id, neg_ids, 0]\n",
    "        for item_id in pos_ids:\n",
    "            df_test = df_test.append({'user_id': user_id, 'item_id': item_id, 'rating': 1}, ignore_index=True)\n",
    "        \n",
    "        for item_id in neg_ids:\n",
    "            df_test = df_test.append({'user_id': user_id, 'item_id': item_id, 'rating': 0}, ignore_index=True)\n",
    "\n",
    "    # Convert back to the correct data types\n",
    "    df_train['user_id'] = df_train['user_id'].astype(int)\n",
    "    df_train['item_id'] = df_train['item_id'].astype(int)\n",
    "    df_train['rating'] = df_train['rating'].astype(int)\n",
    "    \n",
    "    df_test['user_id'] = df_test['user_id'].astype(int)\n",
    "    df_test['item_id'] = df_test['item_id'].astype(int)\n",
    "    df_test['rating'] = df_test['rating'].astype(int)\n",
    "    \n",
    "    return df_train, df_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count(tp, id):\n",
    "    playcount_groupbyid = tp[[id]].groupby(id, as_index=False)\n",
    "    count = playcount_groupbyid.size()\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make dataset implicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>songId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>136</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>188</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>220</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>227</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>228</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>234</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>235</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  songId  rating\n",
       "0       0      72       2\n",
       "1       0     136       2\n",
       "2       0     150       3\n",
       "3       0     171       3\n",
       "4       0     188       3\n",
       "5       0     220       3\n",
       "6       0     227       5\n",
       "7       0     228       4\n",
       "8       0     234       3\n",
       "9       0     235       4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_vd_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>songId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>78</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>104</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>127</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>133</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>145</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  songId  rating\n",
       "0       0      12       4\n",
       "1       0      17       3\n",
       "2       0      74       4\n",
       "3       0      78       2\n",
       "4       0      92       2\n",
       "5       0     104       4\n",
       "6       0     127       4\n",
       "7       0     128       3\n",
       "8       0     133       3\n",
       "9       0     145       2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suggested on the original yahoo's paper\n",
    "POSITIVE_THRESHOLD = 4\n",
    "\n",
    "# Add column to the DataFrame\n",
    "tr_vd_data['ImplicitRating'] = np.where(tr_vd_data['rating'] >= POSITIVE_THRESHOLD, 1, 0)\n",
    "test_data['ImplicitRating'] = np.where(test_data['rating'] >= POSITIVE_THRESHOLD, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>songId</th>\n",
       "      <th>rating</th>\n",
       "      <th>ImplicitRating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>136</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>188</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>220</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>227</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>228</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>234</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>235</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  songId  rating  ImplicitRating\n",
       "0       0      72       2               0\n",
       "1       0     136       2               0\n",
       "2       0     150       3               0\n",
       "3       0     171       3               0\n",
       "4       0     188       3               0\n",
       "5       0     220       3               0\n",
       "6       0     227       5               1\n",
       "7       0     228       4               1\n",
       "8       0     234       3               0\n",
       "9       0     235       4               1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_vd_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_vd_data = tr_vd_data.drop(['rating'],axis=1).rename({\"ImplicitRating\":\"rating\"}, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>songId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>136</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>188</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>220</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>227</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>228</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>234</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>235</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  songId  rating\n",
       "0       0      72       0\n",
       "1       0     136       0\n",
       "2       0     150       0\n",
       "3       0     171       0\n",
       "4       0     188       0\n",
       "5       0     220       0\n",
       "6       0     227       1\n",
       "7       0     228       1\n",
       "8       0     234       0\n",
       "9       0     235       1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_vd_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>songId</th>\n",
       "      <th>rating</th>\n",
       "      <th>ImplicitRating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>78</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>104</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>127</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>133</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>145</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  songId  rating  ImplicitRating\n",
       "0       0      12       4               1\n",
       "1       0      17       3               0\n",
       "2       0      74       4               1\n",
       "3       0      78       2               0\n",
       "4       0      92       2               0\n",
       "5       0     104       4               1\n",
       "6       0     127       4               1\n",
       "7       0     128       3               0\n",
       "8       0     133       3               0\n",
       "9       0     145       2               0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.drop(['rating'],axis=1).rename({\"ImplicitRating\":\"rating\"}, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>songId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>104</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>127</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>133</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  songId  rating\n",
       "0       0      12       1\n",
       "1       0      17       0\n",
       "2       0      74       1\n",
       "3       0      78       0\n",
       "4       0      92       0\n",
       "5       0     104       1\n",
       "6       0     127       1\n",
       "7       0     128       0\n",
       "8       0     133       0\n",
       "9       0     145       0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   userId  songId  rating\n",
       " 0       0      72       0\n",
       " 1       0     136       0\n",
       " 2       0     150       0\n",
       " 3       0     171       0\n",
       " 4       0     188       0,\n",
       " (6960, 3))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_vd_data.head(), tr_vd_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   userId  songId  rating\n",
       " 0       0      12       1\n",
       " 1       0      17       0\n",
       " 2       0      74       1\n",
       " 3       0      78       0\n",
       " 4       0      92       0,\n",
       " (4640, 3))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head(), test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4640 entries, 0 to 4639\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   userId  4640 non-null   int32\n",
      " 1   songId  4640 non-null   int32\n",
      " 2   rating  4640 non-null   int64\n",
      "dtypes: int32(2), int64(1)\n",
      "memory usage: 72.6 KB\n"
     ]
    }
   ],
   "source": [
    "test_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_activity = get_count(tr_vd_data, 'userId')\n",
    "item_popularity = get_count(tr_vd_data, 'songId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_uid = user_activity.index\n",
    "unique_sid = item_popularity.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = len(unique_uid)\n",
    "n_items = len(unique_sid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(290, 300)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_users, n_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing eventual songs and users from the test set not present in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "song2id = dict((sid, i) for (i, sid) in enumerate(unique_sid))\n",
    "user2id = dict((uid, i) for (i, uid) in enumerate(unique_uid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the test set, only keep the users/items from the training set\n",
    "\n",
    "test_data = test_data.loc[test_data['userId'].isin(unique_uid)]\n",
    "test_data = test_data.loc[test_data['songId'].isin(unique_sid)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn userId and songId to 0-based index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerize(tp):\n",
    "    uid = list(map(lambda x: user2id[x], tp['userId']))\n",
    "    sid = list(map(lambda x: song2id[x], tp['songId']))\n",
    "    tp.loc[:, 'user_id'] = uid\n",
    "    tp.loc[:, 'item_id'] = sid\n",
    "    return tp[['user_id', 'item_id', 'rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_vd_data = numerize(tr_vd_data)\n",
    "test_data = numerize(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do we need the validation for our purpose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data, vad_data = split_train_test_proportion(tr_vd_data, 'user_id', test_prop=0.7, random_seed=12345)\n",
    "#obs_test_data, vad_data = split_train_test_proportion(vad_data, 'user_id', test_prop=0.5, random_seed=12345)\n",
    "train_data, obs_test_data = split_train_test_proportion(tr_vd_data, random_seed=12345)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are total of 290 unique users in the training set and 290 unique users in the entire dataset\n"
     ]
    }
   ],
   "source": [
    "print(\"There are total of %d unique users in the training set and %d unique users in the entire dataset\" % (len(pd.unique(train_data['user_id'])), len(unique_uid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are total of 300 unique items in the training set and 300 unique items in the entire dataset\n"
     ]
    }
   ],
   "source": [
    "print(\"There are total of %d unique items in the training set and %d unique items in the entire dataset\" % (len(pd.unique(train_data['item_id'])), len(unique_sid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_to_fill(part_data_1, part_data_2, unique_id, key):\n",
    "    # move the data from part_data_2 to part_data_1 so that part_data_1 has the same number of unique \"key\" as unique_id\n",
    "    part_id = set(pd.unique(part_data_1[key]))\n",
    "    \n",
    "    left_id = list()\n",
    "    for i, _id in enumerate(unique_id):\n",
    "        if _id not in part_id:\n",
    "            left_id.append(_id)\n",
    "            \n",
    "    move_idx = part_data_2[key].isin(left_id)\n",
    "    part_data_1 = part_data_1.append(part_data_2[move_idx])\n",
    "    part_data_2 = part_data_2[~move_idx]\n",
    "    return part_data_1, part_data_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The move_to_fill function is used to ensure that train_data ends up with a complete set of unique IDs as specified by unique_id, by \"moving\" the necessary rows from another dataset (part_data_2 like vad_data or obs_test_data) and updating both DataFrames accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data, vad_data = move_to_fill(train_data, vad_data, np.arange(n_items), 'item_id')\n",
    "train_data, obs_test_data = move_to_fill(train_data, obs_test_data, np.arange(n_items), 'item_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are total of 300 unique items in the training set and 300 unique items in the entire dataset\n"
     ]
    }
   ],
   "source": [
    "print(\"There are total of %d unique items in the training set and %d unique items in the entire dataset\" % (len(pd.unique(train_data['item_id'])), len(unique_sid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store datasets in csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv(os.path.join(output_name, 'train.csv'), index=False)\n",
    "#vad_data.to_csv(os.path.join(output_name, 'validation.csv'), index=False)\n",
    "tr_vd_data.to_csv(os.path.join(output_name, 'train_full.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_test_data.to_csv(os.path.join(output_name, 'obs_test_full.csv'), index=False)\n",
    "test_data.to_csv(os.path.join(output_name, 'test_full.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now *obs_test_data* is our biased testset extracted by the original dataset, while *test_data* is our unbiased test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>298</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>251</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>228</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>236</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>257</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26095</th>\n",
       "      <td>289</td>\n",
       "      <td>237</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26096</th>\n",
       "      <td>289</td>\n",
       "      <td>239</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26097</th>\n",
       "      <td>289</td>\n",
       "      <td>244</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26098</th>\n",
       "      <td>289</td>\n",
       "      <td>249</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26099</th>\n",
       "      <td>289</td>\n",
       "      <td>254</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26100 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id  item_id  rating\n",
       "0            0      298       1\n",
       "1            0      251       1\n",
       "2            0      228       1\n",
       "3            0      236       1\n",
       "4            0      257       0\n",
       "...        ...      ...     ...\n",
       "26095      289      237       0\n",
       "26096      289      239       0\n",
       "26097      289      244       0\n",
       "26098      289      249       0\n",
       "26099      289      254       0\n",
       "\n",
       "[26100 rows x 3 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build files for creating dataset for the openrec library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init empty\n",
    "pos_test_set = []\n",
    "neg_test_set = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create masks for positive and negative ratings\n",
    "pos_mask = obs_test_data['rating'] == 1\n",
    "neg_mask = obs_test_data['rating'] != 1\n",
    "\n",
    "# Extract the user_id and item_id pairs for positive and negative ratings\n",
    "pos_test_set = obs_test_data.loc[pos_mask, ['user_id', 'item_id']].values.tolist()\n",
    "neg_test_set = obs_test_data.loc[neg_mask, ['user_id', 'item_id']].values.tolist()\n",
    "\n",
    "# pos_test_set and neg_test_set now contain the lists of [user_id, item_id] for positive and negative ratings, respectively.\n",
    "# Get np arrays\n",
    "pos_test_set = np.array(pos_test_set)\n",
    "neg_test_set = np.array(neg_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0, 298],\n",
       "       [  0, 251],\n",
       "       [  0, 228],\n",
       "       ...,\n",
       "       [287, 138],\n",
       "       [287, 120],\n",
       "       [288,  36]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataframe\n",
    "pos_test_set_df = pd.DataFrame(pos_test_set)\n",
    "neg_test_set_df = pd.DataFrame(neg_test_set)\n",
    "\n",
    "# Get couples user-item\n",
    "pos_test_set_df.columns = [\"user_id\",\"item_id\"]\n",
    "neg_test_set_df.columns = [\"user_id\",\"item_id\"]\n",
    "\n",
    "# Turn into records\n",
    "structured_data_pos_test_set = pos_test_set_df.to_records(index=False)\n",
    "structured_data_neg_test_set = neg_test_set_df.to_records(index=False)\n",
    "\n",
    "# Save\n",
    "np.save(output_name + \"biased-test_arr_pos.npy\", structured_data_pos_test_set)\n",
    "np.save(output_name + \"biased-test_arr_neg.npy\", structured_data_neg_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unbiased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init empty\n",
    "pos_test_set = []\n",
    "neg_test_set = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create masks for positive and negative ratings\n",
    "pos_mask = test_data['rating'] == 1\n",
    "neg_mask = test_data['rating'] != 1\n",
    "\n",
    "# Extract the user_id and item_id pairs for positive and negative ratings\n",
    "pos_test_set = test_data.loc[pos_mask, ['user_id', 'item_id']].values.tolist()\n",
    "neg_test_set = test_data.loc[neg_mask, ['user_id', 'item_id']].values.tolist()\n",
    "\n",
    "# pos_test_set and neg_test_set now contain the lists of [user_id, item_id] for positive and negative ratings, respectively.\n",
    "# Get np arrays\n",
    "pos_test_set = np.array(pos_test_set)\n",
    "neg_test_set = np.array(neg_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataframe\n",
    "pos_test_set_df = pd.DataFrame(pos_test_set)\n",
    "neg_test_set_df = pd.DataFrame(neg_test_set)\n",
    "\n",
    "# Get couples user-item\n",
    "pos_test_set_df.columns = [\"user_id\",\"item_id\"]\n",
    "neg_test_set_df.columns = [\"user_id\",\"item_id\"]\n",
    "\n",
    "# Turn into records\n",
    "structured_data_pos_test_set = pos_test_set_df.to_records(index=False)\n",
    "structured_data_neg_test_set = neg_test_set_df.to_records(index=False)\n",
    "\n",
    "# Save\n",
    "np.save(output_name + \"unbiased-test_arr_pos.npy\", structured_data_pos_test_set)\n",
    "np.save(output_name + \"unbiased-test_arr_neg.npy\", structured_data_neg_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_trainset = train_data[train_data['rating'] != 0]\n",
    "positive_trainset = positive_trainset.drop(columns=['rating'])\n",
    "\n",
    "# Convert the DataFrame to a structured array\n",
    "positive_trainset = positive_trainset.to_records(index=False) \n",
    "\n",
    "# Save\n",
    "np.save(output_name + \"training_arr.npy\", positive_trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(290, 290, 290)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"user_id\"].unique().size, test_data[\"user_id\"].unique().size, obs_test_data[\"user_id\"].unique().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(289, 289, 289)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"user_id\"].unique().max(), test_data[\"user_id\"].unique().max(), obs_test_data[\"user_id\"].unique().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 300, 300)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"item_id\"].unique().size, test_data[\"item_id\"].unique().size, obs_test_data[\"item_id\"].unique().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(299, 299, 299)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"item_id\"].unique().max(), test_data[\"item_id\"].unique().max(), obs_test_data[\"item_id\"].unique().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MODEL CHOICE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I won't comment anything, we are just using the code provided by the authors of the paper\n",
    "\n",
    "raw_data = dict()\n",
    "raw_data['train_data'] = np.load(output_name + \"training_arr.npy\" )\n",
    "raw_data['max_user'] = 290\n",
    "raw_data['max_item'] = 300\n",
    "batch_size = 8000\n",
    "test_batch_size = 1000\n",
    "display_itr = 1000\n",
    "\n",
    "train_dataset = ImplicitDataset(raw_data['train_data'], raw_data['max_user'], raw_data['max_item'], name='Train')\n",
    "\n",
    "MODEL_CLASS = CML\n",
    "MODEL_PREFIX = \"cml\"\n",
    "DATASET_NAME = \"coat\"\n",
    "OUTPUT_FOLDER = output_name\n",
    "OUTPUT_PATH = OUTPUT_FOLDER + MODEL_PREFIX + \"-\" + DATASET_NAME + \"/\"\n",
    "OUTPUT_PREFIX = str(OUTPUT_PATH) + str(MODEL_PREFIX) + \"-\" + str(DATASET_NAME)\n",
    "\n",
    "\n",
    "if os.path.exists(OUTPUT_PATH) == False:\n",
    "    os.makedirs(OUTPUT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:391: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:31: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:41: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:43: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:33: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/interactions/pairwise_eu_dist.py:71: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:596: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:75: The name tf.scatter_update is deprecated. Please use tf.compat.v1.scatter_update instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:144: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:365: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:148: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-18 23:17:56.885403: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2024-08-18 23:17:56.923878: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fb567549220 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2024-08-18 23:17:56.923894: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Start training with FULL evaluation ==\n",
      "[Itr 100] Finished\n",
      "[Itr 200] Finished\n",
      "[Itr 300] Finished\n",
      "[Itr 400] Finished\n",
      "[Itr 500] Finished\n",
      "[Itr 600] Finished\n",
      "[Itr 700] Finished\n",
      "[Itr 800] Finished\n",
      "[Itr 900] Finished\n",
      "[Itr 1000] Finished\n",
      "INFO:tensorflow:./generated_data/cml-coat/coat-1000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 1000] loss: 6818.894574\n",
      "[Itr 1100] Finished\n",
      "[Itr 1200] Finished\n",
      "[Itr 1300] Finished\n",
      "[Itr 1400] Finished\n",
      "[Itr 1500] Finished\n",
      "[Itr 1600] Finished\n",
      "[Itr 1700] Finished\n",
      "[Itr 1800] Finished\n",
      "[Itr 1900] Finished\n",
      "[Itr 2000] Finished\n",
      "INFO:tensorflow:./generated_data/cml-coat/coat-2000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 2000] loss: 6674.742854\n",
      "[Itr 2100] Finished\n",
      "[Itr 2200] Finished\n",
      "[Itr 2300] Finished\n",
      "[Itr 2400] Finished\n"
     ]
    }
   ],
   "source": [
    "# Prevent tensorflow from using cached embeddings\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "# Define the model\n",
    "model = MODEL_CLASS(batch_size=batch_size, max_user=train_dataset.max_user(), max_item=train_dataset.max_item(), dim_embed=50, l2_reg=0.001, opt='Adam', sess_config=None)\n",
    "sampler = PairwiseSampler(batch_size=batch_size, dataset=train_dataset, num_process=4)\n",
    "model_trainer = ImplicitModelTrainer(batch_size=batch_size, test_batch_size=test_batch_size, train_dataset=train_dataset, model=model, sampler=sampler, eval_save_prefix=OUTPUT_PATH + DATASET_NAME, item_serving_size=500)\n",
    "auc_evaluator = AUC()\n",
    "\n",
    "# Train the model\n",
    "model_trainer.train(num_itr=10001, display_itr=display_itr)\n",
    "\n",
    "# Save in the output folder\n",
    "model.save(OUTPUT_PATH,None)\n",
    "\n",
    "# Delete the model from the memory\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DEFINING FUNCTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eq(infilename, infilename_neg, trainfilename, gamma=-1.0, K=1):\n",
    "\n",
    "    # Read pickles\n",
    "    infile = open(infilename, 'rb')\n",
    "    infile_neg = open(infilename_neg, 'rb')\n",
    "    P = pickle.load(infile)\n",
    "    infile.close()\n",
    "    P_neg = pickle.load(infile_neg)\n",
    "    infile_neg.close()\n",
    "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
    "    \n",
    "    # Merge P and P_neg\n",
    "    for theuser in P[\"users\"]:\n",
    "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "    \n",
    "    Zui = dict()\n",
    "    Ni = dict()\n",
    "    \n",
    "    # Compute frequencies of items in the training set\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "    del trainset\n",
    "\n",
    "    # Count #users with non-zero item frequencies\n",
    "    nonzero_user_count = 0\n",
    "    for theuser in P[\"users\"]:\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for pos_item in pos_items:\n",
    "            if pos_item in Ni:\n",
    "                nonzero_user_count += 1\n",
    "                break\n",
    "    \n",
    "    # Compute recommendations for each user\n",
    "    for theuser in P[\"users\"]:\n",
    "        all_scores = np.array(P[\"results\"][theuser])\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for i, pos_item in enumerate(pos_items):\n",
    "            pos_score = pos_scores[i]\n",
    "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
    "\n",
    "    \n",
    "    # Calculate per-user scores\n",
    "    sum_user_auc = 0.0\n",
    "    sum_user_recall = 0.0\n",
    "    for theuser in P[\"users\"]:\n",
    "        numerator_auc = 0.0\n",
    "        numerator_recall = 0.0\n",
    "        denominator = 0.0\n",
    "\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            if theitem not in Ni:\n",
    "                continue\n",
    "            pui = np.power(Ni[theitem], (gamma + 1) / 2.0)\n",
    "\n",
    "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser])) / pui\n",
    "            \n",
    "            if Zui[(theuser, theitem)] < K:\n",
    "                numerator_recall += 1.0 / pui\n",
    "            denominator += 1 / pui\n",
    "                \n",
    "        if denominator > 0:\n",
    "            sum_user_auc += numerator_auc / denominator\n",
    "            sum_user_recall += numerator_recall / denominator \n",
    "\n",
    "    # Return\n",
    "    return {\n",
    "        \"auc\"       : sum_user_auc / nonzero_user_count,\n",
    "        \"recall\"    : sum_user_recall / nonzero_user_count\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aoa(infilename, infilename_neg, trainfilename, K=1):\n",
    "\n",
    "    # Read pickles\n",
    "    infile = open(infilename, 'rb')\n",
    "    infile_neg = open(infilename_neg, 'rb')\n",
    "    P = pickle.load(infile)\n",
    "    infile.close()\n",
    "    P_neg = pickle.load(infile_neg)\n",
    "    infile_neg.close()\n",
    "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
    "    print(NUM_NEGATIVES)\n",
    "    \n",
    "    # Merge P and P_neg\n",
    "    for theuser in P[\"users\"]:\n",
    "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "    \n",
    "    Zui = dict()\n",
    "    Ni = dict()\n",
    "\n",
    "    # Compute frequencies of items in the training set\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "    del trainset\n",
    "    \n",
    "    # Count #users with non-zero item frequencies\n",
    "    nonzero_user_count = 0\n",
    "    for theuser in P[\"users\"]:\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for pos_item in pos_items:\n",
    "            if pos_item in Ni:\n",
    "                nonzero_user_count += 1\n",
    "                break\n",
    "    \n",
    "    # Compute recommendations for each user\n",
    "    for theuser in P[\"users\"]:\n",
    "        all_scores = np.array(P[\"results\"][theuser])\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for i, pos_item in enumerate(pos_items):\n",
    "            pos_score = pos_scores[i]\n",
    "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
    "\n",
    "    # Calculate per-user scores\n",
    "    sum_user_auc = 0.0\n",
    "    sum_user_recall = 0.0\n",
    "    for theuser in P[\"users\"]:\n",
    "        numerator_auc = 0.0\n",
    "        numerator_recall = 0.0\n",
    "        denominator = 0.0\n",
    "\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            if theitem not in Ni:\n",
    "                continue\n",
    "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser]))\n",
    "            # Calcolo il Recall a 30, vedi nota 6 paper\n",
    "            if Zui[(theuser, theitem)] < K:\n",
    "                numerator_recall += 1.0\n",
    "            denominator += 1 \n",
    "\n",
    "        if denominator > 0:\n",
    "            sum_user_auc += numerator_auc / denominator\n",
    "            sum_user_recall += numerator_recall / denominator\n",
    "\n",
    "    # Return\n",
    "    return {\n",
    "        \"auc\"       : sum_user_auc / nonzero_user_count,\n",
    "        \"recall\"    : sum_user_recall / nonzero_user_count\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified(infilename, infilename_neg, trainfilename, gamma=1.0, K=30, partition=10, delta=0.1):\n",
    "\n",
    "    # Read pickles\n",
    "    infile = open(infilename, 'rb')\n",
    "    infile_neg = open(infilename_neg, 'rb')\n",
    "    P = pickle.load(infile)\n",
    "    infile.close()\n",
    "    P_neg = pickle.load(infile_neg)\n",
    "    infile_neg.close()\n",
    "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
    "\n",
    "    # Merge P and P_neg\n",
    "    for theuser in P[\"users\"]:\n",
    "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "\n",
    "    Zui = dict()\n",
    "    Ni = dict()\n",
    "\n",
    "    # Compute frequencies of items in the training set\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "    del trainset\n",
    "\n",
    "    # Compute recommendations for each user\n",
    "    for theuser in P[\"users\"]:\n",
    "        all_scores = np.array(P[\"results\"][theuser])\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for i, pos_item in enumerate(pos_items):\n",
    "            pos_score = pos_scores[i]\n",
    "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
    "\n",
    "    pui = dict()\n",
    "    w = dict()\n",
    "\n",
    "    # Compute dictionary of propensity scores\n",
    "    for theuser in P[\"users\"]:\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            if theitem not in Ni:\n",
    "                continue\n",
    "            if theitem in pui:\n",
    "                continue\n",
    "            pui[theitem] = np.power(Ni[theitem], (gamma + 1) / 2.0)\n",
    "\n",
    "    # Take the list of items (not tuples) in pui sorted by value\n",
    "    items_sorted_by_value = sorted(pui, key=pui.get, reverse=True)\n",
    "\n",
    "    # Compute linspace between the pui[0] and pui[-1] \n",
    "    linspace = np.linspace(pui[items_sorted_by_value[0]], pui[items_sorted_by_value[-1]], partition+1)\n",
    "   \n",
    "    # Compute dictionary w, that is, for each item, assigns the average of the puis in the partition it belongs to\n",
    "    i=0\n",
    "    j = 0\n",
    "    while i < len(items_sorted_by_value):\n",
    "                            \n",
    "        avg = 0\n",
    "        start = i\n",
    "        end = i\n",
    "    \n",
    "        while i < len(items_sorted_by_value) and pui[items_sorted_by_value[i]] >= linspace[j+1]:\n",
    "            avg += 1.0 / pui[items_sorted_by_value[i]]\n",
    "            end = i\n",
    "            i += 1\n",
    "        avg = avg / (end - start + 1)\n",
    "\n",
    "        for k in range(start, end+1):\n",
    "            w[items_sorted_by_value[k]] = avg\n",
    "\n",
    "        j += 1\n",
    "\n",
    "    # Compute bias' numerator\n",
    "    bias = 0.0\n",
    "    for k in pui.keys():\n",
    "        # add |pui*w - 1!|\n",
    "        bias += abs(pui[k] * w[k] - 1)\n",
    "    # Multiply by number of users\n",
    "    bias *= len(P[\"users\"])\n",
    "\n",
    "    # Compute concentrations numerator (for each user)\n",
    "    concentrations = {}\n",
    "    max_w = max(w.values())\n",
    "    # ... by computing the sum of squares of w for each user\n",
    "    for user, item in zip(trainset['user_id'], trainset['item_id']):\n",
    "        # Iterate over the trainset to compute the sum of squares for each user\n",
    "        if item in w:\n",
    "            if user not in concentrations:\n",
    "                concentrations[user] = 0\n",
    "            concentrations[user] += w[item] ** 2\n",
    "    # ... and then applying the formula\n",
    "    for user in concentrations:\n",
    "        concentrations[user] = math.sqrt(concentrations[user] * 2 * math.log(2/delta)) + max_w * 7 * math.log(2/delta)\n",
    "    # Now sum all the concentrations\n",
    "    concentration = sum(concentrations.values())\n",
    "\n",
    "    # Calculate per-user scores\n",
    "    nonzero_user_count = 0\n",
    "    sum_user_auc = 0.0\n",
    "    sum_user_recall = 0.0\n",
    "    for theuser in P[\"users\"]:\n",
    "        numerator_auc = 0.0\n",
    "        numerator_recall = 0.0\n",
    "        denominator = 0.0\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            # Skip items with null frequency\n",
    "            if  theitem not in Ni:\n",
    "                continue\n",
    "            # Add things to be summed for each item\n",
    "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser])) * w[theitem]\n",
    "            # Add things for recall\n",
    "            if Zui[(theuser, theitem)] < K:\n",
    "                numerator_recall += 1.0 * w[theitem] #Â spetta\n",
    "            # Increment denominator that the sum must be divided by \n",
    "            denominator += 1 / pui[theitem]\n",
    "\n",
    "\n",
    "        # If there was at least one item for the user, count the user and sum the results\n",
    "        if denominator > 0:\n",
    "            nonzero_user_count += 1\n",
    "            sum_user_auc += numerator_auc / denominator\n",
    "            sum_user_recall += numerator_recall / denominator \n",
    "\n",
    "    # Return\n",
    "    return {\n",
    "        \"auc\"       : sum_user_auc / nonzero_user_count, \n",
    "        \"recall\"    : sum_user_recall / nonzero_user_count,\n",
    "        \"bias\"      : bias,\n",
    "        \"concentration\" : concentration\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_logspace(infilename, infilename_neg, trainfilename, gamma=1.0, K=30, partition=10, delta=0.1):\n",
    "\n",
    "    # Read pickles\n",
    "    infile = open(infilename, 'rb')\n",
    "    infile_neg = open(infilename_neg, 'rb')\n",
    "    P = pickle.load(infile)\n",
    "    infile.close()\n",
    "    P_neg = pickle.load(infile_neg)\n",
    "    infile_neg.close()\n",
    "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
    "\n",
    "    # Merge P and P_neg\n",
    "    for theuser in P[\"users\"]:\n",
    "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "\n",
    "    Zui = dict()\n",
    "    Ni = dict()\n",
    "\n",
    "    # Compute frequencies of items in the training set\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "    del trainset\n",
    "\n",
    "    # Compute recommendations for each user\n",
    "    for theuser in P[\"users\"]:\n",
    "        all_scores = np.array(P[\"results\"][theuser])\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for i, pos_item in enumerate(pos_items):\n",
    "            pos_score = pos_scores[i]\n",
    "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
    "\n",
    "    pui = dict()\n",
    "    w = dict()\n",
    "\n",
    "    # Compute dictionary of propensity scores\n",
    "    for theuser in P[\"users\"]:\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            if theitem not in Ni:\n",
    "                continue\n",
    "            if theitem in pui:\n",
    "                continue\n",
    "            pui[theitem] = np.power(Ni[theitem], (gamma + 1) / 2.0)\n",
    "\n",
    "    # Take the list of items (not tuples) in pui sorted by value\n",
    "    items_sorted_by_value = sorted(pui, key=pui.get, reverse=True)\n",
    "\n",
    "    # Compute linspace between the pui[0] and pui[-1] \n",
    "\n",
    "    # Maybe try to split the logspace instead of the linspace?\n",
    "    logspace = np.logspace(pui[items_sorted_by_value[0]], pui[items_sorted_by_value[-1]], partition+1)\n",
    "   \n",
    "    # Compute dictionary w, that is, for each item, assigns the average of the puis in the partition it belongs to\n",
    "    i=0\n",
    "    j = 0\n",
    "    while i < len(items_sorted_by_value):\n",
    "                            \n",
    "        avg = 0\n",
    "        start = i\n",
    "        end = i\n",
    "    \n",
    "        while i < len(items_sorted_by_value) and pui[items_sorted_by_value[i]] >= logspace[j+1]:\n",
    "            avg += 1.0 / pui[items_sorted_by_value[i]]\n",
    "            end = i\n",
    "            i += 1\n",
    "        \n",
    "        # Is the average the only good choice? even with the log space split?\n",
    "        avg = avg / (end - start + 1)\n",
    "\n",
    "        for k in range(start, end+1):\n",
    "            w[items_sorted_by_value[k]] = avg\n",
    "\n",
    "        j += 1\n",
    "\n",
    "        # Compute bias' numerator\n",
    "        bias = 0.0\n",
    "        for k in pui.keys():\n",
    "            # add |pui*w - 1!|\n",
    "            bias += abs(pui[k] * w[k] - 1)\n",
    "        # Multiply by number of users\n",
    "        bias *= len(P[\"users\"])\n",
    "\n",
    "        # Compute concentrations numerator (for each user)\n",
    "        concentrations = {}\n",
    "        max_w = max(w.values())\n",
    "        # ... by computing the sum of squares of w for each user\n",
    "        for user, item in zip(trainset['user_id'], trainset['item_id']):\n",
    "            # Iterate over the trainset to compute the sum of squares for each user\n",
    "            if item in w:\n",
    "                if user not in concentrations:\n",
    "                    concentrations[user] = 0\n",
    "                concentrations[user] += w[item] ** 2\n",
    "        # ... and then applying the formula\n",
    "        for user in concentrations:\n",
    "            concentrations[user] = math.sqrt(concentrations[user] * 2 * math.log(2/delta)) + max_w * 7 * math.log(2/delta)\n",
    "        # Now sum all the concentrations\n",
    "        concentration = sum(concentrations.values())\n",
    "\n",
    "    # Compute score with AUC and compute Recall\n",
    "    nonzero_user_count = 0\n",
    "    sum_user_auc = 0.0\n",
    "    sum_user_recall = 0.0\n",
    "    for theuser in P[\"users\"]:\n",
    "        numerator_auc = 0.0\n",
    "        numerator_recall = 0.0\n",
    "        denominator = 0.0\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            # Skip items with null frequency\n",
    "            if  theitem not in Ni:\n",
    "                continue\n",
    "            # Add things to be summed for each item\n",
    "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser])) * w[theitem]\n",
    "            # Add things for recall\n",
    "            if Zui[(theuser, theitem)] < K:\n",
    "                numerator_recall += 1.0 * w[theitem] #Â spetta\n",
    "            # Increment denominator that the sum must be divided by \n",
    "            denominator += 1 / pui[theitem]\n",
    "\n",
    "\n",
    "        # If there was at least one item for the user, count the user and sum the results\n",
    "        if denominator > 0:\n",
    "            nonzero_user_count += 1\n",
    "            sum_user_auc += numerator_auc / denominator\n",
    "            sum_user_recall += numerator_recall / denominator \n",
    "\n",
    "    # Return\n",
    "    return {\n",
    "        \"auc\"       : sum_user_auc / nonzero_user_count, \n",
    "        \"recall\"    : sum_user_recall / nonzero_user_count,\n",
    "        \"bias\"      : bias,\n",
    "        \"concentration\" : concentration\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This version uses the linspace of the number of number of items used for evaluation, not of the propensities\n",
    "def stratified_2(infilename, infilename_neg, trainfilename, gamma=1.0, K=30, partition=10, delta=0.1):\n",
    "\n",
    "    # Read pickles\n",
    "    infile = open(infilename, 'rb')\n",
    "    infile_neg = open(infilename_neg, 'rb')\n",
    "    P = pickle.load(infile)\n",
    "    infile.close()\n",
    "    P_neg = pickle.load(infile_neg)\n",
    "    infile_neg.close()\n",
    "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
    "\n",
    "    # Merge P and P_neg\n",
    "    for theuser in P[\"users\"]:\n",
    "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "\n",
    "    Zui = dict()\n",
    "    Ni = dict()\n",
    "\n",
    "    # Compute frequencies of items in the training set\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "    del trainset\n",
    "\n",
    "    # Compute recommendations for each user\n",
    "    for theuser in P[\"users\"]:\n",
    "        all_scores = np.array(P[\"results\"][theuser])\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for i, pos_item in enumerate(pos_items):\n",
    "            pos_score = pos_scores[i]\n",
    "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
    "\n",
    "    pui = dict()\n",
    "    w = dict()\n",
    "\n",
    "    # Compute dictionary of propensity scores\n",
    "    for theuser in P[\"users\"]:\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            if theitem not in Ni:\n",
    "                continue\n",
    "            if theitem in pui:\n",
    "                continue\n",
    "            pui[theitem] = np.power(Ni[theitem], (gamma + 1) / 2.0)\n",
    "\n",
    "    # Take the list of items (not tuples) in pui sorted by value\n",
    "    items_sorted_by_value = sorted(pui, key=pui.get, reverse=True)\n",
    "\n",
    "    # Compute linspace between the 0 to len(item_sorted...)\n",
    "    linspace = np.linspace(0, len(items_sorted_by_value), partition+1)\n",
    "   \n",
    "    # Compute dictionary w, that is, for each item, assigns the average of the puis in the partition it belongs to\n",
    "    i=0\n",
    "    j = 0\n",
    "    while i < len(items_sorted_by_value):\n",
    "                            \n",
    "        avg = 0\n",
    "        start = i\n",
    "        end = i\n",
    "    \n",
    "        while i < len(items_sorted_by_value) and i < linspace[j+1]:\n",
    "            avg += 1.0 / pui[items_sorted_by_value[i]]\n",
    "            end = i\n",
    "            i += 1\n",
    "        \n",
    "        avg = avg / (end - start + 1)\n",
    "\n",
    "        for k in range(start, end+1):\n",
    "            w[items_sorted_by_value[k]] = avg\n",
    "\n",
    "        j += 1\n",
    "\n",
    "    # Compute bias' numerator\n",
    "    bias = 0.0\n",
    "    for k in pui.keys():\n",
    "        # add |pui*w - 1!|\n",
    "        bias += abs(pui[k] * w[k] - 1)\n",
    "    # Multiply by number of users\n",
    "    bias *= len(P[\"users\"])\n",
    "\n",
    "    # Compute concentrations numerator (for each user)\n",
    "    concentrations = {}\n",
    "    max_w = max(w.values())\n",
    "    # ... by computing the sum of squares of w for each user\n",
    "    for user, item in zip(trainset['user_id'], trainset['item_id']):\n",
    "        # Iterate over the trainset to compute the sum of squares for each user\n",
    "        if item in w:\n",
    "            if user not in concentrations:\n",
    "                concentrations[user] = 0\n",
    "            concentrations[user] += w[item] ** 2\n",
    "    # ... and then applying the formula\n",
    "    for user in concentrations:\n",
    "        concentrations[user] = math.sqrt(concentrations[user] * 2 * math.log(2/delta)) + max_w * 7 * math.log(2/delta)\n",
    "    # Now sum all the concentrations\n",
    "    concentration = sum(concentrations.values())\n",
    "\n",
    "    # Compute score with AUC and compute Recall\n",
    "    nonzero_user_count = 0\n",
    "    sum_user_auc = 0.0\n",
    "    sum_user_recall = 0.0\n",
    "    for theuser in P[\"users\"]:\n",
    "        numerator_auc = 0.0\n",
    "        numerator_recall = 0.0\n",
    "        denominator = 0.0\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            # Skip items with null frequency\n",
    "            if  theitem not in Ni:\n",
    "                continue\n",
    "            # Add things to be summed for each item\n",
    "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser])) * w[theitem]\n",
    "            # Add things for recall\n",
    "            if Zui[(theuser, theitem)] < K:\n",
    "                numerator_recall += 1.0 * w[theitem] #Â spetta\n",
    "            # Increment denominator that the sum must be divided by \n",
    "            denominator += 1 / pui[theitem]\n",
    "\n",
    "\n",
    "        # If there was at least one item for the user, count the user and sum the results\n",
    "        if denominator > 0:\n",
    "            nonzero_user_count += 1\n",
    "            sum_user_auc += numerator_auc / denominator\n",
    "            sum_user_recall += numerator_recall / denominator \n",
    "\n",
    "    return {\n",
    "        \"auc\"       : sum_user_auc / nonzero_user_count, \n",
    "        \"recall\"    : sum_user_recall / nonzero_user_count,\n",
    "        \"bias\"      : bias,\n",
    "        \"concentration\" : concentration\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **EVALUATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "raw_data = dict()\n",
    "raw_data['train_data'] = np.load(output_name + \"training_arr.npy\")\n",
    "raw_data['test_data_pos_biased'] = np.load(output_name + \"biased-test_arr_pos.npy\")\n",
    "raw_data['test_data_neg_biased'] = np.load(output_name + \"biased-test_arr_neg.npy\")\n",
    "raw_data['test_data_pos_unbiased'] = np.load(output_name + \"unbiased-test_arr_pos.npy\")\n",
    "raw_data['test_data_neg_unbiased'] = np.load(output_name + \"unbiased-test_arr_neg.npy\")\n",
    "raw_data['max_user'] = 290\n",
    "raw_data['max_item'] = 300\n",
    "batch_size = 8000\n",
    "test_batch_size = 1000\n",
    "display_itr = 1000\n",
    "\n",
    "# Load data\n",
    "train_dataset = ImplicitDataset(raw_data['train_data'], raw_data['max_user'], raw_data['max_item'], name='Train')\n",
    "test_dataset_pos_biased = ImplicitDataset(raw_data['test_data_pos_biased'], raw_data['max_user'], raw_data['max_item'])\n",
    "test_dataset_neg_biased = ImplicitDataset(raw_data['test_data_neg_biased'], raw_data['max_user'], raw_data['max_item'])\n",
    "test_dataset_pos_unbiased = ImplicitDataset(raw_data['test_data_pos_unbiased'], raw_data['max_user'], raw_data['max_item'])\n",
    "test_dataset_neg_unbiased = ImplicitDataset(raw_data['test_data_neg_unbiased'], raw_data['max_user'], raw_data['max_item'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./generated_data/cml-coat/\n",
      "[Subsampling negative items]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    }
   ],
   "source": [
    "# Prevent tensorflow from using cached embeddings\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "# Define the model\n",
    "model = MODEL_CLASS(batch_size=batch_size, max_user=train_dataset.max_user(), max_item=train_dataset.max_item(), dim_embed=50, l2_reg=0.001, opt='Adam', sess_config=None)\n",
    "sampler = PairwiseSampler(batch_size=batch_size, dataset=train_dataset, num_process=4)\n",
    "model_trainer = ImplicitModelTrainer(batch_size=batch_size, test_batch_size=test_batch_size, train_dataset=train_dataset, model=model, sampler=sampler, eval_save_prefix=OUTPUT_PATH + DATASET_NAME, item_serving_size=500)\n",
    "auc_evaluator = AUC()\n",
    "\n",
    "# Load model\n",
    "model.load(OUTPUT_PATH)\n",
    "\n",
    "# Set parameters\n",
    "model_trainer._eval_manager = ImplicitEvalManager(evaluators=[auc_evaluator])\n",
    "model_trainer._num_negatives = 60 # in yahoo they were 200 on 1000 items, so let's keep a 1/5 ratio on 300 items\n",
    "model_trainer._exclude_positives([train_dataset, test_dataset_pos_biased, test_dataset_neg_biased])\n",
    "model_trainer._sample_negatives(seed=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biased Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 206/206 [00:00<00:00, 994.86it/s] \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 290/290 [00:00<00:00, 347.46it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AUC': [0.512156862745098,\n",
       "  0.4923220973782772,\n",
       "  0.48518518518518505,\n",
       "  0.49176029962546824,\n",
       "  0.5863636363636363,\n",
       "  0.5869318181818182,\n",
       "  0.4511111111111111,\n",
       "  0.4692592592592594,\n",
       "  0.45224719101123595,\n",
       "  0.5944444444444443,\n",
       "  0.4861423220973783,\n",
       "  0.5597701149425288,\n",
       "  0.40018939393939396,\n",
       "  0.4712962962962963,\n",
       "  0.4685393258426966,\n",
       "  0.4879310344827586,\n",
       "  0.46367041198501874,\n",
       "  0.4689138576779026,\n",
       "  0.5335185185185184,\n",
       "  0.5237827715355805,\n",
       "  0.4407407407407407,\n",
       "  0.46423220973782775,\n",
       "  0.5505681818181819,\n",
       "  0.5385185185185185,\n",
       "  0.5795880149812733,\n",
       "  0.5068518518518519,\n",
       "  0.47074074074074085,\n",
       "  0.5018939393939394,\n",
       "  0.422962962962963,\n",
       "  0.4897727272727273,\n",
       "  0.5490421455938698,\n",
       "  0.5470930232558139,\n",
       "  0.5185185185185186,\n",
       "  0.5227272727272727,\n",
       "  0.46799242424242427,\n",
       "  0.5367041198501873,\n",
       "  0.48693181818181813,\n",
       "  0.5096296296296297,\n",
       "  0.4921348314606742,\n",
       "  0.5354166666666668,\n",
       "  0.5548689138576779,\n",
       "  0.5432584269662921,\n",
       "  0.44531835205992515,\n",
       "  0.44810606060606056,\n",
       "  0.5051136363636364,\n",
       "  0.4751893939393939,\n",
       "  0.5401851851851852,\n",
       "  0.45767790262172275,\n",
       "  0.427037037037037,\n",
       "  0.5146067415730338,\n",
       "  0.4853703703703704,\n",
       "  0.41704119850187266,\n",
       "  0.5180076628352491,\n",
       "  0.5448863636363637,\n",
       "  0.5498148148148149,\n",
       "  0.4693181818181818,\n",
       "  0.5915730337078652,\n",
       "  0.510077519379845,\n",
       "  0.5352059925093633,\n",
       "  0.506367041198502,\n",
       "  0.5228464419475656,\n",
       "  0.5986590038314176,\n",
       "  0.5278409090909091,\n",
       "  0.4285714285714285,\n",
       "  0.5752808988764045,\n",
       "  0.4462121212121212,\n",
       "  0.5029962546816479,\n",
       "  0.55187265917603,\n",
       "  0.472093023255814,\n",
       "  0.5346296296296297,\n",
       "  0.5160984848484849,\n",
       "  0.451498127340824,\n",
       "  0.4711111111111111,\n",
       "  0.5448863636363637,\n",
       "  0.5202651515151516,\n",
       "  0.5468164794007491,\n",
       "  0.5266666666666666,\n",
       "  0.4446360153256705,\n",
       "  0.5616104868913858,\n",
       "  0.5823529411764706,\n",
       "  0.5646067415730336,\n",
       "  0.500374531835206,\n",
       "  0.4003745318352059,\n",
       "  0.48084291187739464,\n",
       "  0.520925925925926,\n",
       "  0.544074074074074,\n",
       "  0.571111111111111,\n",
       "  0.48501872659176026,\n",
       "  0.4657303370786517,\n",
       "  0.42746212121212124,\n",
       "  0.5408239700374531,\n",
       "  0.4061797752808989,\n",
       "  0.5250936329588015,\n",
       "  0.5738888888888889,\n",
       "  0.5318352059925093,\n",
       "  0.4967432950191572,\n",
       "  0.44943820224719094,\n",
       "  0.5397003745318352,\n",
       "  0.4301851851851852,\n",
       "  0.5082397003745318,\n",
       "  0.49259259259259264,\n",
       "  0.4116858237547893,\n",
       "  0.507962962962963,\n",
       "  0.5217054263565891,\n",
       "  0.5228464419475656,\n",
       "  0.47528089887640457,\n",
       "  0.4625968992248062,\n",
       "  0.47771535580524344,\n",
       "  0.4546296296296296,\n",
       "  0.4644194756554307,\n",
       "  0.5268518518518518,\n",
       "  0.47314814814814815,\n",
       "  0.5042145593869732,\n",
       "  0.4792134831460674,\n",
       "  0.5078651685393258,\n",
       "  0.47413793103448276,\n",
       "  0.5189138576779027,\n",
       "  0.447003745318352,\n",
       "  0.4633333333333334,\n",
       "  0.48219696969696973,\n",
       "  0.46988636363636355,\n",
       "  0.5456439393939395,\n",
       "  0.46988636363636355,\n",
       "  0.49341085271317825,\n",
       "  0.5244444444444444,\n",
       "  0.55,\n",
       "  0.48507751937984506,\n",
       "  0.4696296296296296,\n",
       "  0.4159176029962547,\n",
       "  0.47018518518518515,\n",
       "  0.5498106060606062,\n",
       "  0.49777777777777776,\n",
       "  0.5007407407407408,\n",
       "  0.404074074074074,\n",
       "  0.5311111111111111,\n",
       "  0.4553639846743296,\n",
       "  0.44999999999999996,\n",
       "  0.5288389513108613,\n",
       "  0.505056179775281,\n",
       "  0.5099616858237547,\n",
       "  0.40555555555555556,\n",
       "  0.4689138576779026,\n",
       "  0.5359550561797753,\n",
       "  0.6228464419475656,\n",
       "  0.49101123595505614,\n",
       "  0.45325670498084286,\n",
       "  0.4683333333333333,\n",
       "  0.4638888888888889,\n",
       "  0.4994318181818182,\n",
       "  0.5711111111111111,\n",
       "  0.5243445692883895,\n",
       "  0.5104868913857679,\n",
       "  0.4655430711610487,\n",
       "  0.480925925925926,\n",
       "  0.4693181818181818,\n",
       "  0.575,\n",
       "  0.4875478927203066,\n",
       "  0.49737827715355803,\n",
       "  0.4853703703703704,\n",
       "  0.4292592592592592,\n",
       "  0.44425925925925924,\n",
       "  0.4279026217228464,\n",
       "  0.4918518518518518,\n",
       "  0.46837121212121213,\n",
       "  0.5032196969696969,\n",
       "  0.43666666666666665,\n",
       "  0.4600775193798449,\n",
       "  0.43754789272030636,\n",
       "  0.5059925093632959,\n",
       "  0.4831481481481481,\n",
       "  0.5363636363636364,\n",
       "  0.4909961685823755,\n",
       "  0.546111111111111,\n",
       "  0.5674242424242425,\n",
       "  0.50187265917603,\n",
       "  0.547752808988764,\n",
       "  0.47940074906367036,\n",
       "  0.44242424242424244,\n",
       "  0.5122222222222222,\n",
       "  0.4149425287356322,\n",
       "  0.5781007751937984,\n",
       "  0.4670370370370371,\n",
       "  0.45530303030303027,\n",
       "  0.5443820224719101,\n",
       "  0.5400749063670411,\n",
       "  0.5668518518518518,\n",
       "  0.449250936329588,\n",
       "  0.41740740740740734,\n",
       "  0.44185185185185183,\n",
       "  0.44272030651340993,\n",
       "  0.540909090909091,\n",
       "  0.5742424242424242,\n",
       "  0.46907407407407403,\n",
       "  0.4789772727272728,\n",
       "  0.48920454545454545,\n",
       "  0.5457854406130268,\n",
       "  0.5288888888888889,\n",
       "  0.45625000000000004,\n",
       "  0.5162921348314607,\n",
       "  0.5308988764044944,\n",
       "  0.5048449612403101,\n",
       "  0.40685185185185185,\n",
       "  0.4800387596899224,\n",
       "  0.4567415730337078,\n",
       "  0.6286516853932584,\n",
       "  0.48726591760299626,\n",
       "  0.5419475655430712,\n",
       "  0.5390196078431372,\n",
       "  0.453370786516854,\n",
       "  0.3492424242424242,\n",
       "  0.45786516853932585,\n",
       "  0.44833333333333336,\n",
       "  0.5308429118773946,\n",
       "  0.49129629629629634,\n",
       "  0.45355805243445685,\n",
       "  0.49606741573033714,\n",
       "  0.49999999999999994,\n",
       "  0.49272030651341,\n",
       "  0.5096296296296298,\n",
       "  0.4792592592592593,\n",
       "  0.5005555555555555,\n",
       "  0.4494444444444444,\n",
       "  0.48815261044176705,\n",
       "  0.4580524344569287,\n",
       "  0.4024904214559388,\n",
       "  0.5086142322097378,\n",
       "  0.518939393939394,\n",
       "  0.5347058823529411,\n",
       "  0.5704545454545454,\n",
       "  0.4788389513108614,\n",
       "  0.44450757575757577,\n",
       "  0.4981273408239701,\n",
       "  0.39082397003745317,\n",
       "  0.4948148148148148,\n",
       "  0.5906367041198501,\n",
       "  0.482962962962963,\n",
       "  0.47215686274509805,\n",
       "  0.5846743295019157,\n",
       "  0.44681647940074903,\n",
       "  0.4633333333333334,\n",
       "  0.5005747126436779,\n",
       "  0.49888888888888877,\n",
       "  0.5361111111111112,\n",
       "  0.4252808988764045,\n",
       "  0.43425925925925923,\n",
       "  0.398780487804878,\n",
       "  0.45259259259259255,\n",
       "  0.4886973180076628,\n",
       "  0.44907407407407407,\n",
       "  0.5283524904214559,\n",
       "  0.5468164794007491,\n",
       "  0.5450757575757575,\n",
       "  0.491111111111111,\n",
       "  0.46148148148148155,\n",
       "  0.49375,\n",
       "  0.5365530303030304,\n",
       "  0.5442592592592592,\n",
       "  0.4492424242424242,\n",
       "  0.3910112359550562,\n",
       "  0.49240740740740735,\n",
       "  0.4935606060606061,\n",
       "  0.4025193798449612,\n",
       "  0.5443181818181819,\n",
       "  0.47518518518518527,\n",
       "  0.5180232558139535,\n",
       "  0.506179775280899,\n",
       "  0.500383141762452,\n",
       "  0.4664750957854405,\n",
       "  0.4475925925925926,\n",
       "  0.5459259259259258,\n",
       "  0.4755555555555556,\n",
       "  0.49583333333333335,\n",
       "  0.4786516853932584,\n",
       "  0.45625,\n",
       "  0.5078544061302682,\n",
       "  0.43294573643410844,\n",
       "  0.4746212121212121,\n",
       "  0.5430894308943088,\n",
       "  0.5153703703703705,\n",
       "  0.4672222222222222,\n",
       "  0.5662962962962963,\n",
       "  0.5814176245210727,\n",
       "  0.4969696969696969,\n",
       "  0.5262172284644195,\n",
       "  0.5216475095785442,\n",
       "  0.4737827715355805,\n",
       "  0.572093023255814,\n",
       "  0.5592592592592593,\n",
       "  0.5104868913857677,\n",
       "  0.5441947565543072]}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_trainer._eval_save_prefix = OUTPUT_PREFIX + \"-test-pos-biased\"\n",
    "model_trainer._evaluate_partial(test_dataset_pos_biased)\n",
    "\n",
    "model_trainer._eval_save_prefix = OUTPUT_PREFIX +  \"-test-neg-biased\"\n",
    "model_trainer._evaluate_partial(test_dataset_neg_biased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unbiased Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 237/237 [00:00<00:00, 1761.05it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 290/290 [00:00<00:00, 1331.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AUC': [0.5433333333333333,\n",
       "  0.3952380952380953,\n",
       "  0.44615384615384623,\n",
       "  0.5230769230769231,\n",
       "  0.558974358974359,\n",
       "  0.511904761904762,\n",
       "  0.46444444444444444,\n",
       "  0.5916666666666667,\n",
       "  0.5114583333333333,\n",
       "  0.4871794871794872,\n",
       "  0.3269230769230769,\n",
       "  0.5333333333333333,\n",
       "  0.4511904761904761,\n",
       "  0.47083333333333327,\n",
       "  0.5211111111111112,\n",
       "  0.6071428571428571,\n",
       "  0.37604166666666666,\n",
       "  0.4488095238095238,\n",
       "  0.5714285714285714,\n",
       "  0.5777777777777777,\n",
       "  0.49666666666666665,\n",
       "  0.6749999999999999,\n",
       "  0.5770833333333334,\n",
       "  0.6239583333333333,\n",
       "  0.4984848484848484,\n",
       "  0.4166666666666667,\n",
       "  0.5351851851851852,\n",
       "  0.481111111111111,\n",
       "  0.4226190476190476,\n",
       "  0.4833333333333334,\n",
       "  0.5621212121212121,\n",
       "  0.375,\n",
       "  0.5700000000000001,\n",
       "  0.5242424242424243,\n",
       "  0.43571428571428567,\n",
       "  0.3796296296296296,\n",
       "  0.45694444444444443,\n",
       "  0.4708333333333333,\n",
       "  0.4922222222222222,\n",
       "  0.3266666666666666,\n",
       "  0.4166666666666667,\n",
       "  0.517948717948718,\n",
       "  0.2777777777777778,\n",
       "  0.4444444444444444,\n",
       "  0.4666666666666666,\n",
       "  0.45777777777777773,\n",
       "  0.4714285714285714,\n",
       "  0.525,\n",
       "  0.709090909090909,\n",
       "  0.4766666666666667,\n",
       "  0.37499999999999994,\n",
       "  0.6814814814814815,\n",
       "  0.3654761904761905,\n",
       "  0.5566666666666668,\n",
       "  0.425,\n",
       "  0.41888888888888887,\n",
       "  0.39999999999999997,\n",
       "  0.3555555555555555,\n",
       "  0.5444444444444444,\n",
       "  0.48666666666666664,\n",
       "  0.5106060606060606,\n",
       "  0.5187499999999999,\n",
       "  0.54375,\n",
       "  0.45,\n",
       "  0.5880952380952381,\n",
       "  0.3833333333333333,\n",
       "  0.4740740740740741,\n",
       "  0.5729166666666667,\n",
       "  0.36190476190476184,\n",
       "  0.5000000000000001,\n",
       "  0.5136363636363636,\n",
       "  0.4263888888888889,\n",
       "  0.5011111111111111,\n",
       "  0.425,\n",
       "  0.42444444444444446,\n",
       "  0.5611111111111112,\n",
       "  0.4547619047619048,\n",
       "  0.48205128205128206,\n",
       "  0.6833333333333335,\n",
       "  0.4533333333333333,\n",
       "  0.5377777777777778,\n",
       "  0.40833333333333327,\n",
       "  0.5051282051282051,\n",
       "  0.4424242424242424,\n",
       "  0.5607142857142857,\n",
       "  0.44791666666666663,\n",
       "  0.39642857142857146,\n",
       "  0.46538461538461545,\n",
       "  0.5761904761904761,\n",
       "  0.3128205128205129,\n",
       "  0.6961538461538461,\n",
       "  0.5114583333333333,\n",
       "  0.4822222222222222,\n",
       "  0.5888888888888889,\n",
       "  0.5604166666666666,\n",
       "  0.5377777777777779,\n",
       "  0.51875,\n",
       "  0.5666666666666665,\n",
       "  0.284375,\n",
       "  0.5549999999999999,\n",
       "  0.51875,\n",
       "  0.43333333333333335,\n",
       "  0.4211111111111111,\n",
       "  0.5551282051282052,\n",
       "  0.5062500000000001,\n",
       "  0.5011904761904763,\n",
       "  0.49487179487179483,\n",
       "  0.6909090909090909,\n",
       "  0.378125,\n",
       "  0.43787878787878787,\n",
       "  0.6064102564102565,\n",
       "  0.37307692307692303,\n",
       "  0.534375,\n",
       "  0.4577777777777778,\n",
       "  0.6384615384615385,\n",
       "  0.4575757575757576,\n",
       "  0.5722222222222223,\n",
       "  0.5527777777777778,\n",
       "  0.515625,\n",
       "  0.4763888888888889,\n",
       "  0.5958333333333333,\n",
       "  0.46145833333333336,\n",
       "  0.4477777777777778,\n",
       "  0.4869047619047619,\n",
       "  0.49333333333333335,\n",
       "  0.7346153846153846,\n",
       "  0.5833333333333333,\n",
       "  0.5416666666666667,\n",
       "  0.5239583333333333,\n",
       "  0.5192307692307693,\n",
       "  0.4261904761904762,\n",
       "  0.4499999999999999,\n",
       "  0.4544444444444444,\n",
       "  0.5628205128205128,\n",
       "  0.5448717948717948,\n",
       "  0.38,\n",
       "  0.5928571428571427,\n",
       "  0.609375,\n",
       "  0.49166666666666664,\n",
       "  0.40952380952380946,\n",
       "  0.4782051282051282,\n",
       "  0.3851851851851852,\n",
       "  0.5107142857142856,\n",
       "  0.75,\n",
       "  0.44583333333333336,\n",
       "  0.3269230769230769,\n",
       "  0.675,\n",
       "  0.43333333333333335,\n",
       "  0.5638888888888889,\n",
       "  0.5520833333333334,\n",
       "  0.53125,\n",
       "  0.5854166666666667,\n",
       "  0.52,\n",
       "  0.37976190476190474,\n",
       "  0.3962962962962963,\n",
       "  0.5433333333333332,\n",
       "  0.5226190476190475,\n",
       "  0.44833333333333325,\n",
       "  0.3736111111111111,\n",
       "  0.5142857142857143,\n",
       "  0.43020833333333336,\n",
       "  0.39999999999999997,\n",
       "  0.621875,\n",
       "  0.4287878787878787,\n",
       "  0.5227272727272728,\n",
       "  0.40625,\n",
       "  0.39404761904761904,\n",
       "  0.46562499999999996,\n",
       "  0.4666666666666666,\n",
       "  0.5155555555555557,\n",
       "  0.4711111111111111,\n",
       "  0.4966666666666667,\n",
       "  0.4155555555555556,\n",
       "  0.6294871794871795,\n",
       "  0.47916666666666663,\n",
       "  0.546875,\n",
       "  0.3025641025641025,\n",
       "  0.38611111111111107,\n",
       "  0.5440476190476191,\n",
       "  0.23333333333333336,\n",
       "  0.5454545454545454,\n",
       "  0.30999999999999994,\n",
       "  0.6097222222222222,\n",
       "  0.4641025641025641,\n",
       "  0.6012820512820513,\n",
       "  0.47,\n",
       "  0.5322222222222222,\n",
       "  0.49,\n",
       "  0.496875,\n",
       "  0.5283333333333333,\n",
       "  0.6,\n",
       "  0.6583333333333333,\n",
       "  0.4510416666666666,\n",
       "  0.5033333333333333,\n",
       "  0.48589743589743595,\n",
       "  0.5270833333333333,\n",
       "  0.43437499999999996,\n",
       "  0.5064102564102564,\n",
       "  0.5999999999999999,\n",
       "  0.6861111111111112,\n",
       "  0.45769230769230773,\n",
       "  0.4916666666666667,\n",
       "  0.638888888888889,\n",
       "  0.4377777777777778,\n",
       "  0.6044444444444446,\n",
       "  0.4236111111111111,\n",
       "  0.49666666666666665,\n",
       "  0.5916666666666667,\n",
       "  0.48333333333333345,\n",
       "  0.5190476190476191,\n",
       "  0.46309523809523806,\n",
       "  0.467948717948718,\n",
       "  0.5166666666666667,\n",
       "  0.4179487179487179,\n",
       "  0.3055555555555555,\n",
       "  0.6270833333333333,\n",
       "  0.4239583333333333,\n",
       "  0.4654761904761905,\n",
       "  0.3979166666666667,\n",
       "  0.46547619047619043,\n",
       "  0.384375,\n",
       "  0.5422222222222223,\n",
       "  0.675,\n",
       "  0.4916666666666667,\n",
       "  0.37307692307692303,\n",
       "  0.5794871794871794,\n",
       "  0.5230769230769231,\n",
       "  0.49444444444444446,\n",
       "  0.6302083333333333,\n",
       "  0.58125,\n",
       "  0.5010416666666666,\n",
       "  0.43,\n",
       "  0.5059523809523808,\n",
       "  0.6022222222222221,\n",
       "  0.5422222222222222,\n",
       "  0.35520833333333335,\n",
       "  0.5777777777777778,\n",
       "  0.49333333333333335,\n",
       "  0.38974358974358964,\n",
       "  0.5809523809523809,\n",
       "  0.48999999999999994,\n",
       "  0.38999999999999996,\n",
       "  0.36363636363636365,\n",
       "  0.4083333333333333,\n",
       "  0.28541666666666665,\n",
       "  0.6805555555555557,\n",
       "  0.40444444444444444,\n",
       "  0.49444444444444435,\n",
       "  0.5122222222222221,\n",
       "  0.4766666666666666,\n",
       "  0.6533333333333334,\n",
       "  0.4948717948717949,\n",
       "  0.49062500000000003,\n",
       "  0.4321428571428571,\n",
       "  0.46458333333333335,\n",
       "  0.6124999999999999,\n",
       "  0.4303030303030303,\n",
       "  0.561111111111111,\n",
       "  0.36875,\n",
       "  0.5145833333333333,\n",
       "  0.45729166666666665,\n",
       "  0.48333333333333334,\n",
       "  0.4309523809523809,\n",
       "  0.48333333333333334,\n",
       "  0.4537037037037038,\n",
       "  0.4833333333333333,\n",
       "  0.5909090909090909,\n",
       "  0.35,\n",
       "  0.37666666666666665,\n",
       "  0.5822916666666667,\n",
       "  0.43125,\n",
       "  0.5652777777777779,\n",
       "  0.5375,\n",
       "  0.39615384615384613,\n",
       "  0.47564102564102567,\n",
       "  0.4511904761904761,\n",
       "  0.6366666666666668,\n",
       "  0.5416666666666667,\n",
       "  0.5433333333333332,\n",
       "  0.528125,\n",
       "  0.5533333333333332,\n",
       "  0.6535714285714286,\n",
       "  0.48939393939393944,\n",
       "  0.5871794871794871,\n",
       "  0.4644444444444444,\n",
       "  0.425,\n",
       "  0.5722222222222222,\n",
       "  0.5733333333333334,\n",
       "  0.44166666666666665,\n",
       "  0.6447916666666667]}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_trainer._eval_save_prefix = OUTPUT_PREFIX + \"-test-pos-unbiased\"\n",
    "model_trainer._evaluate_partial(test_dataset_pos_unbiased)\n",
    "\n",
    "model_trainer._eval_save_prefix = OUTPUT_PREFIX +  \"-test-neg-unbiased\"\n",
    "model_trainer._evaluate_partial(test_dataset_neg_unbiased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute AOA and unbiased evaluator metrics with biased testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    }
   ],
   "source": [
    "biased_results = dict()\n",
    "\n",
    "# biased_results[\"STRATIFIED_15\"] = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=1.5, K=30, partition=100)\n",
    "biased_results[\"AOA\"] = aoa(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", K=30)\n",
    "biased_results[\"UB_15\"] = eq(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=1.5, K=30)\n",
    "biased_results[\"UB_2\"] =  eq(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2, K=30)\n",
    "biased_results[\"UB_25\"] =  eq(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2.5, K=30)\n",
    "biased_results[\"UB_3\"] =  eq(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=3, K=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute AOA and unbiased evaluator metrics with unbiased testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    }
   ],
   "source": [
    "unbiased_results = dict()\n",
    "\n",
    "# unbiased_results[\"STRATIFIED_15\"] = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=1.5, K=1, partition=100)\n",
    "unbiased_results[\"AOA\"] = aoa(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", K=1)\n",
    "unbiased_results[\"UB_15\"] = eq(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=1.5, K=1)\n",
    "unbiased_results[\"UB_2\"] =  eq(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2, K=1)\n",
    "unbiased_results[\"UB_25\"] =  eq(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2.5, K=1)\n",
    "unbiased_results[\"UB_3\"] =  eq(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=3, K=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([118, 198, 186,  96, 232, 124, 241, 253, 235,  68, 135,  63, 179,\n",
       "       236, 243, 128,  87, 225, 162, 239,  97,  83,  23, 256, 192, 209,\n",
       "       150,   1, 153, 216, 294, 290, 143,  45, 287,  12, 114, 102, 180,\n",
       "         5,  90, 140, 234, 184,  70, 175,   2, 246, 211, 100, 300,  36,\n",
       "        85, 240, 108, 213,  39,  69,  66, 201, 101, 152, 210,  11, 110,\n",
       "       220,  72,  43,  79, 200,  13, 270, 295,  48,  91,  92, 166,  31,\n",
       "       167, 119, 190,  86, 194, 127, 106, 227,  61, 269, 161,  22, 204,\n",
       "        20, 264, 296, 172, 156, 202, 218, 147,  82])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get number of items\n",
    "num_items = 300\n",
    "\n",
    "# Get the n_p partitions\n",
    "n_p = 100\n",
    "nums = np.arange(1, num_items+1)\n",
    "partitions = np.random.choice(nums, n_p, replace=False)\n",
    "\n",
    "# Visualize\n",
    "partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the partition which minimizes the sum of AUC and Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84d0475fb87c47538c23437c92c71af4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'trainset' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kj/3wy9xr4j2vlcr59wg5_cqy200000gn/T/ipykernel_16093/3203257642.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Compute the results (AUC and Recall) for both biased and unbiased test sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mtemp_unbiased\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstratified\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUTPUT_PREFIX\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"-test-pos-unbiased_evaluate_partial.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUTPUT_PREFIX\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"-test-neg-unbiased_evaluate_partial.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"training_arr.npy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mtemp_biased\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstratified\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUTPUT_PREFIX\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"-test-pos-biased_evaluate_partial.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUTPUT_PREFIX\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"-test-neg-biased_evaluate_partial.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"training_arr.npy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# If first iteration...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/kj/3wy9xr4j2vlcr59wg5_cqy200000gn/T/ipykernel_16093/3748199493.py\u001b[0m in \u001b[0;36mstratified\u001b[0;34m(infilename, infilename_neg, trainfilename, gamma, K, partition, delta)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mmax_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;31m# ... by computing the sum of squares of w for each user\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'item_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0;31m# Iterate over the trainset to compute the sum of squares for each user\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'trainset' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# Compute biased and unbiased results with stratified for each partition\n",
    "# and store biased and unbiased results such that the sum of AUC and Recall is minimized\n",
    "\n",
    "# Value of gamma to use for minimization\n",
    "gamma = 15\n",
    "\n",
    "# To print :)\n",
    "key = \"STRATIFIED_\" + str(gamma).replace(\".\",\"\")\n",
    "\n",
    "# Initialize results\n",
    "unbiased_results[key] = dict()\n",
    "biased_results[key] = dict()\n",
    "best_partition = np.random.choice(nums, 1)[0]\n",
    "\n",
    "# For each partition\n",
    "for p in tqdm(partitions):\n",
    "    # Compute the results (AUC and Recall) for both biased and unbiased test sets\n",
    "    temp_unbiased = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=gamma, K=10, partition=p)\n",
    "    temp_biased = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=gamma, K=10, partition=p)\n",
    "    # If first iteration...\n",
    "    if not unbiased_results[key]:\n",
    "        unbiased_results[key] = temp_unbiased\n",
    "    if not biased_results[key]:\n",
    "        biased_results[key] = temp_biased\n",
    "    # Else if a better partition was found, update the results\n",
    "    elif temp_unbiased['bias'] + temp_unbiased['concentration'] + temp_biased['bias'] + temp_biased['concentration'] < biased_results[key]['bias'] + biased_results[key]['concentration'] + unbiased_results[key]['bias'] + unbiased_results[key]['concentration']:\n",
    "        biased_results[key]['auc'] = temp_biased['auc']\n",
    "        biased_results[key]['recall'] = temp_biased['recall']\n",
    "        biased_results[key]['bias'] = temp_biased['bias']\n",
    "        biased_results[key]['concentration'] = temp_biased['concentration']\n",
    "        unbiased_results[key]['auc'] = temp_unbiased['auc']\n",
    "        unbiased_results[key]['recall'] = temp_unbiased['recall']\n",
    "        biased_results[key]['bias'] = temp_biased['bias']\n",
    "        biased_results[key]['concentration'] = temp_biased['concentration']\n",
    "        best_partition = p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, for the chosen value of gamma, the best partition is..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "best_partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute stratified metrics with unbiased testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbiased_results[\"STRATIFIED_15\"] = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=1.5, K=1, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_15\"] = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=1.5, K=30, partition=best_partition)\n",
    "\n",
    "unbiased_results[\"STRATIFIED_2\"] = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2, K=1, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_2\"] = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2, K=30, partition=best_partition)\n",
    "\n",
    "unbiased_results[\"STRATIFIED_25\"] = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2.5, K=1, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_25\"] = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2.5, K=30, partition=best_partition)\n",
    "\n",
    "unbiased_results[\"STRATIFIED_3\"] = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=3, K=1, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_3\"] = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=3, K=30, partition=best_partition)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version uses the linspace of items instead of linspace of propensities to make the partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbiased_results[\"STRATIFIED_v2_15\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=1.5, K=1, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_v2_15\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=1.5, K=30, partition=best_partition)\n",
    "\n",
    "unbiased_results[\"STRATIFIED_v2_2\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2, K=1, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_v2_2\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2, K=30, partition=best_partition)\n",
    "\n",
    "unbiased_results[\"STRATIFIED_v2_25\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2.5, K=1, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_v2_25\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2.5, K=30, partition=best_partition)\n",
    "\n",
    "unbiased_results[\"STRATIFIED_v2_3\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=3, K=1, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_v2_3\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=3, K=30, partition=best_partition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare table for results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(list(value.keys()))\n",
    "rows = 2 \n",
    "#len(list(biased_results.items()))\n",
    "columns = 13\n",
    "\n",
    "# Init results\n",
    "results_array = np.zeros((rows,columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the table with the MAE results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init dictionary\n",
    "mae_results = dict()\n",
    "\n",
    "# Get the names of the rows\n",
    "list_biased_res = list(biased_results.keys())\n",
    "\n",
    "# For each row\n",
    "for i in range(len(list_biased_res)):\n",
    "    key = list_biased_res[i]\n",
    "\n",
    "    # For each column\n",
    "    for j in range(len(list(biased_results[key].keys()))):\n",
    "        key_2 = list(biased_results[key].keys())[j]\n",
    "\n",
    "        # Compute MAE\n",
    "        results_array[j][i] = abs(biased_results[key][key_2] - unbiased_results[key][key_2])\n",
    "\n",
    "# Make it a DataFrame\n",
    "mae_df = pd.DataFrame(columns=list(biased_results.keys()), data=results_array)\n",
    "metric_values = list(biased_results[list(biased_results.keys())[0]].keys())\n",
    "mae_df.insert(0, \"metric\", metric_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **RESULTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "mae_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RecSysEvaluation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
