{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some notes: \n",
    "- i saw that some user, item tuples of the random test set are present in the training set, is this ok?\n",
    "- is negative subsampling of 200 items ok? i switched to 60 to keep the ration\n",
    "- is K=4 and K=20 ok for recall?\n",
    "- grid search for gamma=1.5 gives better results on Stratified with gamma=3, then by looking for the number of partitions that minimizes gamma=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy.random as npr\n",
    "from scipy import sparse, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from openrec.tf1.legacy import ImplicitModelTrainer\n",
    "from openrec.tf1.legacy.utils.evaluators import ImplicitEvalManager\n",
    "from openrec.tf1.legacy.utils import ImplicitDataset\n",
    "from openrec.tf1.legacy.recommenders import CML, BPR, PMF\n",
    "from openrec.tf1.legacy.utils.evaluators import AUC\n",
    "from openrec.tf1.legacy.utils.samplers import PairwiseSampler\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "seed = 2384795\n",
    "np.random.seed(seed=seed)\n",
    "\n",
    "# Preparing folder for output data\n",
    "output_name = f\"./generated_data/\"\n",
    "if os.path.exists(output_name) == False:\n",
    "    os.makedirs(output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './original_files/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(os.path.join(DATA_DIR, 'train.ascii'), sep=\" \", header=None, engine=\"python\")\n",
    "test_data = pd.read_csv(os.path.join(DATA_DIR, 'test.ascii'), sep=\" \", header=None, engine=\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_vd_data = pd.DataFrame({\"userId\": sparse.coo_matrix(raw_data).row,                            \"songId\": sparse.coo_matrix(raw_data).col,                           \"rating\": sparse.coo_matrix(raw_data).data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.DataFrame({\"userId\": sparse.coo_matrix(test_data).row,                            \"songId\": sparse.coo_matrix(test_data).col,                           \"rating\": sparse.coo_matrix(test_data).data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_proportion_old(data, uid, test_prop=0.5, random_seed=0):\n",
    "    data_grouped_by_user = data.groupby(uid)\n",
    "    tr_list, te_list = list(), list()\n",
    "\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    for u, (_, group) in enumerate(data_grouped_by_user):\n",
    "        n_items_u = len(group)\n",
    "\n",
    "        if n_items_u >= 5:\n",
    "            idx = np.zeros(n_items_u, dtype='bool')\n",
    "            idx[np.random.choice(n_items_u, size=int(test_prop * n_items_u), replace=False).astype('int64')] = True\n",
    "\n",
    "            tr_list.append(group[np.logical_not(idx)])\n",
    "            te_list.append(group[idx])\n",
    "        else:\n",
    "            tr_list.append(group)\n",
    "\n",
    "        if u % 5000 == 0:\n",
    "            print(\"%d users sampled\" % u)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    data_tr = pd.concat(tr_list)\n",
    "    data_te = pd.concat(te_list)\n",
    "    \n",
    "    return data_tr, data_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_proportion(data, random_seed=0):\n",
    "\n",
    "    df_train = data\n",
    "\n",
    "    # Create a test df\n",
    "    df_test = pd.DataFrame(columns=data.columns)\n",
    "\n",
    "    # Precompute, for each user, the list of songs with a relevant rating\n",
    "    user_positive_ratings = data[data[\"rating\"] == 1].groupby(\"user_id\")[\"item_id\"].apply(set)\n",
    "    \n",
    "    min_item, max_item = data['item_id'].min(), data['item_id'].max()\n",
    "\n",
    "    # Initialize the range of indexes for the items\n",
    "    items_ids = np.arange(min_item, max_item + 1)\n",
    "\n",
    "    # Set the number of songs for each user\n",
    "    SONGS_FOR_BIASED_TEST = 90\n",
    "\n",
    "    users = set(data[\"user_id\"].unique())\n",
    "\n",
    "    # Extract the biased test set\n",
    "    for user_id in users:\n",
    "\n",
    "        # Get SONGS_FOR_BIASED_TEST items\n",
    "        np.random.shuffle(items_ids)\n",
    "        test_items = set(items_ids[-SONGS_FOR_BIASED_TEST:])\n",
    "\n",
    "        # Get which are positive\n",
    "        pos_ids = user_positive_ratings.get(user_id, set()) & test_items\n",
    "\n",
    "        # Get which are negative but in test_items\n",
    "        neg_ids = test_items - pos_ids\n",
    "\n",
    "        # Set the positive ones to 0 in the training set (extract)\n",
    "        df_train.loc[(df_train['item_id'].isin(pos_ids)) & (df_train['user_id'] == user_id), 'rating'] = 0\n",
    "\n",
    "        # now add them in the test set\n",
    "        # add to df_test the rows made of [user_id, pos_ids, 1] and [user_id, neg_ids, 0]\n",
    "        for item_id in pos_ids:\n",
    "            df_test = df_test.append({'user_id': user_id, 'item_id': item_id, 'rating': 1}, ignore_index=True)\n",
    "        \n",
    "        for item_id in neg_ids:\n",
    "            df_test = df_test.append({'user_id': user_id, 'item_id': item_id, 'rating': 0}, ignore_index=True)\n",
    "\n",
    "    # Convert back to the correct data types\n",
    "    df_train['user_id'] = df_train['user_id'].astype(int)\n",
    "    df_train['item_id'] = df_train['item_id'].astype(int)\n",
    "    df_train['rating'] = df_train['rating'].astype(int)\n",
    "    \n",
    "    df_test['user_id'] = df_test['user_id'].astype(int)\n",
    "    df_test['item_id'] = df_test['item_id'].astype(int)\n",
    "    df_test['rating'] = df_test['rating'].astype(int)\n",
    "    \n",
    "    return df_train, df_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count(tp, id):\n",
    "    playcount_groupbyid = tp[[id]].groupby(id, as_index=False)\n",
    "    count = playcount_groupbyid.size()\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make dataset implicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>songId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>136</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>188</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>220</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>227</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>228</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>234</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>235</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  songId  rating\n",
       "0       0      72       2\n",
       "1       0     136       2\n",
       "2       0     150       3\n",
       "3       0     171       3\n",
       "4       0     188       3\n",
       "5       0     220       3\n",
       "6       0     227       5\n",
       "7       0     228       4\n",
       "8       0     234       3\n",
       "9       0     235       4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_vd_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>songId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>78</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>104</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>127</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>133</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>145</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  songId  rating\n",
       "0       0      12       4\n",
       "1       0      17       3\n",
       "2       0      74       4\n",
       "3       0      78       2\n",
       "4       0      92       2\n",
       "5       0     104       4\n",
       "6       0     127       4\n",
       "7       0     128       3\n",
       "8       0     133       3\n",
       "9       0     145       2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suggested on the original yahoo's paper\n",
    "POSITIVE_THRESHOLD = 4\n",
    "\n",
    "# Add column to the DataFrame\n",
    "tr_vd_data['ImplicitRating'] = np.where(tr_vd_data['rating'] >= POSITIVE_THRESHOLD, 1, 0)\n",
    "test_data['ImplicitRating'] = np.where(test_data['rating'] >= POSITIVE_THRESHOLD, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>songId</th>\n",
       "      <th>rating</th>\n",
       "      <th>ImplicitRating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>136</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>188</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>220</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>227</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>228</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>234</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>235</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  songId  rating  ImplicitRating\n",
       "0       0      72       2               0\n",
       "1       0     136       2               0\n",
       "2       0     150       3               0\n",
       "3       0     171       3               0\n",
       "4       0     188       3               0\n",
       "5       0     220       3               0\n",
       "6       0     227       5               1\n",
       "7       0     228       4               1\n",
       "8       0     234       3               0\n",
       "9       0     235       4               1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_vd_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_vd_data = tr_vd_data.drop(['rating'],axis=1).rename({\"ImplicitRating\":\"rating\"}, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>songId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>136</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>188</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>220</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>227</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>228</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>234</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>235</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  songId  rating\n",
       "0       0      72       0\n",
       "1       0     136       0\n",
       "2       0     150       0\n",
       "3       0     171       0\n",
       "4       0     188       0\n",
       "5       0     220       0\n",
       "6       0     227       1\n",
       "7       0     228       1\n",
       "8       0     234       0\n",
       "9       0     235       1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_vd_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>songId</th>\n",
       "      <th>rating</th>\n",
       "      <th>ImplicitRating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>78</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>104</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>127</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>133</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>145</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  songId  rating  ImplicitRating\n",
       "0       0      12       4               1\n",
       "1       0      17       3               0\n",
       "2       0      74       4               1\n",
       "3       0      78       2               0\n",
       "4       0      92       2               0\n",
       "5       0     104       4               1\n",
       "6       0     127       4               1\n",
       "7       0     128       3               0\n",
       "8       0     133       3               0\n",
       "9       0     145       2               0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.drop(['rating'],axis=1).rename({\"ImplicitRating\":\"rating\"}, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>songId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>104</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>127</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>133</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  songId  rating\n",
       "0       0      12       1\n",
       "1       0      17       0\n",
       "2       0      74       1\n",
       "3       0      78       0\n",
       "4       0      92       0\n",
       "5       0     104       1\n",
       "6       0     127       1\n",
       "7       0     128       0\n",
       "8       0     133       0\n",
       "9       0     145       0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4640 entries, 0 to 4639\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   userId  4640 non-null   int32\n",
      " 1   songId  4640 non-null   int32\n",
      " 2   rating  4640 non-null   int64\n",
      "dtypes: int32(2), int64(1)\n",
      "memory usage: 72.6 KB\n"
     ]
    }
   ],
   "source": [
    "test_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   userId  songId  rating\n",
       " 0       0      72       0\n",
       " 1       0     136       0\n",
       " 2       0     150       0\n",
       " 3       0     171       0\n",
       " 4       0     188       0,\n",
       " (6960, 3))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_vd_data.head(), tr_vd_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   userId  songId  rating\n",
       " 0       0      12       1\n",
       " 1       0      17       0\n",
       " 2       0      74       1\n",
       " 3       0      78       0\n",
       " 4       0      92       0,\n",
       " (4640, 3))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head(), test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4640 entries, 0 to 4639\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   userId  4640 non-null   int32\n",
      " 1   songId  4640 non-null   int32\n",
      " 2   rating  4640 non-null   int64\n",
      "dtypes: int32(2), int64(1)\n",
      "memory usage: 72.6 KB\n"
     ]
    }
   ],
   "source": [
    "test_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_activity = get_count(tr_vd_data, 'userId')\n",
    "item_popularity = get_count(tr_vd_data, 'songId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_uid = user_activity.index\n",
    "unique_sid = item_popularity.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = len(unique_uid)\n",
    "n_items = len(unique_sid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(290, 300)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_users, n_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing eventual songs and users from the test set not present in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "song2id = dict((sid, i) for (i, sid) in enumerate(unique_sid))\n",
    "user2id = dict((uid, i) for (i, uid) in enumerate(unique_uid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the test set, only keep the users/items from the training set\n",
    "\n",
    "test_data = test_data.loc[test_data['userId'].isin(unique_uid)]\n",
    "test_data = test_data.loc[test_data['songId'].isin(unique_sid)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn userId and songId to 0-based index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerize(tp):\n",
    "    uid = list(map(lambda x: user2id[x], tp['userId']))\n",
    "    sid = list(map(lambda x: song2id[x], tp['songId']))\n",
    "    tp.loc[:, 'user_id'] = uid\n",
    "    tp.loc[:, 'item_id'] = sid\n",
    "    return tp[['user_id', 'item_id', 'rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_vd_data = numerize(tr_vd_data)\n",
    "test_data = numerize(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do we need the validation for our purpose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data, vad_data = split_train_test_proportion(tr_vd_data, 'user_id', test_prop=0.7, random_seed=12345)\n",
    "#obs_test_data, vad_data = split_train_test_proportion(vad_data, 'user_id', test_prop=0.5, random_seed=12345)\n",
    "train_data, obs_test_data = split_train_test_proportion(tr_vd_data, random_seed=12345)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are total of 290 unique users in the training set and 290 unique users in the entire dataset\n"
     ]
    }
   ],
   "source": [
    "print(\"There are total of %d unique users in the training set and %d unique users in the entire dataset\" % (len(pd.unique(train_data['user_id'])), len(unique_uid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are total of 300 unique items in the training set and 300 unique items in the entire dataset\n"
     ]
    }
   ],
   "source": [
    "print(\"There are total of %d unique items in the training set and %d unique items in the entire dataset\" % (len(pd.unique(train_data['item_id'])), len(unique_sid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_to_fill(part_data_1, part_data_2, unique_id, key):\n",
    "    # move the data from part_data_2 to part_data_1 so that part_data_1 has the same number of unique \"key\" as unique_id\n",
    "    part_id = set(pd.unique(part_data_1[key]))\n",
    "    \n",
    "    left_id = list()\n",
    "    for i, _id in enumerate(unique_id):\n",
    "        if _id not in part_id:\n",
    "            left_id.append(_id)\n",
    "            \n",
    "    move_idx = part_data_2[key].isin(left_id)\n",
    "    part_data_1 = part_data_1.append(part_data_2[move_idx])\n",
    "    part_data_2 = part_data_2[~move_idx]\n",
    "    return part_data_1, part_data_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The move_to_fill function is used to ensure that train_data ends up with a complete set of unique IDs as specified by unique_id, by \"moving\" the necessary rows from another dataset (part_data_2 like vad_data or obs_test_data) and updating both DataFrames accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data, vad_data = move_to_fill(train_data, vad_data, np.arange(n_items), 'item_id')\n",
    "train_data, obs_test_data = move_to_fill(train_data, obs_test_data, np.arange(n_items), 'item_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are total of 300 unique items in the training set and 300 unique items in the entire dataset\n"
     ]
    }
   ],
   "source": [
    "print(\"There are total of %d unique items in the training set and %d unique items in the entire dataset\" % (len(pd.unique(train_data['item_id'])), len(unique_sid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store datasets in csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv(os.path.join(output_name, 'train.csv'), index=False)\n",
    "#vad_data.to_csv(os.path.join(output_name, 'validation.csv'), index=False)\n",
    "tr_vd_data.to_csv(os.path.join(output_name, 'train_full.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_test_data.to_csv(os.path.join(output_name, 'obs_test_full.csv'), index=False)\n",
    "test_data.to_csv(os.path.join(output_name, 'test_full.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now *obs_test_data* is our biased testset extracted by the original dataset, while *test_data* is our unbiased test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>298</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>251</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>228</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>236</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>257</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26095</th>\n",
       "      <td>289</td>\n",
       "      <td>237</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26096</th>\n",
       "      <td>289</td>\n",
       "      <td>239</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26097</th>\n",
       "      <td>289</td>\n",
       "      <td>244</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26098</th>\n",
       "      <td>289</td>\n",
       "      <td>249</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26099</th>\n",
       "      <td>289</td>\n",
       "      <td>254</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26100 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id  item_id  rating\n",
       "0            0      298       1\n",
       "1            0      251       1\n",
       "2            0      228       1\n",
       "3            0      236       1\n",
       "4            0      257       0\n",
       "...        ...      ...     ...\n",
       "26095      289      237       0\n",
       "26096      289      239       0\n",
       "26097      289      244       0\n",
       "26098      289      249       0\n",
       "26099      289      254       0\n",
       "\n",
       "[26100 rows x 3 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build files for creating dataset for the openrec library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init empty\n",
    "pos_test_set = []\n",
    "neg_test_set = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create masks for positive and negative ratings\n",
    "pos_mask = obs_test_data['rating'] == 1\n",
    "neg_mask = obs_test_data['rating'] != 1\n",
    "\n",
    "# Extract the user_id and item_id pairs for positive and negative ratings\n",
    "pos_test_set = obs_test_data.loc[pos_mask, ['user_id', 'item_id']].values.tolist()\n",
    "neg_test_set = obs_test_data.loc[neg_mask, ['user_id', 'item_id']].values.tolist()\n",
    "\n",
    "# pos_test_set and neg_test_set now contain the lists of [user_id, item_id] for positive and negative ratings, respectively.\n",
    "# Get np arrays\n",
    "pos_test_set = np.array(pos_test_set)\n",
    "neg_test_set = np.array(neg_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0, 298],\n",
       "       [  0, 251],\n",
       "       [  0, 228],\n",
       "       ...,\n",
       "       [287, 138],\n",
       "       [287, 120],\n",
       "       [288,  36]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataframe\n",
    "pos_test_set_df = pd.DataFrame(pos_test_set)\n",
    "neg_test_set_df = pd.DataFrame(neg_test_set)\n",
    "\n",
    "# Get couples user-item\n",
    "pos_test_set_df.columns = [\"user_id\",\"item_id\"]\n",
    "neg_test_set_df.columns = [\"user_id\",\"item_id\"]\n",
    "\n",
    "# Turn into records\n",
    "structured_data_pos_test_set = pos_test_set_df.to_records(index=False)\n",
    "structured_data_neg_test_set = neg_test_set_df.to_records(index=False)\n",
    "\n",
    "# Save\n",
    "np.save(output_name + \"biased-test_arr_pos.npy\", structured_data_pos_test_set)\n",
    "np.save(output_name + \"biased-test_arr_neg.npy\", structured_data_neg_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unbiased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init empty\n",
    "pos_test_set = []\n",
    "neg_test_set = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create masks for positive and negative ratings\n",
    "pos_mask = test_data['rating'] == 1\n",
    "neg_mask = test_data['rating'] != 1\n",
    "\n",
    "# Extract the user_id and item_id pairs for positive and negative ratings\n",
    "pos_test_set = test_data.loc[pos_mask, ['user_id', 'item_id']].values.tolist()\n",
    "neg_test_set = test_data.loc[neg_mask, ['user_id', 'item_id']].values.tolist()\n",
    "\n",
    "# pos_test_set and neg_test_set now contain the lists of [user_id, item_id] for positive and negative ratings, respectively.\n",
    "# Get np arrays\n",
    "pos_test_set = np.array(pos_test_set)\n",
    "neg_test_set = np.array(neg_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataframe\n",
    "pos_test_set_df = pd.DataFrame(pos_test_set)\n",
    "neg_test_set_df = pd.DataFrame(neg_test_set)\n",
    "\n",
    "# Get couples user-item\n",
    "pos_test_set_df.columns = [\"user_id\",\"item_id\"]\n",
    "neg_test_set_df.columns = [\"user_id\",\"item_id\"]\n",
    "\n",
    "# Turn into records\n",
    "structured_data_pos_test_set = pos_test_set_df.to_records(index=False)\n",
    "structured_data_neg_test_set = neg_test_set_df.to_records(index=False)\n",
    "\n",
    "# Save\n",
    "np.save(output_name + \"unbiased-test_arr_pos.npy\", structured_data_pos_test_set)\n",
    "np.save(output_name + \"unbiased-test_arr_neg.npy\", structured_data_neg_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_trainset = train_data[train_data['rating'] != 0]\n",
    "positive_trainset = positive_trainset.drop(columns=['rating'])\n",
    "\n",
    "# Convert the DataFrame to a structured array\n",
    "positive_trainset = positive_trainset.to_records(index=False) \n",
    "\n",
    "# Save\n",
    "np.save(output_name + \"training_arr.npy\", positive_trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(290, 290, 290)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"user_id\"].unique().size, test_data[\"user_id\"].unique().size, obs_test_data[\"user_id\"].unique().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(289, 289, 289)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_trainset[\"user_id\"].unique().max(), test_data[\"user_id\"].unique().max(), obs_test_data[\"user_id\"].unique().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 300, 300)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"item_id\"].unique().size, test_data[\"item_id\"].unique().size, obs_test_data[\"item_id\"].unique().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(299, 299, 299)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"item_id\"].unique().max(), test_data[\"item_id\"].unique().max(), obs_test_data[\"item_id\"].unique().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MODEL CHOICE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I won't comment anything, we are just using the code provided by the authors of the paper\n",
    "\n",
    "raw_data = dict()\n",
    "raw_data['train_data'] = np.load(output_name + \"training_arr.npy\" )\n",
    "raw_data['max_user'] = 290\n",
    "raw_data['max_item'] = 300\n",
    "batch_size = 8000\n",
    "test_batch_size = 1000\n",
    "display_itr = 1000\n",
    "\n",
    "train_dataset = ImplicitDataset(raw_data['train_data'], raw_data['max_user'], raw_data['max_item'], name='Train')\n",
    "\n",
    "MODEL_CLASS = CML\n",
    "MODEL_PREFIX = \"cml\"\n",
    "DATASET_NAME = \"coat\"\n",
    "OUTPUT_FOLDER = output_name\n",
    "OUTPUT_PATH = OUTPUT_FOLDER + MODEL_PREFIX + \"-\" + DATASET_NAME + \"/\"\n",
    "OUTPUT_PREFIX = str(OUTPUT_PATH) + str(MODEL_PREFIX) + \"-\" + str(DATASET_NAME)\n",
    "\n",
    "\n",
    "if os.path.exists(OUTPUT_PATH) == False:\n",
    "    os.makedirs(OUTPUT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:391: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:31: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:41: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:43: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:33: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/interactions/pairwise_eu_dist.py:71: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:596: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:75: The name tf.scatter_update is deprecated. Please use tf.compat.v1.scatter_update instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:144: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:365: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:148: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-19 16:42:49.303013: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2024-08-19 16:42:49.328104: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f8f6f414310 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2024-08-19 16:42:49.328120: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Start training with FULL evaluation ==\n",
      "[Itr 100] Finished\n",
      "[Itr 200] Finished\n",
      "[Itr 300] Finished\n",
      "[Itr 400] Finished\n",
      "[Itr 500] Finished\n",
      "[Itr 600] Finished\n",
      "[Itr 700] Finished\n",
      "[Itr 800] Finished\n",
      "[Itr 900] Finished\n",
      "[Itr 1000] Finished\n",
      "INFO:tensorflow:./generated_data/cml-coat/coat-1000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 1000] loss: 6817.881247\n",
      "[Itr 1100] Finished\n",
      "[Itr 1200] Finished\n",
      "[Itr 1300] Finished\n",
      "[Itr 1400] Finished\n",
      "[Itr 1500] Finished\n",
      "[Itr 1600] Finished\n",
      "[Itr 1700] Finished\n",
      "[Itr 1800] Finished\n",
      "[Itr 1900] Finished\n",
      "[Itr 2000] Finished\n",
      "INFO:tensorflow:./generated_data/cml-coat/coat-2000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 2000] loss: 6674.632245\n",
      "[Itr 2100] Finished\n",
      "[Itr 2200] Finished\n",
      "[Itr 2300] Finished\n",
      "[Itr 2400] Finished\n",
      "[Itr 2500] Finished\n",
      "[Itr 2600] Finished\n",
      "[Itr 2700] Finished\n",
      "[Itr 2800] Finished\n",
      "[Itr 2900] Finished\n",
      "[Itr 3000] Finished\n",
      "INFO:tensorflow:./generated_data/cml-coat/coat-3000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 3000] loss: 6671.247231\n",
      "[Itr 3100] Finished\n",
      "[Itr 3200] Finished\n",
      "[Itr 3300] Finished\n",
      "[Itr 3400] Finished\n",
      "[Itr 3500] Finished\n",
      "[Itr 3600] Finished\n",
      "[Itr 3700] Finished\n",
      "[Itr 3800] Finished\n",
      "[Itr 3900] Finished\n",
      "[Itr 4000] Finished\n",
      "INFO:tensorflow:./generated_data/cml-coat/coat-4000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 4000] loss: 6670.158671\n",
      "[Itr 4100] Finished\n",
      "[Itr 4200] Finished\n",
      "[Itr 4300] Finished\n",
      "[Itr 4400] Finished\n",
      "[Itr 4500] Finished\n",
      "[Itr 4600] Finished\n",
      "[Itr 4700] Finished\n",
      "[Itr 4800] Finished\n",
      "[Itr 4900] Finished\n",
      "[Itr 5000] Finished\n",
      "INFO:tensorflow:./generated_data/cml-coat/coat-5000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 5000] loss: 6669.546193\n",
      "[Itr 5100] Finished\n",
      "[Itr 5200] Finished\n",
      "[Itr 5300] Finished\n",
      "[Itr 5400] Finished\n",
      "[Itr 5500] Finished\n",
      "[Itr 5600] Finished\n",
      "[Itr 5700] Finished\n",
      "[Itr 5800] Finished\n",
      "[Itr 5900] Finished\n",
      "[Itr 6000] Finished\n",
      "INFO:tensorflow:./generated_data/cml-coat/coat-6000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 6000] loss: 6668.988957\n",
      "[Itr 6100] Finished\n",
      "[Itr 6200] Finished\n",
      "[Itr 6300] Finished\n",
      "[Itr 6400] Finished\n",
      "[Itr 6500] Finished\n",
      "[Itr 6600] Finished\n",
      "[Itr 6700] Finished\n",
      "[Itr 6800] Finished\n",
      "[Itr 6900] Finished\n",
      "[Itr 7000] Finished\n",
      "INFO:tensorflow:./generated_data/cml-coat/coat-7000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 7000] loss: 6668.849633\n",
      "[Itr 7100] Finished\n",
      "[Itr 7200] Finished\n",
      "[Itr 7300] Finished\n",
      "[Itr 7400] Finished\n",
      "[Itr 7500] Finished\n",
      "[Itr 7600] Finished\n",
      "[Itr 7700] Finished\n",
      "[Itr 7800] Finished\n",
      "[Itr 7900] Finished\n",
      "[Itr 8000] Finished\n",
      "INFO:tensorflow:./generated_data/cml-coat/coat-8000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 8000] loss: 6668.678166\n",
      "[Itr 8100] Finished\n",
      "[Itr 8200] Finished\n",
      "[Itr 8300] Finished\n",
      "[Itr 8400] Finished\n",
      "[Itr 8500] Finished\n",
      "[Itr 8600] Finished\n",
      "[Itr 8700] Finished\n",
      "[Itr 8800] Finished\n",
      "[Itr 8900] Finished\n",
      "[Itr 9000] Finished\n",
      "INFO:tensorflow:./generated_data/cml-coat/coat-9000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 9000] loss: 6668.474175\n",
      "[Itr 9100] Finished\n",
      "[Itr 9200] Finished\n",
      "[Itr 9300] Finished\n",
      "[Itr 9400] Finished\n",
      "[Itr 9500] Finished\n",
      "[Itr 9600] Finished\n",
      "[Itr 9700] Finished\n",
      "[Itr 9800] Finished\n",
      "[Itr 9900] Finished\n",
      "[Itr 10000] Finished\n",
      "INFO:tensorflow:./generated_data/cml-coat/coat-10000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 10000] loss: 6668.361782\n",
      "INFO:tensorflow:./generated_data/cml-coat/ is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "# Prevent tensorflow from using cached embeddings\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "# Define the model\n",
    "model = MODEL_CLASS(batch_size=batch_size, max_user=train_dataset.max_user(), max_item=train_dataset.max_item(), dim_embed=50, l2_reg=0.001, opt='Adam', sess_config=None)\n",
    "sampler = PairwiseSampler(batch_size=batch_size, dataset=train_dataset, num_process=4)\n",
    "model_trainer = ImplicitModelTrainer(batch_size=batch_size, test_batch_size=test_batch_size, train_dataset=train_dataset, model=model, sampler=sampler, eval_save_prefix=OUTPUT_PATH + DATASET_NAME, item_serving_size=500)\n",
    "auc_evaluator = AUC()\n",
    "\n",
    "# Train the model\n",
    "model_trainer.train(num_itr=10001, display_itr=display_itr)\n",
    "\n",
    "# Save in the output folder\n",
    "model.save(OUTPUT_PATH,None)\n",
    "\n",
    "# Delete the model from the memory\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DEFINING FUNCTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_propensities(n_users, n_items, trainfilename, gammas=[1.5, 2, 2.5, 3], normalize=True):\n",
    "\n",
    "    Ni = dict()\n",
    "    propensities = dict()\n",
    "\n",
    "    # Compute frequencies of items in the training set\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "\n",
    "    for gamma in gammas:\n",
    "        propensities[gamma] = np.zeros((n_users,n_items))\n",
    "\n",
    "    for theitem in range(n_items):\n",
    "        if theitem not in Ni:\n",
    "            continue\n",
    "        for gamma in gammas:\n",
    "            propensities[gamma][:,theitem] = np.power(Ni[theitem], (gamma + 1) / 2.0)\n",
    "\n",
    "    if normalize:\n",
    "        for gamma in gammas:\n",
    "            propensities[gamma] /= propensities[gamma].max()\n",
    "\n",
    "    \n",
    "\n",
    "    return propensities\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eq(infilename, infilename_neg, trainfilename, propensities, K=4):\n",
    "\n",
    "    # Read pickles\n",
    "    infile = open(infilename, 'rb')\n",
    "    infile_neg = open(infilename_neg, 'rb')\n",
    "    P = pickle.load(infile)\n",
    "    infile.close()\n",
    "    P_neg = pickle.load(infile_neg)\n",
    "    infile_neg.close()\n",
    "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
    "    \n",
    "    # Merge P and P_neg\n",
    "    for theuser in P[\"users\"]:\n",
    "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "    \n",
    "    Zui = dict()\n",
    "    Ni = dict()\n",
    "    \n",
    "    # Compute frequencies of items in the training set\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "    del trainset\n",
    "\n",
    "    # Count #users with non-zero item frequencies\n",
    "    nonzero_user_count = 0\n",
    "    for theuser in P[\"users\"]:\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for pos_item in pos_items:\n",
    "            if pos_item in Ni:\n",
    "                nonzero_user_count += 1\n",
    "                break\n",
    "    \n",
    "    # Compute recommendations for each user\n",
    "    for theuser in P[\"users\"]:\n",
    "        all_scores = np.array(P[\"results\"][theuser])\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for i, pos_item in enumerate(pos_items):\n",
    "            pos_score = pos_scores[i]\n",
    "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
    "\n",
    "    \n",
    "    # Calculate per-user scores\n",
    "    sum_user_auc = 0.0\n",
    "    sum_user_recall = 0.0\n",
    "    for theuser in P[\"users\"]:\n",
    "        numerator_auc = 0.0\n",
    "        numerator_recall = 0.0\n",
    "        denominator = 0.0\n",
    "\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            if theitem not in Ni:\n",
    "                continue\n",
    "            pui = propensities[theuser][theitem]\n",
    "            \n",
    "\n",
    "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser])) / pui\n",
    "            \n",
    "            if Zui[(theuser, theitem)] < K:\n",
    "                numerator_recall += 1.0 / pui\n",
    "            denominator += 1 / pui\n",
    "                \n",
    "        if denominator > 0:\n",
    "            sum_user_auc += numerator_auc / denominator\n",
    "            sum_user_recall += numerator_recall / denominator \n",
    "\n",
    "    # Return\n",
    "    return {\n",
    "        \"auc\"       : sum_user_auc / nonzero_user_count,\n",
    "        \"recall\"    : sum_user_recall / nonzero_user_count,\n",
    "        \"bias\"      : -1,\n",
    "        \"concentration\" : -1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aoa(infilename, infilename_neg, trainfilename, K=4):\n",
    "\n",
    "    # Read pickles\n",
    "    infile = open(infilename, 'rb')\n",
    "    infile_neg = open(infilename_neg, 'rb')\n",
    "    P = pickle.load(infile)\n",
    "    infile.close()\n",
    "    P_neg = pickle.load(infile_neg)\n",
    "    infile_neg.close()\n",
    "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
    "    \n",
    "    # Merge P and P_neg\n",
    "    for theuser in P[\"users\"]:\n",
    "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "    \n",
    "    Zui = dict()\n",
    "    Ni = dict()\n",
    "\n",
    "    # Compute frequencies of items in the training set\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "    del trainset\n",
    "    \n",
    "    # Count #users with non-zero item frequencies\n",
    "    nonzero_user_count = 0\n",
    "    for theuser in P[\"users\"]:\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for pos_item in pos_items:\n",
    "            if pos_item in Ni:\n",
    "                nonzero_user_count += 1\n",
    "                break\n",
    "    \n",
    "    # Compute recommendations for each user\n",
    "    for theuser in P[\"users\"]:\n",
    "        all_scores = np.array(P[\"results\"][theuser])\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for i, pos_item in enumerate(pos_items):\n",
    "            pos_score = pos_scores[i]\n",
    "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
    "\n",
    "    # Calculate per-user scores\n",
    "    sum_user_auc = 0.0\n",
    "    sum_user_recall = 0.0\n",
    "    for theuser in P[\"users\"]:\n",
    "        numerator_auc = 0.0\n",
    "        numerator_recall = 0.0\n",
    "        denominator = 0.0\n",
    "\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            if theitem not in Ni:\n",
    "                continue\n",
    "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser]))\n",
    "            # Calcolo il Recall a 30, vedi nota 6 paper\n",
    "            if Zui[(theuser, theitem)] < K:\n",
    "                numerator_recall += 1.0\n",
    "            denominator += 1 \n",
    "\n",
    "        if denominator > 0:\n",
    "            sum_user_auc += numerator_auc / denominator\n",
    "            sum_user_recall += numerator_recall / denominator\n",
    "\n",
    "    # Return\n",
    "    return {\n",
    "        \"auc\"       : sum_user_auc / nonzero_user_count,\n",
    "        \"recall\"    : sum_user_recall / nonzero_user_count,\n",
    "        \"bias\"      : -1,\n",
    "        \"concentration\" : -1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified(infilename, infilename_neg, trainfilename, propensities, K=20, partition=10, delta=0.1):\n",
    "\n",
    "    # Read pickles\n",
    "    infile = open(infilename, 'rb')\n",
    "    infile_neg = open(infilename_neg, 'rb')\n",
    "    P = pickle.load(infile)\n",
    "    infile.close()\n",
    "    P_neg = pickle.load(infile_neg)\n",
    "    infile_neg.close()\n",
    "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
    "\n",
    "    # Merge P and P_neg\n",
    "    for theuser in P[\"users\"]:\n",
    "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "\n",
    "    Zui = dict()\n",
    "    Ni = dict()\n",
    "\n",
    "    # Compute frequencies of items in the training set\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "\n",
    "    # Compute recommendations for each user\n",
    "    for theuser in P[\"users\"]:\n",
    "        all_scores = np.array(P[\"results\"][theuser])\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for i, pos_item in enumerate(pos_items):\n",
    "            pos_score = pos_scores[i]\n",
    "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
    "\n",
    "    w = dict()\n",
    "\n",
    "    # Using as pui a single row of propensities, as we assumed propensities to be user independent\n",
    "    pui = propensities[0,:]\n",
    "\n",
    "    # Take the list of items (not tuples) in pui sorted by value\n",
    "    items_sorted_by_value = np.argsort(pui)[::-1]\n",
    "\n",
    "    # Filter out indices where the value in pui is 0\n",
    "    items_sorted_by_value = items_sorted_by_value[pui[items_sorted_by_value] > 0]\n",
    "\n",
    "    #items_sorted_by_value = sorted(pui, key=np.arange(0,pui.shape[0]), reverse=True)\n",
    "\n",
    "    # Compute linspace between the pui[0] and pui[-1] \n",
    "    linspace = np.linspace(pui[items_sorted_by_value[0]], pui[items_sorted_by_value[-1]], partition+1)\n",
    "   \n",
    "    # Compute dictionary w, that is, for each item, assigns the average of the puis in the partition it belongs to\n",
    "    i=0\n",
    "    j = 0\n",
    "    while i < len(items_sorted_by_value):\n",
    "                            \n",
    "        avg = 0\n",
    "        start = i\n",
    "        end = i\n",
    "    \n",
    "        while i < len(items_sorted_by_value) and pui[items_sorted_by_value[i]] >= linspace[j+1]:\n",
    "            avg += 1.0 / pui[items_sorted_by_value[i]]\n",
    "            end = i\n",
    "            i += 1\n",
    "        avg = avg / (end - start + 1)\n",
    "\n",
    "        for k in range(start, end+1):\n",
    "            w[items_sorted_by_value[k]] = avg\n",
    "\n",
    "        j += 1\n",
    "\n",
    "    # Compute bias' numerator\n",
    "    bias = 0.0\n",
    "    for k in items_sorted_by_value:\n",
    "        # add |pui*w - 1!|\n",
    "        bias += abs(pui[k] * w[k] - 1)\n",
    "    # Multiply by number of users\n",
    "    bias *= len(P[\"users\"])\n",
    "\n",
    "    # Compute concentrations numerator (for each user)\n",
    "    concentrations = {}\n",
    "    max_w = max(w.values())\n",
    "    # ... by computing the sum of squares of w for each user\n",
    "    for user, item in zip(trainset['user_id'], trainset['item_id']):\n",
    "        # Iterate over the trainset to compute the sum of squares for each user\n",
    "        if item in w:\n",
    "            if user not in concentrations:\n",
    "                concentrations[user] = 0\n",
    "            concentrations[user] += w[item] ** 2\n",
    "    # ... and then applying the formula\n",
    "    for user in concentrations:\n",
    "        concentrations[user] = math.sqrt(concentrations[user] * 2 * math.log(2/delta)) + max_w * 7 * math.log(2/delta)\n",
    "    # Now sum all the concentrations\n",
    "    concentration = sum(concentrations.values())\n",
    "\n",
    "    # Calculate per-user scores\n",
    "    nonzero_user_count = 0\n",
    "    sum_user_auc = 0.0\n",
    "    sum_user_recall = 0.0\n",
    "    for theuser in P[\"users\"]:\n",
    "        numerator_auc = 0.0\n",
    "        numerator_recall = 0.0\n",
    "        denominator = 0.0\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            # Skip items with null frequency\n",
    "            if  theitem not in Ni:\n",
    "                continue\n",
    "            #if pui[theitem] == 0:\n",
    "            #    continue\n",
    "\n",
    "            # Add things to be summed for each item\n",
    "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser])) * w[theitem]\n",
    "            # Add things for recall\n",
    "            if Zui[(theuser, theitem)] < K:\n",
    "                numerator_recall += 1.0 * w[theitem] #Â spetta\n",
    "            # Increment denominator that the sum must be divided by \n",
    "            denominator += 1 / pui[theitem]\n",
    "\n",
    "\n",
    "        # If there was at least one item for the user, count the user and sum the results\n",
    "        if denominator > 0:\n",
    "            nonzero_user_count += 1\n",
    "            sum_user_auc += numerator_auc / denominator\n",
    "            sum_user_recall += numerator_recall / denominator \n",
    "\n",
    "    # Return\n",
    "    return {\n",
    "        \"auc\"       : sum_user_auc / nonzero_user_count, \n",
    "        \"recall\"    : sum_user_recall / nonzero_user_count,\n",
    "        \"bias\"      : bias,\n",
    "        \"concentration\" : concentration\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_logspace(infilename, infilename_neg, trainfilename, propensities, K=20, partition=10, delta=0.1):\n",
    "\n",
    "    # Read pickles\n",
    "    infile = open(infilename, 'rb')\n",
    "    infile_neg = open(infilename_neg, 'rb')\n",
    "    P = pickle.load(infile)\n",
    "    infile.close()\n",
    "    P_neg = pickle.load(infile_neg)\n",
    "    infile_neg.close()\n",
    "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
    "\n",
    "    # Merge P and P_neg\n",
    "    for theuser in P[\"users\"]:\n",
    "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "\n",
    "    Zui = dict()\n",
    "    Ni = dict()\n",
    "\n",
    "    # Compute frequencies of items in the training set\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "\n",
    "    # Compute recommendations for each user\n",
    "    for theuser in P[\"users\"]:\n",
    "        all_scores = np.array(P[\"results\"][theuser])\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for i, pos_item in enumerate(pos_items):\n",
    "            pos_score = pos_scores[i]\n",
    "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
    "\n",
    "    w = dict()\n",
    "\n",
    "    pui = propensities[0,:]\n",
    "\n",
    "    # Take the list of items (not tuples) in pui sorted by value\n",
    "    items_sorted_by_value = np.argsort(pui)[::-1]\n",
    "\n",
    "    # Filter out indices where the value in pui is 0\n",
    "    items_sorted_by_value = items_sorted_by_value[pui[items_sorted_by_value] > 0]\n",
    "\n",
    "    #items_sorted_by_value = sorted(pui, key=pui.get, reverse=True)\n",
    "\n",
    "    # Compute linspace between the pui[0] and pui[-1] \n",
    "\n",
    "    # Maybe try to split the logspace instead of the linspace?\n",
    "    logspace = np.logspace(pui[items_sorted_by_value[0]], pui[items_sorted_by_value[-1]], partition+1)\n",
    "   \n",
    "    # Compute dictionary w, that is, for each item, assigns the average of the puis in the partition it belongs to\n",
    "    i=0\n",
    "    j = 0\n",
    "    while i < len(items_sorted_by_value):\n",
    "                            \n",
    "        avg = 0\n",
    "        start = i\n",
    "        end = i\n",
    "    \n",
    "        while i < len(items_sorted_by_value) and pui[items_sorted_by_value[i]] >= logspace[j+1]:\n",
    "            avg += 1.0 / pui[items_sorted_by_value[i]]\n",
    "            end = i\n",
    "            i += 1\n",
    "        \n",
    "        # Is the average the only good choice? even with the log space split?\n",
    "        avg = avg / (end - start + 1)\n",
    "\n",
    "        for k in range(start, end+1):\n",
    "            w[items_sorted_by_value[k]] = avg\n",
    "\n",
    "        j += 1\n",
    "\n",
    "        # Compute bias' numerator\n",
    "        bias = 0.0\n",
    "        for k in items_sorted_by_value:\n",
    "            # add |pui*w - 1!|\n",
    "            bias += abs(pui[k] * w[k] - 1)\n",
    "        # Multiply by number of users\n",
    "        bias *= len(P[\"users\"])\n",
    "\n",
    "        # Compute concentrations numerator (for each user)\n",
    "        concentrations = {}\n",
    "        max_w = max(w.values())\n",
    "        # ... by computing the sum of squares of w for each user\n",
    "        for user, item in zip(trainset['user_id'], trainset['item_id']):\n",
    "            # Iterate over the trainset to compute the sum of squares for each user\n",
    "            if item in w:\n",
    "                if user not in concentrations:\n",
    "                    concentrations[user] = 0\n",
    "                concentrations[user] += w[item] ** 2\n",
    "        # ... and then applying the formula\n",
    "        for user in concentrations:\n",
    "            concentrations[user] = math.sqrt(concentrations[user] * 2 * math.log(2/delta)) + max_w * 7 * math.log(2/delta)\n",
    "        # Now sum all the concentrations\n",
    "        concentration = sum(concentrations.values())\n",
    "\n",
    "    # Compute score with AUC and compute Recall\n",
    "    nonzero_user_count = 0\n",
    "    sum_user_auc = 0.0\n",
    "    sum_user_recall = 0.0\n",
    "    for theuser in P[\"users\"]:\n",
    "        numerator_auc = 0.0\n",
    "        numerator_recall = 0.0\n",
    "        denominator = 0.0\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            # Skip items with null frequency\n",
    "            if  theitem not in Ni:\n",
    "                continue\n",
    "            # Add things to be summed for each item\n",
    "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser])) * w[theitem]\n",
    "            # Add things for recall\n",
    "            if Zui[(theuser, theitem)] < K:\n",
    "                numerator_recall += 1.0 * w[theitem] #Â spetta\n",
    "            # Increment denominator that the sum must be divided by \n",
    "            denominator += 1 / pui[theitem]\n",
    "\n",
    "\n",
    "        # If there was at least one item for the user, count the user and sum the results\n",
    "        if denominator > 0:\n",
    "            nonzero_user_count += 1\n",
    "            sum_user_auc += numerator_auc / denominator\n",
    "            sum_user_recall += numerator_recall / denominator \n",
    "\n",
    "    # Return\n",
    "    return {\n",
    "        \"auc\"       : sum_user_auc / nonzero_user_count, \n",
    "        \"recall\"    : sum_user_recall / nonzero_user_count,\n",
    "        \"bias\"      : bias,\n",
    "        \"concentration\" : concentration\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This version uses the linspace of the number of number of items used for evaluation, not of the propensities\n",
    "def stratified_2(infilename, infilename_neg, trainfilename, propensities, K=20, partition=10, delta=0.1):\n",
    "\n",
    "    # Read pickles\n",
    "    infile = open(infilename, 'rb')\n",
    "    infile_neg = open(infilename_neg, 'rb')\n",
    "    P = pickle.load(infile)\n",
    "    infile.close()\n",
    "    P_neg = pickle.load(infile_neg)\n",
    "    infile_neg.close()\n",
    "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
    "\n",
    "    # Merge P and P_neg\n",
    "    for theuser in P[\"users\"]:\n",
    "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "\n",
    "    Zui = dict()\n",
    "    Ni = dict()\n",
    "\n",
    "    # Compute frequencies of items in the training set\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "\n",
    "    # Compute recommendations for each user\n",
    "    for theuser in P[\"users\"]:\n",
    "        all_scores = np.array(P[\"results\"][theuser])\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for i, pos_item in enumerate(pos_items):\n",
    "            pos_score = pos_scores[i]\n",
    "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
    "\n",
    "    pui = propensities[0,:]\n",
    "    w = dict()\n",
    "\n",
    "    # Take the list of items (not tuples) in pui sorted by value\n",
    "    items_sorted_by_value = np.argsort(pui)[::-1]\n",
    "\n",
    "    # Filter out indices where the value in pui is 0\n",
    "    items_sorted_by_value = items_sorted_by_value[pui[items_sorted_by_value] > 0]\n",
    "\n",
    "    #items_sorted_by_value = sorted(pui, key=pui.get, reverse=True)\n",
    "\n",
    "    # Compute linspace between the 0 to len(item_sorted...)\n",
    "    linspace = np.linspace(0, len(items_sorted_by_value), partition+1)\n",
    "   \n",
    "    # Compute dictionary w, that is, for each item, assigns the average of the puis in the partition it belongs to\n",
    "    i=0\n",
    "    j = 0\n",
    "    while i < len(items_sorted_by_value):\n",
    "                            \n",
    "        avg = 0\n",
    "        start = i\n",
    "        end = i\n",
    "    \n",
    "        while i < len(items_sorted_by_value) and i < linspace[j+1]:\n",
    "            avg += 1.0 / pui[items_sorted_by_value[i]]\n",
    "            end = i\n",
    "            i += 1\n",
    "        \n",
    "        avg = avg / (end - start + 1)\n",
    "\n",
    "        for k in range(start, end+1):\n",
    "            w[items_sorted_by_value[k]] = avg\n",
    "\n",
    "        j += 1\n",
    "\n",
    "    # Compute bias' numerator\n",
    "    bias = 0.0\n",
    "    for k in items_sorted_by_value:\n",
    "        # add |pui*w - 1!|\n",
    "        bias += abs(pui[k] * w[k] - 1)\n",
    "    # Multiply by number of users\n",
    "    bias *= len(P[\"users\"])\n",
    "\n",
    "    # Compute concentrations numerator (for each user)\n",
    "    concentrations = {}\n",
    "    max_w = max(w.values())\n",
    "    # ... by computing the sum of squares of w for each user\n",
    "    for user, item in zip(trainset['user_id'], trainset['item_id']):\n",
    "        # Iterate over the trainset to compute the sum of squares for each user\n",
    "        if item in w:\n",
    "            if user not in concentrations:\n",
    "                concentrations[user] = 0\n",
    "            concentrations[user] += w[item] ** 2\n",
    "    # ... and then applying the formula\n",
    "    for user in concentrations:\n",
    "        concentrations[user] = math.sqrt(concentrations[user] * 2 * math.log(2/delta)) + max_w * 7 * math.log(2/delta)\n",
    "    # Now sum all the concentrations\n",
    "    concentration = sum(concentrations.values())\n",
    "\n",
    "    # Compute score with AUC and compute Recall\n",
    "    nonzero_user_count = 0\n",
    "    sum_user_auc = 0.0\n",
    "    sum_user_recall = 0.0\n",
    "    for theuser in P[\"users\"]:\n",
    "        numerator_auc = 0.0\n",
    "        numerator_recall = 0.0\n",
    "        denominator = 0.0\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            # Skip items with null frequency\n",
    "            if  theitem not in Ni:\n",
    "                continue\n",
    "            # Add things to be summed for each item\n",
    "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser])) * w[theitem]\n",
    "            # Add things for recall\n",
    "            if Zui[(theuser, theitem)] < K:\n",
    "                numerator_recall += 1.0 * w[theitem] #Â spetta\n",
    "            # Increment denominator that the sum must be divided by \n",
    "            denominator += 1 / pui[theitem]\n",
    "\n",
    "\n",
    "        # If there was at least one item for the user, count the user and sum the results\n",
    "        if denominator > 0:\n",
    "            nonzero_user_count += 1\n",
    "            sum_user_auc += numerator_auc / denominator\n",
    "            sum_user_recall += numerator_recall / denominator \n",
    "\n",
    "    return {\n",
    "        \"auc\"       : sum_user_auc / nonzero_user_count, \n",
    "        \"recall\"    : sum_user_recall / nonzero_user_count,\n",
    "        \"bias\"      : bias,\n",
    "        \"concentration\" : concentration\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **EVALUATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMAS = [1.5,2,2.5,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "raw_data = dict()\n",
    "raw_data['train_data'] = np.load(output_name + \"training_arr.npy\")\n",
    "raw_data['test_data_pos_biased'] = np.load(output_name + \"biased-test_arr_pos.npy\")\n",
    "raw_data['test_data_neg_biased'] = np.load(output_name + \"biased-test_arr_neg.npy\")\n",
    "raw_data['test_data_pos_unbiased'] = np.load(output_name + \"unbiased-test_arr_pos.npy\")\n",
    "raw_data['test_data_neg_unbiased'] = np.load(output_name + \"unbiased-test_arr_neg.npy\")\n",
    "raw_data['max_user'] = 290\n",
    "raw_data['max_item'] = 300\n",
    "batch_size = 8000\n",
    "test_batch_size = 1000\n",
    "display_itr = 1000\n",
    "\n",
    "# Load data\n",
    "train_dataset = ImplicitDataset(raw_data['train_data'], raw_data['max_user'], raw_data['max_item'], name='Train')\n",
    "test_dataset_pos_biased = ImplicitDataset(raw_data['test_data_pos_biased'], raw_data['max_user'], raw_data['max_item'])\n",
    "test_dataset_neg_biased = ImplicitDataset(raw_data['test_data_neg_biased'], raw_data['max_user'], raw_data['max_item'])\n",
    "test_dataset_pos_unbiased = ImplicitDataset(raw_data['test_data_pos_unbiased'], raw_data['max_user'], raw_data['max_item'])\n",
    "test_dataset_neg_unbiased = ImplicitDataset(raw_data['test_data_neg_unbiased'], raw_data['max_user'], raw_data['max_item'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./generated_data/cml-coat/\n",
      "[Subsampling negative items]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    }
   ],
   "source": [
    "# Prevent tensorflow from using cached embeddings\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "# Define the model\n",
    "model = MODEL_CLASS(batch_size=batch_size, max_user=train_dataset.max_user(), max_item=train_dataset.max_item(), dim_embed=50, l2_reg=0.001, opt='Adam', sess_config=None)\n",
    "sampler = PairwiseSampler(batch_size=batch_size, dataset=train_dataset, num_process=4)\n",
    "model_trainer = ImplicitModelTrainer(batch_size=batch_size, test_batch_size=test_batch_size, train_dataset=train_dataset, model=model, sampler=sampler, eval_save_prefix=OUTPUT_PATH + DATASET_NAME, item_serving_size=500)\n",
    "auc_evaluator = AUC()\n",
    "\n",
    "# Load model\n",
    "model.load(OUTPUT_PATH)\n",
    "\n",
    "# Set parameters\n",
    "model_trainer._eval_manager = ImplicitEvalManager(evaluators=[auc_evaluator])\n",
    "model_trainer._num_negatives = 60 # in yahoo they were 200 on 1000 items, so let's keep a 1/5 ratio on 300 items\n",
    "model_trainer._exclude_positives([train_dataset, test_dataset_pos_biased, test_dataset_neg_biased])\n",
    "model_trainer._sample_negatives(seed=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biased Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 231/231 [00:00<00:00, 1835.36it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 290/290 [00:00<00:00, 333.90it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AUC': [0.5265503875968993,\n",
       "  0.5900749063670412,\n",
       "  0.44356060606060604,\n",
       "  0.45378787878787885,\n",
       "  0.47659176029962536,\n",
       "  0.5618773946360153,\n",
       "  0.4011363636363636,\n",
       "  0.5239700374531835,\n",
       "  0.500766283524904,\n",
       "  0.5718390804597702,\n",
       "  0.5188888888888888,\n",
       "  0.4957364341085272,\n",
       "  0.5272222222222223,\n",
       "  0.5503745318352059,\n",
       "  0.5527777777777778,\n",
       "  0.5543071161048688,\n",
       "  0.507962962962963,\n",
       "  0.4275280898876404,\n",
       "  0.4767045454545455,\n",
       "  0.48183520599250934,\n",
       "  0.5218992248062017,\n",
       "  0.5155038759689923,\n",
       "  0.5100000000000001,\n",
       "  0.48033707865168546,\n",
       "  0.46329365079365076,\n",
       "  0.42827715355805246,\n",
       "  0.45191570881226056,\n",
       "  0.48689138576779023,\n",
       "  0.548689138576779,\n",
       "  0.5335205992509363,\n",
       "  0.5372549019607844,\n",
       "  0.5235955056179775,\n",
       "  0.5003787878787879,\n",
       "  0.5196296296296297,\n",
       "  0.5271535580524345,\n",
       "  0.5096590909090909,\n",
       "  0.5965909090909092,\n",
       "  0.416851851851852,\n",
       "  0.581992337164751,\n",
       "  0.4886274509803921,\n",
       "  0.4691011235955056,\n",
       "  0.5051136363636364,\n",
       "  0.5034883720930233,\n",
       "  0.3691011235955056,\n",
       "  0.5220973782771535,\n",
       "  0.4925093632958801,\n",
       "  0.5285185185185185,\n",
       "  0.54515503875969,\n",
       "  0.548876404494382,\n",
       "  0.5201550387596899,\n",
       "  0.5022222222222221,\n",
       "  0.467816091954023,\n",
       "  0.4386973180076628,\n",
       "  0.44213483146067417,\n",
       "  0.4496031746031746,\n",
       "  0.5577651515151515,\n",
       "  0.4433712121212121,\n",
       "  0.4790740740740741,\n",
       "  0.47253787878787884,\n",
       "  0.4434456928838951,\n",
       "  0.53125,\n",
       "  0.4712962962962963,\n",
       "  0.45449438202247194,\n",
       "  0.48215686274509806,\n",
       "  0.6183333333333334,\n",
       "  0.4389513108614232,\n",
       "  0.506201550387597,\n",
       "  0.5325925925925926,\n",
       "  0.5333333333333333,\n",
       "  0.449250936329588,\n",
       "  0.4840996168582375,\n",
       "  0.538627450980392,\n",
       "  0.4248148148148148,\n",
       "  0.49753787878787875,\n",
       "  0.3972222222222222,\n",
       "  0.45243445692883894,\n",
       "  0.551123595505618,\n",
       "  0.46893939393939393,\n",
       "  0.4816287878787879,\n",
       "  0.479563492063492,\n",
       "  0.4485185185185186,\n",
       "  0.5467871485943777,\n",
       "  0.5151851851851853,\n",
       "  0.5635185185185184,\n",
       "  0.5460227272727273,\n",
       "  0.4136704119850187,\n",
       "  0.6172619047619048,\n",
       "  0.4943820224719101,\n",
       "  0.547093023255814,\n",
       "  0.5368518518518519,\n",
       "  0.4044943820224719,\n",
       "  0.5408239700374532,\n",
       "  0.5706439393939394,\n",
       "  0.5568181818181818,\n",
       "  0.44419475655430707,\n",
       "  0.5327715355805244,\n",
       "  0.4277153558052435,\n",
       "  0.49337121212121215,\n",
       "  0.46985018726591765,\n",
       "  0.4250936329588014,\n",
       "  0.5155555555555557,\n",
       "  0.41969696969696973,\n",
       "  0.5138888888888888,\n",
       "  0.4780392156862745,\n",
       "  0.46628787878787875,\n",
       "  0.6521072796934866,\n",
       "  0.4377394636015326,\n",
       "  0.5729166666666667,\n",
       "  0.5249063670411985,\n",
       "  0.550392156862745,\n",
       "  0.4720930232558138,\n",
       "  0.4798148148148148,\n",
       "  0.41966292134831457,\n",
       "  0.5019157088122604,\n",
       "  0.5267790262172285,\n",
       "  0.5024621212121212,\n",
       "  0.5982558139534884,\n",
       "  0.4450191570881226,\n",
       "  0.5166666666666667,\n",
       "  0.5181818181818183,\n",
       "  0.5919475655430712,\n",
       "  0.5274074074074074,\n",
       "  0.5038314176245209,\n",
       "  0.5581481481481483,\n",
       "  0.4747126436781608,\n",
       "  0.497191011235955,\n",
       "  0.48914728682170544,\n",
       "  0.5856321839080461,\n",
       "  0.4802681992337163,\n",
       "  0.4913793103448275,\n",
       "  0.5242424242424243,\n",
       "  0.5101123595505619,\n",
       "  0.5729411764705884,\n",
       "  0.4864341085271318,\n",
       "  0.4849206349206349,\n",
       "  0.5130268199233716,\n",
       "  0.4971264367816091,\n",
       "  0.448501872659176,\n",
       "  0.48183520599250934,\n",
       "  0.5625468164794007,\n",
       "  0.3931726907630522,\n",
       "  0.47196969696969704,\n",
       "  0.5560606060606061,\n",
       "  0.5597378277153559,\n",
       "  0.47267441860465115,\n",
       "  0.55,\n",
       "  0.5155555555555555,\n",
       "  0.5525925925925926,\n",
       "  0.48295454545454547,\n",
       "  0.48238636363636367,\n",
       "  0.4897003745318351,\n",
       "  0.5388257575757577,\n",
       "  0.5201851851851852,\n",
       "  0.552621722846442,\n",
       "  0.5159003831417626,\n",
       "  0.49962962962962965,\n",
       "  0.5080524344569288,\n",
       "  0.5140562248995985,\n",
       "  0.5431818181818182,\n",
       "  0.45037037037037037,\n",
       "  0.5211111111111111,\n",
       "  0.5664814814814816,\n",
       "  0.6570370370370371,\n",
       "  0.48662790697674413,\n",
       "  0.452996254681648,\n",
       "  0.44812734082397004,\n",
       "  0.4259469696969697,\n",
       "  0.40574712643678157,\n",
       "  0.4548689138576779,\n",
       "  0.49166666666666664,\n",
       "  0.6100378787878789,\n",
       "  0.6124521072796936,\n",
       "  0.4533333333333332,\n",
       "  0.48823529411764716,\n",
       "  0.4882575757575758,\n",
       "  0.4588014981273408,\n",
       "  0.5259259259259259,\n",
       "  0.4990636704119849,\n",
       "  0.4639846743295019,\n",
       "  0.44846743295019154,\n",
       "  0.45156862745098036,\n",
       "  0.512310606060606,\n",
       "  0.4666666666666667,\n",
       "  0.47126436781609204,\n",
       "  0.4660984848484848,\n",
       "  0.48055555555555557,\n",
       "  0.4544444444444445,\n",
       "  0.4214814814814815,\n",
       "  0.5848148148148148,\n",
       "  0.6172348484848486,\n",
       "  0.5184108527131783,\n",
       "  0.5106060606060606,\n",
       "  0.5796296296296297,\n",
       "  0.5666666666666667,\n",
       "  0.4907407407407408,\n",
       "  0.598876404494382,\n",
       "  0.49850187265917606,\n",
       "  0.4768939393939393,\n",
       "  0.4268199233716476,\n",
       "  0.45484496124031,\n",
       "  0.49507575757575756,\n",
       "  0.48071161048689137,\n",
       "  0.43486590038314177,\n",
       "  0.43820224719101125,\n",
       "  0.5437500000000001,\n",
       "  0.41179775280898884,\n",
       "  0.5538888888888889,\n",
       "  0.4848314606741573,\n",
       "  0.47962962962962963,\n",
       "  0.4074509803921569,\n",
       "  0.47840909090909084,\n",
       "  0.41761363636363635,\n",
       "  0.4956439393939394,\n",
       "  0.5854651162790698,\n",
       "  0.4240310077519379,\n",
       "  0.4941947565543071,\n",
       "  0.46348314606741564,\n",
       "  0.4434456928838951,\n",
       "  0.55,\n",
       "  0.4723484848484848,\n",
       "  0.45203703703703707,\n",
       "  0.4625468164794007,\n",
       "  0.5815261044176707,\n",
       "  0.6047348484848485,\n",
       "  0.534280303030303,\n",
       "  0.5310861423220974,\n",
       "  0.5070075757575758,\n",
       "  0.5125000000000002,\n",
       "  0.5414772727272728,\n",
       "  0.4986742424242424,\n",
       "  0.5369318181818182,\n",
       "  0.46629213483146065,\n",
       "  0.5646067415730337,\n",
       "  0.4846590909090909,\n",
       "  0.584469696969697,\n",
       "  0.5114341085271318,\n",
       "  0.5895833333333333,\n",
       "  0.48544061302681996,\n",
       "  0.5581439393939394,\n",
       "  0.4978927203065134,\n",
       "  0.4698412698412698,\n",
       "  0.5436329588014981,\n",
       "  0.5057471264367817,\n",
       "  0.47253787878787873,\n",
       "  0.5367041198501873,\n",
       "  0.5069767441860464,\n",
       "  0.4642045454545455,\n",
       "  0.4835205992509363,\n",
       "  0.49703703703703705,\n",
       "  0.4760299625468165,\n",
       "  0.5066666666666667,\n",
       "  0.4409961685823755,\n",
       "  0.47870370370370363,\n",
       "  0.46370370370370373,\n",
       "  0.43444444444444447,\n",
       "  0.5293103448275862,\n",
       "  0.5059925093632959,\n",
       "  0.4744318181818182,\n",
       "  0.4435185185185185,\n",
       "  0.5405555555555556,\n",
       "  0.4057407407407408,\n",
       "  0.41003787878787873,\n",
       "  0.4636363636363636,\n",
       "  0.5615530303030304,\n",
       "  0.4572796934865901,\n",
       "  0.5267045454545455,\n",
       "  0.5249063670411986,\n",
       "  0.4044943820224719,\n",
       "  0.44961240310077527,\n",
       "  0.6580524344569288,\n",
       "  0.5382575757575758,\n",
       "  0.5481060606060606,\n",
       "  0.512310606060606,\n",
       "  0.5085271317829457,\n",
       "  0.4505747126436781,\n",
       "  0.4763257575757576,\n",
       "  0.4384469696969697,\n",
       "  0.6170498084291189,\n",
       "  0.44240740740740747,\n",
       "  0.5133333333333334,\n",
       "  0.39981481481481485,\n",
       "  0.38707865168539324,\n",
       "  0.5054307116104869,\n",
       "  0.5053030303030304,\n",
       "  0.46382575757575756,\n",
       "  0.5124521072796935,\n",
       "  0.4724206349206349,\n",
       "  0.46800766283524914,\n",
       "  0.47528089887640457,\n",
       "  0.47407407407407404]}"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_trainer._eval_save_prefix = OUTPUT_PREFIX + \"-test-pos-biased\"\n",
    "model_trainer._evaluate_partial(test_dataset_pos_biased)\n",
    "\n",
    "model_trainer._eval_save_prefix = OUTPUT_PREFIX +  \"-test-neg-biased\"\n",
    "model_trainer._evaluate_partial(test_dataset_neg_biased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unbiased Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 237/237 [00:00<00:00, 1929.87it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 290/290 [00:00<00:00, 1127.05it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AUC': [0.5533333333333333,\n",
       "  0.5357142857142858,\n",
       "  0.6000000000000001,\n",
       "  0.49102564102564106,\n",
       "  0.4243589743589744,\n",
       "  0.5988095238095238,\n",
       "  0.4744444444444445,\n",
       "  0.49583333333333335,\n",
       "  0.42500000000000004,\n",
       "  0.4948717948717949,\n",
       "  0.4358974358974359,\n",
       "  0.42833333333333323,\n",
       "  0.5190476190476191,\n",
       "  0.5666666666666667,\n",
       "  0.4844444444444444,\n",
       "  0.6369047619047619,\n",
       "  0.5208333333333334,\n",
       "  0.3178571428571429,\n",
       "  0.5750000000000001,\n",
       "  0.6100000000000001,\n",
       "  0.5155555555555555,\n",
       "  0.5361111111111111,\n",
       "  0.32291666666666663,\n",
       "  0.6083333333333333,\n",
       "  0.49090909090909085,\n",
       "  0.40238095238095234,\n",
       "  0.5870370370370371,\n",
       "  0.5144444444444444,\n",
       "  0.5928571428571429,\n",
       "  0.4714285714285714,\n",
       "  0.4272727272727273,\n",
       "  0.5178571428571429,\n",
       "  0.4644444444444444,\n",
       "  0.5212121212121213,\n",
       "  0.5047619047619046,\n",
       "  0.46481481481481485,\n",
       "  0.5680555555555556,\n",
       "  0.4395833333333333,\n",
       "  0.44000000000000006,\n",
       "  0.39,\n",
       "  0.3208333333333333,\n",
       "  0.4782051282051282,\n",
       "  0.5911111111111113,\n",
       "  0.5255555555555556,\n",
       "  0.5266666666666667,\n",
       "  0.5322222222222222,\n",
       "  0.4845238095238096,\n",
       "  0.5773809523809524,\n",
       "  0.5287878787878788,\n",
       "  0.495,\n",
       "  0.41190476190476194,\n",
       "  0.4962962962962963,\n",
       "  0.5333333333333333,\n",
       "  0.6077777777777776,\n",
       "  0.38833333333333336,\n",
       "  0.5133333333333333,\n",
       "  0.36363636363636354,\n",
       "  0.5511111111111112,\n",
       "  0.4988888888888889,\n",
       "  0.3588888888888889,\n",
       "  0.5575757575757575,\n",
       "  0.43854166666666666,\n",
       "  0.4645833333333333,\n",
       "  0.41666666666666663,\n",
       "  0.6773809523809523,\n",
       "  0.3202380952380953,\n",
       "  0.5407407407407407,\n",
       "  0.66875,\n",
       "  0.4476190476190477,\n",
       "  0.4871794871794871,\n",
       "  0.43787878787878787,\n",
       "  0.5208333333333333,\n",
       "  0.558888888888889,\n",
       "  0.36388888888888893,\n",
       "  0.3044444444444444,\n",
       "  0.518888888888889,\n",
       "  0.5702380952380952,\n",
       "  0.4846153846153846,\n",
       "  0.5761904761904761,\n",
       "  0.3366666666666667,\n",
       "  0.5555555555555556,\n",
       "  0.5347222222222222,\n",
       "  0.6871794871794871,\n",
       "  0.2924242424242424,\n",
       "  0.5476190476190477,\n",
       "  0.35520833333333335,\n",
       "  0.4880952380952381,\n",
       "  0.5358974358974359,\n",
       "  0.6452380952380953,\n",
       "  0.49743589743589745,\n",
       "  0.5564102564102564,\n",
       "  0.5447916666666666,\n",
       "  0.45999999999999996,\n",
       "  0.558888888888889,\n",
       "  0.47604166666666664,\n",
       "  0.5311111111111112,\n",
       "  0.33958333333333335,\n",
       "  0.41944444444444445,\n",
       "  0.40208333333333335,\n",
       "  0.3233333333333333,\n",
       "  0.5208333333333334,\n",
       "  0.3979166666666667,\n",
       "  0.5388888888888889,\n",
       "  0.5910256410256409,\n",
       "  0.575,\n",
       "  0.5095238095238094,\n",
       "  0.35,\n",
       "  0.6424242424242425,\n",
       "  0.48020833333333335,\n",
       "  0.5,\n",
       "  0.6115384615384616,\n",
       "  0.5192307692307693,\n",
       "  0.36979166666666663,\n",
       "  0.4900000000000001,\n",
       "  0.6756410256410257,\n",
       "  0.45454545454545453,\n",
       "  0.5222222222222223,\n",
       "  0.4986111111111111,\n",
       "  0.4864583333333333,\n",
       "  0.501388888888889,\n",
       "  0.7263888888888889,\n",
       "  0.5885416666666666,\n",
       "  0.4444444444444445,\n",
       "  0.5452380952380952,\n",
       "  0.39888888888888896,\n",
       "  0.5243589743589744,\n",
       "  0.78,\n",
       "  0.4916666666666667,\n",
       "  0.6375,\n",
       "  0.5282051282051282,\n",
       "  0.4714285714285715,\n",
       "  0.3874999999999999,\n",
       "  0.5511111111111112,\n",
       "  0.5076923076923077,\n",
       "  0.5128205128205129,\n",
       "  0.44833333333333336,\n",
       "  0.49999999999999994,\n",
       "  0.4947916666666667,\n",
       "  0.4166666666666667,\n",
       "  0.4964285714285714,\n",
       "  0.4987179487179488,\n",
       "  0.5111111111111112,\n",
       "  0.5059523809523808,\n",
       "  0.494047619047619,\n",
       "  0.4916666666666667,\n",
       "  0.43717948717948707,\n",
       "  0.5895833333333332,\n",
       "  0.5010416666666666,\n",
       "  0.48333333333333334,\n",
       "  0.5291666666666667,\n",
       "  0.5583333333333332,\n",
       "  0.4375,\n",
       "  0.5466666666666667,\n",
       "  0.49523809523809526,\n",
       "  0.5129629629629631,\n",
       "  0.4466666666666666,\n",
       "  0.5571428571428572,\n",
       "  0.37833333333333335,\n",
       "  0.5,\n",
       "  0.369047619047619,\n",
       "  0.5145833333333333,\n",
       "  0.4035714285714285,\n",
       "  0.6375,\n",
       "  0.37121212121212116,\n",
       "  0.5606060606060606,\n",
       "  0.453125,\n",
       "  0.3345238095238095,\n",
       "  0.48020833333333335,\n",
       "  0.391111111111111,\n",
       "  0.5244444444444444,\n",
       "  0.6366666666666666,\n",
       "  0.5266666666666666,\n",
       "  0.3877777777777778,\n",
       "  0.558974358974359,\n",
       "  0.5145833333333334,\n",
       "  0.5947916666666666,\n",
       "  0.5,\n",
       "  0.33055555555555555,\n",
       "  0.39404761904761904,\n",
       "  0.3222222222222222,\n",
       "  0.4818181818181818,\n",
       "  0.42555555555555563,\n",
       "  0.5263888888888889,\n",
       "  0.4371794871794872,\n",
       "  0.4192307692307693,\n",
       "  0.3733333333333333,\n",
       "  0.4444444444444444,\n",
       "  0.6277777777777778,\n",
       "  0.45729166666666665,\n",
       "  0.5083333333333333,\n",
       "  0.6282051282051282,\n",
       "  0.6133333333333333,\n",
       "  0.59375,\n",
       "  0.3722222222222222,\n",
       "  0.5282051282051282,\n",
       "  0.48020833333333335,\n",
       "  0.4354166666666667,\n",
       "  0.5012820512820513,\n",
       "  0.4392857142857144,\n",
       "  0.3541666666666667,\n",
       "  0.41794871794871796,\n",
       "  0.4152777777777778,\n",
       "  0.6166666666666667,\n",
       "  0.4277777777777778,\n",
       "  0.4788888888888889,\n",
       "  0.37222222222222223,\n",
       "  0.56,\n",
       "  0.5895833333333333,\n",
       "  0.5630952380952381,\n",
       "  0.3976190476190476,\n",
       "  0.41785714285714287,\n",
       "  0.41923076923076924,\n",
       "  0.47575757575757577,\n",
       "  0.45641025641025645,\n",
       "  0.2574074074074074,\n",
       "  0.5958333333333333,\n",
       "  0.44270833333333337,\n",
       "  0.4273809523809523,\n",
       "  0.525,\n",
       "  0.43809523809523815,\n",
       "  0.3458333333333333,\n",
       "  0.6288888888888889,\n",
       "  0.175,\n",
       "  0.5523809523809524,\n",
       "  0.55,\n",
       "  0.5320512820512822,\n",
       "  0.6948717948717948,\n",
       "  0.6305555555555555,\n",
       "  0.6729166666666667,\n",
       "  0.6354166666666666,\n",
       "  0.590625,\n",
       "  0.3888888888888888,\n",
       "  0.5428571428571428,\n",
       "  0.5577777777777778,\n",
       "  0.5466666666666667,\n",
       "  0.5520833333333334,\n",
       "  0.5444444444444444,\n",
       "  0.4033333333333333,\n",
       "  0.5358974358974359,\n",
       "  0.5202380952380953,\n",
       "  0.48,\n",
       "  0.5433333333333333,\n",
       "  0.5,\n",
       "  0.43571428571428567,\n",
       "  0.453125,\n",
       "  0.6583333333333332,\n",
       "  0.38,\n",
       "  0.46481481481481485,\n",
       "  0.5533333333333333,\n",
       "  0.48333333333333334,\n",
       "  0.6155555555555556,\n",
       "  0.4461538461538461,\n",
       "  0.4145833333333333,\n",
       "  0.46904761904761905,\n",
       "  0.4041666666666667,\n",
       "  0.4458333333333333,\n",
       "  0.5651515151515152,\n",
       "  0.49861111111111117,\n",
       "  0.3947916666666667,\n",
       "  0.5479166666666666,\n",
       "  0.2604166666666667,\n",
       "  0.45128205128205134,\n",
       "  0.3880952380952381,\n",
       "  0.5803030303030302,\n",
       "  0.5074074074074074,\n",
       "  0.41923076923076924,\n",
       "  0.3772727272727273,\n",
       "  0.362962962962963,\n",
       "  0.38,\n",
       "  0.6052083333333333,\n",
       "  0.43333333333333335,\n",
       "  0.573611111111111,\n",
       "  0.4430555555555557,\n",
       "  0.4038461538461539,\n",
       "  0.37179487179487186,\n",
       "  0.5107142857142857,\n",
       "  0.45333333333333337,\n",
       "  0.5583333333333333,\n",
       "  0.4233333333333334,\n",
       "  0.5708333333333333,\n",
       "  0.39555555555555555,\n",
       "  0.4345238095238095,\n",
       "  0.6151515151515151,\n",
       "  0.5012820512820514,\n",
       "  0.4755555555555555,\n",
       "  0.325,\n",
       "  0.4222222222222222,\n",
       "  0.5033333333333333,\n",
       "  0.36458333333333337,\n",
       "  0.53125]}"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_trainer._eval_save_prefix = OUTPUT_PREFIX + \"-test-pos-unbiased\"\n",
    "model_trainer._evaluate_partial(test_dataset_pos_unbiased)\n",
    "\n",
    "model_trainer._eval_save_prefix = OUTPUT_PREFIX +  \"-test-neg-unbiased\"\n",
    "model_trainer._evaluate_partial(test_dataset_neg_unbiased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Propensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "propensities = calculate_propensities(290,300, output_name+\"training_arr.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1.5: array([[1.        , 0.        , 0.09453009, ..., 0.19708824, 0.04991823,\n",
       "         0.04991823],\n",
       "        [1.        , 0.        , 0.09453009, ..., 0.19708824, 0.04991823,\n",
       "         0.04991823],\n",
       "        [1.        , 0.        , 0.09453009, ..., 0.19708824, 0.04991823,\n",
       "         0.04991823],\n",
       "        ...,\n",
       "        [1.        , 0.        , 0.09453009, ..., 0.19708824, 0.04991823,\n",
       "         0.04991823],\n",
       "        [1.        , 0.        , 0.09453009, ..., 0.19708824, 0.04991823,\n",
       "         0.04991823],\n",
       "        [1.        , 0.        , 0.09453009, ..., 0.19708824, 0.04991823,\n",
       "         0.04991823]]),\n",
       " 2: array([[1.        , 0.        , 0.05897719, ..., 0.14242717, 0.02741012,\n",
       "         0.02741012],\n",
       "        [1.        , 0.        , 0.05897719, ..., 0.14242717, 0.02741012,\n",
       "         0.02741012],\n",
       "        [1.        , 0.        , 0.05897719, ..., 0.14242717, 0.02741012,\n",
       "         0.02741012],\n",
       "        ...,\n",
       "        [1.        , 0.        , 0.05897719, ..., 0.14242717, 0.02741012,\n",
       "         0.02741012],\n",
       "        [1.        , 0.        , 0.05897719, ..., 0.14242717, 0.02741012,\n",
       "         0.02741012],\n",
       "        [1.        , 0.        , 0.05897719, ..., 0.14242717, 0.02741012,\n",
       "         0.02741012]]),\n",
       " 2.5: array([[1.        , 0.        , 0.03679579, ..., 0.10292598, 0.01505091,\n",
       "         0.01505091],\n",
       "        [1.        , 0.        , 0.03679579, ..., 0.10292598, 0.01505091,\n",
       "         0.01505091],\n",
       "        [1.        , 0.        , 0.03679579, ..., 0.10292598, 0.01505091,\n",
       "         0.01505091],\n",
       "        ...,\n",
       "        [1.        , 0.        , 0.03679579, ..., 0.10292598, 0.01505091,\n",
       "         0.01505091],\n",
       "        [1.        , 0.        , 0.03679579, ..., 0.10292598, 0.01505091,\n",
       "         0.01505091],\n",
       "        [1.        , 0.        , 0.03679579, ..., 0.10292598, 0.01505091,\n",
       "         0.01505091]]),\n",
       " 3: array([[1.        , 0.        , 0.02295684, ..., 0.07438017, 0.00826446,\n",
       "         0.00826446],\n",
       "        [1.        , 0.        , 0.02295684, ..., 0.07438017, 0.00826446,\n",
       "         0.00826446],\n",
       "        [1.        , 0.        , 0.02295684, ..., 0.07438017, 0.00826446,\n",
       "         0.00826446],\n",
       "        ...,\n",
       "        [1.        , 0.        , 0.02295684, ..., 0.07438017, 0.00826446,\n",
       "         0.00826446],\n",
       "        [1.        , 0.        , 0.02295684, ..., 0.07438017, 0.00826446,\n",
       "         0.00826446],\n",
       "        [1.        , 0.        , 0.02295684, ..., 0.07438017, 0.00826446,\n",
       "         0.00826446]])}"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "propensities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute AOA and unbiased evaluator metrics with biased testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "biased_results = dict()\n",
    "\n",
    "biased_results[\"AOA\"] = aoa(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", K=20)\n",
    "\n",
    "for gamma in GAMMAS:\n",
    "    key = \"UB_\" + str(gamma).replace(\".\",\"\")\n",
    "    biased_results[key] = eq(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", propensities[gamma], K=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute AOA and unbiased evaluator metrics with unbiased testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbiased_results = dict()\n",
    "\n",
    "# unbiased_results[\"STRATIFIED_15\"] = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=1.5, K=4, partition=100)\n",
    "unbiased_results[\"AOA\"] = aoa(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", K=4)\n",
    "for gamma in GAMMAS:\n",
    "    key = \"UB_\" + str(gamma).replace(\".\",\"\")\n",
    "    unbiased_results[key] = eq(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", propensities[gamma], K=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([118, 198, 186,  96, 232, 124, 241, 253, 235,  68, 135,  63, 179,\n",
       "       236, 243, 128,  87, 225, 162, 239,  97,  83,  23, 256, 192, 209,\n",
       "       150,   1, 153, 216, 294, 290, 143,  45, 287,  12, 114, 102, 180,\n",
       "         5,  90, 140, 234, 184,  70, 175,   2, 246, 211, 100, 300,  36,\n",
       "        85, 240, 108, 213,  39,  69,  66, 201, 101, 152, 210,  11, 110,\n",
       "       220,  72,  43,  79, 200,  13, 270, 295,  48,  91,  92, 166,  31,\n",
       "       167, 119, 190,  86, 194, 127, 106, 227,  61, 269, 161,  22, 204,\n",
       "        20, 264, 296, 172, 156, 202, 218, 147,  82,  50, 163, 158, 196,\n",
       "       176, 242, 120, 117, 168, 267, 151,  56, 206, 197, 277, 231, 178,\n",
       "        67, 284, 297, 224, 229,  99, 221, 142,   4,  44, 136, 169, 165,\n",
       "       203,  64,  15, 268, 164, 195, 145, 272, 257, 115,  51,   6, 134,\n",
       "        88, 207,  78, 271, 278, 181,  80,  93,  28, 208,  10, 137, 177,\n",
       "        54, 286, 121,  42, 233, 237,  84, 248,  19, 149, 171, 191,  89,\n",
       "       199, 238, 293, 188,  53, 266, 273, 173, 155,   3, 130,  76, 126,\n",
       "       262,  98, 125,   7, 281, 107, 148, 265,  32,  59, 154, 104, 123,\n",
       "       116,  17, 291, 185, 226,  60,  71, 244, 183,  73, 122, 182, 157,\n",
       "        16, 260, 105, 250, 141,  57, 214,  95, 219, 247,  24, 132, 133,\n",
       "       223, 254,  38, 215, 283, 289, 280, 112, 159,  35,  25,  18, 103,\n",
       "        14,  46,  26, 193, 138, 288,  27, 251, 275,  75,  33, 263,  29,\n",
       "       282,  21, 189, 139, 298, 205,  47, 285, 259, 131,  77, 252,  52,\n",
       "       222, 160,  58, 113, 174, 261, 109,  30, 230,  81, 292,   9, 245,\n",
       "        65, 212,  41, 187,  37, 279, 146,  74,  49, 249, 228, 111,   8,\n",
       "        34, 217, 276, 299,  55,  62, 170,  40, 144, 129, 274,  94, 258,\n",
       "       255])"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get number of items\n",
    "num_items = 300\n",
    "\n",
    "# Get the n_p partitions\n",
    "n_p = 300\n",
    "nums = np.arange(1, num_items+1)\n",
    "partitions = np.random.choice(nums, n_p, replace=False)\n",
    "\n",
    "# Visualize\n",
    "partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the partition which minimizes the sum of AUC and Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b772148aa1204f008caa12b986d9169c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute biased and unbiased results with stratified for each partition\n",
    "# and store biased and unbiased results such that the sum of AUC and Recall is minimized\n",
    "\n",
    "# Value of gamma to use for minimization\n",
    "gamma = 1.5\n",
    "\n",
    "# To print :)\n",
    "key = \"STRATIFIED_\" + str(gamma).replace(\".\",\"\")\n",
    "\n",
    "# Initialize results\n",
    "unbiased_results[key] = dict()\n",
    "biased_results[key] = dict()\n",
    "best_partition = np.random.choice(nums, 1)[0]\n",
    "\n",
    "history = np.empty(300)\n",
    "\n",
    "\n",
    "# For each partition\n",
    "for p in tqdm(partitions):\n",
    "    # Compute the results (AUC and Recall) for both biased and unbiased test sets\n",
    "    temp_unbiased = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", propensities[gamma], K=4, partition=p)\n",
    "    temp_biased = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", propensities[gamma], K=20, partition=p)\n",
    "    \n",
    "    # If first iteration...\n",
    "    if not unbiased_results[key]:\n",
    "        unbiased_results[key] = temp_unbiased\n",
    "    if not biased_results[key]:\n",
    "        biased_results[key] = temp_biased\n",
    "    # Else if a better partition was found, update the results\n",
    "    elif temp_unbiased['bias'] + temp_unbiased['concentration'] + temp_biased['bias'] + temp_biased['concentration'] < biased_results[key]['bias'] + biased_results[key]['concentration'] + unbiased_results[key]['bias'] + unbiased_results[key]['concentration']:\n",
    "        biased_results[key]['auc'] = temp_biased['auc']\n",
    "        biased_results[key]['recall'] = temp_biased['recall']\n",
    "        biased_results[key]['bias'] = temp_biased['bias']\n",
    "        biased_results[key]['concentration'] = temp_biased['concentration']\n",
    "        unbiased_results[key]['auc'] = temp_unbiased['auc']\n",
    "        unbiased_results[key]['recall'] = temp_unbiased['recall']\n",
    "        biased_results[key]['bias'] = temp_biased['bias']\n",
    "        biased_results[key]['concentration'] = temp_biased['concentration']\n",
    "        best_partition = p\n",
    "    #print(temp_unbiased['bias'], temp_biased['bias'], temp_unbiased['concentration'], temp_biased['concentration'])\n",
    "    history[p-1] = temp_unbiased['bias'] + temp_unbiased['concentration'] + temp_biased['bias'] + temp_biased['concentration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([641011.54693651, 603912.3230429 , 588295.2528227 , 582320.76007935,\n",
       "       559976.65631857, 553877.8557791 , 551699.14915709, 553484.47087527,\n",
       "       551988.15528552, 560933.7340873 , 559917.88016321, 559845.75288849,\n",
       "       576838.45525953, 576277.96372417, 576131.16930133, 575674.34817573,\n",
       "       635318.6685278 , 635429.04908042, 632632.93142109, 632552.364756  ,\n",
       "       632228.20875435, 631868.46756663, 631758.40937598, 631550.07540528,\n",
       "       629880.57165335, 629492.25976169, 758224.432574  , 757635.02443088,\n",
       "       757298.76434645, 757056.76823315, 756831.92573438, 756638.01423427,\n",
       "       755894.53755586, 751828.83934589, 751828.83934589, 751828.83934589,\n",
       "       750442.54205681, 748706.95422175, 748706.95422175, 748706.95422175,\n",
       "       748706.95422175, 748706.95422175, 748706.95422175, 748706.95422175,\n",
       "       748706.95422175, 748706.95422175, 748706.95422175, 748706.95422175,\n",
       "       748706.95422175, 748706.95422175, 748706.95422175, 748706.95422175,\n",
       "       748706.95422175, 748706.95422175, 748706.95422175, 748706.95422175,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748,\n",
       "       977995.00547748, 977995.00547748, 977995.00547748, 977995.00547748])"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the histogram\n",
    "plt.hist(history, bins=10, edgecolor='black')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Data')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, for the chosen value of gamma, the best partition is..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize\n",
    "best_partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auc': 1.4189840271895555,\n",
       " 'recall': 0.8928710694377643,\n",
       " 'bias': 113898.68354593145,\n",
       " 'concentration': 1039373.6137849429}"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", propensities[2.5], K=4, partition=14)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute stratified metrics with unbiased testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gamma in GAMMAS:\n",
    "    key = \"STRATIFIED_\" + str(gamma).replace(\".\",\"\")\n",
    "    unbiased_results[key] = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", propensities[gamma], K=4, partition=best_partition)\n",
    "    biased_results[key] = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", propensities[gamma], K=20, partition=best_partition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version uses the linspace of items instead of linspace of propensities to make the partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gamma in GAMMAS:\n",
    "    key = \"STRATIFIED_v2_\" + str(gamma).replace(\".\",\"\")\n",
    "    unbiased_results[key] = stratified_2(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", propensities[gamma], K=4, partition=best_partition)\n",
    "    biased_results[key] = stratified_2(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", propensities[gamma], K=20, partition=best_partition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 13)"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = 0\n",
    "columns = len(biased_results.keys())\n",
    "\n",
    "for key in biased_results.keys():\n",
    "    rows = max(rows, len(biased_results[key].keys()))\n",
    "\n",
    "for key in unbiased_results.keys():\n",
    "    rows = max(rows, len(biased_results[key].keys()))\n",
    "\n",
    "rows, columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init dictionary\n",
    "mae_results = dict()\n",
    "\n",
    "# Get the names of the rows\n",
    "list_biased_res = list(biased_results.keys())\n",
    "\n",
    "\n",
    "# Init results\n",
    "results_array = np.zeros((rows,columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the table with the MAE results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each row\n",
    "for i in range(len(list_biased_res)):\n",
    "    key = list_biased_res[i]\n",
    "\n",
    "    # For each column\n",
    "    for j in range(len(list(biased_results[key].keys()))):\n",
    "        key_2 = list(biased_results[key].keys())[j]\n",
    "\n",
    "        # Compute MAE\n",
    "        results_array[j][i] = abs(biased_results[key][key_2] - unbiased_results[key][key_2])\n",
    "\n",
    "# Make it a DataFrame\n",
    "mae_df = pd.DataFrame(columns=list(biased_results.keys()), data=results_array)\n",
    "metric_values = list(biased_results[list(biased_results.keys())[0]].keys())\n",
    "mae_df.insert(0, \"metric\", metric_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **RESULTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>AOA</th>\n",
       "      <th>UB_15</th>\n",
       "      <th>UB_2</th>\n",
       "      <th>UB_25</th>\n",
       "      <th>UB_3</th>\n",
       "      <th>STRATIFIED_15</th>\n",
       "      <th>STRATIFIED_2</th>\n",
       "      <th>STRATIFIED_25</th>\n",
       "      <th>STRATIFIED_3</th>\n",
       "      <th>STRATIFIED_v2_15</th>\n",
       "      <th>STRATIFIED_v2_2</th>\n",
       "      <th>STRATIFIED_v2_25</th>\n",
       "      <th>STRATIFIED_v2_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>auc</td>\n",
       "      <td>0.054800</td>\n",
       "      <td>0.090975</td>\n",
       "      <td>0.095139</td>\n",
       "      <td>0.098290</td>\n",
       "      <td>0.100626</td>\n",
       "      <td>0.082059</td>\n",
       "      <td>0.068984</td>\n",
       "      <td>0.043827</td>\n",
       "      <td>0.111492</td>\n",
       "      <td>0.108315</td>\n",
       "      <td>0.116557</td>\n",
       "      <td>0.124141</td>\n",
       "      <td>0.131343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>recall</td>\n",
       "      <td>0.076203</td>\n",
       "      <td>0.113201</td>\n",
       "      <td>0.117820</td>\n",
       "      <td>0.121511</td>\n",
       "      <td>0.124447</td>\n",
       "      <td>0.119133</td>\n",
       "      <td>0.132512</td>\n",
       "      <td>0.119046</td>\n",
       "      <td>0.126275</td>\n",
       "      <td>0.133115</td>\n",
       "      <td>0.142441</td>\n",
       "      <td>0.151204</td>\n",
       "      <td>0.159656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bias</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>257.250036</td>\n",
       "      <td>744.882188</td>\n",
       "      <td>1573.780198</td>\n",
       "      <td>2701.574056</td>\n",
       "      <td>49.682358</td>\n",
       "      <td>60.269106</td>\n",
       "      <td>71.212897</td>\n",
       "      <td>82.580909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>concentration</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          metric       AOA     UB_15      UB_2     UB_25      UB_3  \\\n",
       "0            auc  0.054800  0.090975  0.095139  0.098290  0.100626   \n",
       "1         recall  0.076203  0.113201  0.117820  0.121511  0.124447   \n",
       "2           bias  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3  concentration  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "   STRATIFIED_15  STRATIFIED_2  STRATIFIED_25  STRATIFIED_3  STRATIFIED_v2_15  \\\n",
       "0       0.082059      0.068984       0.043827      0.111492          0.108315   \n",
       "1       0.119133      0.132512       0.119046      0.126275          0.133115   \n",
       "2     257.250036    744.882188    1573.780198   2701.574056         49.682358   \n",
       "3       0.000000      0.000000       0.000000      0.000000          0.000000   \n",
       "\n",
       "   STRATIFIED_v2_2  STRATIFIED_v2_25  STRATIFIED_v2_3  \n",
       "0         0.116557          0.124141         0.131343  \n",
       "1         0.142441          0.151204         0.159656  \n",
       "2        60.269106         71.212897        82.580909  \n",
       "3         0.000000          0.000000         0.000000  "
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize\n",
    "mae_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RecSysEvaluation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
