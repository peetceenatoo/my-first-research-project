{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some notes: \n",
    "- i saw that some user, item tuples of the random test set are present in the training set, is this ok?\n",
    "- is negative subsampling of 200 items ok? i switched to 60 to keep the ration\n",
    "- is K=4 and K=20 ok for recall?\n",
    "- grid search for gamma=1.5 gives better results on Stratified with gamma=3, then by looking for the number of partitions that minimizes gamma=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy.random as npr\n",
    "from scipy import sparse, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from openrec.tf1.legacy import ImplicitModelTrainer\n",
    "from openrec.tf1.legacy.utils.evaluators import ImplicitEvalManager\n",
    "from openrec.tf1.legacy.utils import ImplicitDataset\n",
    "from openrec.tf1.legacy.recommenders import CML, BPR, PMF\n",
    "from openrec.tf1.legacy.utils.evaluators import AUC\n",
    "from openrec.tf1.legacy.utils.samplers import PairwiseSampler\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "seed = 2384795\n",
    "np.random.seed(seed=seed)\n",
    "\n",
    "# Preparing folder for output data\n",
    "output_name = f\"./generated_data/\"\n",
    "if os.path.exists(output_name) == False:\n",
    "    os.makedirs(output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './original_files/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(os.path.join(DATA_DIR, 'train.ascii'), sep=\" \", header=None, engine=\"python\")\n",
    "test_data = pd.read_csv(os.path.join(DATA_DIR, 'test.ascii'), sep=\" \", header=None, engine=\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_vd_data = pd.DataFrame({\"userId\": sparse.coo_matrix(raw_data).row,                            \"songId\": sparse.coo_matrix(raw_data).col,                           \"rating\": sparse.coo_matrix(raw_data).data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.DataFrame({\"userId\": sparse.coo_matrix(test_data).row,                            \"songId\": sparse.coo_matrix(test_data).col,                           \"rating\": sparse.coo_matrix(test_data).data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_proportion_old(data, uid, test_prop=0.5, random_seed=0):\n",
    "    data_grouped_by_user = data.groupby(uid)\n",
    "    tr_list, te_list = list(), list()\n",
    "\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    for u, (_, group) in enumerate(data_grouped_by_user):\n",
    "        n_items_u = len(group)\n",
    "\n",
    "        if n_items_u >= 5:\n",
    "            idx = np.zeros(n_items_u, dtype='bool')\n",
    "            idx[np.random.choice(n_items_u, size=int(test_prop * n_items_u), replace=False).astype('int64')] = True\n",
    "\n",
    "            tr_list.append(group[np.logical_not(idx)])\n",
    "            te_list.append(group[idx])\n",
    "        else:\n",
    "            tr_list.append(group)\n",
    "\n",
    "        if u % 5000 == 0:\n",
    "            print(\"%d users sampled\" % u)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    data_tr = pd.concat(tr_list)\n",
    "    data_te = pd.concat(te_list)\n",
    "    \n",
    "    return data_tr, data_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_proportion(data, random_seed=0):\n",
    "\n",
    "    df_train = data\n",
    "\n",
    "    # Create a test df\n",
    "    df_test = pd.DataFrame(columns=data.columns)\n",
    "\n",
    "    # Precompute, for each user, the list of songs with a relevant rating\n",
    "    user_positive_ratings = data[data[\"rating\"] == 1].groupby(\"user_id\")[\"item_id\"].apply(set)\n",
    "    \n",
    "    min_item, max_item = data['item_id'].min(), data['item_id'].max()\n",
    "\n",
    "    # Initialize the range of indexes for the items\n",
    "    items_ids = np.arange(min_item, max_item + 1)\n",
    "\n",
    "    # Set the number of songs for each user\n",
    "    SONGS_FOR_BIASED_TEST = 90\n",
    "\n",
    "    users = set(data[\"user_id\"].unique())\n",
    "\n",
    "    # Extract the biased test set\n",
    "    for user_id in users:\n",
    "\n",
    "        # Get SONGS_FOR_BIASED_TEST items\n",
    "        np.random.shuffle(items_ids)\n",
    "        test_items = set(items_ids[-SONGS_FOR_BIASED_TEST:])\n",
    "\n",
    "        # Get which are positive\n",
    "        pos_ids = user_positive_ratings.get(user_id, set()) & test_items\n",
    "\n",
    "        # Get which are negative but in test_items\n",
    "        neg_ids = test_items - pos_ids\n",
    "\n",
    "        # Set the positive ones to 0 in the training set (extract)\n",
    "        df_train.loc[(df_train['item_id'].isin(pos_ids)) & (df_train['user_id'] == user_id), 'rating'] = 0\n",
    "\n",
    "        # now add them in the test set\n",
    "        # add to df_test the rows made of [user_id, pos_ids, 1] and [user_id, neg_ids, 0]\n",
    "        for item_id in pos_ids:\n",
    "            df_test = df_test.append({'user_id': user_id, 'item_id': item_id, 'rating': 1}, ignore_index=True)\n",
    "        \n",
    "        for item_id in neg_ids:\n",
    "            df_test = df_test.append({'user_id': user_id, 'item_id': item_id, 'rating': 0}, ignore_index=True)\n",
    "\n",
    "    # Convert back to the correct data types\n",
    "    df_train['user_id'] = df_train['user_id'].astype(int)\n",
    "    df_train['item_id'] = df_train['item_id'].astype(int)\n",
    "    df_train['rating'] = df_train['rating'].astype(int)\n",
    "    \n",
    "    df_test['user_id'] = df_test['user_id'].astype(int)\n",
    "    df_test['item_id'] = df_test['item_id'].astype(int)\n",
    "    df_test['rating'] = df_test['rating'].astype(int)\n",
    "    \n",
    "    return df_train, df_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count(tp, id):\n",
    "    playcount_groupbyid = tp[[id]].groupby(id, as_index=False)\n",
    "    count = playcount_groupbyid.size()\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make dataset implicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>songId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>136</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>188</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>220</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>227</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>228</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>234</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>235</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  songId  rating\n",
       "0       0      72       2\n",
       "1       0     136       2\n",
       "2       0     150       3\n",
       "3       0     171       3\n",
       "4       0     188       3\n",
       "5       0     220       3\n",
       "6       0     227       5\n",
       "7       0     228       4\n",
       "8       0     234       3\n",
       "9       0     235       4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_vd_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>songId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>78</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>104</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>127</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>133</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>145</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  songId  rating\n",
       "0       0      12       4\n",
       "1       0      17       3\n",
       "2       0      74       4\n",
       "3       0      78       2\n",
       "4       0      92       2\n",
       "5       0     104       4\n",
       "6       0     127       4\n",
       "7       0     128       3\n",
       "8       0     133       3\n",
       "9       0     145       2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suggested on the original yahoo's paper\n",
    "POSITIVE_THRESHOLD = 4\n",
    "\n",
    "# Add column to the DataFrame\n",
    "tr_vd_data['ImplicitRating'] = np.where(tr_vd_data['rating'] >= POSITIVE_THRESHOLD, 1, 0)\n",
    "test_data['ImplicitRating'] = np.where(test_data['rating'] >= POSITIVE_THRESHOLD, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>songId</th>\n",
       "      <th>rating</th>\n",
       "      <th>ImplicitRating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>136</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>188</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>220</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>227</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>228</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>234</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>235</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  songId  rating  ImplicitRating\n",
       "0       0      72       2               0\n",
       "1       0     136       2               0\n",
       "2       0     150       3               0\n",
       "3       0     171       3               0\n",
       "4       0     188       3               0\n",
       "5       0     220       3               0\n",
       "6       0     227       5               1\n",
       "7       0     228       4               1\n",
       "8       0     234       3               0\n",
       "9       0     235       4               1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_vd_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_vd_data = tr_vd_data.drop(['rating'],axis=1).rename({\"ImplicitRating\":\"rating\"}, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>songId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>136</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>188</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>220</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>227</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>228</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>234</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>235</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  songId  rating\n",
       "0       0      72       0\n",
       "1       0     136       0\n",
       "2       0     150       0\n",
       "3       0     171       0\n",
       "4       0     188       0\n",
       "5       0     220       0\n",
       "6       0     227       1\n",
       "7       0     228       1\n",
       "8       0     234       0\n",
       "9       0     235       1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_vd_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>songId</th>\n",
       "      <th>rating</th>\n",
       "      <th>ImplicitRating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>78</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>104</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>127</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>133</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>145</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  songId  rating  ImplicitRating\n",
       "0       0      12       4               1\n",
       "1       0      17       3               0\n",
       "2       0      74       4               1\n",
       "3       0      78       2               0\n",
       "4       0      92       2               0\n",
       "5       0     104       4               1\n",
       "6       0     127       4               1\n",
       "7       0     128       3               0\n",
       "8       0     133       3               0\n",
       "9       0     145       2               0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.drop(['rating'],axis=1).rename({\"ImplicitRating\":\"rating\"}, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>songId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>104</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>127</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>133</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  songId  rating\n",
       "0       0      12       1\n",
       "1       0      17       0\n",
       "2       0      74       1\n",
       "3       0      78       0\n",
       "4       0      92       0\n",
       "5       0     104       1\n",
       "6       0     127       1\n",
       "7       0     128       0\n",
       "8       0     133       0\n",
       "9       0     145       0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4640 entries, 0 to 4639\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   userId  4640 non-null   int32\n",
      " 1   songId  4640 non-null   int32\n",
      " 2   rating  4640 non-null   int64\n",
      "dtypes: int32(2), int64(1)\n",
      "memory usage: 72.6 KB\n"
     ]
    }
   ],
   "source": [
    "test_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   userId  songId  rating\n",
       " 0       0      72       0\n",
       " 1       0     136       0\n",
       " 2       0     150       0\n",
       " 3       0     171       0\n",
       " 4       0     188       0,\n",
       " (6960, 3))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_vd_data.head(), tr_vd_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   userId  songId  rating\n",
       " 0       0      12       1\n",
       " 1       0      17       0\n",
       " 2       0      74       1\n",
       " 3       0      78       0\n",
       " 4       0      92       0,\n",
       " (4640, 3))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head(), test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4640 entries, 0 to 4639\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   userId  4640 non-null   int32\n",
      " 1   songId  4640 non-null   int32\n",
      " 2   rating  4640 non-null   int64\n",
      "dtypes: int32(2), int64(1)\n",
      "memory usage: 72.6 KB\n"
     ]
    }
   ],
   "source": [
    "test_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_activity = get_count(tr_vd_data, 'userId')\n",
    "item_popularity = get_count(tr_vd_data, 'songId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_uid = user_activity.index\n",
    "unique_sid = item_popularity.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = len(unique_uid)\n",
    "n_items = len(unique_sid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(290, 300)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_users, n_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing eventual songs and users from the test set not present in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "song2id = dict((sid, i) for (i, sid) in enumerate(unique_sid))\n",
    "user2id = dict((uid, i) for (i, uid) in enumerate(unique_uid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the test set, only keep the users/items from the training set\n",
    "\n",
    "test_data = test_data.loc[test_data['userId'].isin(unique_uid)]\n",
    "test_data = test_data.loc[test_data['songId'].isin(unique_sid)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn userId and songId to 0-based index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerize(tp):\n",
    "    uid = list(map(lambda x: user2id[x], tp['userId']))\n",
    "    sid = list(map(lambda x: song2id[x], tp['songId']))\n",
    "    tp.loc[:, 'user_id'] = uid\n",
    "    tp.loc[:, 'item_id'] = sid\n",
    "    return tp[['user_id', 'item_id', 'rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_vd_data = numerize(tr_vd_data)\n",
    "test_data = numerize(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do we need the validation for our purpose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data, vad_data = split_train_test_proportion(tr_vd_data, 'user_id', test_prop=0.7, random_seed=12345)\n",
    "#obs_test_data, vad_data = split_train_test_proportion(vad_data, 'user_id', test_prop=0.5, random_seed=12345)\n",
    "train_data, obs_test_data = split_train_test_proportion(tr_vd_data, random_seed=12345)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are total of 290 unique users in the training set and 290 unique users in the entire dataset\n"
     ]
    }
   ],
   "source": [
    "print(\"There are total of %d unique users in the training set and %d unique users in the entire dataset\" % (len(pd.unique(train_data['user_id'])), len(unique_uid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are total of 300 unique items in the training set and 300 unique items in the entire dataset\n"
     ]
    }
   ],
   "source": [
    "print(\"There are total of %d unique items in the training set and %d unique items in the entire dataset\" % (len(pd.unique(train_data['item_id'])), len(unique_sid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_to_fill(part_data_1, part_data_2, unique_id, key):\n",
    "    # move the data from part_data_2 to part_data_1 so that part_data_1 has the same number of unique \"key\" as unique_id\n",
    "    part_id = set(pd.unique(part_data_1[key]))\n",
    "    \n",
    "    left_id = list()\n",
    "    for i, _id in enumerate(unique_id):\n",
    "        if _id not in part_id:\n",
    "            left_id.append(_id)\n",
    "            \n",
    "    move_idx = part_data_2[key].isin(left_id)\n",
    "    part_data_1 = part_data_1.append(part_data_2[move_idx])\n",
    "    part_data_2 = part_data_2[~move_idx]\n",
    "    return part_data_1, part_data_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The move_to_fill function is used to ensure that train_data ends up with a complete set of unique IDs as specified by unique_id, by \"moving\" the necessary rows from another dataset (part_data_2 like vad_data or obs_test_data) and updating both DataFrames accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data, vad_data = move_to_fill(train_data, vad_data, np.arange(n_items), 'item_id')\n",
    "train_data, obs_test_data = move_to_fill(train_data, obs_test_data, np.arange(n_items), 'item_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are total of 300 unique items in the training set and 300 unique items in the entire dataset\n"
     ]
    }
   ],
   "source": [
    "print(\"There are total of %d unique items in the training set and %d unique items in the entire dataset\" % (len(pd.unique(train_data['item_id'])), len(unique_sid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store datasets in csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv(os.path.join(output_name, 'train.csv'), index=False)\n",
    "#vad_data.to_csv(os.path.join(output_name, 'validation.csv'), index=False)\n",
    "tr_vd_data.to_csv(os.path.join(output_name, 'train_full.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_test_data.to_csv(os.path.join(output_name, 'obs_test_full.csv'), index=False)\n",
    "test_data.to_csv(os.path.join(output_name, 'test_full.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now *obs_test_data* is our biased testset extracted by the original dataset, while *test_data* is our unbiased test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>298</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>251</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>228</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>236</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>257</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26095</th>\n",
       "      <td>289</td>\n",
       "      <td>237</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26096</th>\n",
       "      <td>289</td>\n",
       "      <td>239</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26097</th>\n",
       "      <td>289</td>\n",
       "      <td>244</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26098</th>\n",
       "      <td>289</td>\n",
       "      <td>249</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26099</th>\n",
       "      <td>289</td>\n",
       "      <td>254</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26100 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id  item_id  rating\n",
       "0            0      298       1\n",
       "1            0      251       1\n",
       "2            0      228       1\n",
       "3            0      236       1\n",
       "4            0      257       0\n",
       "...        ...      ...     ...\n",
       "26095      289      237       0\n",
       "26096      289      239       0\n",
       "26097      289      244       0\n",
       "26098      289      249       0\n",
       "26099      289      254       0\n",
       "\n",
       "[26100 rows x 3 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build files for creating dataset for the openrec library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init empty\n",
    "pos_test_set = []\n",
    "neg_test_set = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create masks for positive and negative ratings\n",
    "pos_mask = obs_test_data['rating'] == 1\n",
    "neg_mask = obs_test_data['rating'] != 1\n",
    "\n",
    "# Extract the user_id and item_id pairs for positive and negative ratings\n",
    "pos_test_set = obs_test_data.loc[pos_mask, ['user_id', 'item_id']].values.tolist()\n",
    "neg_test_set = obs_test_data.loc[neg_mask, ['user_id', 'item_id']].values.tolist()\n",
    "\n",
    "# pos_test_set and neg_test_set now contain the lists of [user_id, item_id] for positive and negative ratings, respectively.\n",
    "# Get np arrays\n",
    "pos_test_set = np.array(pos_test_set)\n",
    "neg_test_set = np.array(neg_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0, 298],\n",
       "       [  0, 251],\n",
       "       [  0, 228],\n",
       "       ...,\n",
       "       [287, 138],\n",
       "       [287, 120],\n",
       "       [288,  36]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataframe\n",
    "pos_test_set_df = pd.DataFrame(pos_test_set)\n",
    "neg_test_set_df = pd.DataFrame(neg_test_set)\n",
    "\n",
    "# Get couples user-item\n",
    "pos_test_set_df.columns = [\"user_id\",\"item_id\"]\n",
    "neg_test_set_df.columns = [\"user_id\",\"item_id\"]\n",
    "\n",
    "# Turn into records\n",
    "structured_data_pos_test_set = pos_test_set_df.to_records(index=False)\n",
    "structured_data_neg_test_set = neg_test_set_df.to_records(index=False)\n",
    "\n",
    "# Save\n",
    "np.save(output_name + \"biased-test_arr_pos.npy\", structured_data_pos_test_set)\n",
    "np.save(output_name + \"biased-test_arr_neg.npy\", structured_data_neg_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unbiased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init empty\n",
    "pos_test_set = []\n",
    "neg_test_set = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create masks for positive and negative ratings\n",
    "pos_mask = test_data['rating'] == 1\n",
    "neg_mask = test_data['rating'] != 1\n",
    "\n",
    "# Extract the user_id and item_id pairs for positive and negative ratings\n",
    "pos_test_set = test_data.loc[pos_mask, ['user_id', 'item_id']].values.tolist()\n",
    "neg_test_set = test_data.loc[neg_mask, ['user_id', 'item_id']].values.tolist()\n",
    "\n",
    "# pos_test_set and neg_test_set now contain the lists of [user_id, item_id] for positive and negative ratings, respectively.\n",
    "# Get np arrays\n",
    "pos_test_set = np.array(pos_test_set)\n",
    "neg_test_set = np.array(neg_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataframe\n",
    "pos_test_set_df = pd.DataFrame(pos_test_set)\n",
    "neg_test_set_df = pd.DataFrame(neg_test_set)\n",
    "\n",
    "# Get couples user-item\n",
    "pos_test_set_df.columns = [\"user_id\",\"item_id\"]\n",
    "neg_test_set_df.columns = [\"user_id\",\"item_id\"]\n",
    "\n",
    "# Turn into records\n",
    "structured_data_pos_test_set = pos_test_set_df.to_records(index=False)\n",
    "structured_data_neg_test_set = neg_test_set_df.to_records(index=False)\n",
    "\n",
    "# Save\n",
    "np.save(output_name + \"unbiased-test_arr_pos.npy\", structured_data_pos_test_set)\n",
    "np.save(output_name + \"unbiased-test_arr_neg.npy\", structured_data_neg_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_trainset = train_data[train_data['rating'] != 0]\n",
    "positive_trainset = positive_trainset.drop(columns=['rating'])\n",
    "\n",
    "# Convert the DataFrame to a structured array\n",
    "positive_trainset = positive_trainset.to_records(index=False) \n",
    "\n",
    "# Save\n",
    "np.save(output_name + \"training_arr.npy\", positive_trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(290, 290, 290)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"user_id\"].unique().size, test_data[\"user_id\"].unique().size, obs_test_data[\"user_id\"].unique().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(289, 289, 289)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_trainset[\"user_id\"].unique().max(), test_data[\"user_id\"].unique().max(), obs_test_data[\"user_id\"].unique().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 300, 300)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"item_id\"].unique().size, test_data[\"item_id\"].unique().size, obs_test_data[\"item_id\"].unique().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(299, 299, 299)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"item_id\"].unique().max(), test_data[\"item_id\"].unique().max(), obs_test_data[\"item_id\"].unique().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MODEL CHOICE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I won't comment anything, we are just using the code provided by the authors of the paper\n",
    "\n",
    "raw_data = dict()\n",
    "raw_data['train_data'] = np.load(output_name + \"training_arr.npy\" )\n",
    "raw_data['max_user'] = 290\n",
    "raw_data['max_item'] = 300\n",
    "batch_size = 8000\n",
    "test_batch_size = 1000\n",
    "display_itr = 1000\n",
    "\n",
    "train_dataset = ImplicitDataset(raw_data['train_data'], raw_data['max_user'], raw_data['max_item'], name='Train')\n",
    "\n",
    "MODEL_CLASS = CML\n",
    "MODEL_PREFIX = \"cml\"\n",
    "DATASET_NAME = \"coat\"\n",
    "OUTPUT_FOLDER = output_name\n",
    "OUTPUT_PATH = OUTPUT_FOLDER + MODEL_PREFIX + \"-\" + DATASET_NAME + \"/\"\n",
    "OUTPUT_PREFIX = str(OUTPUT_PATH) + str(MODEL_PREFIX) + \"-\" + str(DATASET_NAME)\n",
    "\n",
    "\n",
    "if os.path.exists(OUTPUT_PATH) == False:\n",
    "    os.makedirs(OUTPUT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:391: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:31: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:41: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:43: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:33: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/interactions/pairwise_eu_dist.py:71: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:596: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:75: The name tf.scatter_update is deprecated. Please use tf.compat.v1.scatter_update instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:144: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:365: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:148: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-19 16:42:49.303013: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2024-08-19 16:42:49.328104: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f8f6f414310 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2024-08-19 16:42:49.328120: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Start training with FULL evaluation ==\n",
      "[Itr 100] Finished\n",
      "[Itr 200] Finished\n",
      "[Itr 300] Finished\n",
      "[Itr 400] Finished\n",
      "[Itr 500] Finished\n",
      "[Itr 600] Finished\n",
      "[Itr 700] Finished\n",
      "[Itr 800] Finished\n",
      "[Itr 900] Finished\n",
      "[Itr 1000] Finished\n",
      "INFO:tensorflow:./generated_data/cml-coat/coat-1000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 1000] loss: 6817.881247\n",
      "[Itr 1100] Finished\n",
      "[Itr 1200] Finished\n",
      "[Itr 1300] Finished\n",
      "[Itr 1400] Finished\n",
      "[Itr 1500] Finished\n",
      "[Itr 1600] Finished\n",
      "[Itr 1700] Finished\n",
      "[Itr 1800] Finished\n",
      "[Itr 1900] Finished\n",
      "[Itr 2000] Finished\n",
      "INFO:tensorflow:./generated_data/cml-coat/coat-2000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 2000] loss: 6674.632245\n",
      "[Itr 2100] Finished\n",
      "[Itr 2200] Finished\n",
      "[Itr 2300] Finished\n",
      "[Itr 2400] Finished\n",
      "[Itr 2500] Finished\n",
      "[Itr 2600] Finished\n",
      "[Itr 2700] Finished\n",
      "[Itr 2800] Finished\n",
      "[Itr 2900] Finished\n",
      "[Itr 3000] Finished\n",
      "INFO:tensorflow:./generated_data/cml-coat/coat-3000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 3000] loss: 6671.247231\n",
      "[Itr 3100] Finished\n",
      "[Itr 3200] Finished\n",
      "[Itr 3300] Finished\n",
      "[Itr 3400] Finished\n",
      "[Itr 3500] Finished\n",
      "[Itr 3600] Finished\n",
      "[Itr 3700] Finished\n",
      "[Itr 3800] Finished\n",
      "[Itr 3900] Finished\n",
      "[Itr 4000] Finished\n",
      "INFO:tensorflow:./generated_data/cml-coat/coat-4000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 4000] loss: 6670.158671\n",
      "[Itr 4100] Finished\n",
      "[Itr 4200] Finished\n",
      "[Itr 4300] Finished\n",
      "[Itr 4400] Finished\n",
      "[Itr 4500] Finished\n",
      "[Itr 4600] Finished\n",
      "[Itr 4700] Finished\n",
      "[Itr 4800] Finished\n",
      "[Itr 4900] Finished\n",
      "[Itr 5000] Finished\n",
      "INFO:tensorflow:./generated_data/cml-coat/coat-5000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 5000] loss: 6669.546193\n",
      "[Itr 5100] Finished\n",
      "[Itr 5200] Finished\n",
      "[Itr 5300] Finished\n",
      "[Itr 5400] Finished\n",
      "[Itr 5500] Finished\n",
      "[Itr 5600] Finished\n",
      "[Itr 5700] Finished\n",
      "[Itr 5800] Finished\n",
      "[Itr 5900] Finished\n",
      "[Itr 6000] Finished\n",
      "INFO:tensorflow:./generated_data/cml-coat/coat-6000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 6000] loss: 6668.988957\n",
      "[Itr 6100] Finished\n",
      "[Itr 6200] Finished\n",
      "[Itr 6300] Finished\n",
      "[Itr 6400] Finished\n",
      "[Itr 6500] Finished\n",
      "[Itr 6600] Finished\n",
      "[Itr 6700] Finished\n",
      "[Itr 6800] Finished\n",
      "[Itr 6900] Finished\n",
      "[Itr 7000] Finished\n",
      "INFO:tensorflow:./generated_data/cml-coat/coat-7000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 7000] loss: 6668.849633\n",
      "[Itr 7100] Finished\n",
      "[Itr 7200] Finished\n",
      "[Itr 7300] Finished\n",
      "[Itr 7400] Finished\n",
      "[Itr 7500] Finished\n",
      "[Itr 7600] Finished\n",
      "[Itr 7700] Finished\n",
      "[Itr 7800] Finished\n",
      "[Itr 7900] Finished\n",
      "[Itr 8000] Finished\n",
      "INFO:tensorflow:./generated_data/cml-coat/coat-8000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 8000] loss: 6668.678166\n",
      "[Itr 8100] Finished\n",
      "[Itr 8200] Finished\n",
      "[Itr 8300] Finished\n",
      "[Itr 8400] Finished\n",
      "[Itr 8500] Finished\n",
      "[Itr 8600] Finished\n",
      "[Itr 8700] Finished\n",
      "[Itr 8800] Finished\n",
      "[Itr 8900] Finished\n",
      "[Itr 9000] Finished\n",
      "INFO:tensorflow:./generated_data/cml-coat/coat-9000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 9000] loss: 6668.474175\n",
      "[Itr 9100] Finished\n",
      "[Itr 9200] Finished\n",
      "[Itr 9300] Finished\n",
      "[Itr 9400] Finished\n",
      "[Itr 9500] Finished\n",
      "[Itr 9600] Finished\n",
      "[Itr 9700] Finished\n",
      "[Itr 9800] Finished\n",
      "[Itr 9900] Finished\n",
      "[Itr 10000] Finished\n",
      "INFO:tensorflow:./generated_data/cml-coat/coat-10000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 10000] loss: 6668.361782\n",
      "INFO:tensorflow:./generated_data/cml-coat/ is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "# Prevent tensorflow from using cached embeddings\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "# Define the model\n",
    "model = MODEL_CLASS(batch_size=batch_size, max_user=train_dataset.max_user(), max_item=train_dataset.max_item(), dim_embed=50, l2_reg=0.001, opt='Adam', sess_config=None)\n",
    "sampler = PairwiseSampler(batch_size=batch_size, dataset=train_dataset, num_process=4)\n",
    "model_trainer = ImplicitModelTrainer(batch_size=batch_size, test_batch_size=test_batch_size, train_dataset=train_dataset, model=model, sampler=sampler, eval_save_prefix=OUTPUT_PATH + DATASET_NAME, item_serving_size=500)\n",
    "auc_evaluator = AUC()\n",
    "\n",
    "# Train the model\n",
    "model_trainer.train(num_itr=10001, display_itr=display_itr)\n",
    "\n",
    "# Save in the output folder\n",
    "model.save(OUTPUT_PATH,None)\n",
    "\n",
    "# Delete the model from the memory\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DEFINING FUNCTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_propensities(n_users, n_items, trainfilename, gammas=[1.5, 2, 2.5, 3], normalize=True):\n",
    "\n",
    "    Ni = dict()\n",
    "    propensities = dict()\n",
    "\n",
    "    # Compute frequencies of items in the training set\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "\n",
    "    for gamma in gammas:\n",
    "        propensities[gamma] = np.empty((n_users,n_items))\n",
    "\n",
    "    for theitem in range(n_items):\n",
    "        if theitem not in Ni:\n",
    "            continue\n",
    "        for gamma in gammas:\n",
    "            propensities[gamma][:,theitem] = np.power(Ni[theitem], (gamma + 1) / 2.0)\n",
    "\n",
    "    if normalize:\n",
    "        for gamma in gammas:\n",
    "            propensities[gamma] /= propensities[gamma].max()\n",
    "\n",
    "    return propensities\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eq(infilename, infilename_neg, trainfilename, propensities, K=4):\n",
    "\n",
    "    # Read pickles\n",
    "    infile = open(infilename, 'rb')\n",
    "    infile_neg = open(infilename_neg, 'rb')\n",
    "    P = pickle.load(infile)\n",
    "    infile.close()\n",
    "    P_neg = pickle.load(infile_neg)\n",
    "    infile_neg.close()\n",
    "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
    "    \n",
    "    # Merge P and P_neg\n",
    "    for theuser in P[\"users\"]:\n",
    "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "    \n",
    "    Zui = dict()\n",
    "    Ni = dict()\n",
    "    \n",
    "    # Compute frequencies of items in the training set\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "    del trainset\n",
    "\n",
    "    # Count #users with non-zero item frequencies\n",
    "    nonzero_user_count = 0\n",
    "    for theuser in P[\"users\"]:\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for pos_item in pos_items:\n",
    "            if pos_item in Ni:\n",
    "                nonzero_user_count += 1\n",
    "                break\n",
    "    \n",
    "    # Compute recommendations for each user\n",
    "    for theuser in P[\"users\"]:\n",
    "        all_scores = np.array(P[\"results\"][theuser])\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for i, pos_item in enumerate(pos_items):\n",
    "            pos_score = pos_scores[i]\n",
    "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
    "\n",
    "    \n",
    "    # Calculate per-user scores\n",
    "    sum_user_auc = 0.0\n",
    "    sum_user_recall = 0.0\n",
    "    for theuser in P[\"users\"]:\n",
    "        numerator_auc = 0.0\n",
    "        numerator_recall = 0.0\n",
    "        denominator = 0.0\n",
    "\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            if theitem not in Ni:\n",
    "                continue\n",
    "            pui = propensities[theuser][theitem]\n",
    "            if pui == 0:\n",
    "                continue\n",
    "\n",
    "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser])) / pui\n",
    "            \n",
    "            if Zui[(theuser, theitem)] < K:\n",
    "                numerator_recall += 1.0 / pui\n",
    "            denominator += 1 / pui\n",
    "                \n",
    "        if denominator > 0:\n",
    "            sum_user_auc += numerator_auc / denominator\n",
    "            sum_user_recall += numerator_recall / denominator \n",
    "\n",
    "    # Return\n",
    "    return {\n",
    "        \"auc\"       : sum_user_auc / nonzero_user_count,\n",
    "        \"recall\"    : sum_user_recall / nonzero_user_count,\n",
    "        \"bias\"      : -1,\n",
    "        \"concentration\" : -1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aoa(infilename, infilename_neg, trainfilename, K=4):\n",
    "\n",
    "    # Read pickles\n",
    "    infile = open(infilename, 'rb')\n",
    "    infile_neg = open(infilename_neg, 'rb')\n",
    "    P = pickle.load(infile)\n",
    "    infile.close()\n",
    "    P_neg = pickle.load(infile_neg)\n",
    "    infile_neg.close()\n",
    "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
    "    \n",
    "    # Merge P and P_neg\n",
    "    for theuser in P[\"users\"]:\n",
    "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "    \n",
    "    Zui = dict()\n",
    "    Ni = dict()\n",
    "\n",
    "    # Compute frequencies of items in the training set\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "    del trainset\n",
    "    \n",
    "    # Count #users with non-zero item frequencies\n",
    "    nonzero_user_count = 0\n",
    "    for theuser in P[\"users\"]:\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for pos_item in pos_items:\n",
    "            if pos_item in Ni:\n",
    "                nonzero_user_count += 1\n",
    "                break\n",
    "    \n",
    "    # Compute recommendations for each user\n",
    "    for theuser in P[\"users\"]:\n",
    "        all_scores = np.array(P[\"results\"][theuser])\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for i, pos_item in enumerate(pos_items):\n",
    "            pos_score = pos_scores[i]\n",
    "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
    "\n",
    "    # Calculate per-user scores\n",
    "    sum_user_auc = 0.0\n",
    "    sum_user_recall = 0.0\n",
    "    for theuser in P[\"users\"]:\n",
    "        numerator_auc = 0.0\n",
    "        numerator_recall = 0.0\n",
    "        denominator = 0.0\n",
    "\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            if theitem not in Ni:\n",
    "                continue\n",
    "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser]))\n",
    "            # Calcolo il Recall a 30, vedi nota 6 paper\n",
    "            if Zui[(theuser, theitem)] < K:\n",
    "                numerator_recall += 1.0\n",
    "            denominator += 1 \n",
    "\n",
    "        if denominator > 0:\n",
    "            sum_user_auc += numerator_auc / denominator\n",
    "            sum_user_recall += numerator_recall / denominator\n",
    "\n",
    "    # Return\n",
    "    return {\n",
    "        \"auc\"       : sum_user_auc / nonzero_user_count,\n",
    "        \"recall\"    : sum_user_recall / nonzero_user_count,\n",
    "        \"bias\"      : -1,\n",
    "        \"concentration\" : -1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified(infilename, infilename_neg, trainfilename, propensities, K=20, partition=10, delta=0.1):\n",
    "\n",
    "    # Read pickles\n",
    "    infile = open(infilename, 'rb')\n",
    "    infile_neg = open(infilename_neg, 'rb')\n",
    "    P = pickle.load(infile)\n",
    "    infile.close()\n",
    "    P_neg = pickle.load(infile_neg)\n",
    "    infile_neg.close()\n",
    "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
    "\n",
    "    # Merge P and P_neg\n",
    "    for theuser in P[\"users\"]:\n",
    "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "\n",
    "    Zui = dict()\n",
    "    Ni = dict()\n",
    "\n",
    "    # Compute frequencies of items in the training set\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "\n",
    "    # Compute recommendations for each user\n",
    "    for theuser in P[\"users\"]:\n",
    "        all_scores = np.array(P[\"results\"][theuser])\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for i, pos_item in enumerate(pos_items):\n",
    "            pos_score = pos_scores[i]\n",
    "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
    "\n",
    "    w = dict()\n",
    "\n",
    "    # Using as pui a single row of propensities, as we assumed propensities to be user independent\n",
    "    pui = propensities[0,:]\n",
    "\n",
    "    # Take the list of items (not tuples) in pui sorted by value\n",
    "    items_sorted_by_value = np.argsort(pui)[::-1]\n",
    "\n",
    "    # Filter out indices where the value in pui is 0\n",
    "    items_sorted_by_value = items_sorted_by_value[pui[items_sorted_by_value] > 0]\n",
    "\n",
    "    #items_sorted_by_value = sorted(pui, key=np.arange(0,pui.shape[0]), reverse=True)\n",
    "\n",
    "    # Compute linspace between the pui[0] and pui[-1] \n",
    "    linspace = np.linspace(pui[items_sorted_by_value[0]], pui[items_sorted_by_value[-1]], partition+1)\n",
    "   \n",
    "    # Compute dictionary w, that is, for each item, assigns the average of the puis in the partition it belongs to\n",
    "    i=0\n",
    "    j = 0\n",
    "    while i < len(items_sorted_by_value):\n",
    "                            \n",
    "        avg = 0\n",
    "        start = i\n",
    "        end = i\n",
    "    \n",
    "        while i < len(items_sorted_by_value) and pui[items_sorted_by_value[i]] >= linspace[j+1]:\n",
    "            avg += 1.0 / pui[items_sorted_by_value[i]]\n",
    "            end = i\n",
    "            i += 1\n",
    "        avg = avg / (end - start + 1)\n",
    "\n",
    "        for k in range(start, end+1):\n",
    "            w[items_sorted_by_value[k]] = avg\n",
    "\n",
    "        j += 1\n",
    "\n",
    "    # Compute bias' numerator\n",
    "    bias = 0.0\n",
    "    for k in items_sorted_by_value:\n",
    "        # add |pui*w - 1!|\n",
    "        bias += abs(pui[k] * w[k] - 1)\n",
    "    # Multiply by number of users\n",
    "    bias *= len(P[\"users\"])\n",
    "\n",
    "    # Compute concentrations numerator (for each user)\n",
    "    concentrations = {}\n",
    "    max_w = max(w.values())\n",
    "    # ... by computing the sum of squares of w for each user\n",
    "    for user, item in zip(trainset['user_id'], trainset['item_id']):\n",
    "        # Iterate over the trainset to compute the sum of squares for each user\n",
    "        if item in w:\n",
    "            if user not in concentrations:\n",
    "                concentrations[user] = 0\n",
    "            concentrations[user] += w[item] ** 2\n",
    "    # ... and then applying the formula\n",
    "    for user in concentrations:\n",
    "        concentrations[user] = math.sqrt(concentrations[user] * 2 * math.log(2/delta)) + max_w * 7 * math.log(2/delta)\n",
    "    # Now sum all the concentrations\n",
    "    concentration = sum(concentrations.values())\n",
    "\n",
    "    # Calculate per-user scores\n",
    "    nonzero_user_count = 0\n",
    "    sum_user_auc = 0.0\n",
    "    sum_user_recall = 0.0\n",
    "    for theuser in P[\"users\"]:\n",
    "        numerator_auc = 0.0\n",
    "        numerator_recall = 0.0\n",
    "        denominator = 0.0\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            # Skip items with null frequency\n",
    "            if  theitem not in Ni:\n",
    "                continue\n",
    "            if pui[theitem] == 0:\n",
    "                continue\n",
    "\n",
    "            # Add things to be summed for each item\n",
    "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser])) * w[theitem]\n",
    "            # Add things for recall\n",
    "            if Zui[(theuser, theitem)] < K:\n",
    "                numerator_recall += 1.0 * w[theitem] #Â spetta\n",
    "            # Increment denominator that the sum must be divided by \n",
    "            denominator += 1 / pui[theitem]\n",
    "\n",
    "\n",
    "        # If there was at least one item for the user, count the user and sum the results\n",
    "        if denominator > 0:\n",
    "            nonzero_user_count += 1\n",
    "            sum_user_auc += numerator_auc / denominator\n",
    "            sum_user_recall += numerator_recall / denominator \n",
    "\n",
    "    # Return\n",
    "    return {\n",
    "        \"auc\"       : sum_user_auc / nonzero_user_count, \n",
    "        \"recall\"    : sum_user_recall / nonzero_user_count,\n",
    "        \"bias\"      : bias,\n",
    "        \"concentration\" : concentration\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_logspace(infilename, infilename_neg, trainfilename, propensities, K=20, partition=10, delta=0.1):\n",
    "\n",
    "    # Read pickles\n",
    "    infile = open(infilename, 'rb')\n",
    "    infile_neg = open(infilename_neg, 'rb')\n",
    "    P = pickle.load(infile)\n",
    "    infile.close()\n",
    "    P_neg = pickle.load(infile_neg)\n",
    "    infile_neg.close()\n",
    "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
    "\n",
    "    # Merge P and P_neg\n",
    "    for theuser in P[\"users\"]:\n",
    "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "\n",
    "    Zui = dict()\n",
    "    Ni = dict()\n",
    "\n",
    "    # Compute frequencies of items in the training set\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "\n",
    "    # Compute recommendations for each user\n",
    "    for theuser in P[\"users\"]:\n",
    "        all_scores = np.array(P[\"results\"][theuser])\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for i, pos_item in enumerate(pos_items):\n",
    "            pos_score = pos_scores[i]\n",
    "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
    "\n",
    "    w = dict()\n",
    "\n",
    "    pui = propensities[0,:]\n",
    "\n",
    "    # Take the list of items (not tuples) in pui sorted by value\n",
    "    items_sorted_by_value = np.argsort(pui)[::-1]\n",
    "\n",
    "    # Filter out indices where the value in pui is 0\n",
    "    items_sorted_by_value = items_sorted_by_value[pui[items_sorted_by_value] > 0]\n",
    "\n",
    "    #items_sorted_by_value = sorted(pui, key=pui.get, reverse=True)\n",
    "\n",
    "    # Compute linspace between the pui[0] and pui[-1] \n",
    "\n",
    "    # Maybe try to split the logspace instead of the linspace?\n",
    "    logspace = np.logspace(pui[items_sorted_by_value[0]], pui[items_sorted_by_value[-1]], partition+1)\n",
    "   \n",
    "    # Compute dictionary w, that is, for each item, assigns the average of the puis in the partition it belongs to\n",
    "    i=0\n",
    "    j = 0\n",
    "    while i < len(items_sorted_by_value):\n",
    "                            \n",
    "        avg = 0\n",
    "        start = i\n",
    "        end = i\n",
    "    \n",
    "        while i < len(items_sorted_by_value) and pui[items_sorted_by_value[i]] >= logspace[j+1]:\n",
    "            avg += 1.0 / pui[items_sorted_by_value[i]]\n",
    "            end = i\n",
    "            i += 1\n",
    "        \n",
    "        # Is the average the only good choice? even with the log space split?\n",
    "        avg = avg / (end - start + 1)\n",
    "\n",
    "        for k in range(start, end+1):\n",
    "            w[items_sorted_by_value[k]] = avg\n",
    "\n",
    "        j += 1\n",
    "\n",
    "        # Compute bias' numerator\n",
    "        bias = 0.0\n",
    "        for k in items_sorted_by_value:\n",
    "            # add |pui*w - 1!|\n",
    "            bias += abs(pui[k] * w[k] - 1)\n",
    "        # Multiply by number of users\n",
    "        bias *= len(P[\"users\"])\n",
    "\n",
    "        # Compute concentrations numerator (for each user)\n",
    "        concentrations = {}\n",
    "        max_w = max(w.values())\n",
    "        # ... by computing the sum of squares of w for each user\n",
    "        for user, item in zip(trainset['user_id'], trainset['item_id']):\n",
    "            # Iterate over the trainset to compute the sum of squares for each user\n",
    "            if item in w:\n",
    "                if user not in concentrations:\n",
    "                    concentrations[user] = 0\n",
    "                concentrations[user] += w[item] ** 2\n",
    "        # ... and then applying the formula\n",
    "        for user in concentrations:\n",
    "            concentrations[user] = math.sqrt(concentrations[user] * 2 * math.log(2/delta)) + max_w * 7 * math.log(2/delta)\n",
    "        # Now sum all the concentrations\n",
    "        concentration = sum(concentrations.values())\n",
    "\n",
    "    # Compute score with AUC and compute Recall\n",
    "    nonzero_user_count = 0\n",
    "    sum_user_auc = 0.0\n",
    "    sum_user_recall = 0.0\n",
    "    for theuser in P[\"users\"]:\n",
    "        numerator_auc = 0.0\n",
    "        numerator_recall = 0.0\n",
    "        denominator = 0.0\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            # Skip items with null frequency\n",
    "            if  theitem not in Ni:\n",
    "                continue\n",
    "            # Add things to be summed for each item\n",
    "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser])) * w[theitem]\n",
    "            # Add things for recall\n",
    "            if Zui[(theuser, theitem)] < K:\n",
    "                numerator_recall += 1.0 * w[theitem] #Â spetta\n",
    "            # Increment denominator that the sum must be divided by \n",
    "            denominator += 1 / pui[theitem]\n",
    "\n",
    "\n",
    "        # If there was at least one item for the user, count the user and sum the results\n",
    "        if denominator > 0:\n",
    "            nonzero_user_count += 1\n",
    "            sum_user_auc += numerator_auc / denominator\n",
    "            sum_user_recall += numerator_recall / denominator \n",
    "\n",
    "    # Return\n",
    "    return {\n",
    "        \"auc\"       : sum_user_auc / nonzero_user_count, \n",
    "        \"recall\"    : sum_user_recall / nonzero_user_count,\n",
    "        \"bias\"      : bias,\n",
    "        \"concentration\" : concentration\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This version uses the linspace of the number of number of items used for evaluation, not of the propensities\n",
    "def stratified_2(infilename, infilename_neg, trainfilename, propensities, K=20, partition=10, delta=0.1):\n",
    "\n",
    "    # Read pickles\n",
    "    infile = open(infilename, 'rb')\n",
    "    infile_neg = open(infilename_neg, 'rb')\n",
    "    P = pickle.load(infile)\n",
    "    infile.close()\n",
    "    P_neg = pickle.load(infile_neg)\n",
    "    infile_neg.close()\n",
    "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
    "\n",
    "    # Merge P and P_neg\n",
    "    for theuser in P[\"users\"]:\n",
    "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "\n",
    "    Zui = dict()\n",
    "    Ni = dict()\n",
    "\n",
    "    # Compute frequencies of items in the training set\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "\n",
    "    # Compute recommendations for each user\n",
    "    for theuser in P[\"users\"]:\n",
    "        all_scores = np.array(P[\"results\"][theuser])\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for i, pos_item in enumerate(pos_items):\n",
    "            pos_score = pos_scores[i]\n",
    "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
    "\n",
    "    pui = propensities[0,:]\n",
    "    w = dict()\n",
    "\n",
    "    # Take the list of items (not tuples) in pui sorted by value\n",
    "    items_sorted_by_value = np.argsort(pui)[::-1]\n",
    "\n",
    "    # Filter out indices where the value in pui is 0\n",
    "    items_sorted_by_value = items_sorted_by_value[pui[items_sorted_by_value] > 0]\n",
    "\n",
    "    #items_sorted_by_value = sorted(pui, key=pui.get, reverse=True)\n",
    "\n",
    "    # Compute linspace between the 0 to len(item_sorted...)\n",
    "    linspace = np.linspace(0, len(items_sorted_by_value), partition+1)\n",
    "   \n",
    "    # Compute dictionary w, that is, for each item, assigns the average of the puis in the partition it belongs to\n",
    "    i=0\n",
    "    j = 0\n",
    "    while i < len(items_sorted_by_value):\n",
    "                            \n",
    "        avg = 0\n",
    "        start = i\n",
    "        end = i\n",
    "    \n",
    "        while i < len(items_sorted_by_value) and i < linspace[j+1]:\n",
    "            avg += 1.0 / pui[items_sorted_by_value[i]]\n",
    "            end = i\n",
    "            i += 1\n",
    "        \n",
    "        avg = avg / (end - start + 1)\n",
    "\n",
    "        for k in range(start, end+1):\n",
    "            w[items_sorted_by_value[k]] = avg\n",
    "\n",
    "        j += 1\n",
    "\n",
    "    # Compute bias' numerator\n",
    "    bias = 0.0\n",
    "    for k in items_sorted_by_value:\n",
    "        # add |pui*w - 1!|\n",
    "        bias += abs(pui[k] * w[k] - 1)\n",
    "    # Multiply by number of users\n",
    "    bias *= len(P[\"users\"])\n",
    "\n",
    "    # Compute concentrations numerator (for each user)\n",
    "    concentrations = {}\n",
    "    max_w = max(w.values())\n",
    "    # ... by computing the sum of squares of w for each user\n",
    "    for user, item in zip(trainset['user_id'], trainset['item_id']):\n",
    "        # Iterate over the trainset to compute the sum of squares for each user\n",
    "        if item in w:\n",
    "            if user not in concentrations:\n",
    "                concentrations[user] = 0\n",
    "            concentrations[user] += w[item] ** 2\n",
    "    # ... and then applying the formula\n",
    "    for user in concentrations:\n",
    "        concentrations[user] = math.sqrt(concentrations[user] * 2 * math.log(2/delta)) + max_w * 7 * math.log(2/delta)\n",
    "    # Now sum all the concentrations\n",
    "    concentration = sum(concentrations.values())\n",
    "\n",
    "    # Compute score with AUC and compute Recall\n",
    "    nonzero_user_count = 0\n",
    "    sum_user_auc = 0.0\n",
    "    sum_user_recall = 0.0\n",
    "    for theuser in P[\"users\"]:\n",
    "        numerator_auc = 0.0\n",
    "        numerator_recall = 0.0\n",
    "        denominator = 0.0\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            # Skip items with null frequency\n",
    "            if  theitem not in Ni:\n",
    "                continue\n",
    "            # Add things to be summed for each item\n",
    "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser])) * w[theitem]\n",
    "            # Add things for recall\n",
    "            if Zui[(theuser, theitem)] < K:\n",
    "                numerator_recall += 1.0 * w[theitem] #Â spetta\n",
    "            # Increment denominator that the sum must be divided by \n",
    "            denominator += 1 / pui[theitem]\n",
    "\n",
    "\n",
    "        # If there was at least one item for the user, count the user and sum the results\n",
    "        if denominator > 0:\n",
    "            nonzero_user_count += 1\n",
    "            sum_user_auc += numerator_auc / denominator\n",
    "            sum_user_recall += numerator_recall / denominator \n",
    "\n",
    "    return {\n",
    "        \"auc\"       : sum_user_auc / nonzero_user_count, \n",
    "        \"recall\"    : sum_user_recall / nonzero_user_count,\n",
    "        \"bias\"      : bias,\n",
    "        \"concentration\" : concentration\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **EVALUATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "raw_data = dict()\n",
    "raw_data['train_data'] = np.load(output_name + \"training_arr.npy\")\n",
    "raw_data['test_data_pos_biased'] = np.load(output_name + \"biased-test_arr_pos.npy\")\n",
    "raw_data['test_data_neg_biased'] = np.load(output_name + \"biased-test_arr_neg.npy\")\n",
    "raw_data['test_data_pos_unbiased'] = np.load(output_name + \"unbiased-test_arr_pos.npy\")\n",
    "raw_data['test_data_neg_unbiased'] = np.load(output_name + \"unbiased-test_arr_neg.npy\")\n",
    "raw_data['max_user'] = 290\n",
    "raw_data['max_item'] = 300\n",
    "batch_size = 8000\n",
    "test_batch_size = 1000\n",
    "display_itr = 1000\n",
    "\n",
    "# Load data\n",
    "train_dataset = ImplicitDataset(raw_data['train_data'], raw_data['max_user'], raw_data['max_item'], name='Train')\n",
    "test_dataset_pos_biased = ImplicitDataset(raw_data['test_data_pos_biased'], raw_data['max_user'], raw_data['max_item'])\n",
    "test_dataset_neg_biased = ImplicitDataset(raw_data['test_data_neg_biased'], raw_data['max_user'], raw_data['max_item'])\n",
    "test_dataset_pos_unbiased = ImplicitDataset(raw_data['test_data_pos_unbiased'], raw_data['max_user'], raw_data['max_item'])\n",
    "test_dataset_neg_unbiased = ImplicitDataset(raw_data['test_data_neg_unbiased'], raw_data['max_user'], raw_data['max_item'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./generated_data/cml-coat/\n",
      "[Subsampling negative items]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    }
   ],
   "source": [
    "# Prevent tensorflow from using cached embeddings\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "# Define the model\n",
    "model = MODEL_CLASS(batch_size=batch_size, max_user=train_dataset.max_user(), max_item=train_dataset.max_item(), dim_embed=50, l2_reg=0.001, opt='Adam', sess_config=None)\n",
    "sampler = PairwiseSampler(batch_size=batch_size, dataset=train_dataset, num_process=4)\n",
    "model_trainer = ImplicitModelTrainer(batch_size=batch_size, test_batch_size=test_batch_size, train_dataset=train_dataset, model=model, sampler=sampler, eval_save_prefix=OUTPUT_PATH + DATASET_NAME, item_serving_size=500)\n",
    "auc_evaluator = AUC()\n",
    "\n",
    "# Load model\n",
    "model.load(OUTPUT_PATH)\n",
    "\n",
    "# Set parameters\n",
    "model_trainer._eval_manager = ImplicitEvalManager(evaluators=[auc_evaluator])\n",
    "model_trainer._num_negatives = 60 # in yahoo they were 200 on 1000 items, so let's keep a 1/5 ratio on 300 items\n",
    "model_trainer._exclude_positives([train_dataset, test_dataset_pos_biased, test_dataset_neg_biased])\n",
    "model_trainer._sample_negatives(seed=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biased Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 231/231 [00:00<00:00, 1725.73it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 290/290 [00:01<00:00, 270.18it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AUC': [0.5265503875968993,\n",
       "  0.5900749063670412,\n",
       "  0.44356060606060604,\n",
       "  0.45378787878787885,\n",
       "  0.47659176029962536,\n",
       "  0.5618773946360153,\n",
       "  0.4011363636363636,\n",
       "  0.5239700374531835,\n",
       "  0.500766283524904,\n",
       "  0.5718390804597702,\n",
       "  0.5188888888888888,\n",
       "  0.4957364341085272,\n",
       "  0.5272222222222223,\n",
       "  0.5503745318352059,\n",
       "  0.5527777777777778,\n",
       "  0.5543071161048688,\n",
       "  0.507962962962963,\n",
       "  0.4275280898876404,\n",
       "  0.4767045454545455,\n",
       "  0.48183520599250934,\n",
       "  0.5218992248062017,\n",
       "  0.5155038759689923,\n",
       "  0.5100000000000001,\n",
       "  0.48033707865168546,\n",
       "  0.46329365079365076,\n",
       "  0.42827715355805246,\n",
       "  0.45191570881226056,\n",
       "  0.48689138576779023,\n",
       "  0.548689138576779,\n",
       "  0.5335205992509363,\n",
       "  0.5372549019607844,\n",
       "  0.5235955056179775,\n",
       "  0.5003787878787879,\n",
       "  0.5196296296296297,\n",
       "  0.5271535580524345,\n",
       "  0.5096590909090909,\n",
       "  0.5965909090909092,\n",
       "  0.416851851851852,\n",
       "  0.581992337164751,\n",
       "  0.4886274509803921,\n",
       "  0.4691011235955056,\n",
       "  0.5051136363636364,\n",
       "  0.5034883720930233,\n",
       "  0.3691011235955056,\n",
       "  0.5220973782771535,\n",
       "  0.4925093632958801,\n",
       "  0.5285185185185185,\n",
       "  0.54515503875969,\n",
       "  0.548876404494382,\n",
       "  0.5201550387596899,\n",
       "  0.5022222222222221,\n",
       "  0.467816091954023,\n",
       "  0.4386973180076628,\n",
       "  0.44213483146067417,\n",
       "  0.4496031746031746,\n",
       "  0.5577651515151515,\n",
       "  0.4433712121212121,\n",
       "  0.4790740740740741,\n",
       "  0.47253787878787884,\n",
       "  0.4434456928838951,\n",
       "  0.53125,\n",
       "  0.4712962962962963,\n",
       "  0.45449438202247194,\n",
       "  0.48215686274509806,\n",
       "  0.6183333333333334,\n",
       "  0.4389513108614232,\n",
       "  0.506201550387597,\n",
       "  0.5325925925925926,\n",
       "  0.5333333333333333,\n",
       "  0.449250936329588,\n",
       "  0.4840996168582375,\n",
       "  0.538627450980392,\n",
       "  0.4248148148148148,\n",
       "  0.49753787878787875,\n",
       "  0.3972222222222222,\n",
       "  0.45243445692883894,\n",
       "  0.551123595505618,\n",
       "  0.46893939393939393,\n",
       "  0.4816287878787879,\n",
       "  0.479563492063492,\n",
       "  0.4485185185185186,\n",
       "  0.5467871485943777,\n",
       "  0.5151851851851853,\n",
       "  0.5635185185185184,\n",
       "  0.5460227272727273,\n",
       "  0.4136704119850187,\n",
       "  0.6172619047619048,\n",
       "  0.4943820224719101,\n",
       "  0.547093023255814,\n",
       "  0.5368518518518519,\n",
       "  0.4044943820224719,\n",
       "  0.5408239700374532,\n",
       "  0.5706439393939394,\n",
       "  0.5568181818181818,\n",
       "  0.44419475655430707,\n",
       "  0.5327715355805244,\n",
       "  0.4277153558052435,\n",
       "  0.49337121212121215,\n",
       "  0.46985018726591765,\n",
       "  0.4250936329588014,\n",
       "  0.5155555555555557,\n",
       "  0.41969696969696973,\n",
       "  0.5138888888888888,\n",
       "  0.4780392156862745,\n",
       "  0.46628787878787875,\n",
       "  0.6521072796934866,\n",
       "  0.4377394636015326,\n",
       "  0.5729166666666667,\n",
       "  0.5249063670411985,\n",
       "  0.550392156862745,\n",
       "  0.4720930232558138,\n",
       "  0.4798148148148148,\n",
       "  0.41966292134831457,\n",
       "  0.5019157088122604,\n",
       "  0.5267790262172285,\n",
       "  0.5024621212121212,\n",
       "  0.5982558139534884,\n",
       "  0.4450191570881226,\n",
       "  0.5166666666666667,\n",
       "  0.5181818181818183,\n",
       "  0.5919475655430712,\n",
       "  0.5274074074074074,\n",
       "  0.5038314176245209,\n",
       "  0.5581481481481483,\n",
       "  0.4747126436781608,\n",
       "  0.497191011235955,\n",
       "  0.48914728682170544,\n",
       "  0.5856321839080461,\n",
       "  0.4802681992337163,\n",
       "  0.4913793103448275,\n",
       "  0.5242424242424243,\n",
       "  0.5101123595505619,\n",
       "  0.5729411764705884,\n",
       "  0.4864341085271318,\n",
       "  0.4849206349206349,\n",
       "  0.5130268199233716,\n",
       "  0.4971264367816091,\n",
       "  0.448501872659176,\n",
       "  0.48183520599250934,\n",
       "  0.5625468164794007,\n",
       "  0.3931726907630522,\n",
       "  0.47196969696969704,\n",
       "  0.5560606060606061,\n",
       "  0.5597378277153559,\n",
       "  0.47267441860465115,\n",
       "  0.55,\n",
       "  0.5155555555555555,\n",
       "  0.5525925925925926,\n",
       "  0.48295454545454547,\n",
       "  0.48238636363636367,\n",
       "  0.4897003745318351,\n",
       "  0.5388257575757577,\n",
       "  0.5201851851851852,\n",
       "  0.552621722846442,\n",
       "  0.5159003831417626,\n",
       "  0.49962962962962965,\n",
       "  0.5080524344569288,\n",
       "  0.5140562248995985,\n",
       "  0.5431818181818182,\n",
       "  0.45037037037037037,\n",
       "  0.5211111111111111,\n",
       "  0.5664814814814816,\n",
       "  0.6570370370370371,\n",
       "  0.48662790697674413,\n",
       "  0.452996254681648,\n",
       "  0.44812734082397004,\n",
       "  0.4259469696969697,\n",
       "  0.40574712643678157,\n",
       "  0.4548689138576779,\n",
       "  0.49166666666666664,\n",
       "  0.6100378787878789,\n",
       "  0.6124521072796936,\n",
       "  0.4533333333333332,\n",
       "  0.48823529411764716,\n",
       "  0.4882575757575758,\n",
       "  0.4588014981273408,\n",
       "  0.5259259259259259,\n",
       "  0.4990636704119849,\n",
       "  0.4639846743295019,\n",
       "  0.44846743295019154,\n",
       "  0.45156862745098036,\n",
       "  0.512310606060606,\n",
       "  0.4666666666666667,\n",
       "  0.47126436781609204,\n",
       "  0.4660984848484848,\n",
       "  0.48055555555555557,\n",
       "  0.4544444444444445,\n",
       "  0.4214814814814815,\n",
       "  0.5848148148148148,\n",
       "  0.6172348484848486,\n",
       "  0.5184108527131783,\n",
       "  0.5106060606060606,\n",
       "  0.5796296296296297,\n",
       "  0.5666666666666667,\n",
       "  0.4907407407407408,\n",
       "  0.598876404494382,\n",
       "  0.49850187265917606,\n",
       "  0.4768939393939393,\n",
       "  0.4268199233716476,\n",
       "  0.45484496124031,\n",
       "  0.49507575757575756,\n",
       "  0.48071161048689137,\n",
       "  0.43486590038314177,\n",
       "  0.43820224719101125,\n",
       "  0.5437500000000001,\n",
       "  0.41179775280898884,\n",
       "  0.5538888888888889,\n",
       "  0.4848314606741573,\n",
       "  0.47962962962962963,\n",
       "  0.4074509803921569,\n",
       "  0.47840909090909084,\n",
       "  0.41761363636363635,\n",
       "  0.4956439393939394,\n",
       "  0.5854651162790698,\n",
       "  0.4240310077519379,\n",
       "  0.4941947565543071,\n",
       "  0.46348314606741564,\n",
       "  0.4434456928838951,\n",
       "  0.55,\n",
       "  0.4723484848484848,\n",
       "  0.45203703703703707,\n",
       "  0.4625468164794007,\n",
       "  0.5815261044176707,\n",
       "  0.6047348484848485,\n",
       "  0.534280303030303,\n",
       "  0.5310861423220974,\n",
       "  0.5070075757575758,\n",
       "  0.5125000000000002,\n",
       "  0.5414772727272728,\n",
       "  0.4986742424242424,\n",
       "  0.5369318181818182,\n",
       "  0.46629213483146065,\n",
       "  0.5646067415730337,\n",
       "  0.4846590909090909,\n",
       "  0.584469696969697,\n",
       "  0.5114341085271318,\n",
       "  0.5895833333333333,\n",
       "  0.48544061302681996,\n",
       "  0.5581439393939394,\n",
       "  0.4978927203065134,\n",
       "  0.4698412698412698,\n",
       "  0.5436329588014981,\n",
       "  0.5057471264367817,\n",
       "  0.47253787878787873,\n",
       "  0.5367041198501873,\n",
       "  0.5069767441860464,\n",
       "  0.4642045454545455,\n",
       "  0.4835205992509363,\n",
       "  0.49703703703703705,\n",
       "  0.4760299625468165,\n",
       "  0.5066666666666667,\n",
       "  0.4409961685823755,\n",
       "  0.47870370370370363,\n",
       "  0.46370370370370373,\n",
       "  0.43444444444444447,\n",
       "  0.5293103448275862,\n",
       "  0.5059925093632959,\n",
       "  0.4744318181818182,\n",
       "  0.4435185185185185,\n",
       "  0.5405555555555556,\n",
       "  0.4057407407407408,\n",
       "  0.41003787878787873,\n",
       "  0.4636363636363636,\n",
       "  0.5615530303030304,\n",
       "  0.4572796934865901,\n",
       "  0.5267045454545455,\n",
       "  0.5249063670411986,\n",
       "  0.4044943820224719,\n",
       "  0.44961240310077527,\n",
       "  0.6580524344569288,\n",
       "  0.5382575757575758,\n",
       "  0.5481060606060606,\n",
       "  0.512310606060606,\n",
       "  0.5085271317829457,\n",
       "  0.4505747126436781,\n",
       "  0.4763257575757576,\n",
       "  0.4384469696969697,\n",
       "  0.6170498084291189,\n",
       "  0.44240740740740747,\n",
       "  0.5133333333333334,\n",
       "  0.39981481481481485,\n",
       "  0.38707865168539324,\n",
       "  0.5054307116104869,\n",
       "  0.5053030303030304,\n",
       "  0.46382575757575756,\n",
       "  0.5124521072796935,\n",
       "  0.4724206349206349,\n",
       "  0.46800766283524914,\n",
       "  0.47528089887640457,\n",
       "  0.47407407407407404]}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_trainer._eval_save_prefix = OUTPUT_PREFIX + \"-test-pos-biased\"\n",
    "model_trainer._evaluate_partial(test_dataset_pos_biased)\n",
    "\n",
    "model_trainer._eval_save_prefix = OUTPUT_PREFIX +  \"-test-neg-biased\"\n",
    "model_trainer._evaluate_partial(test_dataset_neg_biased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unbiased Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 237/237 [00:00<00:00, 1222.06it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 290/290 [00:00<00:00, 936.06it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AUC': [0.5533333333333333,\n",
       "  0.5357142857142858,\n",
       "  0.6000000000000001,\n",
       "  0.49102564102564106,\n",
       "  0.4243589743589744,\n",
       "  0.5988095238095238,\n",
       "  0.4744444444444445,\n",
       "  0.49583333333333335,\n",
       "  0.42500000000000004,\n",
       "  0.4948717948717949,\n",
       "  0.4358974358974359,\n",
       "  0.42833333333333323,\n",
       "  0.5190476190476191,\n",
       "  0.5666666666666667,\n",
       "  0.4844444444444444,\n",
       "  0.6369047619047619,\n",
       "  0.5208333333333334,\n",
       "  0.3178571428571429,\n",
       "  0.5750000000000001,\n",
       "  0.6100000000000001,\n",
       "  0.5155555555555555,\n",
       "  0.5361111111111111,\n",
       "  0.32291666666666663,\n",
       "  0.6083333333333333,\n",
       "  0.49090909090909085,\n",
       "  0.40238095238095234,\n",
       "  0.5870370370370371,\n",
       "  0.5144444444444444,\n",
       "  0.5928571428571429,\n",
       "  0.4714285714285714,\n",
       "  0.4272727272727273,\n",
       "  0.5178571428571429,\n",
       "  0.4644444444444444,\n",
       "  0.5212121212121213,\n",
       "  0.5047619047619046,\n",
       "  0.46481481481481485,\n",
       "  0.5680555555555556,\n",
       "  0.4395833333333333,\n",
       "  0.44000000000000006,\n",
       "  0.39,\n",
       "  0.3208333333333333,\n",
       "  0.4782051282051282,\n",
       "  0.5911111111111113,\n",
       "  0.5255555555555556,\n",
       "  0.5266666666666667,\n",
       "  0.5322222222222222,\n",
       "  0.4845238095238096,\n",
       "  0.5773809523809524,\n",
       "  0.5287878787878788,\n",
       "  0.495,\n",
       "  0.41190476190476194,\n",
       "  0.4962962962962963,\n",
       "  0.5333333333333333,\n",
       "  0.6077777777777776,\n",
       "  0.38833333333333336,\n",
       "  0.5133333333333333,\n",
       "  0.36363636363636354,\n",
       "  0.5511111111111112,\n",
       "  0.4988888888888889,\n",
       "  0.3588888888888889,\n",
       "  0.5575757575757575,\n",
       "  0.43854166666666666,\n",
       "  0.4645833333333333,\n",
       "  0.41666666666666663,\n",
       "  0.6773809523809523,\n",
       "  0.3202380952380953,\n",
       "  0.5407407407407407,\n",
       "  0.66875,\n",
       "  0.4476190476190477,\n",
       "  0.4871794871794871,\n",
       "  0.43787878787878787,\n",
       "  0.5208333333333333,\n",
       "  0.558888888888889,\n",
       "  0.36388888888888893,\n",
       "  0.3044444444444444,\n",
       "  0.518888888888889,\n",
       "  0.5702380952380952,\n",
       "  0.4846153846153846,\n",
       "  0.5761904761904761,\n",
       "  0.3366666666666667,\n",
       "  0.5555555555555556,\n",
       "  0.5347222222222222,\n",
       "  0.6871794871794871,\n",
       "  0.2924242424242424,\n",
       "  0.5476190476190477,\n",
       "  0.35520833333333335,\n",
       "  0.4880952380952381,\n",
       "  0.5358974358974359,\n",
       "  0.6452380952380953,\n",
       "  0.49743589743589745,\n",
       "  0.5564102564102564,\n",
       "  0.5447916666666666,\n",
       "  0.45999999999999996,\n",
       "  0.558888888888889,\n",
       "  0.47604166666666664,\n",
       "  0.5311111111111112,\n",
       "  0.33958333333333335,\n",
       "  0.41944444444444445,\n",
       "  0.40208333333333335,\n",
       "  0.3233333333333333,\n",
       "  0.5208333333333334,\n",
       "  0.3979166666666667,\n",
       "  0.5388888888888889,\n",
       "  0.5910256410256409,\n",
       "  0.575,\n",
       "  0.5095238095238094,\n",
       "  0.35,\n",
       "  0.6424242424242425,\n",
       "  0.48020833333333335,\n",
       "  0.5,\n",
       "  0.6115384615384616,\n",
       "  0.5192307692307693,\n",
       "  0.36979166666666663,\n",
       "  0.4900000000000001,\n",
       "  0.6756410256410257,\n",
       "  0.45454545454545453,\n",
       "  0.5222222222222223,\n",
       "  0.4986111111111111,\n",
       "  0.4864583333333333,\n",
       "  0.501388888888889,\n",
       "  0.7263888888888889,\n",
       "  0.5885416666666666,\n",
       "  0.4444444444444445,\n",
       "  0.5452380952380952,\n",
       "  0.39888888888888896,\n",
       "  0.5243589743589744,\n",
       "  0.78,\n",
       "  0.4916666666666667,\n",
       "  0.6375,\n",
       "  0.5282051282051282,\n",
       "  0.4714285714285715,\n",
       "  0.3874999999999999,\n",
       "  0.5511111111111112,\n",
       "  0.5076923076923077,\n",
       "  0.5128205128205129,\n",
       "  0.44833333333333336,\n",
       "  0.49999999999999994,\n",
       "  0.4947916666666667,\n",
       "  0.4166666666666667,\n",
       "  0.4964285714285714,\n",
       "  0.4987179487179488,\n",
       "  0.5111111111111112,\n",
       "  0.5059523809523808,\n",
       "  0.494047619047619,\n",
       "  0.4916666666666667,\n",
       "  0.43717948717948707,\n",
       "  0.5895833333333332,\n",
       "  0.5010416666666666,\n",
       "  0.48333333333333334,\n",
       "  0.5291666666666667,\n",
       "  0.5583333333333332,\n",
       "  0.4375,\n",
       "  0.5466666666666667,\n",
       "  0.49523809523809526,\n",
       "  0.5129629629629631,\n",
       "  0.4466666666666666,\n",
       "  0.5571428571428572,\n",
       "  0.37833333333333335,\n",
       "  0.5,\n",
       "  0.369047619047619,\n",
       "  0.5145833333333333,\n",
       "  0.4035714285714285,\n",
       "  0.6375,\n",
       "  0.37121212121212116,\n",
       "  0.5606060606060606,\n",
       "  0.453125,\n",
       "  0.3345238095238095,\n",
       "  0.48020833333333335,\n",
       "  0.391111111111111,\n",
       "  0.5244444444444444,\n",
       "  0.6366666666666666,\n",
       "  0.5266666666666666,\n",
       "  0.3877777777777778,\n",
       "  0.558974358974359,\n",
       "  0.5145833333333334,\n",
       "  0.5947916666666666,\n",
       "  0.5,\n",
       "  0.33055555555555555,\n",
       "  0.39404761904761904,\n",
       "  0.3222222222222222,\n",
       "  0.4818181818181818,\n",
       "  0.42555555555555563,\n",
       "  0.5263888888888889,\n",
       "  0.4371794871794872,\n",
       "  0.4192307692307693,\n",
       "  0.3733333333333333,\n",
       "  0.4444444444444444,\n",
       "  0.6277777777777778,\n",
       "  0.45729166666666665,\n",
       "  0.5083333333333333,\n",
       "  0.6282051282051282,\n",
       "  0.6133333333333333,\n",
       "  0.59375,\n",
       "  0.3722222222222222,\n",
       "  0.5282051282051282,\n",
       "  0.48020833333333335,\n",
       "  0.4354166666666667,\n",
       "  0.5012820512820513,\n",
       "  0.4392857142857144,\n",
       "  0.3541666666666667,\n",
       "  0.41794871794871796,\n",
       "  0.4152777777777778,\n",
       "  0.6166666666666667,\n",
       "  0.4277777777777778,\n",
       "  0.4788888888888889,\n",
       "  0.37222222222222223,\n",
       "  0.56,\n",
       "  0.5895833333333333,\n",
       "  0.5630952380952381,\n",
       "  0.3976190476190476,\n",
       "  0.41785714285714287,\n",
       "  0.41923076923076924,\n",
       "  0.47575757575757577,\n",
       "  0.45641025641025645,\n",
       "  0.2574074074074074,\n",
       "  0.5958333333333333,\n",
       "  0.44270833333333337,\n",
       "  0.4273809523809523,\n",
       "  0.525,\n",
       "  0.43809523809523815,\n",
       "  0.3458333333333333,\n",
       "  0.6288888888888889,\n",
       "  0.175,\n",
       "  0.5523809523809524,\n",
       "  0.55,\n",
       "  0.5320512820512822,\n",
       "  0.6948717948717948,\n",
       "  0.6305555555555555,\n",
       "  0.6729166666666667,\n",
       "  0.6354166666666666,\n",
       "  0.590625,\n",
       "  0.3888888888888888,\n",
       "  0.5428571428571428,\n",
       "  0.5577777777777778,\n",
       "  0.5466666666666667,\n",
       "  0.5520833333333334,\n",
       "  0.5444444444444444,\n",
       "  0.4033333333333333,\n",
       "  0.5358974358974359,\n",
       "  0.5202380952380953,\n",
       "  0.48,\n",
       "  0.5433333333333333,\n",
       "  0.5,\n",
       "  0.43571428571428567,\n",
       "  0.453125,\n",
       "  0.6583333333333332,\n",
       "  0.38,\n",
       "  0.46481481481481485,\n",
       "  0.5533333333333333,\n",
       "  0.48333333333333334,\n",
       "  0.6155555555555556,\n",
       "  0.4461538461538461,\n",
       "  0.4145833333333333,\n",
       "  0.46904761904761905,\n",
       "  0.4041666666666667,\n",
       "  0.4458333333333333,\n",
       "  0.5651515151515152,\n",
       "  0.49861111111111117,\n",
       "  0.3947916666666667,\n",
       "  0.5479166666666666,\n",
       "  0.2604166666666667,\n",
       "  0.45128205128205134,\n",
       "  0.3880952380952381,\n",
       "  0.5803030303030302,\n",
       "  0.5074074074074074,\n",
       "  0.41923076923076924,\n",
       "  0.3772727272727273,\n",
       "  0.362962962962963,\n",
       "  0.38,\n",
       "  0.6052083333333333,\n",
       "  0.43333333333333335,\n",
       "  0.573611111111111,\n",
       "  0.4430555555555557,\n",
       "  0.4038461538461539,\n",
       "  0.37179487179487186,\n",
       "  0.5107142857142857,\n",
       "  0.45333333333333337,\n",
       "  0.5583333333333333,\n",
       "  0.4233333333333334,\n",
       "  0.5708333333333333,\n",
       "  0.39555555555555555,\n",
       "  0.4345238095238095,\n",
       "  0.6151515151515151,\n",
       "  0.5012820512820514,\n",
       "  0.4755555555555555,\n",
       "  0.325,\n",
       "  0.4222222222222222,\n",
       "  0.5033333333333333,\n",
       "  0.36458333333333337,\n",
       "  0.53125]}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_trainer._eval_save_prefix = OUTPUT_PREFIX + \"-test-pos-unbiased\"\n",
    "model_trainer._evaluate_partial(test_dataset_pos_unbiased)\n",
    "\n",
    "model_trainer._eval_save_prefix = OUTPUT_PREFIX +  \"-test-neg-unbiased\"\n",
    "model_trainer._evaluate_partial(test_dataset_neg_unbiased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Propensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "propensities = calculate_propensities(290,300, output_name+\"training_arr.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.03679579, ..., 0.10292598, 0.01505091,\n",
       "        0.01505091],\n",
       "       [1.        , 0.        , 0.03679579, ..., 0.10292598, 0.01505091,\n",
       "        0.01505091],\n",
       "       [1.        , 0.        , 0.03679579, ..., 0.10292598, 0.01505091,\n",
       "        0.01505091],\n",
       "       ...,\n",
       "       [1.        , 0.        , 0.03679579, ..., 0.10292598, 0.01505091,\n",
       "        0.01505091],\n",
       "       [1.        , 0.        , 0.03679579, ..., 0.10292598, 0.01505091,\n",
       "        0.01505091],\n",
       "       [1.        , 0.        , 0.03679579, ..., 0.10292598, 0.01505091,\n",
       "        0.01505091]])"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "propensities[2.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute AOA and unbiased evaluator metrics with biased testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "biased_results = dict()\n",
    "\n",
    "# biased_results[\"STRATIFIED_15\"] = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=1.5, K=20, partition=100)\n",
    "biased_results[\"AOA\"] = aoa(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", K=20)\n",
    "biased_results[\"UB_15\"] = eq(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", propensities[1.5], K=20)\n",
    "biased_results[\"UB_2\"] =  eq(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", propensities[2], K=20)\n",
    "biased_results[\"UB_25\"] =  eq(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", propensities[2.5], K=20)\n",
    "biased_results[\"UB_3\"] =  eq(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", propensities[3], K=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute AOA and unbiased evaluator metrics with unbiased testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbiased_results = dict()\n",
    "\n",
    "# unbiased_results[\"STRATIFIED_15\"] = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=1.5, K=4, partition=100)\n",
    "unbiased_results[\"AOA\"] = aoa(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", K=4)\n",
    "unbiased_results[\"UB_15\"] = eq(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", propensities[1.5], K=4)\n",
    "unbiased_results[\"UB_2\"] =  eq(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", propensities[2], K=4)\n",
    "unbiased_results[\"UB_25\"] =  eq(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", propensities[2.5], K=4)\n",
    "unbiased_results[\"UB_3\"] =  eq(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", propensities[3], K=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 45,   4, 218, 195, 161,  29, 192,  34,  70, 108, 202, 248,  66,\n",
       "       160,  53, 204, 284,   7,  59,  30, 112, 273, 256, 243, 245, 262,\n",
       "        91, 185, 240, 236, 242, 296,  94, 119, 260, 138, 139, 182,  24,\n",
       "        28,  33, 145, 259, 203,  92, 166,  96, 193, 181, 147,  68, 174,\n",
       "        36, 201, 276, 232, 130,  82,  73, 252,  67, 215, 184, 230,  55,\n",
       "       229, 261, 265, 105,  90,  56, 272,  78, 257, 142, 198,  12,  39,\n",
       "        75, 167,  22,  26,  51, 278, 155, 104,  98,  41,  77,  88,  42,\n",
       "       213, 189, 178, 297, 171, 127, 295, 234, 118, 250, 183, 298,  63,\n",
       "       164, 146, 115, 173, 177, 220, 287, 168,  79, 114, 212, 209, 176,\n",
       "       141, 293,  72, 211,  10, 285, 190,  11, 148,   8, 264, 291, 131,\n",
       "       191,  19, 280, 206, 251, 133,  31, 279,  71, 180, 246, 123,  61,\n",
       "       239, 290, 109, 196,  87, 137,  74, 216, 144, 149, 170, 281, 244,\n",
       "        58, 188, 158, 179,   1,  50,  85, 126,  21, 282,  95, 103,  46,\n",
       "       254, 267, 132,  89, 152,   3, 228, 223, 113,  40, 134, 156,  48,\n",
       "       135, 153,  25,   2, 143, 263, 237, 249,  32, 124, 286, 283,  86,\n",
       "       107, 270,  15,  80, 175, 172, 299, 210, 111, 159,  52, 219, 231,\n",
       "       157, 225, 253, 288,  27,   5, 120, 238, 128,  93, 235,  57,  23,\n",
       "        65, 292, 227,  44, 199, 226,  35, 106, 289,  47,  84, 207, 200,\n",
       "       247, 194, 205,   6, 224, 221, 217,  13, 110,  38, 197, 271, 165,\n",
       "       268,  64,  97, 186,  37,  16, 140,  14, 136, 294, 129, 241,  69,\n",
       "       300, 208, 121,  99, 214, 100,  62, 277, 163, 101, 275,  83, 169,\n",
       "        81, 151,  17, 125, 117, 116, 122, 266,  49, 258, 150,  54, 162,\n",
       "       274, 222, 269,  76,   9,  20,  18,  43, 187, 154, 255,  60, 233,\n",
       "       102])"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get number of items\n",
    "num_items = 300\n",
    "\n",
    "# Get the n_p partitions\n",
    "n_p = 300\n",
    "nums = np.arange(1, num_items+1)\n",
    "partitions = np.random.choice(nums, n_p, replace=False)\n",
    "\n",
    "# Visualize\n",
    "partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the partition which minimizes the sum of AUC and Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "707fc81a07684045822b163ae8fe5030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/ipykernel_launcher.py:61: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "/Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/ipykernel_launcher.py:75: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': inf, 'recall': inf, 'bias': nan, 'concentration': inf}\n",
      "{'auc': 0.6625307358484721, 'recall': 0.44778919568149717, 'bias': nan, 'concentration': inf}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kj/3wy9xr4j2vlcr59wg5_cqy200000gn/T/ipykernel_57007/2621334674.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Compute the results (AUC and Recall) for both biased and unbiased test sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mtemp_unbiased\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstratified\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUTPUT_PREFIX\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"-test-pos-unbiased_evaluate_partial.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUTPUT_PREFIX\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"-test-neg-unbiased_evaluate_partial.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"training_arr.npy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpropensities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mtemp_biased\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstratified\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUTPUT_PREFIX\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"-test-pos-biased_evaluate_partial.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUTPUT_PREFIX\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"-test-neg-biased_evaluate_partial.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"training_arr.npy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpropensities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_biased\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/kj/3wy9xr4j2vlcr59wg5_cqy200000gn/T/ipykernel_57007/2444578338.py\u001b[0m in \u001b[0;36mstratified\u001b[0;34m(infilename, infilename_neg, trainfilename, propensities, K, partition, delta)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0minfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0minfile_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfilename_neg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0minfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mP_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfile_neg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Compute biased and unbiased results with stratified for each partition\n",
    "# and store biased and unbiased results such that the sum of AUC and Recall is minimized\n",
    "\n",
    "# Value of gamma to use for minimization\n",
    "gamma = 1.5\n",
    "\n",
    "# To print :)\n",
    "key = \"STRATIFIED_\" + str(gamma).replace(\".\",\"\")\n",
    "\n",
    "# Initialize results\n",
    "unbiased_results[key] = dict()\n",
    "biased_results[key] = dict()\n",
    "best_partition = np.random.choice(nums, 1)[0]\n",
    "\n",
    "history = np.empty(300)\n",
    "\n",
    "\n",
    "# For each partition\n",
    "for p in tqdm(partitions):\n",
    "    # Compute the results (AUC and Recall) for both biased and unbiased test sets\n",
    "    temp_unbiased = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", propensities[gamma], K=4, partition=p)\n",
    "    temp_biased = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", propensities[gamma], K=20, partition=p)\n",
    "    \n",
    "    # If first iteration...\n",
    "    if not unbiased_results[key]:\n",
    "        unbiased_results[key] = temp_unbiased\n",
    "    if not biased_results[key]:\n",
    "        biased_results[key] = temp_biased\n",
    "    # Else if a better partition was found, update the results\n",
    "    elif temp_unbiased['bias'] + temp_unbiased['concentration'] + temp_biased['bias'] + temp_biased['concentration'] < biased_results[key]['bias'] + biased_results[key]['concentration'] + unbiased_results[key]['bias'] + unbiased_results[key]['concentration']:\n",
    "        biased_results[key]['auc'] = temp_biased['auc']\n",
    "        biased_results[key]['recall'] = temp_biased['recall']\n",
    "        biased_results[key]['bias'] = temp_biased['bias']\n",
    "        biased_results[key]['concentration'] = temp_biased['concentration']\n",
    "        unbiased_results[key]['auc'] = temp_unbiased['auc']\n",
    "        unbiased_results[key]['recall'] = temp_unbiased['recall']\n",
    "        biased_results[key]['bias'] = temp_biased['bias']\n",
    "        biased_results[key]['concentration'] = temp_biased['concentration']\n",
    "        best_partition = p\n",
    "    #print(temp_unbiased['bias'], temp_biased['bias'], temp_unbiased['concentration'], temp_biased['concentration'])\n",
    "    history[p-1] = temp_unbiased['bias'] + temp_unbiased['concentration'] + temp_biased['bias'] + temp_biased['concentration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/matplotlib/axes/_axes.py:6607: RuntimeWarning: All-NaN slice encountered\n",
      "  xmin = min(xmin, np.nanmin(xi))\n",
      "/Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/matplotlib/axes/_axes.py:6608: RuntimeWarning: All-NaN slice encountered\n",
      "  xmax = max(xmax, np.nanmax(xi))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "autodetected range of [nan, nan] is not finite",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kj/3wy9xr4j2vlcr59wg5_cqy200000gn/T/ipykernel_57007/231632241.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Plotting the histogram\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medgecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'black'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Adding labels and title\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mhist\u001b[0;34m(x, bins, range, density, weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label, stacked, data, **kwargs)\u001b[0m\n\u001b[1;32m   2605\u001b[0m         \u001b[0malign\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2606\u001b[0m         \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacked\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2607\u001b[0;31m         **({\"data\": data} if data is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1410\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1412\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mhist\u001b[0;34m(self, x, bins, range, density, weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label, stacked, **kwargs)\u001b[0m\n\u001b[1;32m   6633\u001b[0m             \u001b[0;31m# this will automatically overwrite bins,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6634\u001b[0m             \u001b[0;31m# so that each histogram uses the same bins\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6635\u001b[0;31m             \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhist_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6636\u001b[0m             \u001b[0mtops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6637\u001b[0m         \u001b[0mtops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# causes problems later if it's an int\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mhistogram\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/numpy/lib/histograms.py\u001b[0m in \u001b[0;36mhistogram\u001b[0;34m(a, bins, range, normed, weights, density)\u001b[0m\n\u001b[1;32m    793\u001b[0m     \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ravel_and_check_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m     \u001b[0mbin_edges\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniform_bins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_bin_edges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m     \u001b[0;31m# Histogram is an integer or a float array depending on the weights.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/numpy/lib/histograms.py\u001b[0m in \u001b[0;36m_get_bin_edges\u001b[0;34m(a, bins, range, weights)\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`bins` must be positive, when an integer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m         \u001b[0mfirst_edge\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_edge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_outer_edges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/numpy/lib/histograms.py\u001b[0m in \u001b[0;36m_get_outer_edges\u001b[0;34m(a, range)\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_edge\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_edge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             raise ValueError(\n\u001b[0;32m--> 327\u001b[0;31m                 \"autodetected range of [{}, {}] is not finite\".format(first_edge, last_edge))\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;31m# expand empty range to avoid divide by zero\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: autodetected range of [nan, nan] is not finite"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the histogram\n",
    "plt.hist(history, bins=10, edgecolor='black')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Data')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, for the chosen value of gamma, the best partition is..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize\n",
    "best_partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute stratified metrics with unbiased testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbiased_results[\"STRATIFIED_15\"] = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=1.5, K=4, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_15\"] = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=1.5, K=20, partition=best_partition)\n",
    "\n",
    "unbiased_results[\"STRATIFIED_2\"] = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2, K=4, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_2\"] = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2, K=20, partition=best_partition)\n",
    "\n",
    "unbiased_results[\"STRATIFIED_25\"] = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2.5, K=4, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_25\"] = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2.5, K=20, partition=best_partition)\n",
    "\n",
    "unbiased_results[\"STRATIFIED_3\"] = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=3, K=4, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_3\"] = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=3, K=20, partition=best_partition)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version uses the linspace of items instead of linspace of propensities to make the partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbiased_results[\"STRATIFIED_v2_15\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=1.5, K=4, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_v2_15\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=1.5, K=20, partition=best_partition)\n",
    "\n",
    "unbiased_results[\"STRATIFIED_v2_2\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2, K=4, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_v2_2\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2, K=20, partition=best_partition)\n",
    "\n",
    "unbiased_results[\"STRATIFIED_v2_25\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2.5, K=4, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_v2_25\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2.5, K=20, partition=best_partition)\n",
    "\n",
    "unbiased_results[\"STRATIFIED_v2_3\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=3, K=4, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_v2_3\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=3, K=20, partition=best_partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 13)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = 0\n",
    "columns = len(biased_results.keys())\n",
    "\n",
    "for key in biased_results.keys():\n",
    "    rows = max(rows, len(biased_results[key].keys()))\n",
    "\n",
    "for key in unbiased_results.keys():\n",
    "    rows = max(rows, len(biased_results[key].keys()))\n",
    "\n",
    "rows, columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init dictionary\n",
    "mae_results = dict()\n",
    "\n",
    "# Get the names of the rows\n",
    "list_biased_res = list(biased_results.keys())\n",
    "\n",
    "\n",
    "# Init results\n",
    "results_array = np.zeros((rows,columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the table with the MAE results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each row\n",
    "for i in range(len(list_biased_res)):\n",
    "    key = list_biased_res[i]\n",
    "\n",
    "    # For each column\n",
    "    for j in range(len(list(biased_results[key].keys()))):\n",
    "        key_2 = list(biased_results[key].keys())[j]\n",
    "\n",
    "        # Compute MAE\n",
    "        results_array[j][i] = abs(biased_results[key][key_2] - unbiased_results[key][key_2])\n",
    "\n",
    "# Make it a DataFrame\n",
    "mae_df = pd.DataFrame(columns=list(biased_results.keys()), data=results_array)\n",
    "metric_values = list(biased_results[list(biased_results.keys())[0]].keys())\n",
    "mae_df.insert(0, \"metric\", metric_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **RESULTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>AOA</th>\n",
       "      <th>UB_15</th>\n",
       "      <th>UB_2</th>\n",
       "      <th>UB_25</th>\n",
       "      <th>UB_3</th>\n",
       "      <th>STRATIFIED_15</th>\n",
       "      <th>STRATIFIED_2</th>\n",
       "      <th>STRATIFIED_25</th>\n",
       "      <th>STRATIFIED_3</th>\n",
       "      <th>STRATIFIED_v2_15</th>\n",
       "      <th>STRATIFIED_v2_2</th>\n",
       "      <th>STRATIFIED_v2_25</th>\n",
       "      <th>STRATIFIED_v2_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>auc</td>\n",
       "      <td>0.054800</td>\n",
       "      <td>0.090975</td>\n",
       "      <td>0.095139</td>\n",
       "      <td>0.098290</td>\n",
       "      <td>0.100626</td>\n",
       "      <td>9.097485e-02</td>\n",
       "      <td>9.513882e-02</td>\n",
       "      <td>9.828963e-02</td>\n",
       "      <td>0.072265</td>\n",
       "      <td>0.089552</td>\n",
       "      <td>0.093386</td>\n",
       "      <td>0.096192</td>\n",
       "      <td>0.098169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>recall</td>\n",
       "      <td>0.076203</td>\n",
       "      <td>0.113201</td>\n",
       "      <td>0.117820</td>\n",
       "      <td>0.121511</td>\n",
       "      <td>0.124447</td>\n",
       "      <td>1.132014e-01</td>\n",
       "      <td>1.178203e-01</td>\n",
       "      <td>1.215106e-01</td>\n",
       "      <td>0.133113</td>\n",
       "      <td>0.112005</td>\n",
       "      <td>0.116341</td>\n",
       "      <td>0.119731</td>\n",
       "      <td>0.122350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bias</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.082268e-11</td>\n",
       "      <td>7.539636e-12</td>\n",
       "      <td>4.317213e-12</td>\n",
       "      <td>5425.714286</td>\n",
       "      <td>143.909688</td>\n",
       "      <td>173.446281</td>\n",
       "      <td>203.398273</td>\n",
       "      <td>233.838054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>concentration</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.294130e+02</td>\n",
       "      <td>1.265924e+02</td>\n",
       "      <td>1.243931e+02</td>\n",
       "      <td>231.346606</td>\n",
       "      <td>129.427445</td>\n",
       "      <td>126.599289</td>\n",
       "      <td>124.396269</td>\n",
       "      <td>122.635240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          metric       AOA     UB_15      UB_2     UB_25      UB_3  \\\n",
       "0            auc  0.054800  0.090975  0.095139  0.098290  0.100626   \n",
       "1         recall  0.076203  0.113201  0.117820  0.121511  0.124447   \n",
       "2           bias  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3  concentration  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "   STRATIFIED_15  STRATIFIED_2  STRATIFIED_25  STRATIFIED_3  STRATIFIED_v2_15  \\\n",
       "0   9.097485e-02  9.513882e-02   9.828963e-02      0.072265          0.089552   \n",
       "1   1.132014e-01  1.178203e-01   1.215106e-01      0.133113          0.112005   \n",
       "2   1.082268e-11  7.539636e-12   4.317213e-12   5425.714286        143.909688   \n",
       "3   1.294130e+02  1.265924e+02   1.243931e+02    231.346606        129.427445   \n",
       "\n",
       "   STRATIFIED_v2_2  STRATIFIED_v2_25  STRATIFIED_v2_3  \n",
       "0         0.093386          0.096192         0.098169  \n",
       "1         0.116341          0.119731         0.122350  \n",
       "2       173.446281        203.398273       233.838054  \n",
       "3       126.599289        124.396269       122.635240  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize\n",
    "mae_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RecSysEvaluation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
