{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some notes: \n",
    "- i saw that some user, item tuples of the random test set are present in the training set, is this ok?\n",
    "- is negative subsampling of 200 items ok? i switched to 60 to keep the ration\n",
    "- is K=4 and K=20 ok for recall?\n",
    "- grid search for gamma=1.5 gives better results on Stratified with gamma=3, then by looking for the number of partitions that minimizes gamma=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy.random as npr\n",
    "from scipy import sparse, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from openrec.tf1.legacy import ImplicitModelTrainer\n",
    "from openrec.tf1.legacy.utils.evaluators import ImplicitEvalManager\n",
    "from openrec.tf1.legacy.utils import ImplicitDataset\n",
    "from openrec.tf1.legacy.recommenders import CML, BPR, PMF\n",
    "from openrec.tf1.legacy.utils.evaluators import AUC\n",
    "from openrec.tf1.legacy.utils.samplers import PairwiseSampler\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "seed = 2384795\n",
    "np.random.seed(seed=seed)\n",
    "\n",
    "# Preparing folder for output data\n",
    "output_name = f\"./generated_data/\"\n",
    "if os.path.exists(output_name) == False:\n",
    "    os.makedirs(output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './original_files/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(os.path.join(DATA_DIR, 'train.ascii'), sep=\" \", header=None, engine=\"python\")\n",
    "test_data = pd.read_csv(os.path.join(DATA_DIR, 'test.ascii'), sep=\" \", header=None, engine=\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_vd_data = pd.DataFrame({\"userId\": sparse.coo_matrix(raw_data).row,                            \"songId\": sparse.coo_matrix(raw_data).col,                           \"rating\": sparse.coo_matrix(raw_data).data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.DataFrame({\"userId\": sparse.coo_matrix(test_data).row,                            \"songId\": sparse.coo_matrix(test_data).col,                           \"rating\": sparse.coo_matrix(test_data).data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_proportion_old(data, uid, test_prop=0.5, random_seed=0):\n",
    "    data_grouped_by_user = data.groupby(uid)\n",
    "    tr_list, te_list = list(), list()\n",
    "\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    for u, (_, group) in enumerate(data_grouped_by_user):\n",
    "        n_items_u = len(group)\n",
    "\n",
    "        if n_items_u >= 5:\n",
    "            idx = np.zeros(n_items_u, dtype='bool')\n",
    "            idx[np.random.choice(n_items_u, size=int(test_prop * n_items_u), replace=False).astype('int64')] = True\n",
    "\n",
    "            tr_list.append(group[np.logical_not(idx)])\n",
    "            te_list.append(group[idx])\n",
    "        else:\n",
    "            tr_list.append(group)\n",
    "\n",
    "        if u % 5000 == 0:\n",
    "            print(\"%d users sampled\" % u)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    data_tr = pd.concat(tr_list)\n",
    "    data_te = pd.concat(te_list)\n",
    "    \n",
    "    return data_tr, data_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_proportion(data, random_seed=0):\n",
    "\n",
    "    df_train = data\n",
    "\n",
    "    # Create a test df\n",
    "    df_test = pd.DataFrame(columns=data.columns)\n",
    "\n",
    "    # Precompute, for each user, the list of songs with a relevant rating\n",
    "    user_positive_ratings = data[data[\"rating\"] == 1].groupby(\"user_id\")[\"item_id\"].apply(set)\n",
    "    \n",
    "    min_item, max_item = data['item_id'].min(), data['item_id'].max()\n",
    "\n",
    "    # Initialize the range of indexes for the items\n",
    "    items_ids = np.arange(min_item, max_item + 1)\n",
    "\n",
    "    # Set the number of songs for each user\n",
    "    SONGS_FOR_BIASED_TEST = 90\n",
    "\n",
    "    users = set(data[\"user_id\"].unique())\n",
    "\n",
    "    # Extract the biased test set\n",
    "    for user_id in users:\n",
    "\n",
    "        # Get SONGS_FOR_BIASED_TEST items\n",
    "        np.random.shuffle(items_ids)\n",
    "        test_items = set(items_ids[-SONGS_FOR_BIASED_TEST:])\n",
    "\n",
    "        # Get which are positive\n",
    "        pos_ids = user_positive_ratings.get(user_id, set()) & test_items\n",
    "\n",
    "        # Get which are negative but in test_items\n",
    "        neg_ids = test_items - pos_ids\n",
    "\n",
    "        # Set the positive ones to 0 in the training set (extract)\n",
    "        df_train.loc[(df_train['item_id'].isin(pos_ids)) & (df_train['user_id'] == user_id), 'rating'] = 0\n",
    "\n",
    "        # now add them in the test set\n",
    "        # add to df_test the rows made of [user_id, pos_ids, 1] and [user_id, neg_ids, 0]\n",
    "        for item_id in pos_ids:\n",
    "            df_test = df_test.append({'user_id': user_id, 'item_id': item_id, 'rating': 1}, ignore_index=True)\n",
    "        \n",
    "        for item_id in neg_ids:\n",
    "            df_test = df_test.append({'user_id': user_id, 'item_id': item_id, 'rating': 0}, ignore_index=True)\n",
    "\n",
    "    # Convert back to the correct data types\n",
    "    df_train['user_id'] = df_train['user_id'].astype(int)\n",
    "    df_train['item_id'] = df_train['item_id'].astype(int)\n",
    "    df_train['rating'] = df_train['rating'].astype(int)\n",
    "    \n",
    "    df_test['user_id'] = df_test['user_id'].astype(int)\n",
    "    df_test['item_id'] = df_test['item_id'].astype(int)\n",
    "    df_test['rating'] = df_test['rating'].astype(int)\n",
    "    \n",
    "    return df_train, df_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count(tp, id):\n",
    "    playcount_groupbyid = tp[[id]].groupby(id, as_index=False)\n",
    "    count = playcount_groupbyid.size()\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make dataset implicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>songId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>136</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>188</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>220</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>227</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>228</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>234</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>235</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  songId  rating\n",
       "0       0      72       2\n",
       "1       0     136       2\n",
       "2       0     150       3\n",
       "3       0     171       3\n",
       "4       0     188       3\n",
       "5       0     220       3\n",
       "6       0     227       5\n",
       "7       0     228       4\n",
       "8       0     234       3\n",
       "9       0     235       4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_vd_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>songId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>78</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>104</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>127</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>133</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>145</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  songId  rating\n",
       "0       0      12       4\n",
       "1       0      17       3\n",
       "2       0      74       4\n",
       "3       0      78       2\n",
       "4       0      92       2\n",
       "5       0     104       4\n",
       "6       0     127       4\n",
       "7       0     128       3\n",
       "8       0     133       3\n",
       "9       0     145       2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suggested on the original yahoo's paper\n",
    "POSITIVE_THRESHOLD = 4\n",
    "\n",
    "# Add column to the DataFrame\n",
    "tr_vd_data['ImplicitRating'] = np.where(tr_vd_data['rating'] >= POSITIVE_THRESHOLD, 1, 0)\n",
    "test_data['ImplicitRating'] = np.where(test_data['rating'] >= POSITIVE_THRESHOLD, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>songId</th>\n",
       "      <th>rating</th>\n",
       "      <th>ImplicitRating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>136</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>188</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>220</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>227</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>228</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>234</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>235</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  songId  rating  ImplicitRating\n",
       "0       0      72       2               0\n",
       "1       0     136       2               0\n",
       "2       0     150       3               0\n",
       "3       0     171       3               0\n",
       "4       0     188       3               0\n",
       "5       0     220       3               0\n",
       "6       0     227       5               1\n",
       "7       0     228       4               1\n",
       "8       0     234       3               0\n",
       "9       0     235       4               1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_vd_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_vd_data = tr_vd_data.drop(['rating'],axis=1).rename({\"ImplicitRating\":\"rating\"}, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>songId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>136</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>188</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>220</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>227</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>228</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>234</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>235</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  songId  rating\n",
       "0       0      72       0\n",
       "1       0     136       0\n",
       "2       0     150       0\n",
       "3       0     171       0\n",
       "4       0     188       0\n",
       "5       0     220       0\n",
       "6       0     227       1\n",
       "7       0     228       1\n",
       "8       0     234       0\n",
       "9       0     235       1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_vd_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>songId</th>\n",
       "      <th>rating</th>\n",
       "      <th>ImplicitRating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>78</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>104</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>127</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>133</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>145</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  songId  rating  ImplicitRating\n",
       "0       0      12       4               1\n",
       "1       0      17       3               0\n",
       "2       0      74       4               1\n",
       "3       0      78       2               0\n",
       "4       0      92       2               0\n",
       "5       0     104       4               1\n",
       "6       0     127       4               1\n",
       "7       0     128       3               0\n",
       "8       0     133       3               0\n",
       "9       0     145       2               0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.drop(['rating'],axis=1).rename({\"ImplicitRating\":\"rating\"}, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>songId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>104</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>127</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>133</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  songId  rating\n",
       "0       0      12       1\n",
       "1       0      17       0\n",
       "2       0      74       1\n",
       "3       0      78       0\n",
       "4       0      92       0\n",
       "5       0     104       1\n",
       "6       0     127       1\n",
       "7       0     128       0\n",
       "8       0     133       0\n",
       "9       0     145       0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4640 entries, 0 to 4639\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   userId  4640 non-null   int32\n",
      " 1   songId  4640 non-null   int32\n",
      " 2   rating  4640 non-null   int64\n",
      "dtypes: int32(2), int64(1)\n",
      "memory usage: 72.6 KB\n"
     ]
    }
   ],
   "source": [
    "test_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   userId  songId  rating\n",
       " 0       0      72       0\n",
       " 1       0     136       0\n",
       " 2       0     150       0\n",
       " 3       0     171       0\n",
       " 4       0     188       0,\n",
       " (6960, 3))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_vd_data.head(), tr_vd_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   userId  songId  rating\n",
       " 0       0      12       1\n",
       " 1       0      17       0\n",
       " 2       0      74       1\n",
       " 3       0      78       0\n",
       " 4       0      92       0,\n",
       " (4640, 3))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head(), test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4640 entries, 0 to 4639\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   userId  4640 non-null   int32\n",
      " 1   songId  4640 non-null   int32\n",
      " 2   rating  4640 non-null   int64\n",
      "dtypes: int32(2), int64(1)\n",
      "memory usage: 72.6 KB\n"
     ]
    }
   ],
   "source": [
    "test_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_activity = get_count(tr_vd_data, 'userId')\n",
    "item_popularity = get_count(tr_vd_data, 'songId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_uid = user_activity.index\n",
    "unique_sid = item_popularity.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = len(unique_uid)\n",
    "n_items = len(unique_sid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(290, 300)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_users, n_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing eventual songs and users from the test set not present in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "song2id = dict((sid, i) for (i, sid) in enumerate(unique_sid))\n",
    "user2id = dict((uid, i) for (i, uid) in enumerate(unique_uid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the test set, only keep the users/items from the training set\n",
    "\n",
    "test_data = test_data.loc[test_data['userId'].isin(unique_uid)]\n",
    "test_data = test_data.loc[test_data['songId'].isin(unique_sid)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn userId and songId to 0-based index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerize(tp):\n",
    "    uid = list(map(lambda x: user2id[x], tp['userId']))\n",
    "    sid = list(map(lambda x: song2id[x], tp['songId']))\n",
    "    tp.loc[:, 'user_id'] = uid\n",
    "    tp.loc[:, 'item_id'] = sid\n",
    "    return tp[['user_id', 'item_id', 'rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_vd_data = numerize(tr_vd_data)\n",
    "test_data = numerize(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do we need the validation for our purpose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data, vad_data = split_train_test_proportion(tr_vd_data, 'user_id', test_prop=0.7, random_seed=12345)\n",
    "#obs_test_data, vad_data = split_train_test_proportion(vad_data, 'user_id', test_prop=0.5, random_seed=12345)\n",
    "train_data, obs_test_data = split_train_test_proportion(tr_vd_data, random_seed=12345)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are total of 290 unique users in the training set and 290 unique users in the entire dataset\n"
     ]
    }
   ],
   "source": [
    "print(\"There are total of %d unique users in the training set and %d unique users in the entire dataset\" % (len(pd.unique(train_data['user_id'])), len(unique_uid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are total of 300 unique items in the training set and 300 unique items in the entire dataset\n"
     ]
    }
   ],
   "source": [
    "print(\"There are total of %d unique items in the training set and %d unique items in the entire dataset\" % (len(pd.unique(train_data['item_id'])), len(unique_sid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_to_fill(part_data_1, part_data_2, unique_id, key):\n",
    "    # move the data from part_data_2 to part_data_1 so that part_data_1 has the same number of unique \"key\" as unique_id\n",
    "    part_id = set(pd.unique(part_data_1[key]))\n",
    "    \n",
    "    left_id = list()\n",
    "    for i, _id in enumerate(unique_id):\n",
    "        if _id not in part_id:\n",
    "            left_id.append(_id)\n",
    "            \n",
    "    move_idx = part_data_2[key].isin(left_id)\n",
    "    part_data_1 = part_data_1.append(part_data_2[move_idx])\n",
    "    part_data_2 = part_data_2[~move_idx]\n",
    "    return part_data_1, part_data_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The move_to_fill function is used to ensure that train_data ends up with a complete set of unique IDs as specified by unique_id, by \"moving\" the necessary rows from another dataset (part_data_2 like vad_data or obs_test_data) and updating both DataFrames accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data, vad_data = move_to_fill(train_data, vad_data, np.arange(n_items), 'item_id')\n",
    "train_data, obs_test_data = move_to_fill(train_data, obs_test_data, np.arange(n_items), 'item_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are total of 300 unique items in the training set and 300 unique items in the entire dataset\n"
     ]
    }
   ],
   "source": [
    "print(\"There are total of %d unique items in the training set and %d unique items in the entire dataset\" % (len(pd.unique(train_data['item_id'])), len(unique_sid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store datasets in csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv(os.path.join(output_name, 'train.csv'), index=False)\n",
    "#vad_data.to_csv(os.path.join(output_name, 'validation.csv'), index=False)\n",
    "tr_vd_data.to_csv(os.path.join(output_name, 'train_full.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_test_data.to_csv(os.path.join(output_name, 'obs_test_full.csv'), index=False)\n",
    "test_data.to_csv(os.path.join(output_name, 'test_full.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now *obs_test_data* is our biased testset extracted by the original dataset, while *test_data* is our unbiased test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>298</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>251</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>228</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>236</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>257</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26095</th>\n",
       "      <td>289</td>\n",
       "      <td>237</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26096</th>\n",
       "      <td>289</td>\n",
       "      <td>239</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26097</th>\n",
       "      <td>289</td>\n",
       "      <td>244</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26098</th>\n",
       "      <td>289</td>\n",
       "      <td>249</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26099</th>\n",
       "      <td>289</td>\n",
       "      <td>254</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26100 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id  item_id  rating\n",
       "0            0      298       1\n",
       "1            0      251       1\n",
       "2            0      228       1\n",
       "3            0      236       1\n",
       "4            0      257       0\n",
       "...        ...      ...     ...\n",
       "26095      289      237       0\n",
       "26096      289      239       0\n",
       "26097      289      244       0\n",
       "26098      289      249       0\n",
       "26099      289      254       0\n",
       "\n",
       "[26100 rows x 3 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build files for creating dataset for the openrec library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init empty\n",
    "pos_test_set = []\n",
    "neg_test_set = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create masks for positive and negative ratings\n",
    "pos_mask = obs_test_data['rating'] == 1\n",
    "neg_mask = obs_test_data['rating'] != 1\n",
    "\n",
    "# Extract the user_id and item_id pairs for positive and negative ratings\n",
    "pos_test_set = obs_test_data.loc[pos_mask, ['user_id', 'item_id']].values.tolist()\n",
    "neg_test_set = obs_test_data.loc[neg_mask, ['user_id', 'item_id']].values.tolist()\n",
    "\n",
    "# pos_test_set and neg_test_set now contain the lists of [user_id, item_id] for positive and negative ratings, respectively.\n",
    "# Get np arrays\n",
    "pos_test_set = np.array(pos_test_set)\n",
    "neg_test_set = np.array(neg_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0, 298],\n",
       "       [  0, 251],\n",
       "       [  0, 228],\n",
       "       ...,\n",
       "       [287, 138],\n",
       "       [287, 120],\n",
       "       [288,  36]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataframe\n",
    "pos_test_set_df = pd.DataFrame(pos_test_set)\n",
    "neg_test_set_df = pd.DataFrame(neg_test_set)\n",
    "\n",
    "# Get couples user-item\n",
    "pos_test_set_df.columns = [\"user_id\",\"item_id\"]\n",
    "neg_test_set_df.columns = [\"user_id\",\"item_id\"]\n",
    "\n",
    "# Turn into records\n",
    "structured_data_pos_test_set = pos_test_set_df.to_records(index=False)\n",
    "structured_data_neg_test_set = neg_test_set_df.to_records(index=False)\n",
    "\n",
    "# Save\n",
    "np.save(output_name + \"biased-test_arr_pos.npy\", structured_data_pos_test_set)\n",
    "np.save(output_name + \"biased-test_arr_neg.npy\", structured_data_neg_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unbiased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init empty\n",
    "pos_test_set = []\n",
    "neg_test_set = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create masks for positive and negative ratings\n",
    "pos_mask = test_data['rating'] == 1\n",
    "neg_mask = test_data['rating'] != 1\n",
    "\n",
    "# Extract the user_id and item_id pairs for positive and negative ratings\n",
    "pos_test_set = test_data.loc[pos_mask, ['user_id', 'item_id']].values.tolist()\n",
    "neg_test_set = test_data.loc[neg_mask, ['user_id', 'item_id']].values.tolist()\n",
    "\n",
    "# pos_test_set and neg_test_set now contain the lists of [user_id, item_id] for positive and negative ratings, respectively.\n",
    "# Get np arrays\n",
    "pos_test_set = np.array(pos_test_set)\n",
    "neg_test_set = np.array(neg_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataframe\n",
    "pos_test_set_df = pd.DataFrame(pos_test_set)\n",
    "neg_test_set_df = pd.DataFrame(neg_test_set)\n",
    "\n",
    "# Get couples user-item\n",
    "pos_test_set_df.columns = [\"user_id\",\"item_id\"]\n",
    "neg_test_set_df.columns = [\"user_id\",\"item_id\"]\n",
    "\n",
    "# Turn into records\n",
    "structured_data_pos_test_set = pos_test_set_df.to_records(index=False)\n",
    "structured_data_neg_test_set = neg_test_set_df.to_records(index=False)\n",
    "\n",
    "# Save\n",
    "np.save(output_name + \"unbiased-test_arr_pos.npy\", structured_data_pos_test_set)\n",
    "np.save(output_name + \"unbiased-test_arr_neg.npy\", structured_data_neg_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_trainset = train_data[train_data['rating'] != 0]\n",
    "positive_trainset = positive_trainset.drop(columns=['rating'])\n",
    "\n",
    "# Convert the DataFrame to a structured array\n",
    "positive_trainset = positive_trainset.to_records(index=False) \n",
    "\n",
    "# Save\n",
    "np.save(output_name + \"training_arr.npy\", positive_trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(290, 290, 290)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"user_id\"].unique().size, test_data[\"user_id\"].unique().size, obs_test_data[\"user_id\"].unique().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(289, 289, 289)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_trainset[\"user_id\"].unique().max(), test_data[\"user_id\"].unique().max(), obs_test_data[\"user_id\"].unique().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 300, 300)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"item_id\"].unique().size, test_data[\"item_id\"].unique().size, obs_test_data[\"item_id\"].unique().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(299, 299, 299)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"item_id\"].unique().max(), test_data[\"item_id\"].unique().max(), obs_test_data[\"item_id\"].unique().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MODEL CHOICE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I won't comment anything, we are just using the code provided by the authors of the paper\n",
    "\n",
    "raw_data = dict()\n",
    "raw_data['train_data'] = np.load(output_name + \"training_arr.npy\" )\n",
    "raw_data['max_user'] = 290\n",
    "raw_data['max_item'] = 300\n",
    "batch_size = 8000\n",
    "test_batch_size = 1000\n",
    "display_itr = 1000\n",
    "\n",
    "train_dataset = ImplicitDataset(raw_data['train_data'], raw_data['max_user'], raw_data['max_item'], name='Train')\n",
    "\n",
    "MODEL_CLASS = CML\n",
    "MODEL_PREFIX = \"cml\"\n",
    "DATASET_NAME = \"coat\"\n",
    "OUTPUT_FOLDER = output_name\n",
    "OUTPUT_PATH = OUTPUT_FOLDER + MODEL_PREFIX + \"-\" + DATASET_NAME + \"/\"\n",
    "OUTPUT_PREFIX = str(OUTPUT_PATH) + str(MODEL_PREFIX) + \"-\" + str(DATASET_NAME)\n",
    "\n",
    "\n",
    "if os.path.exists(OUTPUT_PATH) == False:\n",
    "    os.makedirs(OUTPUT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:391: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:31: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:41: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:43: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:33: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/interactions/pairwise_eu_dist.py:71: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:596: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:75: The name tf.scatter_update is deprecated. Please use tf.compat.v1.scatter_update instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:144: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:365: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/japo/miniconda3/envs/RecSysEvaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:148: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-19 16:42:49.303013: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2024-08-19 16:42:49.328104: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f8f6f414310 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2024-08-19 16:42:49.328120: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Start training with FULL evaluation ==\n",
      "[Itr 100] Finished\n",
      "[Itr 200] Finished\n",
      "[Itr 300] Finished\n",
      "[Itr 400] Finished\n",
      "[Itr 500] Finished\n",
      "[Itr 600] Finished\n",
      "[Itr 700] Finished\n",
      "[Itr 800] Finished\n",
      "[Itr 900] Finished\n",
      "[Itr 1000] Finished\n",
      "INFO:tensorflow:./generated_data/cml-coat/coat-1000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 1000] loss: 6817.881247\n",
      "[Itr 1100] Finished\n",
      "[Itr 1200] Finished\n",
      "[Itr 1300] Finished\n",
      "[Itr 1400] Finished\n",
      "[Itr 1500] Finished\n",
      "[Itr 1600] Finished\n",
      "[Itr 1700] Finished\n",
      "[Itr 1800] Finished\n",
      "[Itr 1900] Finished\n",
      "[Itr 2000] Finished\n",
      "INFO:tensorflow:./generated_data/cml-coat/coat-2000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 2000] loss: 6674.632245\n",
      "[Itr 2100] Finished\n",
      "[Itr 2200] Finished\n",
      "[Itr 2300] Finished\n",
      "[Itr 2400] Finished\n",
      "[Itr 2500] Finished\n",
      "[Itr 2600] Finished\n",
      "[Itr 2700] Finished\n",
      "[Itr 2800] Finished\n",
      "[Itr 2900] Finished\n",
      "[Itr 3000] Finished\n",
      "INFO:tensorflow:./generated_data/cml-coat/coat-3000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 3000] loss: 6671.247231\n",
      "[Itr 3100] Finished\n",
      "[Itr 3200] Finished\n",
      "[Itr 3300] Finished\n",
      "[Itr 3400] Finished\n",
      "[Itr 3500] Finished\n",
      "[Itr 3600] Finished\n",
      "[Itr 3700] Finished\n",
      "[Itr 3800] Finished\n",
      "[Itr 3900] Finished\n",
      "[Itr 4000] Finished\n",
      "INFO:tensorflow:./generated_data/cml-coat/coat-4000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 4000] loss: 6670.158671\n",
      "[Itr 4100] Finished\n",
      "[Itr 4200] Finished\n",
      "[Itr 4300] Finished\n",
      "[Itr 4400] Finished\n",
      "[Itr 4500] Finished\n",
      "[Itr 4600] Finished\n",
      "[Itr 4700] Finished\n",
      "[Itr 4800] Finished\n",
      "[Itr 4900] Finished\n",
      "[Itr 5000] Finished\n",
      "INFO:tensorflow:./generated_data/cml-coat/coat-5000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 5000] loss: 6669.546193\n",
      "[Itr 5100] Finished\n",
      "[Itr 5200] Finished\n",
      "[Itr 5300] Finished\n",
      "[Itr 5400] Finished\n",
      "[Itr 5500] Finished\n",
      "[Itr 5600] Finished\n",
      "[Itr 5700] Finished\n",
      "[Itr 5800] Finished\n",
      "[Itr 5900] Finished\n",
      "[Itr 6000] Finished\n",
      "INFO:tensorflow:./generated_data/cml-coat/coat-6000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 6000] loss: 6668.988957\n",
      "[Itr 6100] Finished\n",
      "[Itr 6200] Finished\n",
      "[Itr 6300] Finished\n",
      "[Itr 6400] Finished\n",
      "[Itr 6500] Finished\n",
      "[Itr 6600] Finished\n",
      "[Itr 6700] Finished\n",
      "[Itr 6800] Finished\n",
      "[Itr 6900] Finished\n",
      "[Itr 7000] Finished\n",
      "INFO:tensorflow:./generated_data/cml-coat/coat-7000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 7000] loss: 6668.849633\n",
      "[Itr 7100] Finished\n",
      "[Itr 7200] Finished\n",
      "[Itr 7300] Finished\n",
      "[Itr 7400] Finished\n",
      "[Itr 7500] Finished\n",
      "[Itr 7600] Finished\n",
      "[Itr 7700] Finished\n",
      "[Itr 7800] Finished\n",
      "[Itr 7900] Finished\n",
      "[Itr 8000] Finished\n",
      "INFO:tensorflow:./generated_data/cml-coat/coat-8000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 8000] loss: 6668.678166\n",
      "[Itr 8100] Finished\n",
      "[Itr 8200] Finished\n",
      "[Itr 8300] Finished\n",
      "[Itr 8400] Finished\n",
      "[Itr 8500] Finished\n",
      "[Itr 8600] Finished\n",
      "[Itr 8700] Finished\n",
      "[Itr 8800] Finished\n",
      "[Itr 8900] Finished\n",
      "[Itr 9000] Finished\n",
      "INFO:tensorflow:./generated_data/cml-coat/coat-9000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 9000] loss: 6668.474175\n",
      "[Itr 9100] Finished\n",
      "[Itr 9200] Finished\n",
      "[Itr 9300] Finished\n",
      "[Itr 9400] Finished\n",
      "[Itr 9500] Finished\n",
      "[Itr 9600] Finished\n",
      "[Itr 9700] Finished\n",
      "[Itr 9800] Finished\n",
      "[Itr 9900] Finished\n",
      "[Itr 10000] Finished\n",
      "INFO:tensorflow:./generated_data/cml-coat/coat-10000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "[Itr 10000] loss: 6668.361782\n",
      "INFO:tensorflow:./generated_data/cml-coat/ is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "# Prevent tensorflow from using cached embeddings\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "# Define the model\n",
    "model = MODEL_CLASS(batch_size=batch_size, max_user=train_dataset.max_user(), max_item=train_dataset.max_item(), dim_embed=50, l2_reg=0.001, opt='Adam', sess_config=None)\n",
    "sampler = PairwiseSampler(batch_size=batch_size, dataset=train_dataset, num_process=4)\n",
    "model_trainer = ImplicitModelTrainer(batch_size=batch_size, test_batch_size=test_batch_size, train_dataset=train_dataset, model=model, sampler=sampler, eval_save_prefix=OUTPUT_PATH + DATASET_NAME, item_serving_size=500)\n",
    "auc_evaluator = AUC()\n",
    "\n",
    "# Train the model\n",
    "model_trainer.train(num_itr=10001, display_itr=display_itr)\n",
    "\n",
    "# Save in the output folder\n",
    "model.save(OUTPUT_PATH,None)\n",
    "\n",
    "# Delete the model from the memory\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DEFINING FUNCTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_propensities(n_users, n_items, trainfilename, gammas=[1.5, 2, 2.5, 3], normalize=True):\n",
    "\n",
    "    Ni = dict()\n",
    "    propensities = dict()\n",
    "\n",
    "    # Compute frequencies of items in the training set\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "\n",
    "    for gamma in gammas:\n",
    "        propensities[gamma] = np.empty((n_users,n_items))\n",
    "\n",
    "    for theitem in range(n_items):\n",
    "        if theitem not in Ni:\n",
    "            continue\n",
    "        for gamma in gammas:\n",
    "            propensities[gamma][:,theitem] = np.power(Ni[theitem], (gamma + 1) / 2.0)\n",
    "\n",
    "    if normalize:\n",
    "        for gamma in gammas:\n",
    "            propensities[gamma] /= propensities[gamma].max()\n",
    "\n",
    "    return propensities\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eq(infilename, infilename_neg, trainfilename, gamma=-1.0, K=4):\n",
    "\n",
    "    # Read pickles\n",
    "    infile = open(infilename, 'rb')\n",
    "    infile_neg = open(infilename_neg, 'rb')\n",
    "    P = pickle.load(infile)\n",
    "    infile.close()\n",
    "    P_neg = pickle.load(infile_neg)\n",
    "    infile_neg.close()\n",
    "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
    "    \n",
    "    # Merge P and P_neg\n",
    "    for theuser in P[\"users\"]:\n",
    "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "    \n",
    "    Zui = dict()\n",
    "    Ni = dict()\n",
    "    \n",
    "    # Compute frequencies of items in the training set\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "    del trainset\n",
    "\n",
    "    # Count #users with non-zero item frequencies\n",
    "    nonzero_user_count = 0\n",
    "    for theuser in P[\"users\"]:\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for pos_item in pos_items:\n",
    "            if pos_item in Ni:\n",
    "                nonzero_user_count += 1\n",
    "                break\n",
    "    \n",
    "    # Compute recommendations for each user\n",
    "    for theuser in P[\"users\"]:\n",
    "        all_scores = np.array(P[\"results\"][theuser])\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for i, pos_item in enumerate(pos_items):\n",
    "            pos_score = pos_scores[i]\n",
    "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
    "\n",
    "    \n",
    "    # Calculate per-user scores\n",
    "    sum_user_auc = 0.0\n",
    "    sum_user_recall = 0.0\n",
    "    for theuser in P[\"users\"]:\n",
    "        numerator_auc = 0.0\n",
    "        numerator_recall = 0.0\n",
    "        denominator = 0.0\n",
    "\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            if theitem not in Ni:\n",
    "                continue\n",
    "            pui = np.power(Ni[theitem], (gamma + 1) / 2.0)\n",
    "\n",
    "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser])) / pui\n",
    "            \n",
    "            if Zui[(theuser, theitem)] < K:\n",
    "                numerator_recall += 1.0 / pui\n",
    "            denominator += 1 / pui\n",
    "                \n",
    "        if denominator > 0:\n",
    "            sum_user_auc += numerator_auc / denominator\n",
    "            sum_user_recall += numerator_recall / denominator \n",
    "\n",
    "    # Return\n",
    "    return {\n",
    "        \"auc\"       : sum_user_auc / nonzero_user_count,\n",
    "        \"recall\"    : sum_user_recall / nonzero_user_count,\n",
    "        \"bias\"      : -1,\n",
    "        \"concentration\" : -1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aoa(infilename, infilename_neg, trainfilename, K=4):\n",
    "\n",
    "    # Read pickles\n",
    "    infile = open(infilename, 'rb')\n",
    "    infile_neg = open(infilename_neg, 'rb')\n",
    "    P = pickle.load(infile)\n",
    "    infile.close()\n",
    "    P_neg = pickle.load(infile_neg)\n",
    "    infile_neg.close()\n",
    "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
    "    \n",
    "    # Merge P and P_neg\n",
    "    for theuser in P[\"users\"]:\n",
    "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "    \n",
    "    Zui = dict()\n",
    "    Ni = dict()\n",
    "\n",
    "    # Compute frequencies of items in the training set\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "    del trainset\n",
    "    \n",
    "    # Count #users with non-zero item frequencies\n",
    "    nonzero_user_count = 0\n",
    "    for theuser in P[\"users\"]:\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for pos_item in pos_items:\n",
    "            if pos_item in Ni:\n",
    "                nonzero_user_count += 1\n",
    "                break\n",
    "    \n",
    "    # Compute recommendations for each user\n",
    "    for theuser in P[\"users\"]:\n",
    "        all_scores = np.array(P[\"results\"][theuser])\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for i, pos_item in enumerate(pos_items):\n",
    "            pos_score = pos_scores[i]\n",
    "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
    "\n",
    "    # Calculate per-user scores\n",
    "    sum_user_auc = 0.0\n",
    "    sum_user_recall = 0.0\n",
    "    for theuser in P[\"users\"]:\n",
    "        numerator_auc = 0.0\n",
    "        numerator_recall = 0.0\n",
    "        denominator = 0.0\n",
    "\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            if theitem not in Ni:\n",
    "                continue\n",
    "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser]))\n",
    "            # Calcolo il Recall a 30, vedi nota 6 paper\n",
    "            if Zui[(theuser, theitem)] < K:\n",
    "                numerator_recall += 1.0\n",
    "            denominator += 1 \n",
    "\n",
    "        if denominator > 0:\n",
    "            sum_user_auc += numerator_auc / denominator\n",
    "            sum_user_recall += numerator_recall / denominator\n",
    "\n",
    "    # Return\n",
    "    return {\n",
    "        \"auc\"       : sum_user_auc / nonzero_user_count,\n",
    "        \"recall\"    : sum_user_recall / nonzero_user_count,\n",
    "        \"bias\"      : -1,\n",
    "        \"concentration\" : -1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified(infilename, infilename_neg, trainfilename, gamma=1.0, K=20, partition=10, delta=0.1):\n",
    "\n",
    "    # Read pickles\n",
    "    infile = open(infilename, 'rb')\n",
    "    infile_neg = open(infilename_neg, 'rb')\n",
    "    P = pickle.load(infile)\n",
    "    infile.close()\n",
    "    P_neg = pickle.load(infile_neg)\n",
    "    infile_neg.close()\n",
    "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
    "\n",
    "    # Merge P and P_neg\n",
    "    for theuser in P[\"users\"]:\n",
    "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "\n",
    "    Zui = dict()\n",
    "    Ni = dict()\n",
    "\n",
    "    # Compute frequencies of items in the training set\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "\n",
    "    # Compute recommendations for each user\n",
    "    for theuser in P[\"users\"]:\n",
    "        all_scores = np.array(P[\"results\"][theuser])\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for i, pos_item in enumerate(pos_items):\n",
    "            pos_score = pos_scores[i]\n",
    "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
    "\n",
    "    pui = dict()\n",
    "    w = dict()\n",
    "\n",
    "    # Compute dictionary of propensity scores\n",
    "    for theuser in P[\"users\"]:\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            if theitem not in Ni:\n",
    "                continue\n",
    "            if theitem in pui:\n",
    "                continue\n",
    "            pui[theitem] = np.power(Ni[theitem], (gamma + 1) / 2.0)\n",
    "\n",
    "    # Take the list of items (not tuples) in pui sorted by value\n",
    "    items_sorted_by_value = sorted(pui, key=pui.get, reverse=True)\n",
    "\n",
    "    # Compute linspace between the pui[0] and pui[-1] \n",
    "    linspace = np.linspace(pui[items_sorted_by_value[0]], pui[items_sorted_by_value[-1]], partition+1)\n",
    "   \n",
    "    # Compute dictionary w, that is, for each item, assigns the average of the puis in the partition it belongs to\n",
    "    i=0\n",
    "    j = 0\n",
    "    while i < len(items_sorted_by_value):\n",
    "                            \n",
    "        avg = 0\n",
    "        start = i\n",
    "        end = i\n",
    "    \n",
    "        while i < len(items_sorted_by_value) and pui[items_sorted_by_value[i]] >= linspace[j+1]:\n",
    "            avg += 1.0 / pui[items_sorted_by_value[i]]\n",
    "            end = i\n",
    "            i += 1\n",
    "        avg = avg / (end - start + 1)\n",
    "\n",
    "        for k in range(start, end+1):\n",
    "            w[items_sorted_by_value[k]] = avg\n",
    "\n",
    "        j += 1\n",
    "\n",
    "    # Compute bias' numerator\n",
    "    bias = 0.0\n",
    "    for k in pui.keys():\n",
    "        # add |pui*w - 1!|\n",
    "        bias += abs(pui[k] * w[k] - 1)\n",
    "    # Multiply by number of users\n",
    "    bias *= len(P[\"users\"])\n",
    "\n",
    "    # Compute concentrations numerator (for each user)\n",
    "    concentrations = {}\n",
    "    max_w = max(w.values())\n",
    "    # ... by computing the sum of squares of w for each user\n",
    "    for user, item in zip(trainset['user_id'], trainset['item_id']):\n",
    "        # Iterate over the trainset to compute the sum of squares for each user\n",
    "        if item in w:\n",
    "            if user not in concentrations:\n",
    "                concentrations[user] = 0\n",
    "            concentrations[user] += w[item] ** 2\n",
    "    # ... and then applying the formula\n",
    "    for user in concentrations:\n",
    "        concentrations[user] = math.sqrt(concentrations[user] * 2 * math.log(2/delta)) + max_w * 7 * math.log(2/delta)\n",
    "    # Now sum all the concentrations\n",
    "    concentration = sum(concentrations.values())\n",
    "\n",
    "    # Calculate per-user scores\n",
    "    nonzero_user_count = 0\n",
    "    sum_user_auc = 0.0\n",
    "    sum_user_recall = 0.0\n",
    "    for theuser in P[\"users\"]:\n",
    "        numerator_auc = 0.0\n",
    "        numerator_recall = 0.0\n",
    "        denominator = 0.0\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            # Skip items with null frequency\n",
    "            if  theitem not in Ni:\n",
    "                continue\n",
    "            # Add things to be summed for each item\n",
    "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser])) * w[theitem]\n",
    "            # Add things for recall\n",
    "            if Zui[(theuser, theitem)] < K:\n",
    "                numerator_recall += 1.0 * w[theitem] #Â spetta\n",
    "            # Increment denominator that the sum must be divided by \n",
    "            denominator += 1 / pui[theitem]\n",
    "\n",
    "\n",
    "        # If there was at least one item for the user, count the user and sum the results\n",
    "        if denominator > 0:\n",
    "            nonzero_user_count += 1\n",
    "            sum_user_auc += numerator_auc / denominator\n",
    "            sum_user_recall += numerator_recall / denominator \n",
    "\n",
    "    # Return\n",
    "    return {\n",
    "        \"auc\"       : sum_user_auc / nonzero_user_count, \n",
    "        \"recall\"    : sum_user_recall / nonzero_user_count,\n",
    "        \"bias\"      : bias,\n",
    "        \"concentration\" : concentration\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_logspace(infilename, infilename_neg, trainfilename, gamma=1.0, K=20, partition=10, delta=0.1):\n",
    "\n",
    "    # Read pickles\n",
    "    infile = open(infilename, 'rb')\n",
    "    infile_neg = open(infilename_neg, 'rb')\n",
    "    P = pickle.load(infile)\n",
    "    infile.close()\n",
    "    P_neg = pickle.load(infile_neg)\n",
    "    infile_neg.close()\n",
    "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
    "\n",
    "    # Merge P and P_neg\n",
    "    for theuser in P[\"users\"]:\n",
    "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "\n",
    "    Zui = dict()\n",
    "    Ni = dict()\n",
    "\n",
    "    # Compute frequencies of items in the training set\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "\n",
    "    # Compute recommendations for each user\n",
    "    for theuser in P[\"users\"]:\n",
    "        all_scores = np.array(P[\"results\"][theuser])\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for i, pos_item in enumerate(pos_items):\n",
    "            pos_score = pos_scores[i]\n",
    "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
    "\n",
    "    pui = dict()\n",
    "    w = dict()\n",
    "\n",
    "    # Compute dictionary of propensity scores\n",
    "    for theuser in P[\"users\"]:\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            if theitem not in Ni:\n",
    "                continue\n",
    "            if theitem in pui:\n",
    "                continue\n",
    "            pui[theitem] = np.power(Ni[theitem], (gamma + 1) / 2.0)\n",
    "\n",
    "    # Take the list of items (not tuples) in pui sorted by value\n",
    "    items_sorted_by_value = sorted(pui, key=pui.get, reverse=True)\n",
    "\n",
    "    # Compute linspace between the pui[0] and pui[-1] \n",
    "\n",
    "    # Maybe try to split the logspace instead of the linspace?\n",
    "    logspace = np.logspace(pui[items_sorted_by_value[0]], pui[items_sorted_by_value[-1]], partition+1)\n",
    "   \n",
    "    # Compute dictionary w, that is, for each item, assigns the average of the puis in the partition it belongs to\n",
    "    i=0\n",
    "    j = 0\n",
    "    while i < len(items_sorted_by_value):\n",
    "                            \n",
    "        avg = 0\n",
    "        start = i\n",
    "        end = i\n",
    "    \n",
    "        while i < len(items_sorted_by_value) and pui[items_sorted_by_value[i]] >= logspace[j+1]:\n",
    "            avg += 1.0 / pui[items_sorted_by_value[i]]\n",
    "            end = i\n",
    "            i += 1\n",
    "        \n",
    "        # Is the average the only good choice? even with the log space split?\n",
    "        avg = avg / (end - start + 1)\n",
    "\n",
    "        for k in range(start, end+1):\n",
    "            w[items_sorted_by_value[k]] = avg\n",
    "\n",
    "        j += 1\n",
    "\n",
    "        # Compute bias' numerator\n",
    "        bias = 0.0\n",
    "        for k in pui.keys():\n",
    "            # add |pui*w - 1!|\n",
    "            bias += abs(pui[k] * w[k] - 1)\n",
    "        # Multiply by number of users\n",
    "        bias *= len(P[\"users\"])\n",
    "\n",
    "        # Compute concentrations numerator (for each user)\n",
    "        concentrations = {}\n",
    "        max_w = max(w.values())\n",
    "        # ... by computing the sum of squares of w for each user\n",
    "        for user, item in zip(trainset['user_id'], trainset['item_id']):\n",
    "            # Iterate over the trainset to compute the sum of squares for each user\n",
    "            if item in w:\n",
    "                if user not in concentrations:\n",
    "                    concentrations[user] = 0\n",
    "                concentrations[user] += w[item] ** 2\n",
    "        # ... and then applying the formula\n",
    "        for user in concentrations:\n",
    "            concentrations[user] = math.sqrt(concentrations[user] * 2 * math.log(2/delta)) + max_w * 7 * math.log(2/delta)\n",
    "        # Now sum all the concentrations\n",
    "        concentration = sum(concentrations.values())\n",
    "\n",
    "    # Compute score with AUC and compute Recall\n",
    "    nonzero_user_count = 0\n",
    "    sum_user_auc = 0.0\n",
    "    sum_user_recall = 0.0\n",
    "    for theuser in P[\"users\"]:\n",
    "        numerator_auc = 0.0\n",
    "        numerator_recall = 0.0\n",
    "        denominator = 0.0\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            # Skip items with null frequency\n",
    "            if  theitem not in Ni:\n",
    "                continue\n",
    "            # Add things to be summed for each item\n",
    "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser])) * w[theitem]\n",
    "            # Add things for recall\n",
    "            if Zui[(theuser, theitem)] < K:\n",
    "                numerator_recall += 1.0 * w[theitem] #Â spetta\n",
    "            # Increment denominator that the sum must be divided by \n",
    "            denominator += 1 / pui[theitem]\n",
    "\n",
    "\n",
    "        # If there was at least one item for the user, count the user and sum the results\n",
    "        if denominator > 0:\n",
    "            nonzero_user_count += 1\n",
    "            sum_user_auc += numerator_auc / denominator\n",
    "            sum_user_recall += numerator_recall / denominator \n",
    "\n",
    "    # Return\n",
    "    return {\n",
    "        \"auc\"       : sum_user_auc / nonzero_user_count, \n",
    "        \"recall\"    : sum_user_recall / nonzero_user_count,\n",
    "        \"bias\"      : bias,\n",
    "        \"concentration\" : concentration\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This version uses the linspace of the number of number of items used for evaluation, not of the propensities\n",
    "def stratified_2(infilename, infilename_neg, trainfilename, gamma=1.0, K=20, partition=10, delta=0.1):\n",
    "\n",
    "    # Read pickles\n",
    "    infile = open(infilename, 'rb')\n",
    "    infile_neg = open(infilename_neg, 'rb')\n",
    "    P = pickle.load(infile)\n",
    "    infile.close()\n",
    "    P_neg = pickle.load(infile_neg)\n",
    "    infile_neg.close()\n",
    "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
    "\n",
    "    # Merge P and P_neg\n",
    "    for theuser in P[\"users\"]:\n",
    "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "\n",
    "    Zui = dict()\n",
    "    Ni = dict()\n",
    "\n",
    "    # Compute frequencies of items in the training set\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "\n",
    "    # Compute recommendations for each user\n",
    "    for theuser in P[\"users\"]:\n",
    "        all_scores = np.array(P[\"results\"][theuser])\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for i, pos_item in enumerate(pos_items):\n",
    "            pos_score = pos_scores[i]\n",
    "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
    "\n",
    "    pui = dict()\n",
    "    w = dict()\n",
    "\n",
    "    # Compute dictionary of propensity scores\n",
    "    for theuser in P[\"users\"]:\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            if theitem not in Ni:\n",
    "                continue\n",
    "            if theitem in pui:\n",
    "                continue\n",
    "            pui[theitem] = np.power(Ni[theitem], (gamma + 1) / 2.0)\n",
    "\n",
    "    # Take the list of items (not tuples) in pui sorted by value\n",
    "    items_sorted_by_value = sorted(pui, key=pui.get, reverse=True)\n",
    "\n",
    "    # Compute linspace between the 0 to len(item_sorted...)\n",
    "    linspace = np.linspace(0, len(items_sorted_by_value), partition+1)\n",
    "   \n",
    "    # Compute dictionary w, that is, for each item, assigns the average of the puis in the partition it belongs to\n",
    "    i=0\n",
    "    j = 0\n",
    "    while i < len(items_sorted_by_value):\n",
    "                            \n",
    "        avg = 0\n",
    "        start = i\n",
    "        end = i\n",
    "    \n",
    "        while i < len(items_sorted_by_value) and i < linspace[j+1]:\n",
    "            avg += 1.0 / pui[items_sorted_by_value[i]]\n",
    "            end = i\n",
    "            i += 1\n",
    "        \n",
    "        avg = avg / (end - start + 1)\n",
    "\n",
    "        for k in range(start, end+1):\n",
    "            w[items_sorted_by_value[k]] = avg\n",
    "\n",
    "        j += 1\n",
    "\n",
    "    # Compute bias' numerator\n",
    "    bias = 0.0\n",
    "    for k in pui.keys():\n",
    "        # add |pui*w - 1!|\n",
    "        bias += abs(pui[k] * w[k] - 1)\n",
    "    # Multiply by number of users\n",
    "    bias *= len(P[\"users\"])\n",
    "\n",
    "    # Compute concentrations numerator (for each user)\n",
    "    concentrations = {}\n",
    "    max_w = max(w.values())\n",
    "    # ... by computing the sum of squares of w for each user\n",
    "    for user, item in zip(trainset['user_id'], trainset['item_id']):\n",
    "        # Iterate over the trainset to compute the sum of squares for each user\n",
    "        if item in w:\n",
    "            if user not in concentrations:\n",
    "                concentrations[user] = 0\n",
    "            concentrations[user] += w[item] ** 2\n",
    "    # ... and then applying the formula\n",
    "    for user in concentrations:\n",
    "        concentrations[user] = math.sqrt(concentrations[user] * 2 * math.log(2/delta)) + max_w * 7 * math.log(2/delta)\n",
    "    # Now sum all the concentrations\n",
    "    concentration = sum(concentrations.values())\n",
    "\n",
    "    # Compute score with AUC and compute Recall\n",
    "    nonzero_user_count = 0\n",
    "    sum_user_auc = 0.0\n",
    "    sum_user_recall = 0.0\n",
    "    for theuser in P[\"users\"]:\n",
    "        numerator_auc = 0.0\n",
    "        numerator_recall = 0.0\n",
    "        denominator = 0.0\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            # Skip items with null frequency\n",
    "            if  theitem not in Ni:\n",
    "                continue\n",
    "            # Add things to be summed for each item\n",
    "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser])) * w[theitem]\n",
    "            # Add things for recall\n",
    "            if Zui[(theuser, theitem)] < K:\n",
    "                numerator_recall += 1.0 * w[theitem] #Â spetta\n",
    "            # Increment denominator that the sum must be divided by \n",
    "            denominator += 1 / pui[theitem]\n",
    "\n",
    "\n",
    "        # If there was at least one item for the user, count the user and sum the results\n",
    "        if denominator > 0:\n",
    "            nonzero_user_count += 1\n",
    "            sum_user_auc += numerator_auc / denominator\n",
    "            sum_user_recall += numerator_recall / denominator \n",
    "\n",
    "    return {\n",
    "        \"auc\"       : sum_user_auc / nonzero_user_count, \n",
    "        \"recall\"    : sum_user_recall / nonzero_user_count,\n",
    "        \"bias\"      : bias,\n",
    "        \"concentration\" : concentration\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **EVALUATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "raw_data = dict()\n",
    "raw_data['train_data'] = np.load(output_name + \"training_arr.npy\")\n",
    "raw_data['test_data_pos_biased'] = np.load(output_name + \"biased-test_arr_pos.npy\")\n",
    "raw_data['test_data_neg_biased'] = np.load(output_name + \"biased-test_arr_neg.npy\")\n",
    "raw_data['test_data_pos_unbiased'] = np.load(output_name + \"unbiased-test_arr_pos.npy\")\n",
    "raw_data['test_data_neg_unbiased'] = np.load(output_name + \"unbiased-test_arr_neg.npy\")\n",
    "raw_data['max_user'] = 290\n",
    "raw_data['max_item'] = 300\n",
    "batch_size = 8000\n",
    "test_batch_size = 1000\n",
    "display_itr = 1000\n",
    "\n",
    "# Load data\n",
    "train_dataset = ImplicitDataset(raw_data['train_data'], raw_data['max_user'], raw_data['max_item'], name='Train')\n",
    "test_dataset_pos_biased = ImplicitDataset(raw_data['test_data_pos_biased'], raw_data['max_user'], raw_data['max_item'])\n",
    "test_dataset_neg_biased = ImplicitDataset(raw_data['test_data_neg_biased'], raw_data['max_user'], raw_data['max_item'])\n",
    "test_dataset_pos_unbiased = ImplicitDataset(raw_data['test_data_pos_unbiased'], raw_data['max_user'], raw_data['max_item'])\n",
    "test_dataset_neg_unbiased = ImplicitDataset(raw_data['test_data_neg_unbiased'], raw_data['max_user'], raw_data['max_item'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./generated_data/cml-coat/\n",
      "[Subsampling negative items]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    }
   ],
   "source": [
    "# Prevent tensorflow from using cached embeddings\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "# Define the model\n",
    "model = MODEL_CLASS(batch_size=batch_size, max_user=train_dataset.max_user(), max_item=train_dataset.max_item(), dim_embed=50, l2_reg=0.001, opt='Adam', sess_config=None)\n",
    "sampler = PairwiseSampler(batch_size=batch_size, dataset=train_dataset, num_process=4)\n",
    "model_trainer = ImplicitModelTrainer(batch_size=batch_size, test_batch_size=test_batch_size, train_dataset=train_dataset, model=model, sampler=sampler, eval_save_prefix=OUTPUT_PATH + DATASET_NAME, item_serving_size=500)\n",
    "auc_evaluator = AUC()\n",
    "\n",
    "# Load model\n",
    "model.load(OUTPUT_PATH)\n",
    "\n",
    "# Set parameters\n",
    "model_trainer._eval_manager = ImplicitEvalManager(evaluators=[auc_evaluator])\n",
    "model_trainer._num_negatives = 60 # in yahoo they were 200 on 1000 items, so let's keep a 1/5 ratio on 300 items\n",
    "model_trainer._exclude_positives([train_dataset, test_dataset_pos_biased, test_dataset_neg_biased])\n",
    "model_trainer._sample_negatives(seed=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biased Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 231/231 [00:00<00:00, 1725.73it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 290/290 [00:01<00:00, 270.18it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AUC': [0.5265503875968993,\n",
       "  0.5900749063670412,\n",
       "  0.44356060606060604,\n",
       "  0.45378787878787885,\n",
       "  0.47659176029962536,\n",
       "  0.5618773946360153,\n",
       "  0.4011363636363636,\n",
       "  0.5239700374531835,\n",
       "  0.500766283524904,\n",
       "  0.5718390804597702,\n",
       "  0.5188888888888888,\n",
       "  0.4957364341085272,\n",
       "  0.5272222222222223,\n",
       "  0.5503745318352059,\n",
       "  0.5527777777777778,\n",
       "  0.5543071161048688,\n",
       "  0.507962962962963,\n",
       "  0.4275280898876404,\n",
       "  0.4767045454545455,\n",
       "  0.48183520599250934,\n",
       "  0.5218992248062017,\n",
       "  0.5155038759689923,\n",
       "  0.5100000000000001,\n",
       "  0.48033707865168546,\n",
       "  0.46329365079365076,\n",
       "  0.42827715355805246,\n",
       "  0.45191570881226056,\n",
       "  0.48689138576779023,\n",
       "  0.548689138576779,\n",
       "  0.5335205992509363,\n",
       "  0.5372549019607844,\n",
       "  0.5235955056179775,\n",
       "  0.5003787878787879,\n",
       "  0.5196296296296297,\n",
       "  0.5271535580524345,\n",
       "  0.5096590909090909,\n",
       "  0.5965909090909092,\n",
       "  0.416851851851852,\n",
       "  0.581992337164751,\n",
       "  0.4886274509803921,\n",
       "  0.4691011235955056,\n",
       "  0.5051136363636364,\n",
       "  0.5034883720930233,\n",
       "  0.3691011235955056,\n",
       "  0.5220973782771535,\n",
       "  0.4925093632958801,\n",
       "  0.5285185185185185,\n",
       "  0.54515503875969,\n",
       "  0.548876404494382,\n",
       "  0.5201550387596899,\n",
       "  0.5022222222222221,\n",
       "  0.467816091954023,\n",
       "  0.4386973180076628,\n",
       "  0.44213483146067417,\n",
       "  0.4496031746031746,\n",
       "  0.5577651515151515,\n",
       "  0.4433712121212121,\n",
       "  0.4790740740740741,\n",
       "  0.47253787878787884,\n",
       "  0.4434456928838951,\n",
       "  0.53125,\n",
       "  0.4712962962962963,\n",
       "  0.45449438202247194,\n",
       "  0.48215686274509806,\n",
       "  0.6183333333333334,\n",
       "  0.4389513108614232,\n",
       "  0.506201550387597,\n",
       "  0.5325925925925926,\n",
       "  0.5333333333333333,\n",
       "  0.449250936329588,\n",
       "  0.4840996168582375,\n",
       "  0.538627450980392,\n",
       "  0.4248148148148148,\n",
       "  0.49753787878787875,\n",
       "  0.3972222222222222,\n",
       "  0.45243445692883894,\n",
       "  0.551123595505618,\n",
       "  0.46893939393939393,\n",
       "  0.4816287878787879,\n",
       "  0.479563492063492,\n",
       "  0.4485185185185186,\n",
       "  0.5467871485943777,\n",
       "  0.5151851851851853,\n",
       "  0.5635185185185184,\n",
       "  0.5460227272727273,\n",
       "  0.4136704119850187,\n",
       "  0.6172619047619048,\n",
       "  0.4943820224719101,\n",
       "  0.547093023255814,\n",
       "  0.5368518518518519,\n",
       "  0.4044943820224719,\n",
       "  0.5408239700374532,\n",
       "  0.5706439393939394,\n",
       "  0.5568181818181818,\n",
       "  0.44419475655430707,\n",
       "  0.5327715355805244,\n",
       "  0.4277153558052435,\n",
       "  0.49337121212121215,\n",
       "  0.46985018726591765,\n",
       "  0.4250936329588014,\n",
       "  0.5155555555555557,\n",
       "  0.41969696969696973,\n",
       "  0.5138888888888888,\n",
       "  0.4780392156862745,\n",
       "  0.46628787878787875,\n",
       "  0.6521072796934866,\n",
       "  0.4377394636015326,\n",
       "  0.5729166666666667,\n",
       "  0.5249063670411985,\n",
       "  0.550392156862745,\n",
       "  0.4720930232558138,\n",
       "  0.4798148148148148,\n",
       "  0.41966292134831457,\n",
       "  0.5019157088122604,\n",
       "  0.5267790262172285,\n",
       "  0.5024621212121212,\n",
       "  0.5982558139534884,\n",
       "  0.4450191570881226,\n",
       "  0.5166666666666667,\n",
       "  0.5181818181818183,\n",
       "  0.5919475655430712,\n",
       "  0.5274074074074074,\n",
       "  0.5038314176245209,\n",
       "  0.5581481481481483,\n",
       "  0.4747126436781608,\n",
       "  0.497191011235955,\n",
       "  0.48914728682170544,\n",
       "  0.5856321839080461,\n",
       "  0.4802681992337163,\n",
       "  0.4913793103448275,\n",
       "  0.5242424242424243,\n",
       "  0.5101123595505619,\n",
       "  0.5729411764705884,\n",
       "  0.4864341085271318,\n",
       "  0.4849206349206349,\n",
       "  0.5130268199233716,\n",
       "  0.4971264367816091,\n",
       "  0.448501872659176,\n",
       "  0.48183520599250934,\n",
       "  0.5625468164794007,\n",
       "  0.3931726907630522,\n",
       "  0.47196969696969704,\n",
       "  0.5560606060606061,\n",
       "  0.5597378277153559,\n",
       "  0.47267441860465115,\n",
       "  0.55,\n",
       "  0.5155555555555555,\n",
       "  0.5525925925925926,\n",
       "  0.48295454545454547,\n",
       "  0.48238636363636367,\n",
       "  0.4897003745318351,\n",
       "  0.5388257575757577,\n",
       "  0.5201851851851852,\n",
       "  0.552621722846442,\n",
       "  0.5159003831417626,\n",
       "  0.49962962962962965,\n",
       "  0.5080524344569288,\n",
       "  0.5140562248995985,\n",
       "  0.5431818181818182,\n",
       "  0.45037037037037037,\n",
       "  0.5211111111111111,\n",
       "  0.5664814814814816,\n",
       "  0.6570370370370371,\n",
       "  0.48662790697674413,\n",
       "  0.452996254681648,\n",
       "  0.44812734082397004,\n",
       "  0.4259469696969697,\n",
       "  0.40574712643678157,\n",
       "  0.4548689138576779,\n",
       "  0.49166666666666664,\n",
       "  0.6100378787878789,\n",
       "  0.6124521072796936,\n",
       "  0.4533333333333332,\n",
       "  0.48823529411764716,\n",
       "  0.4882575757575758,\n",
       "  0.4588014981273408,\n",
       "  0.5259259259259259,\n",
       "  0.4990636704119849,\n",
       "  0.4639846743295019,\n",
       "  0.44846743295019154,\n",
       "  0.45156862745098036,\n",
       "  0.512310606060606,\n",
       "  0.4666666666666667,\n",
       "  0.47126436781609204,\n",
       "  0.4660984848484848,\n",
       "  0.48055555555555557,\n",
       "  0.4544444444444445,\n",
       "  0.4214814814814815,\n",
       "  0.5848148148148148,\n",
       "  0.6172348484848486,\n",
       "  0.5184108527131783,\n",
       "  0.5106060606060606,\n",
       "  0.5796296296296297,\n",
       "  0.5666666666666667,\n",
       "  0.4907407407407408,\n",
       "  0.598876404494382,\n",
       "  0.49850187265917606,\n",
       "  0.4768939393939393,\n",
       "  0.4268199233716476,\n",
       "  0.45484496124031,\n",
       "  0.49507575757575756,\n",
       "  0.48071161048689137,\n",
       "  0.43486590038314177,\n",
       "  0.43820224719101125,\n",
       "  0.5437500000000001,\n",
       "  0.41179775280898884,\n",
       "  0.5538888888888889,\n",
       "  0.4848314606741573,\n",
       "  0.47962962962962963,\n",
       "  0.4074509803921569,\n",
       "  0.47840909090909084,\n",
       "  0.41761363636363635,\n",
       "  0.4956439393939394,\n",
       "  0.5854651162790698,\n",
       "  0.4240310077519379,\n",
       "  0.4941947565543071,\n",
       "  0.46348314606741564,\n",
       "  0.4434456928838951,\n",
       "  0.55,\n",
       "  0.4723484848484848,\n",
       "  0.45203703703703707,\n",
       "  0.4625468164794007,\n",
       "  0.5815261044176707,\n",
       "  0.6047348484848485,\n",
       "  0.534280303030303,\n",
       "  0.5310861423220974,\n",
       "  0.5070075757575758,\n",
       "  0.5125000000000002,\n",
       "  0.5414772727272728,\n",
       "  0.4986742424242424,\n",
       "  0.5369318181818182,\n",
       "  0.46629213483146065,\n",
       "  0.5646067415730337,\n",
       "  0.4846590909090909,\n",
       "  0.584469696969697,\n",
       "  0.5114341085271318,\n",
       "  0.5895833333333333,\n",
       "  0.48544061302681996,\n",
       "  0.5581439393939394,\n",
       "  0.4978927203065134,\n",
       "  0.4698412698412698,\n",
       "  0.5436329588014981,\n",
       "  0.5057471264367817,\n",
       "  0.47253787878787873,\n",
       "  0.5367041198501873,\n",
       "  0.5069767441860464,\n",
       "  0.4642045454545455,\n",
       "  0.4835205992509363,\n",
       "  0.49703703703703705,\n",
       "  0.4760299625468165,\n",
       "  0.5066666666666667,\n",
       "  0.4409961685823755,\n",
       "  0.47870370370370363,\n",
       "  0.46370370370370373,\n",
       "  0.43444444444444447,\n",
       "  0.5293103448275862,\n",
       "  0.5059925093632959,\n",
       "  0.4744318181818182,\n",
       "  0.4435185185185185,\n",
       "  0.5405555555555556,\n",
       "  0.4057407407407408,\n",
       "  0.41003787878787873,\n",
       "  0.4636363636363636,\n",
       "  0.5615530303030304,\n",
       "  0.4572796934865901,\n",
       "  0.5267045454545455,\n",
       "  0.5249063670411986,\n",
       "  0.4044943820224719,\n",
       "  0.44961240310077527,\n",
       "  0.6580524344569288,\n",
       "  0.5382575757575758,\n",
       "  0.5481060606060606,\n",
       "  0.512310606060606,\n",
       "  0.5085271317829457,\n",
       "  0.4505747126436781,\n",
       "  0.4763257575757576,\n",
       "  0.4384469696969697,\n",
       "  0.6170498084291189,\n",
       "  0.44240740740740747,\n",
       "  0.5133333333333334,\n",
       "  0.39981481481481485,\n",
       "  0.38707865168539324,\n",
       "  0.5054307116104869,\n",
       "  0.5053030303030304,\n",
       "  0.46382575757575756,\n",
       "  0.5124521072796935,\n",
       "  0.4724206349206349,\n",
       "  0.46800766283524914,\n",
       "  0.47528089887640457,\n",
       "  0.47407407407407404]}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_trainer._eval_save_prefix = OUTPUT_PREFIX + \"-test-pos-biased\"\n",
    "model_trainer._evaluate_partial(test_dataset_pos_biased)\n",
    "\n",
    "model_trainer._eval_save_prefix = OUTPUT_PREFIX +  \"-test-neg-biased\"\n",
    "model_trainer._evaluate_partial(test_dataset_neg_biased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unbiased Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 237/237 [00:00<00:00, 1222.06it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 290/290 [00:00<00:00, 936.06it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AUC': [0.5533333333333333,\n",
       "  0.5357142857142858,\n",
       "  0.6000000000000001,\n",
       "  0.49102564102564106,\n",
       "  0.4243589743589744,\n",
       "  0.5988095238095238,\n",
       "  0.4744444444444445,\n",
       "  0.49583333333333335,\n",
       "  0.42500000000000004,\n",
       "  0.4948717948717949,\n",
       "  0.4358974358974359,\n",
       "  0.42833333333333323,\n",
       "  0.5190476190476191,\n",
       "  0.5666666666666667,\n",
       "  0.4844444444444444,\n",
       "  0.6369047619047619,\n",
       "  0.5208333333333334,\n",
       "  0.3178571428571429,\n",
       "  0.5750000000000001,\n",
       "  0.6100000000000001,\n",
       "  0.5155555555555555,\n",
       "  0.5361111111111111,\n",
       "  0.32291666666666663,\n",
       "  0.6083333333333333,\n",
       "  0.49090909090909085,\n",
       "  0.40238095238095234,\n",
       "  0.5870370370370371,\n",
       "  0.5144444444444444,\n",
       "  0.5928571428571429,\n",
       "  0.4714285714285714,\n",
       "  0.4272727272727273,\n",
       "  0.5178571428571429,\n",
       "  0.4644444444444444,\n",
       "  0.5212121212121213,\n",
       "  0.5047619047619046,\n",
       "  0.46481481481481485,\n",
       "  0.5680555555555556,\n",
       "  0.4395833333333333,\n",
       "  0.44000000000000006,\n",
       "  0.39,\n",
       "  0.3208333333333333,\n",
       "  0.4782051282051282,\n",
       "  0.5911111111111113,\n",
       "  0.5255555555555556,\n",
       "  0.5266666666666667,\n",
       "  0.5322222222222222,\n",
       "  0.4845238095238096,\n",
       "  0.5773809523809524,\n",
       "  0.5287878787878788,\n",
       "  0.495,\n",
       "  0.41190476190476194,\n",
       "  0.4962962962962963,\n",
       "  0.5333333333333333,\n",
       "  0.6077777777777776,\n",
       "  0.38833333333333336,\n",
       "  0.5133333333333333,\n",
       "  0.36363636363636354,\n",
       "  0.5511111111111112,\n",
       "  0.4988888888888889,\n",
       "  0.3588888888888889,\n",
       "  0.5575757575757575,\n",
       "  0.43854166666666666,\n",
       "  0.4645833333333333,\n",
       "  0.41666666666666663,\n",
       "  0.6773809523809523,\n",
       "  0.3202380952380953,\n",
       "  0.5407407407407407,\n",
       "  0.66875,\n",
       "  0.4476190476190477,\n",
       "  0.4871794871794871,\n",
       "  0.43787878787878787,\n",
       "  0.5208333333333333,\n",
       "  0.558888888888889,\n",
       "  0.36388888888888893,\n",
       "  0.3044444444444444,\n",
       "  0.518888888888889,\n",
       "  0.5702380952380952,\n",
       "  0.4846153846153846,\n",
       "  0.5761904761904761,\n",
       "  0.3366666666666667,\n",
       "  0.5555555555555556,\n",
       "  0.5347222222222222,\n",
       "  0.6871794871794871,\n",
       "  0.2924242424242424,\n",
       "  0.5476190476190477,\n",
       "  0.35520833333333335,\n",
       "  0.4880952380952381,\n",
       "  0.5358974358974359,\n",
       "  0.6452380952380953,\n",
       "  0.49743589743589745,\n",
       "  0.5564102564102564,\n",
       "  0.5447916666666666,\n",
       "  0.45999999999999996,\n",
       "  0.558888888888889,\n",
       "  0.47604166666666664,\n",
       "  0.5311111111111112,\n",
       "  0.33958333333333335,\n",
       "  0.41944444444444445,\n",
       "  0.40208333333333335,\n",
       "  0.3233333333333333,\n",
       "  0.5208333333333334,\n",
       "  0.3979166666666667,\n",
       "  0.5388888888888889,\n",
       "  0.5910256410256409,\n",
       "  0.575,\n",
       "  0.5095238095238094,\n",
       "  0.35,\n",
       "  0.6424242424242425,\n",
       "  0.48020833333333335,\n",
       "  0.5,\n",
       "  0.6115384615384616,\n",
       "  0.5192307692307693,\n",
       "  0.36979166666666663,\n",
       "  0.4900000000000001,\n",
       "  0.6756410256410257,\n",
       "  0.45454545454545453,\n",
       "  0.5222222222222223,\n",
       "  0.4986111111111111,\n",
       "  0.4864583333333333,\n",
       "  0.501388888888889,\n",
       "  0.7263888888888889,\n",
       "  0.5885416666666666,\n",
       "  0.4444444444444445,\n",
       "  0.5452380952380952,\n",
       "  0.39888888888888896,\n",
       "  0.5243589743589744,\n",
       "  0.78,\n",
       "  0.4916666666666667,\n",
       "  0.6375,\n",
       "  0.5282051282051282,\n",
       "  0.4714285714285715,\n",
       "  0.3874999999999999,\n",
       "  0.5511111111111112,\n",
       "  0.5076923076923077,\n",
       "  0.5128205128205129,\n",
       "  0.44833333333333336,\n",
       "  0.49999999999999994,\n",
       "  0.4947916666666667,\n",
       "  0.4166666666666667,\n",
       "  0.4964285714285714,\n",
       "  0.4987179487179488,\n",
       "  0.5111111111111112,\n",
       "  0.5059523809523808,\n",
       "  0.494047619047619,\n",
       "  0.4916666666666667,\n",
       "  0.43717948717948707,\n",
       "  0.5895833333333332,\n",
       "  0.5010416666666666,\n",
       "  0.48333333333333334,\n",
       "  0.5291666666666667,\n",
       "  0.5583333333333332,\n",
       "  0.4375,\n",
       "  0.5466666666666667,\n",
       "  0.49523809523809526,\n",
       "  0.5129629629629631,\n",
       "  0.4466666666666666,\n",
       "  0.5571428571428572,\n",
       "  0.37833333333333335,\n",
       "  0.5,\n",
       "  0.369047619047619,\n",
       "  0.5145833333333333,\n",
       "  0.4035714285714285,\n",
       "  0.6375,\n",
       "  0.37121212121212116,\n",
       "  0.5606060606060606,\n",
       "  0.453125,\n",
       "  0.3345238095238095,\n",
       "  0.48020833333333335,\n",
       "  0.391111111111111,\n",
       "  0.5244444444444444,\n",
       "  0.6366666666666666,\n",
       "  0.5266666666666666,\n",
       "  0.3877777777777778,\n",
       "  0.558974358974359,\n",
       "  0.5145833333333334,\n",
       "  0.5947916666666666,\n",
       "  0.5,\n",
       "  0.33055555555555555,\n",
       "  0.39404761904761904,\n",
       "  0.3222222222222222,\n",
       "  0.4818181818181818,\n",
       "  0.42555555555555563,\n",
       "  0.5263888888888889,\n",
       "  0.4371794871794872,\n",
       "  0.4192307692307693,\n",
       "  0.3733333333333333,\n",
       "  0.4444444444444444,\n",
       "  0.6277777777777778,\n",
       "  0.45729166666666665,\n",
       "  0.5083333333333333,\n",
       "  0.6282051282051282,\n",
       "  0.6133333333333333,\n",
       "  0.59375,\n",
       "  0.3722222222222222,\n",
       "  0.5282051282051282,\n",
       "  0.48020833333333335,\n",
       "  0.4354166666666667,\n",
       "  0.5012820512820513,\n",
       "  0.4392857142857144,\n",
       "  0.3541666666666667,\n",
       "  0.41794871794871796,\n",
       "  0.4152777777777778,\n",
       "  0.6166666666666667,\n",
       "  0.4277777777777778,\n",
       "  0.4788888888888889,\n",
       "  0.37222222222222223,\n",
       "  0.56,\n",
       "  0.5895833333333333,\n",
       "  0.5630952380952381,\n",
       "  0.3976190476190476,\n",
       "  0.41785714285714287,\n",
       "  0.41923076923076924,\n",
       "  0.47575757575757577,\n",
       "  0.45641025641025645,\n",
       "  0.2574074074074074,\n",
       "  0.5958333333333333,\n",
       "  0.44270833333333337,\n",
       "  0.4273809523809523,\n",
       "  0.525,\n",
       "  0.43809523809523815,\n",
       "  0.3458333333333333,\n",
       "  0.6288888888888889,\n",
       "  0.175,\n",
       "  0.5523809523809524,\n",
       "  0.55,\n",
       "  0.5320512820512822,\n",
       "  0.6948717948717948,\n",
       "  0.6305555555555555,\n",
       "  0.6729166666666667,\n",
       "  0.6354166666666666,\n",
       "  0.590625,\n",
       "  0.3888888888888888,\n",
       "  0.5428571428571428,\n",
       "  0.5577777777777778,\n",
       "  0.5466666666666667,\n",
       "  0.5520833333333334,\n",
       "  0.5444444444444444,\n",
       "  0.4033333333333333,\n",
       "  0.5358974358974359,\n",
       "  0.5202380952380953,\n",
       "  0.48,\n",
       "  0.5433333333333333,\n",
       "  0.5,\n",
       "  0.43571428571428567,\n",
       "  0.453125,\n",
       "  0.6583333333333332,\n",
       "  0.38,\n",
       "  0.46481481481481485,\n",
       "  0.5533333333333333,\n",
       "  0.48333333333333334,\n",
       "  0.6155555555555556,\n",
       "  0.4461538461538461,\n",
       "  0.4145833333333333,\n",
       "  0.46904761904761905,\n",
       "  0.4041666666666667,\n",
       "  0.4458333333333333,\n",
       "  0.5651515151515152,\n",
       "  0.49861111111111117,\n",
       "  0.3947916666666667,\n",
       "  0.5479166666666666,\n",
       "  0.2604166666666667,\n",
       "  0.45128205128205134,\n",
       "  0.3880952380952381,\n",
       "  0.5803030303030302,\n",
       "  0.5074074074074074,\n",
       "  0.41923076923076924,\n",
       "  0.3772727272727273,\n",
       "  0.362962962962963,\n",
       "  0.38,\n",
       "  0.6052083333333333,\n",
       "  0.43333333333333335,\n",
       "  0.573611111111111,\n",
       "  0.4430555555555557,\n",
       "  0.4038461538461539,\n",
       "  0.37179487179487186,\n",
       "  0.5107142857142857,\n",
       "  0.45333333333333337,\n",
       "  0.5583333333333333,\n",
       "  0.4233333333333334,\n",
       "  0.5708333333333333,\n",
       "  0.39555555555555555,\n",
       "  0.4345238095238095,\n",
       "  0.6151515151515151,\n",
       "  0.5012820512820514,\n",
       "  0.4755555555555555,\n",
       "  0.325,\n",
       "  0.4222222222222222,\n",
       "  0.5033333333333333,\n",
       "  0.36458333333333337,\n",
       "  0.53125]}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_trainer._eval_save_prefix = OUTPUT_PREFIX + \"-test-pos-unbiased\"\n",
    "model_trainer._evaluate_partial(test_dataset_pos_unbiased)\n",
    "\n",
    "model_trainer._eval_save_prefix = OUTPUT_PREFIX +  \"-test-neg-unbiased\"\n",
    "model_trainer._evaluate_partial(test_dataset_neg_unbiased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Propensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "propensities = calculate_propensities(290,300, output_name+\"training_arr.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.09453009, ..., 0.19708824, 0.04991823,\n",
       "        0.04991823],\n",
       "       [1.        , 0.        , 0.09453009, ..., 0.19708824, 0.04991823,\n",
       "        0.04991823],\n",
       "       [1.        , 0.        , 0.09453009, ..., 0.19708824, 0.04991823,\n",
       "        0.04991823],\n",
       "       ...,\n",
       "       [1.        , 0.        , 0.09453009, ..., 0.19708824, 0.04991823,\n",
       "        0.04991823],\n",
       "       [1.        , 0.        , 0.09453009, ..., 0.19708824, 0.04991823,\n",
       "        0.04991823],\n",
       "       [1.        , 0.        , 0.09453009, ..., 0.19708824, 0.04991823,\n",
       "        0.04991823]])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "propensities[1.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute AOA and unbiased evaluator metrics with biased testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "biased_results = dict()\n",
    "\n",
    "# biased_results[\"STRATIFIED_15\"] = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=1.5, K=20, partition=100)\n",
    "biased_results[\"AOA\"] = aoa(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", K=20)\n",
    "biased_results[\"UB_15\"] = eq(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=1.5, K=20)\n",
    "biased_results[\"UB_2\"] =  eq(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2, K=20)\n",
    "biased_results[\"UB_25\"] =  eq(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2.5, K=20)\n",
    "biased_results[\"UB_3\"] =  eq(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=3, K=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute AOA and unbiased evaluator metrics with unbiased testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbiased_results = dict()\n",
    "\n",
    "# unbiased_results[\"STRATIFIED_15\"] = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=1.5, K=4, partition=100)\n",
    "unbiased_results[\"AOA\"] = aoa(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", K=4)\n",
    "unbiased_results[\"UB_15\"] = eq(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=1.5, K=4)\n",
    "unbiased_results[\"UB_2\"] =  eq(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2, K=4)\n",
    "unbiased_results[\"UB_25\"] =  eq(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2.5, K=4)\n",
    "unbiased_results[\"UB_3\"] =  eq(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=3, K=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([192, 274,  62,  90, 150, 228, 162, 184, 249, 126,  95, 288, 163,\n",
       "       117,  26, 205, 171, 214, 210,  49, 236, 168, 276,  38, 189, 250,\n",
       "       123, 299,  67, 277, 278, 283, 222, 141, 176, 213,  94, 231, 110,\n",
       "        93, 134,  81,  20, 257, 137, 208, 161, 287,  68, 140, 127, 291,\n",
       "         8, 146, 155, 202,  40, 174,  45, 198, 139, 217, 203,  87, 247,\n",
       "        66,  76, 120, 193, 170, 221,  27, 116, 285, 215,  88,  72, 275,\n",
       "         7,  24, 240,  84, 122, 248, 220,  11, 166, 252, 244, 261,  69,\n",
       "       279, 263,  60, 280, 233, 230,  59, 100, 177,  77, 169,  46,  33,\n",
       "        61,  41, 253,  39, 282, 284,  98,  82,  96, 154, 188, 207, 256,\n",
       "        17, 136, 129, 152,  10, 199, 183, 260,  97,  16,  15,  80,  12,\n",
       "        34,  28, 224, 156,  53, 209, 294, 245, 235, 179,  14, 219,  65,\n",
       "       106,  50,  13, 178,  48, 242,  23, 153, 149, 112, 218, 238, 147,\n",
       "        47, 225, 157, 124,  89,  74,  56, 237, 115,   3, 138,  52, 119,\n",
       "       243, 111,  57,  54,  86, 298, 107, 158, 216, 196, 258,   2,  58,\n",
       "       164,  99, 271, 190, 187,  71, 135,  63, 295, 267, 104, 292, 239,\n",
       "       131, 255, 165, 173,  29,  91, 148, 265, 286, 262, 172,  73, 197,\n",
       "       206,   4, 113, 182, 201, 133,   1, 191, 259, 281, 118, 254,   9,\n",
       "       128, 109,  18, 142, 121,  70, 160,  32, 272, 296,  64, 143,  37,\n",
       "       251,  22, 159, 273,  31,  55, 266, 130,  35,  36, 297, 268, 211,\n",
       "       186,  19,  51, 290, 289, 212, 132,  21, 232, 200,  30, 103, 151,\n",
       "       270, 229, 226, 102, 144, 264, 246, 108,  43, 204,  79,  78,  75,\n",
       "       227,   6, 125, 269,  25,   5, 175, 293,  44, 114, 234, 195, 223,\n",
       "       101, 105, 180, 167, 241, 185, 145, 300,  42,  92,  85,  83, 194,\n",
       "       181])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get number of items\n",
    "num_items = 300\n",
    "\n",
    "# Get the n_p partitions\n",
    "n_p = 300\n",
    "nums = np.arange(1, num_items+1)\n",
    "partitions = np.random.choice(nums, n_p, replace=False)\n",
    "\n",
    "# Visualize\n",
    "partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the partition which minimizes the sum of AUC and Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f2cf64f15d2459f8741c33fb041d3b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute biased and unbiased results with stratified for each partition\n",
    "# and store biased and unbiased results such that the sum of AUC and Recall is minimized\n",
    "\n",
    "# Value of gamma to use for minimization\n",
    "gamma = 1.5\n",
    "\n",
    "# To print :)\n",
    "key = \"STRATIFIED_\" + str(gamma).replace(\".\",\"\")\n",
    "\n",
    "# Initialize results\n",
    "unbiased_results[key] = dict()\n",
    "biased_results[key] = dict()\n",
    "best_partition = np.random.choice(nums, 1)[0]\n",
    "\n",
    "history = np.empty(300)\n",
    "\n",
    "\n",
    "# For each partition\n",
    "for p in tqdm(partitions):\n",
    "    # Compute the results (AUC and Recall) for both biased and unbiased test sets\n",
    "    temp_unbiased = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=gamma, K=4, partition=p)\n",
    "    temp_biased = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=gamma, K=20, partition=p)\n",
    "    # If first iteration...\n",
    "    if not unbiased_results[key]:\n",
    "        unbiased_results[key] = temp_unbiased\n",
    "    if not biased_results[key]:\n",
    "        biased_results[key] = temp_biased\n",
    "    # Else if a better partition was found, update the results\n",
    "    elif temp_unbiased['bias'] + temp_unbiased['concentration'] + temp_biased['bias'] + temp_biased['concentration'] < biased_results[key]['bias'] + biased_results[key]['concentration'] + unbiased_results[key]['bias'] + unbiased_results[key]['concentration']:\n",
    "        biased_results[key]['auc'] = temp_biased['auc']\n",
    "        biased_results[key]['recall'] = temp_biased['recall']\n",
    "        biased_results[key]['bias'] = temp_biased['bias']\n",
    "        biased_results[key]['concentration'] = temp_biased['concentration']\n",
    "        unbiased_results[key]['auc'] = temp_unbiased['auc']\n",
    "        unbiased_results[key]['recall'] = temp_unbiased['recall']\n",
    "        biased_results[key]['bias'] = temp_biased['bias']\n",
    "        biased_results[key]['concentration'] = temp_biased['concentration']\n",
    "        best_partition = p\n",
    "    #print(temp_unbiased['bias'], temp_biased['bias'], temp_unbiased['concentration'], temp_biased['concentration'])\n",
    "    history[p-1] = temp_unbiased['bias'] + temp_unbiased['concentration'] + temp_biased['bias'] + temp_biased['concentration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAj0AAAHFCAYAAAAZuEjoAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA4h0lEQVR4nO3deXxU5d3///dAFpKQBELIMhACt4ZFEq0sgkhJ2EHDft+AoCwGq2UplHCraPkSWgsIt4A3KtoWAogstSK10opBFqEBC0EFLCJqWJMYgZAQlklIrt8f/pjbIWELA5nkvJ6Px3k8Mte5zjmfKyfDvDlznRmbMcYIAACgmqtR2QUAAADcCYQeAABgCYQeAABgCYQeAABgCYQeAABgCYQeAABgCYQeAABgCYQeAABgCYQeAABgCYQeoApaunSpbDabdu/eXe76xMRENW7c2KWtcePGGjVq1E0dJz09XSkpKTpz5kzFCrWgNWvWqGXLlvLz85PNZtPnn39ebr8tW7bIZrM5Fx8fH9WvX18PPfSQXnjhBR05cqTCNWRlZSklJeWqxwasitADWMR7772nadOm3dQ26enpmjFjBqHnBv3www96/PHHddddd+nDDz/Ujh071LRp02tuM3PmTO3YsUObN2/W4sWLlZCQoCVLlqhFixZ6++23K1RHVlaWZsyYQegBruBV2QUAuDPuv//+yi7hphUXF8tms8nLq2r8U/X111+ruLhYjz32mOLj429om5iYGLVv3975uG/fvkpOTla3bt00atQo3XvvvYqLi7tdJQOWwpUewCKufHurtLRUL774opo1ayY/Pz/VqVNH9957r1555RVJUkpKiv77v/9bktSkSRPn2zBbtmxxbj9nzhw1b95cvr6+CgsL04gRI3T8+HGX4xpjNHPmTEVHR6tWrVpq06aN0tLSlJCQoISEBGe/y2/3vPXWW0pOTlaDBg3k6+urb775Rj/88IPGjh2re+65R7Vr11ZYWJi6dOmibdu2uRzr8OHDstlsmjt3rl566SU1btxYfn5+SkhIcAaS5557Tna7XcHBwRowYIByc3Nv6Pf3/vvv68EHH5S/v78CAwPVvXt37dixw7l+1KhR6tixoyRpyJAhstlsLuO7GSEhIXrzzTd16dIlzZ8/39n+zTffaPTo0YqJiZG/v78aNGigPn36aN++fS6/x7Zt20qSRo8e7TxvKSkpkqTdu3dr6NChzt9N48aN9eijj97S22lAVVE1/vsEoFwlJSW6dOlSmXZjzHW3nTNnjlJSUvSb3/xGnTp1UnFxsb766ivnW1ljxozR6dOntXDhQq1du1aRkZGSpHvuuUeS9Mtf/lJ/+MMfNH78eCUmJurw4cOaNm2atmzZoj179ig0NFSS9MILL2jWrFn6xS9+oYEDB+rYsWMaM2aMiouLy33rZ+rUqXrwwQf1xhtvqEaNGgoLC9MPP/wgSZo+fboiIiJUWFio9957TwkJCfr444/LhIvXXntN9957r1577TWdOXNGycnJ6tOnj9q1aydvb28tWbJER44c0ZQpUzRmzBi9//771/xdrVy5UsOHD1ePHj20atUqORwOzZkzx3n8jh07atq0aXrggQc0btw4zZw5U507d1ZQUNB1z8PVtG3bVpGRkfrkk0+cbVlZWapXr55mz56t+vXr6/Tp01q2bJnatWunzz77TM2aNVOrVq2Umpqq0aNH6ze/+Y0eeeQRSVLDhg0l/RgMmzVrpqFDhyokJETZ2dlatGiR2rZtq3//+9/O8wZUSwZAlZOammokXXOJjo522SY6OtqMHDnS+TgxMdH87Gc/u+Zx5s6daySZzMxMl/YDBw4YSWbs2LEu7Z9++qmRZJ5//nljjDGnT582vr6+ZsiQIS79duzYYSSZ+Ph4Z9vmzZuNJNOpU6frjv/SpUumuLjYdO3a1QwYMMDZnpmZaSSZ++67z5SUlDjbFyxYYCSZvn37uuxn0qRJRpLJz8+/6rFKSkqM3W43cXFxLvs8e/asCQsLMx06dCgzhnfeeee6Y7iRvu3atTN+fn5XXX/p0iVTVFRkYmJizK9//Wtn+65du4wkk5qaet06Ll26ZAoLC01AQIB55ZVXrtsfqMp4ewuowpYvX65du3aVWS6/zXItDzzwgL744guNHTtWGzZsUEFBwQ0fd/PmzZJU5m6wBx54QC1atNDHH38sSdq5c6ccDocGDx7s0q99+/Zl7i67bNCgQeW2v/HGG2rVqpVq1aolLy8veXt76+OPP9aBAwfK9H344YdVo8b//fPWokULSXJe9biy/ejRo1cZqXTw4EFlZWXp8ccfd9ln7dq1NWjQIO3cuVPnz5+/6va3wlxxxe7SpUuaOXOm7rnnHvn4+MjLy0s+Pj46dOhQub+H8hQWFurZZ5/V3XffLS8vL3l5eal27do6d+7cDe8DqKp4ewuowlq0aKE2bdqUaQ8ODtaxY8euue3UqVMVEBCgFStW6I033lDNmjXVqVMnvfTSS+Xu86dOnTolSc63vH7Kbrc754dc7hceHl6mX3ltV9vnvHnzlJycrKefflq/+93vFBoaqpo1a2ratGnlvlCHhIS4PPbx8blm+8WLF8ut5adjuNpYS0tLlZeXJ39//6vuo6KOHj0qu93ufDx58mS99tprevbZZxUfH6+6deuqRo0aGjNmjC5cuHBD+xw2bJg+/vhjTZs2TW3btlVQUJBsNpsefvjhG94HUFURegCL8vLy0uTJkzV58mSdOXNGGzdu1PPPP6+ePXvq2LFj13wRr1evniQpOzvbOVfksqysLOe8kMv9vv/++zL7yMnJKfdqj81mK9O2YsUKJSQkaNGiRS7tZ8+evfYg3eCnY71SVlaWatSoobp167r9uP/617+Uk5OjpKQkZ9uKFSs0YsQIzZw506XvyZMnVadOnevuMz8/Xx988IGmT5+u5557ztnucDh0+vRpt9UOeCre3gKgOnXq6D//8z81btw4nT59WocPH5Yk+fr6SlKZKwBdunSR9OOL8E/t2rVLBw4cUNeuXSVJ7dq1k6+vr9asWePSb+fOnTd1t5DNZnPWctnevXtd7p66XZo1a6YGDRpo5cqVLm83nTt3Tu+++67zji53On36tJ5++ml5e3vr17/+tbO9vN/D+vXrdeLECZe2q503m80mY0yZffzpT39SSUmJO4cAeCSu9AAW1adPH8XGxqpNmzaqX7++jhw5ogULFig6OloxMTGS5Px8mFdeeUUjR46Ut7e3mjVrpmbNmukXv/iFFi5cqBo1aqh3797Ou7eioqKcL9QhISGaPHmyZs2apbp162rAgAE6fvy4ZsyYocjISJc5MteSmJio3/3ud5o+fbri4+N18OBB/fa3v1WTJk3KvXvNnWrUqKE5c+Zo+PDhSkxM1FNPPSWHw6G5c+fqzJkzmj179i3t/9ChQ9q5c6dKS0t16tQpffrpp1q8eLEKCgq0fPlytWzZ0tk3MTFRS5cuVfPmzXXvvfcqIyNDc+fOLXO17a677pKfn5/efvtttWjRQrVr15bdbpfdblenTp00d+5chYaGqnHjxtq6dasWL158Q1eKgCqvkidSA6iAy3dv7dq1q9z1jzzyyHXv3nr55ZdNhw4dTGhoqPHx8TGNGjUySUlJ5vDhwy7bTZ061djtdlOjRg0jyWzevNkY8+NdTS+99JJp2rSp8fb2NqGhoeaxxx4zx44dc9m+tLTUvPjii6Zhw4bGx8fH3HvvveaDDz4w9913n8udV9e6m8nhcJgpU6aYBg0amFq1aplWrVqZdevWmZEjR7qM8/LdW3PnznXZ/mr7vt7v8afWrVtn2rVrZ2rVqmUCAgJM165dzT//+c8bOk55Lve9vHh5eZl69eqZBx980Dz//PNlzoMxxuTl5ZmkpCQTFhZm/P39TceOHc22bdtMfHy8y51wxhizatUq07x5c+Pt7W0kmenTpxtjjDl+/LgZNGiQqVu3rgkMDDS9evUy+/fvL/P3AVRHNmNu4AM9AMCNMjMz1bx5c02fPl3PP/98ZZcDwCIIPQBuqy+++EKrVq1Shw4dFBQUpIMHD2rOnDkqKCjQ/v37r3oXFwC4G3N6ANxWAQEB2r17txYvXqwzZ84oODhYCQkJ+v3vf0/gAXBHcaUHAABYAresAwAASyD0AAAASyD0AAAAS2Ais6TS0lJlZWUpMDCw3I/ABwAAnscYo7Nnz8put9/Qh50SevTj9+dERUVVdhkAAKACjh07VuaTyctD6JEUGBgo6cdfWlBQUCVXAwAAbkRBQYGioqKcr+PXQ+jR/32rc1BQEKEHAIAq5kanpjCRGQAAWAKhBwAAWAKhBwAAWAKhBwAAWAKhBwAAWAKhBwAAWAKhBwAAWAKhBwAAWAKhBwAAWAKhBwAAWAKhBwAAWAKhBwAAWAKhBwAAWAKhBwAAWIJXZRdQ3R09elQnT56s7DJuWmhoqBo1alTZZQAA4DaEntvo6NGjata8hS5eOF/Zpdy0Wn7+OvjVAYIPAKDaIPTcRidPntTFC+dVLzFZ3vWiKrucG1Z86phOffCyTp48SegBAFQbhJ47wLtelHwj7q7sMgAAsDQmMgMAAEsg9AAAAEsg9AAAAEsg9AAAAEsg9AAAAEsg9AAAAEsg9AAAAEsg9AAAAEsg9AAAAEsg9AAAAEsg9AAAAEsg9AAAAEsg9AAAAEsg9AAAAEsg9AAAAEsg9AAAAEsg9AAAAEsg9AAAAEsg9AAAAEsg9AAAAEsg9AAAAEsg9AAAAEsg9AAAAEuo1NAza9YstW3bVoGBgQoLC1P//v118OBBlz6jRo2SzWZzWdq3b+/Sx+FwaMKECQoNDVVAQID69u2r48eP38mhAAAAD1epoWfr1q0aN26cdu7cqbS0NF26dEk9evTQuXPnXPr16tVL2dnZzuXvf/+7y/pJkybpvffe0+rVq7V9+3YVFhYqMTFRJSUld3I4AADAg3lV5sE//PBDl8epqakKCwtTRkaGOnXq5Gz39fVVREREufvIz8/X4sWL9dZbb6lbt26SpBUrVigqKkobN25Uz549b98AAABAleFRc3ry8/MlSSEhIS7tW7ZsUVhYmJo2baonn3xSubm5znUZGRkqLi5Wjx49nG12u12xsbFKT0+/M4UDAACPV6lXen7KGKPJkyerY8eOio2Ndbb37t1b//Vf/6Xo6GhlZmZq2rRp6tKlizIyMuTr66ucnBz5+Piobt26LvsLDw9XTk5OucdyOBxyOBzOxwUFBbdnUAAAwGN4TOgZP3689u7dq+3bt7u0DxkyxPlzbGys2rRpo+joaK1fv14DBw686v6MMbLZbOWumzVrlmbMmOGewgEAQJXgEW9vTZgwQe+//742b96shg0bXrNvZGSkoqOjdejQIUlSRESEioqKlJeX59IvNzdX4eHh5e5j6tSpys/Pdy7Hjh1zz0AAAIDHqtTQY4zR+PHjtXbtWm3atElNmjS57janTp3SsWPHFBkZKUlq3bq1vL29lZaW5uyTnZ2t/fv3q0OHDuXuw9fXV0FBQS4LAACo3ir17a1x48Zp5cqV+utf/6rAwEDnHJzg4GD5+fmpsLBQKSkpGjRokCIjI3X48GE9//zzCg0N1YABA5x9k5KSlJycrHr16ikkJERTpkxRXFyc824uAACASg09ixYtkiQlJCS4tKempmrUqFGqWbOm9u3bp+XLl+vMmTOKjIxU586dtWbNGgUGBjr7z58/X15eXho8eLAuXLigrl27aunSpapZs+adHA4AAPBglRp6jDHXXO/n56cNGzZcdz+1atXSwoULtXDhQneVBgAAqhmPmMgMAABwuxF6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJVRq6Jk1a5batm2rwMBAhYWFqX///jp48KBLH2OMUlJSZLfb5efnp4SEBH355ZcufRwOhyZMmKDQ0FAFBASob9++On78+J0cCgAA8HCVGnq2bt2qcePGaefOnUpLS9OlS5fUo0cPnTt3ztlnzpw5mjdvnl599VXt2rVLERER6t69u86ePevsM2nSJL333ntavXq1tm/frsLCQiUmJqqkpKQyhgUAADyQV2Ue/MMPP3R5nJqaqrCwMGVkZKhTp04yxmjBggV64YUXNHDgQEnSsmXLFB4erpUrV+qpp55Sfn6+Fi9erLfeekvdunWTJK1YsUJRUVHauHGjevbsecfHBQAAPI9HzenJz8+XJIWEhEiSMjMzlZOTox49ejj7+Pr6Kj4+Xunp6ZKkjIwMFRcXu/Sx2+2KjY119gEAAKjUKz0/ZYzR5MmT1bFjR8XGxkqScnJyJEnh4eEufcPDw3XkyBFnHx8fH9WtW7dMn8vbX8nhcMjhcDgfFxQUuG0cAADAM3nMlZ7x48dr7969WrVqVZl1NpvN5bExpkzbla7VZ9asWQoODnYuUVFRFS8cAABUCR4ReiZMmKD3339fmzdvVsOGDZ3tERERklTmik1ubq7z6k9ERISKioqUl5d31T5Xmjp1qvLz853LsWPH3DkcAADggSo19BhjNH78eK1du1abNm1SkyZNXNY3adJEERERSktLc7YVFRVp69at6tChgySpdevW8vb2dumTnZ2t/fv3O/tcydfXV0FBQS4LAACo3ip1Ts+4ceO0cuVK/fWvf1VgYKDzik5wcLD8/Pxks9k0adIkzZw5UzExMYqJidHMmTPl7++vYcOGOfsmJSUpOTlZ9erVU0hIiKZMmaK4uDjn3VwAAACVGnoWLVokSUpISHBpT01N1ahRoyRJzzzzjC5cuKCxY8cqLy9P7dq100cffaTAwEBn//nz58vLy0uDBw/WhQsX1LVrVy1dulQ1a9a8U0MBAAAerlJDjzHmun1sNptSUlKUkpJy1T61atXSwoULtXDhQjdWBwAAqhOPmMgMAABwuxF6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJRB6AACAJVQo9GRmZrq7DgAAgNuqQqHn7rvvVufOnbVixQpdvHjR3TUBAAC4XYVCzxdffKH7779fycnJioiI0FNPPaV//etf7q4NAADAbSoUemJjYzVv3jydOHFCqampysnJUceOHdWyZUvNmzdPP/zwg7vrBAAAuCW3NJHZy8tLAwYM0J///Ge99NJL+vbbbzVlyhQ1bNhQI0aMUHZ2trvqBAAAuCW3FHp2796tsWPHKjIyUvPmzdOUKVP07bffatOmTTpx4oT69evnrjoBAABuiVdFNpo3b55SU1N18OBBPfzww1q+fLkefvhh1ajxY4Zq0qSJ3nzzTTVv3tytxQIAAFRUhULPokWL9MQTT2j06NGKiIgot0+jRo20ePHiWyoOAADAXSoUeg4dOnTdPj4+Pho5cmRFdg8AAOB2FZrTk5qaqnfeeadM+zvvvKNly5bdclEAAADuVqHQM3v2bIWGhpZpDwsL08yZM2+5KAAAAHerUOg5cuSImjRpUqY9OjpaR48eveWiAAAA3K1CoScsLEx79+4t0/7FF1+oXr16t1wUAACAu1Uo9AwdOlS/+tWvtHnzZpWUlKikpESbNm3SxIkTNXToUHfXCAAAcMsqdPfWiy++qCNHjqhr167y8vpxF6WlpRoxYgRzegAAgEeqUOjx8fHRmjVr9Lvf/U5ffPGF/Pz8FBcXp+joaHfXBwAA4BYVCj2XNW3aVE2bNnVXLQAAALdNhUJPSUmJli5dqo8//li5ubkqLS11Wb9p0ya3FAcAAOAuFQo9EydO1NKlS/XII48oNjZWNpvN3XUBAAC4VYVCz+rVq/XnP/9ZDz/8sLvrAQAAuC0qdMu6j4+P7r77bnfXAgAAcNtUKPQkJyfrlVdekTHmlg7+ySefqE+fPrLb7bLZbFq3bp3L+lGjRslms7ks7du3d+njcDg0YcIEhYaGKiAgQH379tXx48dvqS4AAFD9VOjtre3bt2vz5s36xz/+oZYtW8rb29tl/dq1a29oP+fOndN9992n0aNHa9CgQeX26dWrl1JTU52PfXx8XNZPmjRJf/vb37R69WrVq1dPycnJSkxMVEZGhmrWrHmTIwMAANVVhUJPnTp1NGDAgFs+eO/evdW7d+9r9vH19VVERES56/Lz87V48WK99dZb6tatmyRpxYoVioqK0saNG9WzZ89brhEAAFQPFQo9P73ycrtt2bJFYWFhqlOnjuLj4/X73/9eYWFhkqSMjAwVFxerR48ezv52u12xsbFKT0+/auhxOBxyOBzOxwUFBbd3EAAAoNJVaE6PJF26dEkbN27Um2++qbNnz0qSsrKyVFhY6LbievfurbffflubNm3Syy+/rF27dqlLly7OwJKTkyMfHx/VrVvXZbvw8HDl5ORcdb+zZs1ScHCwc4mKinJbzQAAwDNV6ErPkSNH1KtXLx09elQOh0Pdu3dXYGCg5syZo4sXL+qNN95wS3FDhgxx/hwbG6s2bdooOjpa69ev18CBA6+6nTHmmp8dNHXqVE2ePNn5uKCggOADAEA1V6ErPRMnTlSbNm2Ul5cnPz8/Z/uAAQP08ccfu624K0VGRio6OlqHDh2SJEVERKioqEh5eXku/XJzcxUeHn7V/fj6+iooKMhlAQAA1VuFQs/27dv1m9/8psydVNHR0Tpx4oRbCivPqVOndOzYMUVGRkqSWrduLW9vb6WlpTn7ZGdna//+/erQocNtqwMAAFQ9FXp7q7S0VCUlJWXajx8/rsDAwBveT2Fhob755hvn48zMTH3++ecKCQlRSEiIUlJSNGjQIEVGRurw4cN6/vnnFRoa6rxzLDg4WElJSUpOTla9evUUEhKiKVOmKC4uznk3FwAAgFTBKz3du3fXggULnI9tNpsKCws1ffr0m/pqit27d+v+++/X/fffL0maPHmy7r//fv2///f/VLNmTe3bt0/9+vVT06ZNNXLkSDVt2lQ7duxwCVbz589X//79NXjwYD300EPy9/fX3/72Nz6jBwAAuKjQlZ758+erc+fOuueee3Tx4kUNGzZMhw4dUmhoqFatWnXD+0lISLjmpzpv2LDhuvuoVauWFi5cqIULF97wcQEAgPVUKPTY7XZ9/vnnWrVqlfbs2aPS0lIlJSVp+PDhLhObAQAAPEWFQo8k+fn56YknntATTzzhznoAAABuiwqFnuXLl19z/YgRIypUDAAAwO1SodAzceJEl8fFxcU6f/68fHx85O/vT+gBAAAep0J3b+Xl5bkshYWFOnjwoDp27HhTE5kBAADulAp/99aVYmJiNHv27DJXgQAAADyB20KPJNWsWVNZWVnu3CUAAIBbVGhOz/vvv+/y2Bij7Oxsvfrqq3rooYfcUhgAAIA7VSj09O/f3+WxzWZT/fr11aVLF7388svuqAsAAMCtKvzdWwAAAFWJW+f0AAAAeKoKXemZPHnyDfedN29eRQ4BAADgVhUKPZ999pn27NmjS5cuqVmzZpKkr7/+WjVr1lSrVq2c/Ww2m3uqBAAAuEUVCj19+vRRYGCgli1bprp160r68QMLR48erZ///OdKTk52a5EAAAC3qkJzel5++WXNmjXLGXgkqW7dunrxxRe5ewsAAHikCoWegoICff/992Xac3Nzdfbs2VsuCgAAwN0qFHoGDBig0aNH6y9/+YuOHz+u48eP6y9/+YuSkpI0cOBAd9cIAABwyyo0p+eNN97QlClT9Nhjj6m4uPjHHXl5KSkpSXPnznVrgQAAAO5QodDj7++v119/XXPnztW3334rY4zuvvtuBQQEuLs+AAAAt7ilDyfMzs5Wdna2mjZtqoCAABlj3FUXAACAW1Uo9Jw6dUpdu3ZV06ZN9fDDDys7O1uSNGbMGG5XBwAAHqlCoefXv/61vL29dfToUfn7+zvbhwwZog8//NBtxQEAALhLheb0fPTRR9qwYYMaNmzo0h4TE6MjR464pTAAAAB3qtCVnnPnzrlc4bns5MmT8vX1veWiAAAA3K1CoadTp05avny587HNZlNpaanmzp2rzp07u604AAAAd6nQ21tz585VQkKCdu/eraKiIj3zzDP68ssvdfr0af3zn/90d40AAAC3rEJXeu655x7t3btXDzzwgLp3765z585p4MCB+uyzz3TXXXe5u0YAAIBbdtNXeoqLi9WjRw+9+eabmjFjxu2oCQAAwO1u+kqPt7e39u/fL5vNdjvqAQAAuC0q9PbWiBEjtHjxYnfXAgAAcNtUaCJzUVGR/vSnPyktLU1t2rQp851b8+bNc0txAAAA7nJToee7775T48aNtX//frVq1UqS9PXXX7v04W0vAADgiW4q9MTExCg7O1ubN2+W9OPXTvzv//6vwsPDb0txAAAA7nJTc3qu/Bb1f/zjHzp37pxbCwIAALgdKjSR+bIrQxAAAICnuqnQY7PZyszZYQ4PAACoCm5qTo8xRqNGjXJ+qejFixf19NNPl7l7a+3ate6rEAAAwA1uKvSMHDnS5fFjjz3m1mIAAABul5sKPampqberDgAAgNvqliYyAwAAVBWEHgAAYAmEHgAAYAmEHgAAYAmEHgAAYAmEHgAAYAmEHgAAYAmEHgAAYAmEHgAAYAmEHgAAYAmEHgAAYAmEHgAAYAmEHgAAYAmEHgAAYAmVGno++eQT9enTR3a7XTabTevWrXNZb4xRSkqK7Ha7/Pz8lJCQoC+//NKlj8Ph0IQJExQaGqqAgAD17dtXx48fv4OjAAAAVUGlhp5z587pvvvu06uvvlru+jlz5mjevHl69dVXtWvXLkVERKh79+46e/ass8+kSZP03nvvafXq1dq+fbsKCwuVmJiokpKSOzUMAABQBXhV5sF79+6t3r17l7vOGKMFCxbohRde0MCBAyVJy5YtU3h4uFauXKmnnnpK+fn5Wrx4sd566y1169ZNkrRixQpFRUVp48aN6tmz5x0bCwAA8GweO6cnMzNTOTk56tGjh7PN19dX8fHxSk9PlyRlZGSouLjYpY/dbldsbKyzT3kcDocKCgpcFgAAUL15bOjJycmRJIWHh7u0h4eHO9fl5OTIx8dHdevWvWqf8syaNUvBwcHOJSoqys3VAwAAT+Oxoecym83m8tgYU6btStfrM3XqVOXn5zuXY8eOuaVWAADguTw29EREREhSmSs2ubm5zqs/ERERKioqUl5e3lX7lMfX11dBQUEuCwAAqN48NvQ0adJEERERSktLc7YVFRVp69at6tChgySpdevW8vb2dumTnZ2t/fv3O/sAAABIlXz3VmFhob755hvn48zMTH3++ecKCQlRo0aNNGnSJM2cOVMxMTGKiYnRzJkz5e/vr2HDhkmSgoODlZSUpOTkZNWrV08hISGaMmWK4uLinHdzAQAASJUcenbv3q3OnTs7H0+ePFmSNHLkSC1dulTPPPOMLly4oLFjxyovL0/t2rXTRx99pMDAQOc28+fPl5eXlwYPHqwLFy6oa9euWrp0qWrWrHnHxwMAADyXzRhjKruIylZQUKDg4GDl5+e7dX7Pnj171Lp1a0WMXCDfiLvdtt/bzZHzjXKWTVJGRoZatWpV2eUAAFCum3399tg5PQAAAO5E6AEAAJZA6AEAAJZA6AEAAJZA6AEAAJZA6AEAAJZA6AEAAJZA6AEAAJZA6AEAAJZA6AEAAJZA6AEAAJZA6AEAAJZA6AEAAJZA6AEAAJZA6AEAAJZA6AEAAJZA6AEAAJZA6AEAAJZA6AEAAJZA6AEAAJZA6AEAAJZA6AEAAJZA6AEAAJZA6AEAAJZA6AEAAJZA6AEAAJZA6AEAAJZA6AEAAJZA6AEAAJZA6AEAAJZA6AEAAJZA6AEAAJZA6AEAAJZA6AEAAJZA6AEAAJZA6AEAAJZA6AEAAJZA6AEAAJZA6AEAAJZA6AEAAJZA6AEAAJZA6AEAAJZA6AEAAJZA6AEAAJZA6AEAAJZA6AEAAJZA6AEAAJZA6AEAAJZA6AEAAJZA6AEAAJZA6AEAAJZA6AEAAJZA6AEAAJbg0aEnJSVFNpvNZYmIiHCuN8YoJSVFdrtdfn5+SkhI0JdfflmJFQMAAE/l0aFHklq2bKns7Gznsm/fPue6OXPmaN68eXr11Ve1a9cuRUREqHv37jp79mwlVgwAADyRx4ceLy8vRUREOJf69etL+vEqz4IFC/TCCy9o4MCBio2N1bJly3T+/HmtXLmykqsGAACexuNDz6FDh2S329WkSRMNHTpU3333nSQpMzNTOTk56tGjh7Ovr6+v4uPjlZ6efs19OhwOFRQUuCwAAKB68+jQ065dOy1fvlwbNmzQH//4R+Xk5KhDhw46deqUcnJyJEnh4eEu24SHhzvXXc2sWbMUHBzsXKKiom7bGAAAgGfw6NDTu3dvDRo0SHFxcerWrZvWr18vSVq2bJmzj81mc9nGGFOm7UpTp05Vfn6+czl27Jj7iwcAAB7Fo0PPlQICAhQXF6dDhw457+K68qpObm5umas/V/L19VVQUJDLAgAAqrcqFXocDocOHDigyMhINWnSRBEREUpLS3OuLyoq0tatW9WhQ4dKrBIAAHgir8ou4FqmTJmiPn36qFGjRsrNzdWLL76ogoICjRw5UjabTZMmTdLMmTMVExOjmJgYzZw5U/7+/ho2bFhllw4AADyMR4ee48eP69FHH9XJkydVv359tW/fXjt37lR0dLQk6ZlnntGFCxc0duxY5eXlqV27dvroo48UGBhYyZUDAABP49GhZ/Xq1ddcb7PZlJKSopSUlDtTEAAAqLKq1JweAACAiiL0AAAASyD0AAAASyD0AAAASyD0AAAASyD0AAAASyD0AAAASyD0AAAASyD0AAAASyD0AAAASyD0AAAASyD0AAAASyD0AAAASyD0AAAASyD0AAAASyD0AAAASyD0AAAASyD0AAAASyD0AAAASyD0AAAASyD0AAAASyD0AAAASyD0AAAASyD0AAAASyD0AAAASyD0AAAASyD0AAAASyD0AAAASyD0AAAASyD0AAAASyD0AAAASyD0AAAASyD0AAAASyD0AAAASyD0AAAASyD0AAAASyD0AAAASyD0AAAASyD0AAAASyD0AAAASyD0AAAASyD0AAAASyD0AAAASyD0AAAAS/Cq7ALguQ4cOFDZJdyU0NBQNWrUqLLLAAB4KEIPyigpzJNsNj322GOVXcpNqeXnr4NfHSD4AADKRehBGaWOQskY1UtMlne9qMou54YUnzqmUx+8rJMnTxJ6AADlIvTgqrzrRck34u7KLgMAALcg9KBaYR4SAOBqCD2oFpiHBAC4HkIPqgXmIQEArofQg2qFeUgAgKvhwwkBAIAlVJsrPa+//rrmzp2r7OxstWzZUgsWLNDPf/7zyi4LuC4mXwPAnVEtQs+aNWs0adIkvf7663rooYf05ptvqnfv3vr3v//NP87wWEy+BoA7q1qEnnnz5ikpKUljxoyRJC1YsEAbNmzQokWLNGvWrEquDihfVZ58vW3bNrVo0aKyy7lhDodDvr6+lV3GTaHmO4Oa7wxPuUJc5UNPUVGRMjIy9Nxzz7m09+jRQ+np6ZVUFXDjqtLk66p6dUq2GpIprewqbg413xnUfEd4yhXiKh96Tp48qZKSEoWHh7u0h4eHKycnp9xtHA6HHA6H83F+fr4kqaCgwK21FRYW/ni8nG9UWnTRrfu+nYpPHZNUteqm5jvDkXVAMkZBbQeqZnD9yi7nhhRlfa1z/95MzbcZNd8ZVbHmkvwfVLBrrQ4fPqw6deq4dd+XX7eNMTe2ganiTpw4YSSZ9PR0l/YXX3zRNGvWrNxtpk+fbiSxsLCwsLCwVIPl2LFjN5QZqvyVntDQUNWsWbPMVZ3c3NwyV38umzp1qiZPnux8XFpaqtOnT6tevXqy2Wy3td47paCgQFFRUTp27JiCgoIqu5zbzkrjtdJYJcZbnVlprJK1xnunxmqM0dmzZ2W322+of5UPPT4+PmrdurXS0tI0YMAAZ3taWpr69etX7ja+vr5lJoG5+5KbpwgKCqr2T66fstJ4rTRWifFWZ1Yaq2St8d6JsQYHB99w3yofeiRp8uTJevzxx9WmTRs9+OCD+sMf/qCjR4/q6aefruzSAACAh6gWoWfIkCE6deqUfvvb3yo7O1uxsbH6+9//rujo6MouDQAAeIhqEXokaezYsRo7dmxll+ExfH19NX369Cr3WQ4VZaXxWmmsEuOtzqw0Vsla4/XUsdqMudH7vAAAAKouvnAUAABYAqEHAABYAqEHAABYAqEHAABYAqHHQ8yaNUtt27ZVYGCgwsLC1L9/fx08eNClz6hRo2Sz2VyW9u3bu/RxOByaMGGCQkNDFRAQoL59++r48eMuffLy8vT4448rODhYwcHBevzxx3XmzBmXPkePHlWfPn0UEBCg0NBQ/epXv1JRUZHbxpuSklJmLBEREc71xhilpKTIbrfLz89PCQkJ+vLLL6vkWBs3blxmrDabTePGjZNU9c/rJ598oj59+shut8tms2ndunUu6z3tXO7bt0/x8fHy8/NTgwYN9Nvf/vaGv7fnWmMtLi7Ws88+q7i4OAUEBMhut2vEiBHKyspy2UdCQkKZ8z106FCPG+v1xit53t/u7R5vec9jm82muXPnOvtUlfN7I6851em5+9NBwQP07NnTpKammv3795vPP//cPPLII6ZRo0amsLDQ2WfkyJGmV69eJjs727mcOnXKZT9PP/20adCggUlLSzN79uwxnTt3Nvfdd5+5dOmSs0+vXr1MbGysSU9PN+np6SY2NtYkJiY611+6dMnExsaazp07mz179pi0tDRjt9vN+PHj3Tbe6dOnm5YtW7qMJTc317l+9uzZJjAw0Lz77rtm3759ZsiQISYyMtIUFBRUubHm5ua6jDMtLc1IMps3bzbGVP3z+ve//9288MIL5t133zWSzHvvveey3pPOZX5+vgkPDzdDhw41+/btM++++64JDAw0//M//3PLYz1z5ozp1q2bWbNmjfnqq6/Mjh07TLt27Uzr1q1d9hEfH2+efPJJl/N95swZlz6eMNbrjdcYz/rbvRPj/ek4s7OzzZIlS4zNZjPffvuts09VOb838ppTnZ67lxF6PFRubq6RZLZu3epsGzlypOnXr99Vtzlz5ozx9vY2q1evdradOHHC1KhRw3z44YfGGGP+/e9/G0lm586dzj47duwwksxXX31ljPnxiV+jRg1z4sQJZ59Vq1YZX19fk5+f75bxTZ8+3dx3333lristLTURERFm9uzZzraLFy+a4OBg88Ybb1S5sV5p4sSJ5q677jKlpaXGmOp1Xq98ofC0c/n666+b4OBgc/HiRWefWbNmGbvd7jwfFR1ref71r38ZSebIkSPOtvj4eDNx4sSrbuOJYzWm/PF60t/unRjvlfr162e6dOni0lZVz++VrznV9bnL21seKj8/X5IUEhLi0r5lyxaFhYWpadOmevLJJ5Wbm+tcl5GRoeLiYvXo0cPZZrfbFRsbq/T0dEnSjh07FBwcrHbt2jn7tG/fXsHBwS59YmNjXb7ArWfPnnI4HMrIyHDbGA8dOiS73a4mTZpo6NCh+u677yRJmZmZysnJcRmHr6+v4uPjnTVWtbFeVlRUpBUrVuiJJ55w+XLb6nRef8rTzuWOHTsUHx/v8oFpPXv2VFZWlg4fPuz28efn58tms5X5br+3335boaGhatmypaZMmaKzZ88611W1sXrK3+6dPrfff/+91q9fr6SkpDLrquL5vfI1p7o+dwk9HsgYo8mTJ6tjx46KjY11tvfu3Vtvv/22Nm3apJdfflm7du1Sly5d5HA4JEk5OTny8fFR3bp1XfYXHh7u/Bb6nJwchYWFlTlmWFiYS58rv6G+bt268vHxKfNt9hXVrl07LV++XBs2bNAf//hH5eTkqEOHDjp16pTzGFfWcOU4qspYf2rdunU6c+aMRo0a5WyrTuf1Sp52Lsvrc/mxu38HFy9e1HPPPadhw4a5fOHi8OHDtWrVKm3ZskXTpk3Tu+++q4EDBzrXV6WxetLf7p08t5K0bNkyBQYGupw7qWqe3/Jec6rrc7fafA1FdTJ+/Hjt3btX27dvd2kfMmSI8+fY2Fi1adNG0dHRWr9+fZkn3k8ZY1yuKvz051vpcyt69+7t/DkuLk4PPvig7rrrLi1btsw5EfLKY93I8T1xrD+1ePFi9e7d2+V/NNXpvF6NJ53L8mq52rYVVVxcrKFDh6q0tFSvv/66y7onn3zS+XNsbKxiYmLUpk0b7dmzR61atarwOG6kj7vH6ml/u3fi3F62ZMkSDR8+XLVq1XJpr4rn92qvOVc7RlV+7nKlx8NMmDBB77//vjZv3qyGDRtes29kZKSio6N16NAhSVJERISKioqUl5fn0i83N9eZiCMiIvT999+X2dcPP/zg0ufK5JyXl6fi4uIySdtdAgICFBcXp0OHDjnv4rqyhivHUdXGeuTIEW3cuFFjxoy5Zr/qdF497VyW1+fy2zHu+h0UFxdr8ODByszMVFpamstVnvK0atVK3t7eLue7qoz1SpX5t3snx7tt2zYdPHjwus9lyfPP79Vec6rtc/eGZ//gtiotLTXjxo0zdrvdfP311ze0zcmTJ42vr69ZtmyZMeb/JpWtWbPG2ScrK6vcSWWffvqps8/OnTvLnVSWlZXl7LN69erbOrn34sWLpkGDBmbGjBnOCXQvvfSSc73D4Sh3Al1VGuv06dNNRESEKS4uvma/qnxedZWJzJ5yLl9//XVTp04d43A4nH1mz57ttonMRUVFpn///qZly5YudyNey759+1wmkHriWK823itV5t/unRzvyJEjy9yVdzWeen6v95pTXZ+7hB4P8ctf/tIEBwebLVu2uNzqeP78eWOMMWfPnjXJyckmPT3dZGZmms2bN5sHH3zQNGjQoMztgw0bNjQbN240e/bsMV26dCn39sF7773X7Nixw+zYscPExcWVe/tg165dzZ49e8zGjRtNw4YN3Xobd3JystmyZYv57rvvzM6dO01iYqIJDAw0hw8fNsb8+MccHBxs1q5da/bt22ceffTRcm+VrApjNcaYkpIS06hRI/Pss8+6tFeH83r27Fnz2Wefmc8++8xIMvPmzTOfffaZ844lTzqXZ86cMeHh4ebRRx81+/btM2vXrjVBQUE3fNvrtcZaXFxs+vbtaxo2bGg+//xzl+fx5X+ov/nmGzNjxgyza9cuk5mZadavX2+aN29u7r//fo8b6/XG62l/u7d7vJfl5+cbf39/s2jRojLbV6Xze73XHGOq13P3MkKPh5BU7pKammqMMeb8+fOmR48epn79+sbb29s0atTIjBw50hw9etRlPxcuXDDjx483ISEhxs/PzyQmJpbpc+rUKTN8+HATGBhoAgMDzfDhw01eXp5LnyNHjphHHnnE+Pn5mZCQEDN+/HiXWwVv1eXPe/D29jZ2u90MHDjQfPnll871paWlzisjvr6+plOnTmbfvn1VcqzGGLNhwwYjyRw8eNClvTqc182bN5f7tzty5EhjjOedy71795qf//znxtfX10RERJiUlJQb/p/itcaamZl51efx5c9kOnr0qOnUqZMJCQkxPj4+5q677jK/+tWvyny2jSeM9Xrj9cS/3ds53svefPNN4+fnV+azd4ypWuf3eq85xlSv5+5ltv9/8AAAANUaE5kBAIAlEHoAAIAlEHoAAIAlEHoAAIAlEHoAAIAlEHoAAIAlEHoAAIAlEHoAVFsJCQmaNGlSZZcBwEMQegB4pD59+qhbt27lrtuxY4dsNpv27Nlzh6sCUJURegB4pKSkJG3atElHjhwps27JkiX62c9+platWlVCZQCqKkIPAI+UmJiosLAwLV261KX9/PnzWrNmjfr3769HH31UDRs2lL+/v+Li4rRq1apr7tNms2ndunUubXXq1HE5xokTJzRkyBDVrVtX9erVU79+/XT48GH3DApApSL0APBIXl5eGjFihJYuXaqffkXgO++8o6KiIo0ZM0atW7fWBx98oP379+sXv/iFHn/8cX366acVPub58+fVuXNn1a5dW5988om2b9+u2rVrq1evXioqKnLHsABUIkIPAI/1xBNP6PDhw9qyZYuzbcmSJRo4cKAaNGigKVOm6Gc/+5n+4z/+QxMmTFDPnj31zjvvVPh4q1evVo0aNfSnP/1JcXFxatGihVJTU3X06FGXGgBUTV6VXQAAXE3z5s3VoUMHLVmyRJ07d9a3336rbdu26aOPPlJJSYlmz56tNWvW6MSJE3I4HHI4HAoICKjw8TIyMvTNN98oMDDQpf3ixYv69ttvb3U4ACoZoQeAR0tKStL48eP12muvKTU1VdHR0eratavmzp2r+fPna8GCBYqLi1NAQIAmTZp0zbehbDaby1tlklRcXOz8ubS0VK1bt9bbb79dZtv69eu7b1AAKgWhB4BHGzx4sCZOnKiVK1dq2bJlevLJJ2Wz2bRt2zb169dPjz32mKQfA8uhQ4fUokWLq+6rfv36ys7Odj4+dOiQzp8/73zcqlUrrVmzRmFhYQoKCrp9gwJQKZjTA8Cj1a5dW0OGDNHzzz+vrKwsjRo1SpJ09913Ky0tTenp6Tpw4ICeeuop5eTkXHNfXbp00auvvqo9e/Zo9+7devrpp+Xt7e1cP3z4cIWGhqpfv37atm2bMjMztXXrVk2cOFHHjx+/ncMEcAcQegB4vKSkJOXl5albt25q1KiRJGnatGlq1aqVevbsqYSEBEVERKh///7X3M/LL7+sqKgoderUScOGDdOUKVPk7+/vXO/v769PPvlEjRo10sCBA9WiRQs98cQTunDhAld+gGrAZq58gxsAAKAa4koPAACwBEIPAACwBEIPAACwBEIPAACwBEIPAACwBEIPAACwBEIPAACwBEIPAACwBEIPAACwBEIPAACwBEIPAACwBEIPAACwhP8PUjIu2s4oaFMAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the histogram\n",
    "plt.hist(history, bins=10, edgecolor='black')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Data')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, for the chosen value of gamma, the best partition is..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize\n",
    "best_partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute stratified metrics with unbiased testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbiased_results[\"STRATIFIED_15\"] = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=1.5, K=4, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_15\"] = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=1.5, K=20, partition=best_partition)\n",
    "\n",
    "unbiased_results[\"STRATIFIED_2\"] = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2, K=4, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_2\"] = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2, K=20, partition=best_partition)\n",
    "\n",
    "unbiased_results[\"STRATIFIED_25\"] = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2.5, K=4, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_25\"] = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2.5, K=20, partition=best_partition)\n",
    "\n",
    "unbiased_results[\"STRATIFIED_3\"] = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=3, K=4, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_3\"] = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=3, K=20, partition=best_partition)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version uses the linspace of items instead of linspace of propensities to make the partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbiased_results[\"STRATIFIED_v2_15\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=1.5, K=4, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_v2_15\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=1.5, K=20, partition=best_partition)\n",
    "\n",
    "unbiased_results[\"STRATIFIED_v2_2\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2, K=4, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_v2_2\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2, K=20, partition=best_partition)\n",
    "\n",
    "unbiased_results[\"STRATIFIED_v2_25\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2.5, K=4, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_v2_25\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2.5, K=20, partition=best_partition)\n",
    "\n",
    "unbiased_results[\"STRATIFIED_v2_3\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=3, K=4, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_v2_3\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=3, K=20, partition=best_partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 13)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = 0\n",
    "columns = len(biased_results.keys())\n",
    "\n",
    "for key in biased_results.keys():\n",
    "    rows = max(rows, len(biased_results[key].keys()))\n",
    "\n",
    "for key in unbiased_results.keys():\n",
    "    rows = max(rows, len(biased_results[key].keys()))\n",
    "\n",
    "rows, columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init dictionary\n",
    "mae_results = dict()\n",
    "\n",
    "# Get the names of the rows\n",
    "list_biased_res = list(biased_results.keys())\n",
    "\n",
    "\n",
    "# Init results\n",
    "results_array = np.zeros((rows,columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the table with the MAE results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each row\n",
    "for i in range(len(list_biased_res)):\n",
    "    key = list_biased_res[i]\n",
    "\n",
    "    # For each column\n",
    "    for j in range(len(list(biased_results[key].keys()))):\n",
    "        key_2 = list(biased_results[key].keys())[j]\n",
    "\n",
    "        # Compute MAE\n",
    "        results_array[j][i] = abs(biased_results[key][key_2] - unbiased_results[key][key_2])\n",
    "\n",
    "# Make it a DataFrame\n",
    "mae_df = pd.DataFrame(columns=list(biased_results.keys()), data=results_array)\n",
    "metric_values = list(biased_results[list(biased_results.keys())[0]].keys())\n",
    "mae_df.insert(0, \"metric\", metric_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **RESULTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>AOA</th>\n",
       "      <th>UB_15</th>\n",
       "      <th>UB_2</th>\n",
       "      <th>UB_25</th>\n",
       "      <th>UB_3</th>\n",
       "      <th>STRATIFIED_15</th>\n",
       "      <th>STRATIFIED_2</th>\n",
       "      <th>STRATIFIED_25</th>\n",
       "      <th>STRATIFIED_3</th>\n",
       "      <th>STRATIFIED_v2_15</th>\n",
       "      <th>STRATIFIED_v2_2</th>\n",
       "      <th>STRATIFIED_v2_25</th>\n",
       "      <th>STRATIFIED_v2_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>auc</td>\n",
       "      <td>0.054800</td>\n",
       "      <td>0.090975</td>\n",
       "      <td>0.095139</td>\n",
       "      <td>0.098290</td>\n",
       "      <td>0.100626</td>\n",
       "      <td>9.097485e-02</td>\n",
       "      <td>9.513882e-02</td>\n",
       "      <td>9.828963e-02</td>\n",
       "      <td>0.072265</td>\n",
       "      <td>0.089552</td>\n",
       "      <td>0.093386</td>\n",
       "      <td>0.096192</td>\n",
       "      <td>0.098169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>recall</td>\n",
       "      <td>0.076203</td>\n",
       "      <td>0.113201</td>\n",
       "      <td>0.117820</td>\n",
       "      <td>0.121511</td>\n",
       "      <td>0.124447</td>\n",
       "      <td>1.132014e-01</td>\n",
       "      <td>1.178203e-01</td>\n",
       "      <td>1.215106e-01</td>\n",
       "      <td>0.133113</td>\n",
       "      <td>0.112005</td>\n",
       "      <td>0.116341</td>\n",
       "      <td>0.119731</td>\n",
       "      <td>0.122350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bias</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.082268e-11</td>\n",
       "      <td>7.539636e-12</td>\n",
       "      <td>4.317213e-12</td>\n",
       "      <td>5425.714286</td>\n",
       "      <td>143.909688</td>\n",
       "      <td>173.446281</td>\n",
       "      <td>203.398273</td>\n",
       "      <td>233.838054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>concentration</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.294130e+02</td>\n",
       "      <td>1.265924e+02</td>\n",
       "      <td>1.243931e+02</td>\n",
       "      <td>231.346606</td>\n",
       "      <td>129.427445</td>\n",
       "      <td>126.599289</td>\n",
       "      <td>124.396269</td>\n",
       "      <td>122.635240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          metric       AOA     UB_15      UB_2     UB_25      UB_3  \\\n",
       "0            auc  0.054800  0.090975  0.095139  0.098290  0.100626   \n",
       "1         recall  0.076203  0.113201  0.117820  0.121511  0.124447   \n",
       "2           bias  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3  concentration  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "   STRATIFIED_15  STRATIFIED_2  STRATIFIED_25  STRATIFIED_3  STRATIFIED_v2_15  \\\n",
       "0   9.097485e-02  9.513882e-02   9.828963e-02      0.072265          0.089552   \n",
       "1   1.132014e-01  1.178203e-01   1.215106e-01      0.133113          0.112005   \n",
       "2   1.082268e-11  7.539636e-12   4.317213e-12   5425.714286        143.909688   \n",
       "3   1.294130e+02  1.265924e+02   1.243931e+02    231.346606        129.427445   \n",
       "\n",
       "   STRATIFIED_v2_2  STRATIFIED_v2_25  STRATIFIED_v2_3  \n",
       "0         0.093386          0.096192         0.098169  \n",
       "1         0.116341          0.119731         0.122350  \n",
       "2       173.446281        203.398273       233.838054  \n",
       "3       126.599289        124.396269       122.635240  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize\n",
    "mae_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RecSysEvaluation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
