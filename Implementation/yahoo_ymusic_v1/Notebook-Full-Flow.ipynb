{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **IMPORT LIBS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from openrec.tf1.legacy import ImplicitModelTrainer\n",
    "from openrec.tf1.legacy.utils.evaluators import ImplicitEvalManager\n",
    "from openrec.tf1.legacy.utils import ImplicitDataset\n",
    "from openrec.tf1.legacy.recommenders import CML, BPR, PMF\n",
    "from openrec.tf1.legacy.utils.evaluators import AUC\n",
    "from openrec.tf1.legacy.utils.samplers import PairwiseSampler\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPEAT_TRAINING = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **GENERATE THE DATASET**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "seed = 2384795\n",
    "np.random.seed(seed=seed)\n",
    "\n",
    "# Preparing folder for output data\n",
    "output_name = f\"./generated_data/\"\n",
    "if os.path.exists(output_name) == False:\n",
    "    os.makedirs(output_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>SongID</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>83</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>153</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>170</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>184</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>194</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  SongID  Rating\n",
       "0       1      14       5\n",
       "1       1      35       1\n",
       "2       1      46       1\n",
       "3       1      83       1\n",
       "4       1      93       1\n",
       "5       1      94       1\n",
       "6       1     153       5\n",
       "7       1     170       4\n",
       "8       1     184       5\n",
       "9       1     194       5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Name of the dataset paths\n",
    "file_path = 'ydata-ymusic-rating-study-v1_0-train.txt'\n",
    "folder_name = f\"./original_files/\"\n",
    "\n",
    "# Load the training set into a DataFrame\n",
    "df_train = pd.read_csv(folder_name+file_path, sep='\\t',names=[\"UserID\",\"SongID\",\"Rating\"], header=None)  # sep='\\t' for tab-separated values\n",
    "\n",
    "# Visualize\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to implicit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"We treat items rated greater than or equal to 4 as relevant, and others as irrelevant, as suggested by prior literature.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>SongID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>ImplicitRating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>83</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>153</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>170</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>184</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>194</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  SongID  Rating  ImplicitRating\n",
       "0       1      14       5               1\n",
       "1       1      35       1               0\n",
       "2       1      46       1               0\n",
       "3       1      83       1               0\n",
       "4       1      93       1               0\n",
       "5       1      94       1               0\n",
       "6       1     153       5               1\n",
       "7       1     170       4               1\n",
       "8       1     184       5               1\n",
       "9       1     194       5               1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Suggested on paper\n",
    "POSITIVE_THRESHOLD = 4\n",
    "\n",
    "# Add column to the DataFrame\n",
    "df_train['ImplicitRating'] = np.where(df_train['Rating'] >= POSITIVE_THRESHOLD, 1, 0)\n",
    "\n",
    "# Visualize\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the number of users and items in the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The training set contains 300K ratings given by 15.4K users against 1K songs through natural interactions.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 15400)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store the range of ids for users\n",
    "min_user = df_train[\"UserID\"].min()\n",
    "max_user = df_train[\"UserID\"].max()\n",
    "\n",
    "# Store the range of items\n",
    "min_item = df_train[\"SongID\"].min()\n",
    "max_item = df_train[\"SongID\"].max()\n",
    "\n",
    "# Visualize the number of both\n",
    "max_item, max_user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **GET UNBIASED TESTSET**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the unbiased testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the dataset path\n",
    "file_path = folder_name + 'ydata-ymusic-rating-study-v1_0-test.txt'\n",
    "\n",
    "# Load the training set into a DataFrame\n",
    "df_test = pd.read_csv(file_path, sep='\\t',names=[\"UserID\",\"SongID\",\"Rating\"], header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to implicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>SongID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>ImplicitRating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>138</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>141</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>177</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>268</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>511</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>587</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>772</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>941</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  SongID  Rating  ImplicitRating\n",
       "0       1      49       1               0\n",
       "1       1     126       1               0\n",
       "2       1     138       1               0\n",
       "3       1     141       1               0\n",
       "4       1     177       1               0\n",
       "5       1     268       3               0\n",
       "6       1     511       1               0\n",
       "7       1     587       1               0\n",
       "8       1     772       5               1\n",
       "9       1     941       1               0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add column to the DataFrame\n",
    "df_test['ImplicitRating'] = np.where(df_test['Rating'] >= POSITIVE_THRESHOLD, 1, 0)\n",
    "\n",
    "# Visualize\n",
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the number of users and items in the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The testing set is collected by asking a subset of 5.4K users to rate 10 randomly selected songs.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5400, 1000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize\n",
    "df_test[\"UserID\"].max(), df_test[\"SongID\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter unbiased testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"We filter the testing set by retaining users who have at least a relevant and an irrelevant song in the testing set and two relevant songs in the training set.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select users with at least an irrelevant song in the unbiased testset\n",
    "usersWithNegativeInteractionInTest = df_test[df_test[\"ImplicitRating\"] == 0][\"UserID\"].unique()\n",
    "\n",
    "# Select UserID of users with at least a relevant song in testset\n",
    "usersWithPositiveInteractionInTest = df_test[df_test[\"ImplicitRating\"] == 1][\"UserID\"].unique()\n",
    "\n",
    "# Select UserID of users with at least two relevant song in trainset\n",
    "usersWithTwoPositiveInteractions = df_train[df_train[\"ImplicitRating\"] == 1].groupby(\"UserID\").filter(lambda x: len(x) >= 2)['UserID'].unique()\n",
    "\n",
    "# Compute the intersection\n",
    "set1 = set(usersWithNegativeInteractionInTest)\n",
    "set2 = set(usersWithPositiveInteractionInTest)\n",
    "set3 = set(usersWithTwoPositiveInteractions)\n",
    "valid_users_testset = set1 & set2 & set3\n",
    "\n",
    "# Filter the testset\n",
    "df_test_filtered = df_test[df_test[\"UserID\"].isin(valid_users_testset)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"2296 users satisfy these requirements.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2296"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize\n",
    "len(valid_users_testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape the unbiased test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the dataframe, for each row where ImplicitRating is 1, append [userID, itemID] to unbiased_pos_test_set\n",
    "# and for each row where ImplicitRating is 0, append [userID, itemID] to unbiased_neg_test_set\n",
    "unbiased_pos_test_set = df_test_filtered[df_test_filtered[\"ImplicitRating\"] == 1][[\"UserID\", \"SongID\"]].values\n",
    "unbiased_neg_test_set = df_test_filtered[df_test_filtered[\"ImplicitRating\"] == 0][[\"UserID\", \"SongID\"]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save unbiased test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to split pos and neg test set into two separate files\n",
    "\n",
    "# Get the dataframe\n",
    "unbiased_pos_test_set_df = pd.DataFrame(unbiased_pos_test_set)\n",
    "unbiased_neg_test_set_df = pd.DataFrame(unbiased_neg_test_set)\n",
    "\n",
    "# Get couples user-item\n",
    "unbiased_pos_test_set_df.columns = [\"user_id\",\"item_id\"]\n",
    "unbiased_neg_test_set_df.columns = [\"user_id\",\"item_id\"]\n",
    "\n",
    "# Turn into records\n",
    "structured_data_pos_test_set_unbiased = unbiased_pos_test_set_df.to_records(index=False)\n",
    "structured_data_neg_test_set_unbiased = unbiased_neg_test_set_df.to_records(index=False)\n",
    "\n",
    "# Save\n",
    "np.save(output_name + \"unbiased-test_arr_pos.npy\", structured_data_pos_test_set_unbiased)\n",
    "np.save(output_name + \"unbiased-test_arr_neg.npy\", structured_data_neg_test_set_unbiased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **GET BIASED TESTSET**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read again to reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the dataset path\n",
    "file_path = folder_name + 'ydata-ymusic-rating-study-v1_0-train.txt'\n",
    "\n",
    "# Load the training set into a DataFrame\n",
    "df_train = pd.read_csv(file_path, sep='\\t',names=[\"UserID\",\"SongID\",\"Rating\"], header=None)  # sep='\\t' for tab-separated values\n",
    "df_train['ImplicitRating'] = np.where(df_train['Rating'] >= POSITIVE_THRESHOLD, 1, 0)\n",
    "df_train = df_train[df_train[\"UserID\"].isin(valid_users_testset)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the biased test set and shape it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"We additionally held out a biased testing set (biased-testing) from the training set by randomly sampling 300 songs for each user.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompute, for each user, the list of songs with a relevant rating\n",
    "user_positive_ratings = df_train[df_train[\"ImplicitRating\"] == 1].groupby(\"UserID\")[\"SongID\"].apply(set)\n",
    "\n",
    "# Initialize the range of indexes for the items\n",
    "items_ids = np.arange(min_item, max_item + 1)\n",
    "\n",
    "# Set the number of songs for each user\n",
    "SONGS_FOR_BIASED_TEST = 300\n",
    "\n",
    "# Init empty\n",
    "pos_test_set = []\n",
    "neg_test_set = []\n",
    "\n",
    "# Extract the biased test set\n",
    "for user_id in valid_users_testset:\n",
    "\n",
    "    # Get SONGS_FOR_BIASED_TEST items\n",
    "    np.random.shuffle(items_ids)\n",
    "    test_items = set(items_ids[-SONGS_FOR_BIASED_TEST:])\n",
    "\n",
    "    # Get which are positive\n",
    "    pos_ids = user_positive_ratings.get(user_id, set()) & test_items\n",
    "\n",
    "    # Set the positive ones to 0 in the training set (extract)\n",
    "    df_train.loc[(df_train['SongID'].isin(pos_ids)) & (df_train['UserID'] == user_id), 'ImplicitRating'] = 0\n",
    "\n",
    "    # Append items\n",
    "    for id in test_items:\n",
    "        if id in pos_ids:\n",
    "            pos_test_set.append([user_id, id])\n",
    "        else:\n",
    "            neg_test_set.append([user_id, id])\n",
    "\n",
    "# Get np arrays\n",
    "pos_test_set = np.array(pos_test_set)\n",
    "neg_test_set = np.array(neg_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the biased test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to split pos and neg test set into two separate files\n",
    "\n",
    "# Get the dataframe\n",
    "pos_test_set_df = pd.DataFrame(pos_test_set)\n",
    "neg_test_set_df = pd.DataFrame(neg_test_set)\n",
    "\n",
    "# Get couples user-item\n",
    "pos_test_set_df.columns = [\"user_id\",\"item_id\"]\n",
    "neg_test_set_df.columns = [\"user_id\",\"item_id\"]\n",
    "\n",
    "# Turn into records\n",
    "structured_data_pos_test_set = pos_test_set_df.to_records(index=False)\n",
    "structured_data_neg_test_set = neg_test_set_df.to_records(index=False)\n",
    "\n",
    "# Save\n",
    "np.save(output_name + \"biased-test_arr_pos.npy\", structured_data_pos_test_set)\n",
    "np.save(output_name + \"biased-test_arr_neg.npy\", structured_data_neg_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **STORE TRAINSET**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter positive couples (user, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only take the couples (user, item) with relevant rating\n",
    "new_df = df_train[df_train['ImplicitRating'] != 0]\n",
    "new_df = new_df.drop(columns=['Rating', 'ImplicitRating'])\n",
    "\n",
    "# Define a dictionary for renaming columns\n",
    "rename_dict = {\n",
    "    'UserID': 'user_id',\n",
    "    'SongID': 'item_id'\n",
    "}\n",
    "\n",
    "# Rename the columns\n",
    "new_df = new_df.rename(columns=rename_dict)\n",
    "\n",
    "# Convert the DataFrame to a structured array\n",
    "train_data = new_df.to_records(index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "np.save(output_name + \"training_arr.npy\", train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MODEL CHOICE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I won't comment anything, we are just using the code provided by the authors of the paper\n",
    "\n",
    "raw_data = dict()\n",
    "raw_data['train_data'] = np.load(output_name + \"training_arr.npy\")\n",
    "raw_data['max_user'] = 15401\n",
    "raw_data['max_item'] = 1001\n",
    "batch_size = 8000\n",
    "test_batch_size = 1000\n",
    "display_itr = 1000\n",
    "\n",
    "train_dataset = ImplicitDataset(raw_data['train_data'], raw_data['max_user'], raw_data['max_item'], name='Train')\n",
    "\n",
    "MODEL_CLASS = CML\n",
    "MODEL_PREFIX = \"cml\"\n",
    "DATASET_NAME = \"yahoo\"\n",
    "OUTPUT_FOLDER = output_name\n",
    "OUTPUT_PATH = OUTPUT_FOLDER + MODEL_PREFIX + \"-\" + DATASET_NAME + \"/\"\n",
    "OUTPUT_PREFIX = str(OUTPUT_PATH) + str(MODEL_PREFIX) + \"-\" + str(DATASET_NAME)\n",
    "\n",
    "\n",
    "if os.path.exists(OUTPUT_PATH) == False:\n",
    "    os.makedirs(OUTPUT_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TRAIN THE MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_139026/3362066690.py:4: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prevent tensorflow from using cached embeddings\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.reset_default_graph()\n",
    "tf.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REPEAT_TRAINING:\n",
    "    # Define the model\n",
    "    model = MODEL_CLASS(batch_size=batch_size, max_user=train_dataset.max_user(), max_item=train_dataset.max_item(), dim_embed=50, l2_reg=0.001, opt='Adam', sess_config=None)\n",
    "    sampler = PairwiseSampler(batch_size=batch_size, dataset=train_dataset, num_process=4)\n",
    "    model_trainer = ImplicitModelTrainer(batch_size=batch_size, test_batch_size=test_batch_size, train_dataset=train_dataset, model=model, sampler=sampler, eval_save_prefix=OUTPUT_PATH + DATASET_NAME, item_serving_size=500)\n",
    "    auc_evaluator = AUC()\n",
    "\n",
    "    # Train the model\n",
    "    model_trainer.train(num_itr=10001, display_itr=display_itr)\n",
    "\n",
    "    # Save in the output folder\n",
    "    model.save(OUTPUT_PATH,None)\n",
    "\n",
    "    # Delete the model from the memory\n",
    "    del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DEFINING FUNCTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_propensities(n_users, n_items, trainfilename, gammas=[1.5, 2, 2.5, 3], normalize=True):\n",
    "\n",
    "    propensities = dict()\n",
    "    Ni = dict()\n",
    "    \n",
    "    # Compute frequencies of items in the training set\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "    del trainset\n",
    "\n",
    "    for gamma in gammas:\n",
    "        propensities[gamma] = np.zeros((n_users,n_items))\n",
    "\n",
    "  \n",
    "    for theitem in range(n_items):\n",
    "        if theitem not in Ni:\n",
    "            continue\n",
    "        for gamma in gammas:\n",
    "            propensities[gamma][:,theitem] =  np.power(Ni[theitem], (gamma + 1) / 2.0)\n",
    "\n",
    "    if normalize:\n",
    "        for gamma in gammas:\n",
    "            propensities[gamma] /= propensities[gamma].max()\n",
    "\n",
    "    return propensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eq(infilename, infilename_neg, trainfilename, propensities, K=1):\n",
    "\n",
    "    # Read pickles\n",
    "    infile = open(infilename, 'rb')\n",
    "    infile_neg = open(infilename_neg, 'rb')\n",
    "    P = pickle.load(infile)\n",
    "    infile.close()\n",
    "    P_neg = pickle.load(infile_neg)\n",
    "    infile_neg.close()\n",
    "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
    "    \n",
    "    # Merge P and P_neg\n",
    "    for theuser in P[\"users\"]:\n",
    "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "    \n",
    "    Zui = dict()\n",
    "    Ni = dict()\n",
    "    \n",
    "    # Compute frequencies of items in the training set\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "    del trainset\n",
    "\n",
    "    # Count #users with non-zero item frequencies\n",
    "    nonzero_user_count = 0\n",
    "    for theuser in P[\"users\"]:\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for pos_item in pos_items:\n",
    "            if pos_item in Ni:\n",
    "                nonzero_user_count += 1\n",
    "                break\n",
    "    \n",
    "    # Compute recommendations for each user\n",
    "    for theuser in P[\"users\"]:\n",
    "        all_scores = np.array(P[\"results\"][theuser])\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for i, pos_item in enumerate(pos_items):\n",
    "            pos_score = pos_scores[i]\n",
    "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
    "\n",
    "    \n",
    "    # Calculate per-user scores\n",
    "    sum_user_auc = 0.0\n",
    "    sum_user_recall = 0.0\n",
    "    for theuser in P[\"users\"]:\n",
    "        numerator_auc = 0.0\n",
    "        numerator_recall = 0.0\n",
    "        denominator = 0.0\n",
    "\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            # Skip items with null frequency\n",
    "            if theitem not in Ni:\n",
    "                continue\n",
    "            \n",
    "            # Load propensity score\n",
    "            pui = propensities[theuser][theitem]\n",
    "\n",
    "            # Add things to be summed for each item\n",
    "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser])) / pui\n",
    "\n",
    "            # Add things for recall\n",
    "            if Zui[(theuser, theitem)] < K:\n",
    "                numerator_recall += 1.0 / pui\n",
    "\n",
    "            # Increment denominator that the sum must be divided by\n",
    "            denominator += 1 / pui\n",
    "                \n",
    "        # If there was at least one item for the user, count the user and sum the results\n",
    "        if denominator > 0:\n",
    "            sum_user_auc += numerator_auc / denominator\n",
    "            sum_user_recall += numerator_recall / denominator \n",
    "\n",
    "    # Return\n",
    "    return {\n",
    "        \"auc\"       : sum_user_auc / nonzero_user_count,\n",
    "        \"recall\"    : sum_user_recall / nonzero_user_count,\n",
    "        \"bias\"      : -1,\n",
    "        \"concentration\" : -1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aoa(infilename, infilename_neg, trainfilename, K=1):\n",
    "\n",
    "    # Read pickles\n",
    "    infile = open(infilename, 'rb')\n",
    "    infile_neg = open(infilename_neg, 'rb')\n",
    "    P = pickle.load(infile)\n",
    "    infile.close()\n",
    "    P_neg = pickle.load(infile_neg)\n",
    "    infile_neg.close()\n",
    "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
    "    \n",
    "    # Merge P and P_neg\n",
    "    for theuser in P[\"users\"]:\n",
    "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "    \n",
    "    Zui = dict()\n",
    "    Ni = dict()\n",
    "\n",
    "    # Compute frequencies of items in the training set\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "    del trainset\n",
    "    \n",
    "    # Count #users with non-zero item frequencies\n",
    "    nonzero_user_count = 0\n",
    "    for theuser in P[\"users\"]:\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for pos_item in pos_items:\n",
    "            if pos_item in Ni:\n",
    "                nonzero_user_count += 1\n",
    "                break\n",
    "    \n",
    "    # Compute recommendations for each user\n",
    "    for theuser in P[\"users\"]:\n",
    "        all_scores = np.array(P[\"results\"][theuser])\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for i, pos_item in enumerate(pos_items):\n",
    "            pos_score = pos_scores[i]\n",
    "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
    "\n",
    "    # Calculate per-user scores\n",
    "    sum_user_auc = 0.0\n",
    "    sum_user_recall = 0.0\n",
    "    for theuser in P[\"users\"]:\n",
    "        numerator_auc = 0.0\n",
    "        numerator_recall = 0.0\n",
    "        denominator = 0.0\n",
    "\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            # Skip items with null frequency\n",
    "            if theitem not in Ni:\n",
    "                continue\n",
    "\n",
    "            # Add things to be summed for each item\n",
    "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser]))\n",
    "\n",
    "            # Add things for recall\n",
    "            if Zui[(theuser, theitem)] < K:\n",
    "                numerator_recall += 1.0\n",
    "                \n",
    "            # Increment denominator that the sum must be divided by\n",
    "            denominator += 1 \n",
    "\n",
    "        # If there was at least one item for the user, count the user and sum the results\n",
    "        if denominator > 0:\n",
    "            sum_user_auc += numerator_auc / denominator\n",
    "            sum_user_recall += numerator_recall / denominator\n",
    "\n",
    "    # Return\n",
    "    return {\n",
    "        \"auc\"       : sum_user_auc / nonzero_user_count,\n",
    "        \"recall\"    : sum_user_recall / nonzero_user_count,\n",
    "        \"bias\"      : -1,\n",
    "        \"concentration\" : -1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified(infilename, infilename_neg, trainfilename, propensities, K=30, partition=10, delta=0.1):\n",
    "\n",
    "    # Read pickles\n",
    "    infile = open(infilename, 'rb')\n",
    "    infile_neg = open(infilename_neg, 'rb')\n",
    "    P = pickle.load(infile)\n",
    "    infile.close()\n",
    "    P_neg = pickle.load(infile_neg)\n",
    "    infile_neg.close()\n",
    "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
    "\n",
    "    # Merge P and P_neg\n",
    "    for theuser in P[\"users\"]:\n",
    "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "\n",
    "    Zui = dict()\n",
    "    Ni = dict()\n",
    "\n",
    "    # Compute frequencies of items in the training set\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "    #del trainset\n",
    "\n",
    "    # Compute recommendations for each user\n",
    "    for theuser in P[\"users\"]:\n",
    "        all_scores = np.array(P[\"results\"][theuser])\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for i, pos_item in enumerate(pos_items):\n",
    "            pos_score = pos_scores[i]\n",
    "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
    "\n",
    "    w = dict()\n",
    "\n",
    "    # Store the ids of the items to be used to compute the bias\n",
    "    items_of_the_test_set = set()\n",
    "    for theuser in P[\"users\"]:\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            if theitem not in items_of_the_test_set:\n",
    "                items_of_the_test_set.add(theitem)\n",
    "\n",
    "   \n",
    "    # Using as pui a single row of propensities, as we assumed propensities to be user independent\n",
    "    pui = propensities[0,:]\n",
    "\n",
    "    # Take the list of items (not tuples) in pui sorted by value\n",
    "    items_sorted_by_value = np.argsort(pui)[::-1]\n",
    "\n",
    "    # Remove items not of the testset\n",
    "    items_sorted_by_value = np.array([item for item in items_sorted_by_value if item in items_of_the_test_set])\n",
    "\n",
    "    # Filter out indices of the items not in the test set\n",
    "    items_sorted_by_value = items_sorted_by_value[pui[items_sorted_by_value] > 0]\n",
    "\n",
    "    # Compute linspace between the pui[0] and pui[-1] \n",
    "    linspace = np.linspace(pui[items_sorted_by_value[0]], pui[items_sorted_by_value[-1]], partition+1)\n",
    "   \n",
    "    # Compute dictionary w, that is, for each item, assigns the average of the puis in the partition it belongs to\n",
    "    i=0\n",
    "    j = 0\n",
    "    while i < len(items_sorted_by_value):\n",
    "                            \n",
    "        avg = 0\n",
    "        start = i\n",
    "        end = i\n",
    "    \n",
    "        while i < len(items_sorted_by_value) and pui[items_sorted_by_value[i]] >= linspace[j+1]:\n",
    "            avg += 1.0 / pui[items_sorted_by_value[i]]\n",
    "            end = i\n",
    "            i += 1\n",
    "        avg = avg / (end - start + 1)\n",
    "\n",
    "        for k in range(start, end+1):\n",
    "            w[items_sorted_by_value[k]] = avg\n",
    "\n",
    "        j += 1\n",
    "\n",
    "    # Compute bias' numerator\n",
    "    bias = 0.0\n",
    "    for k in items_sorted_by_value:\n",
    "        # add |pui*w - 1!|\n",
    "        bias += abs(pui[k] * w[k] - 1)\n",
    "    # Multiply by number of users\n",
    "    bias *= len(P[\"users\"])\n",
    "\n",
    "    # Compute concentrations numerator (for each user)\n",
    "    concentrations = {}\n",
    "    max_w = max(w.values())\n",
    "    # ... by computing the sum of squares of w for each user\n",
    "    for user, item in zip(trainset['user_id'], trainset['item_id']):\n",
    "        # Iterate over the trainset to compute the sum of squares for each user\n",
    "        if item in w:\n",
    "            if user not in concentrations:\n",
    "                concentrations[user] = 0\n",
    "            concentrations[user] += w[item] ** 2\n",
    "    # ... and then applying the formula\n",
    "    for user in concentrations:\n",
    "        concentrations[user] = math.sqrt(concentrations[user] * 2 * math.log(2/delta)) + max_w * 7 * math.log(2/delta)\n",
    "    # Now sum all the concentrations\n",
    "    concentration = sum(concentrations.values())\n",
    "\n",
    "    # Calculate per-user scores\n",
    "    nonzero_user_count = 0\n",
    "    sum_user_auc = 0.0\n",
    "    sum_user_recall = 0.0\n",
    "    for theuser in P[\"users\"]:\n",
    "        numerator_auc = 0.0\n",
    "        numerator_recall = 0.0\n",
    "        denominator = 0.0\n",
    "        \n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            # Skip items with null frequency\n",
    "            if  theitem not in Ni:\n",
    "                continue\n",
    "\n",
    "            # Add things to be summed for each item\n",
    "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser])) * w[theitem]\n",
    "\n",
    "            # Add things for recall\n",
    "            if Zui[(theuser, theitem)] < K:\n",
    "                numerator_recall += 1.0 * w[theitem]\n",
    "\n",
    "            # Increment denominator that the sum must be divided by \n",
    "            denominator += 1 / pui[theitem]\n",
    "\n",
    "        # If there was at least one item for the user, count the user and sum the results\n",
    "        if denominator > 0:\n",
    "            nonzero_user_count += 1\n",
    "            sum_user_auc += numerator_auc / denominator\n",
    "            sum_user_recall += numerator_recall / denominator \n",
    "\n",
    "    # Return\n",
    "    return {\n",
    "        \"auc\"       : sum_user_auc / nonzero_user_count, \n",
    "        \"recall\"    : sum_user_recall / nonzero_user_count,\n",
    "        \"bias\"      : bias,\n",
    "        \"concentration\" : concentration\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_logspace(infilename, infilename_neg, trainfilename, propensities, K=30, partition=10, delta=0.1):\n",
    "\n",
    "    # Read pickles\n",
    "    infile = open(infilename, 'rb')\n",
    "    infile_neg = open(infilename_neg, 'rb')\n",
    "    P = pickle.load(infile)\n",
    "    infile.close()\n",
    "    P_neg = pickle.load(infile_neg)\n",
    "    infile_neg.close()\n",
    "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
    "\n",
    "    # Merge P and P_neg\n",
    "    for theuser in P[\"users\"]:\n",
    "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "\n",
    "    Zui = dict()\n",
    "    Ni = dict()\n",
    "\n",
    "    # Compute frequencies of items in the training set\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "    #del trainset\n",
    "\n",
    "    # Compute recommendations for each user\n",
    "    for theuser in P[\"users\"]:\n",
    "        all_scores = np.array(P[\"results\"][theuser])\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for i, pos_item in enumerate(pos_items):\n",
    "            pos_score = pos_scores[i]\n",
    "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
    "\n",
    "    w = dict()\n",
    "\n",
    "    # Store the ids of the items to be used to compute the bias\n",
    "    items_of_the_test_set = set()\n",
    "    for theuser in P[\"users\"]:\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            if theitem not in items_of_the_test_set:\n",
    "                items_of_the_test_set.add(theitem)\n",
    "\n",
    "   \n",
    "    # Using as pui a single row of propensities, as we assumed propensities to be user independent\n",
    "    pui = propensities[0,:]\n",
    "\n",
    "    # Take the list of items (not tuples) in pui sorted by value\n",
    "    items_sorted_by_value = np.argsort(pui)[::-1]\n",
    "\n",
    "    # Remove items not of the testset\n",
    "    items_sorted_by_value = np.array([item for item in items_sorted_by_value if item in items_of_the_test_set])\n",
    "\n",
    "    # Filter out indices of the items not in the test set\n",
    "    items_sorted_by_value = items_sorted_by_value[pui[items_sorted_by_value] > 0]\n",
    "\n",
    "    # Maybe try to split the logspace instead of the linspace?\n",
    "    logspace = np.logspace(pui[items_sorted_by_value[0]], pui[items_sorted_by_value[-1]], partition+1)\n",
    "   \n",
    "    # Compute dictionary w, that is, for each item, assigns the average of the puis in the partition it belongs to\n",
    "    i=0\n",
    "    j = 0\n",
    "    while i < len(items_sorted_by_value):\n",
    "                            \n",
    "        avg = 0\n",
    "        start = i\n",
    "        end = i\n",
    "    \n",
    "        while i < len(items_sorted_by_value) and pui[items_sorted_by_value[i]] >= logspace[j+1]:\n",
    "            avg += 1.0 / pui[items_sorted_by_value[i]]\n",
    "            end = i\n",
    "            i += 1\n",
    "        \n",
    "        # Is the average the only good choice? even with the log space split?\n",
    "        avg = avg / (end - start + 1)\n",
    "\n",
    "        for k in range(start, end+1):\n",
    "            w[items_sorted_by_value[k]] = avg\n",
    "\n",
    "        j += 1\n",
    "\n",
    "        # Compute bias' numerator\n",
    "        bias = 0.0\n",
    "        for k in items_sorted_by_value:\n",
    "            # add |pui*w - 1!|\n",
    "            bias += abs(pui[k] * w[k] - 1)\n",
    "        # Multiply by number of users\n",
    "        bias *= len(P[\"users\"])\n",
    "\n",
    "        # Compute concentrations numerator (for each user)\n",
    "        concentrations = {}\n",
    "        max_w = max(w.values())\n",
    "        # ... by computing the sum of squares of w for each user\n",
    "        for user, item in zip(trainset['user_id'], trainset['item_id']):\n",
    "            # Iterate over the trainset to compute the sum of squares for each user\n",
    "            if item in w:\n",
    "                if user not in concentrations:\n",
    "                    concentrations[user] = 0\n",
    "                concentrations[user] += w[item] ** 2\n",
    "        # ... and then applying the formula\n",
    "        for user in concentrations:\n",
    "            concentrations[user] = math.sqrt(concentrations[user] * 2 * math.log(2/delta)) + max_w * 7 * math.log(2/delta)\n",
    "        # Now sum all the concentrations\n",
    "        concentration = sum(concentrations.values())\n",
    "\n",
    "    # Calculate per-user scores\n",
    "    nonzero_user_count = 0\n",
    "    sum_user_auc = 0.0\n",
    "    sum_user_recall = 0.0\n",
    "    for theuser in P[\"users\"]:\n",
    "        numerator_auc = 0.0\n",
    "        numerator_recall = 0.0\n",
    "        denominator = 0.0\n",
    "        \n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            # Skip items with null frequency\n",
    "            if  theitem not in Ni:\n",
    "                continue\n",
    "\n",
    "            # Add things to be summed for each item\n",
    "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser])) * w[theitem]\n",
    "\n",
    "            # Add things for recall\n",
    "            if Zui[(theuser, theitem)] < K:\n",
    "                numerator_recall += 1.0 * w[theitem]\n",
    "\n",
    "            # Increment denominator that the sum must be divided by \n",
    "            denominator += 1 / pui[theitem]\n",
    "\n",
    "        # If there was at least one item for the user, count the user and sum the results\n",
    "        if denominator > 0:\n",
    "            nonzero_user_count += 1\n",
    "            sum_user_auc += numerator_auc / denominator\n",
    "            sum_user_recall += numerator_recall / denominator  \n",
    "\n",
    "    # Return\n",
    "    return {\n",
    "        \"auc\"       : sum_user_auc / nonzero_user_count, \n",
    "        \"recall\"    : sum_user_recall / nonzero_user_count,\n",
    "        \"bias\"      : bias,\n",
    "        \"concentration\" : concentration\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This version uses the linspace of the number of number of items used for evaluation, not of the propensities\n",
    "def stratified_2(infilename, infilename_neg, trainfilename, propensities, K=30, partition=10, delta=0.1):\n",
    "\n",
    "    # Read pickles\n",
    "    infile = open(infilename, 'rb')\n",
    "    infile_neg = open(infilename_neg, 'rb')\n",
    "    P = pickle.load(infile)\n",
    "    infile.close()\n",
    "    P_neg = pickle.load(infile_neg)\n",
    "    infile_neg.close()\n",
    "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
    "\n",
    "    # Merge P and P_neg\n",
    "    for theuser in P[\"users\"]:\n",
    "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "\n",
    "    Zui = dict()\n",
    "    Ni = dict()\n",
    "\n",
    "    # Compute frequencies of items in the training set\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "    #del trainset\n",
    "\n",
    "    # Compute recommendations for each user\n",
    "    for theuser in P[\"users\"]:\n",
    "        all_scores = np.array(P[\"results\"][theuser])\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for i, pos_item in enumerate(pos_items):\n",
    "            pos_score = pos_scores[i]\n",
    "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
    "\n",
    "    w = dict()\n",
    "\n",
    "    # Store the ids of the items to be used to compute the bias\n",
    "    items_of_the_test_set = set()\n",
    "    for theuser in P[\"users\"]:\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            if theitem not in items_of_the_test_set:\n",
    "                items_of_the_test_set.add(theitem)\n",
    "\n",
    "   \n",
    "    # Using as pui a single row of propensities, as we assumed propensities to be user independent\n",
    "    pui = propensities[0,:]\n",
    "\n",
    "    # Take the list of items (not tuples) in pui sorted by value\n",
    "    items_sorted_by_value = np.argsort(pui)[::-1]\n",
    "\n",
    "    # Remove items not of the testset\n",
    "    items_sorted_by_value = np.array([item for item in items_sorted_by_value if item in items_of_the_test_set])\n",
    "\n",
    "    # Filter out indices of the items not in the test set\n",
    "    items_sorted_by_value = items_sorted_by_value[pui[items_sorted_by_value] > 0]\n",
    "\n",
    "    # Compute linspace between the 0 to len(item_sorted...)\n",
    "    linspace = np.linspace(0, len(items_sorted_by_value), partition+1)\n",
    "   \n",
    "    # Compute dictionary w, that is, for each item, assigns the average of the puis in the partition it belongs to\n",
    "    i=0\n",
    "    j = 0\n",
    "    while i < len(items_sorted_by_value):\n",
    "                            \n",
    "        avg = 0\n",
    "        start = i\n",
    "        end = i\n",
    "    \n",
    "        while i < len(items_sorted_by_value) and i < linspace[j+1]:\n",
    "            avg += 1.0 / pui[items_sorted_by_value[i]]\n",
    "            end = i\n",
    "            i += 1\n",
    "        \n",
    "        avg = avg / (end - start + 1)\n",
    "\n",
    "        for k in range(start, end+1):\n",
    "            w[items_sorted_by_value[k]] = avg\n",
    "\n",
    "        j += 1\n",
    "\n",
    "    # Compute bias' numerator\n",
    "    bias = 0.0\n",
    "    for k in items_sorted_by_value:\n",
    "        # add |pui*w - 1!|\n",
    "        bias += abs(pui[k] * w[k] - 1)\n",
    "    # Multiply by number of users\n",
    "    bias *= len(P[\"users\"])\n",
    "\n",
    "    # Compute concentrations numerator (for each user)\n",
    "    concentrations = {}\n",
    "    max_w = max(w.values())\n",
    "    # ... by computing the sum of squares of w for each user\n",
    "    for user, item in zip(trainset['user_id'], trainset['item_id']):\n",
    "        # Iterate over the trainset to compute the sum of squares for each user\n",
    "        if item in w:\n",
    "            if user not in concentrations:\n",
    "                concentrations[user] = 0\n",
    "            concentrations[user] += w[item] ** 2\n",
    "    # ... and then applying the formula\n",
    "    for user in concentrations:\n",
    "        concentrations[user] = math.sqrt(concentrations[user] * 2 * math.log(2/delta)) + max_w * 7 * math.log(2/delta)\n",
    "    # Now sum all the concentrations\n",
    "    concentration = sum(concentrations.values())\n",
    "\n",
    "    # Calculate per-user scores\n",
    "    nonzero_user_count = 0\n",
    "    sum_user_auc = 0.0\n",
    "    sum_user_recall = 0.0\n",
    "    for theuser in P[\"users\"]:\n",
    "        numerator_auc = 0.0\n",
    "        numerator_recall = 0.0\n",
    "        denominator = 0.0\n",
    "        \n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            # Skip items with null frequency\n",
    "            if  theitem not in Ni:\n",
    "                continue\n",
    "\n",
    "            # Add things to be summed for each item\n",
    "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser])) * w[theitem]\n",
    "\n",
    "            # Add things for recall\n",
    "            if Zui[(theuser, theitem)] < K:\n",
    "                numerator_recall += 1.0 * w[theitem]\n",
    "\n",
    "            # Increment denominator that the sum must be divided by \n",
    "            denominator += 1 / pui[theitem]\n",
    "\n",
    "        # If there was at least one item for the user, count the user and sum the results\n",
    "        if denominator > 0:\n",
    "            nonzero_user_count += 1\n",
    "            sum_user_auc += numerator_auc / denominator\n",
    "            sum_user_recall += numerator_recall / denominator \n",
    "\n",
    "    # Return\n",
    "    return {\n",
    "        \"auc\"       : sum_user_auc / nonzero_user_count, \n",
    "        \"recall\"    : sum_user_recall / nonzero_user_count,\n",
    "        \"bias\"      : bias,\n",
    "        \"concentration\" : concentration\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **EVALUATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMAS = [1.5,2,2.5,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "raw_data = dict()\n",
    "raw_data['train_data'] = np.load(output_name + \"training_arr.npy\")\n",
    "raw_data['test_data_pos_biased'] = np.load(output_name + \"biased-test_arr_pos.npy\")\n",
    "raw_data['test_data_neg_biased'] = np.load(output_name + \"biased-test_arr_neg.npy\")\n",
    "raw_data['test_data_pos_unbiased'] = np.load(output_name + \"unbiased-test_arr_pos.npy\")\n",
    "raw_data['test_data_neg_unbiased'] = np.load(output_name + \"unbiased-test_arr_neg.npy\")\n",
    "raw_data['max_user'] = 15401\n",
    "raw_data['max_item'] = 1001\n",
    "batch_size = 8000\n",
    "test_batch_size = 1000\n",
    "display_itr = 1000\n",
    "\n",
    "# Load data\n",
    "train_dataset = ImplicitDataset(raw_data['train_data'], raw_data['max_user'], raw_data['max_item'], name='Train')\n",
    "test_dataset_pos_biased = ImplicitDataset(raw_data['test_data_pos_biased'], raw_data['max_user'], raw_data['max_item'])\n",
    "test_dataset_neg_biased = ImplicitDataset(raw_data['test_data_neg_biased'], raw_data['max_user'], raw_data['max_item'])\n",
    "test_dataset_pos_unbiased = ImplicitDataset(raw_data['test_data_pos_unbiased'], raw_data['max_user'], raw_data['max_item'])\n",
    "test_dataset_neg_unbiased = ImplicitDataset(raw_data['test_data_neg_unbiased'], raw_data['max_user'], raw_data['max_item'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:391: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:31: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:41: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:43: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:33: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/interactions/pairwise_eu_dist.py:71: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:596: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:75: The name tf.scatter_update is deprecated. Please use tf.compat.v1.scatter_update instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:144: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:365: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:148: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-20 14:38:45.341237: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
      "2024-08-20 14:38:45.344760: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 4192055000 Hz\n",
      "2024-08-20 14:38:45.345364: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c1cb36ee50 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2024-08-20 14:38:45.345381: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./generated_data/cml-yahoo/\n",
      "[Subsampling negative items]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = MODEL_CLASS(batch_size=batch_size, max_user=train_dataset.max_user(), max_item=train_dataset.max_item(), dim_embed=50, l2_reg=0.001, opt='Adam', sess_config=None)\n",
    "sampler = PairwiseSampler(batch_size=batch_size, dataset=train_dataset, num_process=4)\n",
    "model_trainer = ImplicitModelTrainer(batch_size=batch_size, test_batch_size=test_batch_size, train_dataset=train_dataset, model=model, sampler=sampler, eval_save_prefix=OUTPUT_PATH + DATASET_NAME, item_serving_size=500)\n",
    "auc_evaluator = AUC()\n",
    "\n",
    "# Load model\n",
    "model.load(OUTPUT_PATH)\n",
    "\n",
    "# Set parameters\n",
    "model_trainer._eval_manager = ImplicitEvalManager(evaluators=[auc_evaluator])\n",
    "model_trainer._num_negatives = 200\n",
    "model_trainer._exclude_positives([train_dataset, test_dataset_pos_biased, test_dataset_neg_biased])\n",
    "model_trainer._sample_negatives(seed=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biased Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2070/2070 [00:00<00:00, 2463.69it/s]\n",
      "100%|| 2296/2296 [00:25<00:00, 90.02it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AUC': [0.5039491525423728,\n",
       "  0.5153220338983051,\n",
       "  0.5071644295302014,\n",
       "  0.5041442953020134,\n",
       "  0.4916666666666667,\n",
       "  0.4769661016949153,\n",
       "  0.5358892617449664,\n",
       "  0.5279222972972972,\n",
       "  0.5011577181208055,\n",
       "  0.4937878787878789,\n",
       "  0.517864406779661,\n",
       "  0.5050675675675675,\n",
       "  0.4627638190954774,\n",
       "  0.4978,\n",
       "  0.5228472222222221,\n",
       "  0.4881879194630873,\n",
       "  0.5278595317725752,\n",
       "  0.48707482993197276,\n",
       "  0.5023322147651007,\n",
       "  0.5379797979797981,\n",
       "  0.5238127090301004,\n",
       "  0.5319349315068492,\n",
       "  0.486571906354515,\n",
       "  0.4471644295302013,\n",
       "  0.4835451505016722,\n",
       "  0.4724295774647888,\n",
       "  0.49429553264604814,\n",
       "  0.48894648829431436,\n",
       "  0.4587074829931973,\n",
       "  0.5340909090909091,\n",
       "  0.4631711409395972,\n",
       "  0.5076094276094276,\n",
       "  0.47067708333333336,\n",
       "  0.5034782608695653,\n",
       "  0.47471476510067107,\n",
       "  0.49164429530201337,\n",
       "  0.5371070234113712,\n",
       "  0.46685374149659864,\n",
       "  0.4741442953020134,\n",
       "  0.5084966216216217,\n",
       "  0.5023986486486486,\n",
       "  0.43272413793103454,\n",
       "  0.552098976109215,\n",
       "  0.47006734006734013,\n",
       "  0.4715604026845638,\n",
       "  0.49927609427609426,\n",
       "  0.5302525252525253,\n",
       "  0.5354166666666667,\n",
       "  0.501510067114094,\n",
       "  0.4929765886287625,\n",
       "  0.5082833333333333,\n",
       "  0.4901010101010101,\n",
       "  0.4989322033898305,\n",
       "  0.48835570469798656,\n",
       "  0.505222602739726,\n",
       "  0.5098154362416107,\n",
       "  0.4819191919191919,\n",
       "  0.49674137931034484,\n",
       "  0.46536912751677856,\n",
       "  0.5564067796610169,\n",
       "  0.501077441077441,\n",
       "  0.4992114093959731,\n",
       "  0.4784523809523809,\n",
       "  0.4560606060606061,\n",
       "  0.4482775919732441,\n",
       "  0.513758389261745,\n",
       "  0.4868518518518519,\n",
       "  0.48255033557046983,\n",
       "  0.4803666666666667,\n",
       "  0.4899163879598662,\n",
       "  0.48789297658862874,\n",
       "  0.5199665551839466,\n",
       "  0.43978114478114483,\n",
       "  0.5009246575342465,\n",
       "  0.48510135135135135,\n",
       "  0.4910338983050847,\n",
       "  0.4663255033557047,\n",
       "  0.4834333333333332,\n",
       "  0.49291946308724827,\n",
       "  0.4860437710437711,\n",
       "  0.4717676767676768,\n",
       "  0.4663265306122449,\n",
       "  0.5138422818791947,\n",
       "  0.5229530201342281,\n",
       "  0.4950505050505051,\n",
       "  0.502104377104377,\n",
       "  0.45449324324324325,\n",
       "  0.49917525773195875,\n",
       "  0.5685304054054054,\n",
       "  0.5117905405405405,\n",
       "  0.5203703703703704,\n",
       "  0.5236912751677852,\n",
       "  0.4340878378378379,\n",
       "  0.48471476510067113,\n",
       "  0.5192087542087541,\n",
       "  0.468167808219178,\n",
       "  0.5025335570469799,\n",
       "  0.48755852842809366,\n",
       "  0.478125,\n",
       "  0.5375838926174498,\n",
       "  0.48533783783783785,\n",
       "  0.49273972602739724,\n",
       "  0.5229761904761905,\n",
       "  0.5329545454545455,\n",
       "  0.5148833333333334,\n",
       "  0.5271186440677966,\n",
       "  0.49339464882943135,\n",
       "  0.498238255033557,\n",
       "  0.5212333333333333,\n",
       "  0.4839057239057239,\n",
       "  0.48159395973154356,\n",
       "  0.4978619528619529,\n",
       "  0.4844612794612794,\n",
       "  0.48930272108843537,\n",
       "  0.5136363636363637,\n",
       "  0.5284500000000001,\n",
       "  0.46464765100671135,\n",
       "  0.4962289562289562,\n",
       "  0.512813559322034,\n",
       "  0.48421666666666663,\n",
       "  0.4463804713804713,\n",
       "  0.4777272727272727,\n",
       "  0.5531270903010033,\n",
       "  0.4930536912751678,\n",
       "  0.48300000000000004,\n",
       "  0.5166329966329967,\n",
       "  0.5321548821548822,\n",
       "  0.5159259259259259,\n",
       "  0.5340604026845638,\n",
       "  0.5417114093959732,\n",
       "  0.4976006711409396,\n",
       "  0.47068728522336767,\n",
       "  0.5149830508474577,\n",
       "  0.5024564459930314,\n",
       "  0.5533724832214765,\n",
       "  0.48598993288590603,\n",
       "  0.5111486486486486,\n",
       "  0.498003355704698,\n",
       "  0.4818288590604026,\n",
       "  0.4687244897959184,\n",
       "  0.5204391891891892,\n",
       "  0.5375,\n",
       "  0.5426779661016949,\n",
       "  0.5323333333333333,\n",
       "  0.5080821917808219,\n",
       "  0.4807357859531773,\n",
       "  0.5149999999999999,\n",
       "  0.5062331081081081,\n",
       "  0.529261744966443,\n",
       "  0.4736073825503355,\n",
       "  0.48456597222222225,\n",
       "  0.49928093645484956,\n",
       "  0.5474328859060402,\n",
       "  0.5032705479452054,\n",
       "  0.5009,\n",
       "  0.52965,\n",
       "  0.5114932885906041,\n",
       "  0.5182154882154882,\n",
       "  0.5104545454545455,\n",
       "  0.4814864864864864,\n",
       "  0.5309427609427609,\n",
       "  0.4904898648648649,\n",
       "  0.5223310810810812,\n",
       "  0.4693288590604026,\n",
       "  0.48721428571428577,\n",
       "  0.547962962962963,\n",
       "  0.5206879194630871,\n",
       "  0.5005084745762713,\n",
       "  0.5179765886287625,\n",
       "  0.5209899328859061,\n",
       "  0.4728716216216216,\n",
       "  0.4871088435374149,\n",
       "  0.5215476190476189,\n",
       "  0.4805405405405406,\n",
       "  0.5123986486486486,\n",
       "  0.4905518394648829,\n",
       "  0.49280201342281876,\n",
       "  0.5124161073825503,\n",
       "  0.5400335570469799,\n",
       "  0.4808528428093646,\n",
       "  0.43393220338983046,\n",
       "  0.48708754208754207,\n",
       "  0.5307457627118645,\n",
       "  0.503973063973064,\n",
       "  0.4994798657718121,\n",
       "  0.4761204013377927,\n",
       "  0.5135906040268456,\n",
       "  0.5107718120805368,\n",
       "  0.4634833333333334,\n",
       "  0.4558999999999999,\n",
       "  0.5477272727272727,\n",
       "  0.5680267558528428,\n",
       "  0.4792929292929294,\n",
       "  0.516864406779661,\n",
       "  0.4476027397260274,\n",
       "  0.5520033670033669,\n",
       "  0.4759726962457338,\n",
       "  0.5128523489932886,\n",
       "  0.5026588628762542,\n",
       "  0.5293074324324325,\n",
       "  0.5235570469798657,\n",
       "  0.5602181208053691,\n",
       "  0.5028187919463087,\n",
       "  0.5020333333333333,\n",
       "  0.4922203389830508,\n",
       "  0.4955555555555556,\n",
       "  0.5040033783783784,\n",
       "  0.5060101010101011,\n",
       "  0.46699999999999997,\n",
       "  0.5175,\n",
       "  0.4676351351351352,\n",
       "  0.4892976588628763,\n",
       "  0.45558528428093653,\n",
       "  0.4696464646464647,\n",
       "  0.5021717171717172,\n",
       "  0.46511705685618726,\n",
       "  0.4873569023569024,\n",
       "  0.5255218855218855,\n",
       "  0.5202027027027026,\n",
       "  0.5198657718120805,\n",
       "  0.4885810810810811,\n",
       "  0.4880936454849498,\n",
       "  0.47459999999999997,\n",
       "  0.5049665551839465,\n",
       "  0.556761744966443,\n",
       "  0.5139527027027027,\n",
       "  0.5177181208053692,\n",
       "  0.5114765100671141,\n",
       "  0.4706802721088435,\n",
       "  0.49824915824915833,\n",
       "  0.4585324232081912,\n",
       "  0.4998657718120805,\n",
       "  0.4962709030100334,\n",
       "  0.5072147651006712,\n",
       "  0.4952006688963211,\n",
       "  0.43776845637583894,\n",
       "  0.508695652173913,\n",
       "  0.4955326460481099,\n",
       "  0.49692953020134223,\n",
       "  0.5042424242424242,\n",
       "  0.49229865771812076,\n",
       "  0.49566666666666664,\n",
       "  0.5019666666666668,\n",
       "  0.5097483221476509,\n",
       "  0.5029833333333333,\n",
       "  0.5340301003344481,\n",
       "  0.45427586206896553,\n",
       "  0.4616442953020134,\n",
       "  0.457768456375839,\n",
       "  0.5180976430976431,\n",
       "  0.5188813559322034,\n",
       "  0.5221794871794873,\n",
       "  0.5348322147651007,\n",
       "  0.4983277591973244,\n",
       "  0.5294636678200692,\n",
       "  0.5026333333333333,\n",
       "  0.47085,\n",
       "  0.4974414715719064,\n",
       "  0.48731418918918923,\n",
       "  0.4833779264214046,\n",
       "  0.47308333333333336,\n",
       "  0.48870748299319733,\n",
       "  0.48124579124579125,\n",
       "  0.5017340067340067,\n",
       "  0.4906879194630872,\n",
       "  0.5333221476510067,\n",
       "  0.5085166666666667,\n",
       "  0.5069295302013422,\n",
       "  0.5365762711864408,\n",
       "  0.5447651006711409,\n",
       "  0.505016835016835,\n",
       "  0.5068620689655172,\n",
       "  0.5263028169014085,\n",
       "  0.49847972972972976,\n",
       "  0.516593220338983,\n",
       "  0.5236195286195287,\n",
       "  0.516996644295302,\n",
       "  0.5083053691275168,\n",
       "  0.4901546391752577,\n",
       "  0.5471477663230241,\n",
       "  0.4791836734693877,\n",
       "  0.4698,\n",
       "  0.52245,\n",
       "  0.46481605351170563,\n",
       "  0.48752508361204006,\n",
       "  0.4587625418060201,\n",
       "  0.5376767676767676,\n",
       "  0.4496296296296296,\n",
       "  0.49961279461279456,\n",
       "  0.5163175675675675,\n",
       "  0.44933110367892987,\n",
       "  0.48034129692832767,\n",
       "  0.5517500000000001,\n",
       "  0.4795945945945945,\n",
       "  0.4551839464882943,\n",
       "  0.4662207357859532,\n",
       "  0.48835570469798656,\n",
       "  0.514113712374582,\n",
       "  0.49450511945392495,\n",
       "  0.5335953177257524,\n",
       "  0.5212372881355932,\n",
       "  0.5463422818791946,\n",
       "  0.514206081081081,\n",
       "  0.5274832214765101,\n",
       "  0.4501689189189189,\n",
       "  0.5060034013605442,\n",
       "  0.5294773519163763,\n",
       "  0.5142307692307693,\n",
       "  0.46547297297297296,\n",
       "  0.46843537414965986,\n",
       "  0.4865833333333332,\n",
       "  0.5041442953020133,\n",
       "  0.5270066889632107,\n",
       "  0.4949665551839465,\n",
       "  0.5170637583892618,\n",
       "  0.444748322147651,\n",
       "  0.5229720279720279,\n",
       "  0.4885521885521885,\n",
       "  0.4820000000000001,\n",
       "  0.5010034013605442,\n",
       "  0.5086610169491526,\n",
       "  0.49687290969899667,\n",
       "  0.4773639455782313,\n",
       "  0.4761705685618729,\n",
       "  0.5182491582491583,\n",
       "  0.5245101351351351,\n",
       "  0.5236531986531986,\n",
       "  0.49834448160535116,\n",
       "  0.5354666666666666,\n",
       "  0.5026013513513513,\n",
       "  0.5170469798657717,\n",
       "  0.47272575250836113,\n",
       "  0.5327551020408163,\n",
       "  0.4980134680134679,\n",
       "  0.544404761904762,\n",
       "  0.48158075601374567,\n",
       "  0.4664393939393939,\n",
       "  0.5208417508417509,\n",
       "  0.5539799331103679,\n",
       "  0.4806228956228956,\n",
       "  0.49545762711864405,\n",
       "  0.47611301369863024,\n",
       "  0.4625084175084174,\n",
       "  0.4419938650306748,\n",
       "  0.4866220735785953,\n",
       "  0.48070469798657717,\n",
       "  0.4409060402684563,\n",
       "  0.4772666666666667,\n",
       "  0.5150503355704699,\n",
       "  0.49377516778523495,\n",
       "  0.5116047297297297,\n",
       "  0.49609999999999993,\n",
       "  0.5277931034482759,\n",
       "  0.5024916387959867,\n",
       "  0.509765100671141,\n",
       "  0.5477586206896552,\n",
       "  0.512125850340136,\n",
       "  0.46603040540540536,\n",
       "  0.5169230769230769,\n",
       "  0.4949,\n",
       "  0.47645484949832767,\n",
       "  0.5344295302013422,\n",
       "  0.5483838383838384,\n",
       "  0.5249498327759198,\n",
       "  0.5190169491525424,\n",
       "  0.49781355932203386,\n",
       "  0.4817892976588629,\n",
       "  0.4717114093959731,\n",
       "  0.4864965986394559,\n",
       "  0.5052341137123746,\n",
       "  0.4739393939393939,\n",
       "  0.4836789297658863,\n",
       "  0.5103833333333334,\n",
       "  0.5212457912457913,\n",
       "  0.5182718120805369,\n",
       "  0.5386531986531987,\n",
       "  0.5098662207357859,\n",
       "  0.5060702341137123,\n",
       "  0.4984589041095891,\n",
       "  0.4993311036789298,\n",
       "  0.5062367491166078,\n",
       "  0.5854119850187266,\n",
       "  0.521986531986532,\n",
       "  0.52006734006734,\n",
       "  0.5258724832214764,\n",
       "  0.486048951048951,\n",
       "  0.5220101351351351,\n",
       "  0.4736655405405406,\n",
       "  0.5076870748299319,\n",
       "  0.4756565656565656,\n",
       "  0.44719594594594597,\n",
       "  0.47978333333333334,\n",
       "  0.5131666666666667,\n",
       "  0.5506996587030717,\n",
       "  0.4710535117056856,\n",
       "  0.48010033444816047,\n",
       "  0.4857770270270271,\n",
       "  0.5075919732441471,\n",
       "  0.516722972972973,\n",
       "  0.5068060200668897,\n",
       "  0.5105067567567568,\n",
       "  0.4967844522968197,\n",
       "  0.5209731543624161,\n",
       "  0.48429999999999995,\n",
       "  0.48713804713804715,\n",
       "  0.5288590604026846,\n",
       "  0.5366555183946489,\n",
       "  0.5046153846153846,\n",
       "  0.4903535353535353,\n",
       "  0.49616554054054046,\n",
       "  0.4827796610169492,\n",
       "  0.47077966101694924,\n",
       "  0.4557457627118644,\n",
       "  0.4282608695652174,\n",
       "  0.5166000000000001,\n",
       "  0.5182711864406779,\n",
       "  0.5022651006711409,\n",
       "  0.45803448275862063,\n",
       "  0.5120066889632107,\n",
       "  0.49745762711864405,\n",
       "  0.504845890410959,\n",
       "  0.5155555555555557,\n",
       "  0.5534511784511784,\n",
       "  0.5141778523489933,\n",
       "  0.49645270270270264,\n",
       "  0.5064548494983278,\n",
       "  0.4978020134228187,\n",
       "  0.5041778523489933,\n",
       "  0.48348993288590597,\n",
       "  0.5604745762711866,\n",
       "  0.5117142857142856,\n",
       "  0.5138795986622073,\n",
       "  0.4871548821548822,\n",
       "  0.5136363636363637,\n",
       "  0.4702372881355931,\n",
       "  0.44501742160278746,\n",
       "  0.48424749163879605,\n",
       "  0.5433035714285714,\n",
       "  0.4663389830508474,\n",
       "  0.5298316498316499,\n",
       "  0.5222569444444445,\n",
       "  0.4950170068027211,\n",
       "  0.549543918918919,\n",
       "  0.5032996632996634,\n",
       "  0.4836287625418061,\n",
       "  0.510738255033557,\n",
       "  0.443597972972973,\n",
       "  0.5244127516778524,\n",
       "  0.45299663299663295,\n",
       "  0.520847750865052,\n",
       "  0.5330369127516777,\n",
       "  0.4768729096989967,\n",
       "  0.47568791946308725,\n",
       "  0.4879761904761905,\n",
       "  0.4855574324324325,\n",
       "  0.49722033898305085,\n",
       "  0.5172666666666667,\n",
       "  0.4588775510204082,\n",
       "  0.525271186440678,\n",
       "  0.487962962962963,\n",
       "  0.48925,\n",
       "  0.4595205479452055,\n",
       "  0.519554794520548,\n",
       "  0.5165436241610738,\n",
       "  0.4869565217391305,\n",
       "  0.4990100671140939,\n",
       "  0.5043559322033898,\n",
       "  0.5251178451178451,\n",
       "  0.47528619528619526,\n",
       "  0.5441047297297298,\n",
       "  0.5376845637583894,\n",
       "  0.4899290780141845,\n",
       "  0.5087354085603113,\n",
       "  0.5175418060200669,\n",
       "  0.49663299663299665,\n",
       "  0.4928938356164384,\n",
       "  0.5427852348993288,\n",
       "  0.47801694915254234,\n",
       "  0.49279264214046825,\n",
       "  0.5214166666666666,\n",
       "  0.5197474747474747,\n",
       "  0.5060535117056856,\n",
       "  0.4745317725752508,\n",
       "  0.5489057239057239,\n",
       "  0.5161734693877551,\n",
       "  0.48935593220338985,\n",
       "  0.5177380952380952,\n",
       "  0.4994763513513513,\n",
       "  0.5454,\n",
       "  0.48784280936454855,\n",
       "  0.5091471571906355,\n",
       "  0.5071061643835616,\n",
       "  0.5102525252525252,\n",
       "  0.543238255033557,\n",
       "  0.49153846153846154,\n",
       "  0.4629251700680272,\n",
       "  0.4936166666666667,\n",
       "  0.5113299663299664,\n",
       "  0.5168394648829431,\n",
       "  0.50395,\n",
       "  0.49142140468227424,\n",
       "  0.5658833333333333,\n",
       "  0.515304054054054,\n",
       "  0.524714765100671,\n",
       "  0.4985284280936454,\n",
       "  0.5114093959731545,\n",
       "  0.533956228956229,\n",
       "  0.4821160409556314,\n",
       "  0.49553511705685627,\n",
       "  0.5201677852348994,\n",
       "  0.4733838383838384,\n",
       "  0.5245221843003413,\n",
       "  0.5178093645484949,\n",
       "  0.508614864864865,\n",
       "  0.4945469798657718,\n",
       "  0.4872986577181208,\n",
       "  0.5076712328767123,\n",
       "  0.5078355704697985,\n",
       "  0.5016498316498317,\n",
       "  0.4919896193771626,\n",
       "  0.4861833333333333,\n",
       "  0.5112,\n",
       "  0.49276666666666663,\n",
       "  0.46846666666666664,\n",
       "  0.5024324324324325,\n",
       "  0.5208193979933111,\n",
       "  0.5411577181208054,\n",
       "  0.5066722408026756,\n",
       "  0.48661073825503354,\n",
       "  0.5266442953020135,\n",
       "  0.49031141868512107,\n",
       "  0.5348979591836734,\n",
       "  0.4704333333333333,\n",
       "  0.449625850340136,\n",
       "  0.5113804713804714,\n",
       "  0.4929530201342282,\n",
       "  0.5101174496644296,\n",
       "  0.4553645833333333,\n",
       "  0.46534129692832765,\n",
       "  0.5184563758389261,\n",
       "  0.4710641891891892,\n",
       "  0.5004026845637584,\n",
       "  0.4911744966442953,\n",
       "  0.5463299663299663,\n",
       "  0.5194127516778524,\n",
       "  0.5152203389830508,\n",
       "  0.48919732441471575,\n",
       "  0.4667118644067797,\n",
       "  0.4713377926421405,\n",
       "  0.5503344481605351,\n",
       "  0.5176421404682273,\n",
       "  0.4577533783783784,\n",
       "  0.49565436241610733,\n",
       "  0.5009030100334448,\n",
       "  0.49201342281879196,\n",
       "  0.5084511784511784,\n",
       "  0.48635135135135127,\n",
       "  0.49415824915824913,\n",
       "  0.4652356902356903,\n",
       "  0.4970648464163823,\n",
       "  0.4632432432432433,\n",
       "  0.4913095238095238,\n",
       "  0.5013636363636363,\n",
       "  0.46749999999999997,\n",
       "  0.4734020618556701,\n",
       "  0.5260535117056857,\n",
       "  0.5331879194630873,\n",
       "  0.5327181208053692,\n",
       "  0.5092662116040957,\n",
       "  0.4890771812080537,\n",
       "  0.48685810810810803,\n",
       "  0.5165136054421768,\n",
       "  0.4662626262626263,\n",
       "  0.4865604026845638,\n",
       "  0.47559523809523807,\n",
       "  0.5091778523489934,\n",
       "  0.4771501706484642,\n",
       "  0.4630555555555556,\n",
       "  0.4847128378378378,\n",
       "  0.5512627986348122,\n",
       "  0.44998333333333335,\n",
       "  0.5209866220735785,\n",
       "  0.524933110367893,\n",
       "  0.5058695652173912,\n",
       "  0.5018197278911565,\n",
       "  0.5274831081081081,\n",
       "  0.4900167224080268,\n",
       "  0.4452533783783784,\n",
       "  0.5150501672240803,\n",
       "  0.5532943143812709,\n",
       "  0.492295918367347,\n",
       "  0.4820875420875421,\n",
       "  0.47704391891891895,\n",
       "  0.5214184397163121,\n",
       "  0.5547147651006712,\n",
       "  0.5239527027027027,\n",
       "  0.48359060402684567,\n",
       "  0.49854515050167225,\n",
       "  0.5421917808219178,\n",
       "  0.5226262626262627,\n",
       "  0.4936166666666667,\n",
       "  0.4794006849315069,\n",
       "  0.5053703703703702,\n",
       "  0.4658026755852842,\n",
       "  0.4923400673400673,\n",
       "  0.5096655518394648,\n",
       "  0.4807666666666666,\n",
       "  0.4556040268456376,\n",
       "  0.49377516778523484,\n",
       "  0.5033108108108107,\n",
       "  0.5071212121212121,\n",
       "  0.5250836120401338,\n",
       "  0.5209333333333334,\n",
       "  0.4270903010033445,\n",
       "  0.5075250836120402,\n",
       "  0.5024916387959867,\n",
       "  0.5215551839464883,\n",
       "  0.5029194630872483,\n",
       "  0.46328231292517,\n",
       "  0.5040925266903915,\n",
       "  0.4779797979797979,\n",
       "  0.5334006734006734,\n",
       "  0.5190771812080537,\n",
       "  0.5014358108108108,\n",
       "  0.5049832214765101,\n",
       "  0.49506688963210693,\n",
       "  0.49722033898305085,\n",
       "  0.5178260869565217,\n",
       "  0.4944666666666666,\n",
       "  0.5718959731543625,\n",
       "  0.5001342281879194,\n",
       "  0.5314695945945946,\n",
       "  0.5392760942760944,\n",
       "  0.45909395973154365,\n",
       "  0.4900349650349651,\n",
       "  0.5485953177257527,\n",
       "  0.4830666666666667,\n",
       "  0.5336734693877551,\n",
       "  0.5157859531772575,\n",
       "  0.5142087542087542,\n",
       "  0.5092424242424243,\n",
       "  0.4798801369863014,\n",
       "  0.4935451505016722,\n",
       "  0.4949655172413794,\n",
       "  0.4898494983277591,\n",
       "  0.44673333333333337,\n",
       "  0.4985714285714286,\n",
       "  0.5038644067796609,\n",
       "  0.4925762711864407,\n",
       "  0.5225000000000001,\n",
       "  0.46190635451505013,\n",
       "  0.4952881355932204,\n",
       "  0.5027609427609427,\n",
       "  0.484057239057239,\n",
       "  0.5236317567567568,\n",
       "  0.5130993150684932,\n",
       "  0.4864478114478114,\n",
       "  0.5003496503496503,\n",
       "  0.5565384615384615,\n",
       "  0.49506779661016953,\n",
       "  0.4908020477815699,\n",
       "  0.480561224489796,\n",
       "  0.5334006734006734,\n",
       "  0.5013087248322147,\n",
       "  0.5092006802721089,\n",
       "  0.5029280821917809,\n",
       "  0.4990202702702703,\n",
       "  0.4981438127090301,\n",
       "  0.47540677966101696,\n",
       "  0.5167171717171717,\n",
       "  0.4895666666666667,\n",
       "  0.46954081632653066,\n",
       "  0.5374579124579124,\n",
       "  0.47515306122448975,\n",
       "  0.4595986622073579,\n",
       "  0.5420066889632107,\n",
       "  0.5178260869565218,\n",
       "  0.5101190476190476,\n",
       "  0.5522635135135135,\n",
       "  0.48538461538461536,\n",
       "  0.46697986577181216,\n",
       "  0.5058333333333334,\n",
       "  0.5114932885906041,\n",
       "  0.4987414965986394,\n",
       "  0.4926254180602006,\n",
       "  0.5186486486486486,\n",
       "  0.5344314381270903,\n",
       "  0.5080968858131488,\n",
       "  0.5259060402684564,\n",
       "  0.5205033557046981,\n",
       "  0.49036666666666673,\n",
       "  0.5016778523489933,\n",
       "  0.5366666666666666,\n",
       "  0.5090666666666667,\n",
       "  0.5156734006734007,\n",
       "  0.5330405405405405,\n",
       "  0.46086666666666665,\n",
       "  0.5203666666666668,\n",
       "  0.5043311036789299,\n",
       "  0.5178885135135135,\n",
       "  0.5010068259385666,\n",
       "  0.47632550335570467,\n",
       "  0.5083559322033898,\n",
       "  0.5023979591836735,\n",
       "  0.5171134020618556,\n",
       "  0.4610641891891893,\n",
       "  0.4976086956521739,\n",
       "  0.48363945578231293,\n",
       "  0.52875,\n",
       "  0.4960234899328859,\n",
       "  0.4963255033557047,\n",
       "  0.4276286764705882,\n",
       "  0.5073244147157191,\n",
       "  0.49747474747474746,\n",
       "  0.47725589225589227,\n",
       "  0.4981103678929766,\n",
       "  0.4639830508474576,\n",
       "  0.5040033783783784,\n",
       "  0.4639833333333333,\n",
       "  0.4945333333333334,\n",
       "  0.45681818181818185,\n",
       "  0.5044237288135593,\n",
       "  0.5139003436426116,\n",
       "  0.5017391304347826,\n",
       "  0.5271621621621622,\n",
       "  0.5036271186440678,\n",
       "  0.49998310810810814,\n",
       "  0.5137,\n",
       "  0.4956440677966102,\n",
       "  0.4747431506849315,\n",
       "  0.4942021276595745,\n",
       "  0.4572241992882562,\n",
       "  0.4288620689655172,\n",
       "  0.5037416107382551,\n",
       "  0.5443478260869565,\n",
       "  0.5198976109215017,\n",
       "  0.5201672240802676,\n",
       "  0.5744915254237288,\n",
       "  0.5441186440677965,\n",
       "  0.4986271186440679,\n",
       "  0.5160906040268457,\n",
       "  0.5324916387959866,\n",
       "  0.5153198653198654,\n",
       "  0.4709491525423729,\n",
       "  0.4444666666666667,\n",
       "  0.47790969899665553,\n",
       "  0.499513422818792,\n",
       "  0.5162203389830509,\n",
       "  0.511728813559322,\n",
       "  0.4964358108108108,\n",
       "  0.461160409556314,\n",
       "  0.4863573883161511,\n",
       "  0.5121404109589042,\n",
       "  0.5020033670033671,\n",
       "  0.44578859060402687,\n",
       "  0.5218537414965986,\n",
       "  0.5153412969283276,\n",
       "  0.5398464163822525,\n",
       "  0.43706081081081083,\n",
       "  0.5076421404682275,\n",
       "  0.4982333333333333,\n",
       "  0.5208710801393729,\n",
       "  0.48115646258503403,\n",
       "  0.5098116438356164,\n",
       "  0.5027257525083612,\n",
       "  0.4597457627118644,\n",
       "  0.4909563758389262,\n",
       "  0.4775783972125435,\n",
       "  0.47449999999999987,\n",
       "  0.5079151943462897,\n",
       "  0.5015901060070671,\n",
       "  0.5111525423728813,\n",
       "  0.5277257525083613,\n",
       "  0.49608695652173906,\n",
       "  0.5270439189189189,\n",
       "  0.5068561872909699,\n",
       "  0.5142642140468227,\n",
       "  0.5523648648648649,\n",
       "  0.5075506756756757,\n",
       "  0.4688813559322035,\n",
       "  0.4942424242424243,\n",
       "  0.48903114186851204,\n",
       "  0.49833333333333335,\n",
       "  0.44852842809364546,\n",
       "  0.4825850340136054,\n",
       "  0.5182931034482758,\n",
       "  0.48,\n",
       "  0.48300000000000004,\n",
       "  0.4875589225589226,\n",
       "  0.4939966555183946,\n",
       "  0.535685618729097,\n",
       "  0.5536744966442954,\n",
       "  0.48734899328859066,\n",
       "  0.5413050847457627,\n",
       "  0.5018456375838927,\n",
       "  0.5383389261744966,\n",
       "  0.5093288590604027,\n",
       "  0.542190635451505,\n",
       "  0.5028787878787879,\n",
       "  0.4830639730639731,\n",
       "  0.5227104377104377,\n",
       "  0.5312627986348124,\n",
       "  0.5167056856187291,\n",
       "  0.528561872909699,\n",
       "  0.5000526315789473,\n",
       "  0.5165371621621621,\n",
       "  0.47162408759124086,\n",
       "  0.46481605351170574,\n",
       "  0.47790268456375845,\n",
       "  0.45591973244147155,\n",
       "  0.5230204778156997,\n",
       "  0.5487331081081082,\n",
       "  0.483628762541806,\n",
       "  0.4997089041095889,\n",
       "  0.4856856187290969,\n",
       "  0.5294387755102041,\n",
       "  0.5184429065743944,\n",
       "  0.4730677966101695,\n",
       "  0.4832601351351352,\n",
       "  0.48913333333333336,\n",
       "  0.5219425675675676,\n",
       "  0.5093624161073825,\n",
       "  0.5100836120401337,\n",
       "  0.5015656565656564,\n",
       "  0.5129026845637584,\n",
       "  0.46793918918918914,\n",
       "  0.48672818791946304,\n",
       "  0.49445392491467577,\n",
       "  0.510945945945946,\n",
       "  0.4636195286195286,\n",
       "  0.452876254180602,\n",
       "  0.5176599326599327,\n",
       "  0.49969491525423726,\n",
       "  0.5076086956521739,\n",
       "  0.48075174825174816,\n",
       "  0.4553716216216217,\n",
       "  0.5139597315436242,\n",
       "  0.4563833333333334,\n",
       "  0.4884121621621622,\n",
       "  0.49107744107744117,\n",
       "  0.5176936026936027,\n",
       "  0.5186622073578595,\n",
       "  0.4808026755852843,\n",
       "  0.5301178451178451,\n",
       "  0.5002356902356903,\n",
       "  0.5032033898305085,\n",
       "  0.5454222972972973,\n",
       "  0.46718333333333334,\n",
       "  0.48998161764705883,\n",
       "  0.4738758389261745,\n",
       "  0.4804347826086956,\n",
       "  0.5443050847457627,\n",
       "  0.47930976430976435,\n",
       "  0.4683500000000001,\n",
       "  0.5372635135135135,\n",
       "  0.5007457627118643,\n",
       "  0.5072,\n",
       "  0.5084899328859059,\n",
       "  0.4846153846153846,\n",
       "  0.5176816608996541,\n",
       "  0.5446724137931035,\n",
       "  0.5189464882943144,\n",
       "  0.5015719063545151,\n",
       "  0.543922558922559,\n",
       "  0.46791095890410955,\n",
       "  0.5322147651006712,\n",
       "  0.5413389830508475,\n",
       "  0.4817966101694915,\n",
       "  0.5249486301369863,\n",
       "  0.47115771812080537,\n",
       "  0.47379999999999994,\n",
       "  0.5074745762711864,\n",
       "  0.4633164983164983,\n",
       "  0.5441095890410959,\n",
       "  0.5003166666666666,\n",
       "  0.5034899328859062,\n",
       "  0.5234310344827586,\n",
       "  0.5081081081081081,\n",
       "  0.4627551020408164,\n",
       "  0.5071140939597315,\n",
       "  0.5184228187919463,\n",
       "  0.48281786941580757,\n",
       "  0.5421428571428571,\n",
       "  0.49348484848484847,\n",
       "  0.5035117056856188,\n",
       "  0.5154882154882154,\n",
       "  0.49091216216216216,\n",
       "  0.4862374581939799,\n",
       "  0.5176297577854672,\n",
       "  0.507864406779661,\n",
       "  0.48951666666666666,\n",
       "  0.4968561872909698,\n",
       "  0.5393771043771044,\n",
       "  0.513078231292517,\n",
       "  0.47577966101694913,\n",
       "  0.5231569965870307,\n",
       "  0.4841778523489933,\n",
       "  0.569438775510204,\n",
       "  0.507820945945946,\n",
       "  0.47878424657534246,\n",
       "  0.47313356164383574,\n",
       "  0.5216021126760563,\n",
       "  0.5065833333333334,\n",
       "  0.5242567567567568,\n",
       "  0.5029124579124579,\n",
       "  0.5251202749140893,\n",
       "  0.47711666666666663,\n",
       "  0.5383166666666667,\n",
       "  0.4962668918918918,\n",
       "  0.5027777777777778,\n",
       "  0.5105236486486486,\n",
       "  0.5580134680134681,\n",
       "  0.49623728813559315,\n",
       "  0.5081184668989547,\n",
       "  0.45476510067114095,\n",
       "  0.5199496644295302,\n",
       "  0.49861301369863004,\n",
       "  0.4580729166666667,\n",
       "  0.4983164983164983,\n",
       "  0.5135690235690236,\n",
       "  0.5229530201342281,\n",
       "  0.5117905405405405,\n",
       "  0.47703020134228197,\n",
       "  0.505135593220339,\n",
       "  0.5260750853242321,\n",
       "  0.5175420875420875,\n",
       "  0.4840499999999999,\n",
       "  0.5028082191780822,\n",
       "  0.504542372881356,\n",
       "  0.4554377104377104,\n",
       "  0.49102836879432626,\n",
       "  0.4949831081081081,\n",
       "  0.5449333333333333,\n",
       "  0.49363333333333337,\n",
       "  0.5542976588628763,\n",
       "  0.46426174496644296,\n",
       "  0.4711666666666666,\n",
       "  0.48785958904109594,\n",
       "  0.5094983277591973,\n",
       "  0.4784129692832765,\n",
       "  0.5519630872483222,\n",
       "  0.482195945945946,\n",
       "  0.49703333333333327,\n",
       "  0.44860068259385666,\n",
       "  0.4632214765100672,\n",
       "  0.4372542372881356,\n",
       "  0.5046938775510204,\n",
       "  0.5289464882943143,\n",
       "  0.5157575757575757,\n",
       "  0.4878956228956229,\n",
       "  0.5050838926174496,\n",
       "  0.5415604026845637,\n",
       "  0.5246949152542373,\n",
       "  0.495959595959596,\n",
       "  0.4446724137931035,\n",
       "  0.48127516778523494,\n",
       "  0.5355574324324325,\n",
       "  0.4975666666666666,\n",
       "  0.47184121621621616,\n",
       "  0.5044666666666667,\n",
       "  0.46586642599277983,\n",
       "  0.49546075085324237,\n",
       "  0.45945762711864413,\n",
       "  0.4607744107744108,\n",
       "  0.47988333333333333,\n",
       "  0.5069565217391305,\n",
       "  0.499983164983165,\n",
       "  0.5228040540540541,\n",
       "  0.4886271186440677,\n",
       "  0.4968918918918919,\n",
       "  0.5145563139931741,\n",
       "  0.5250499999999999,\n",
       "  0.4909364548494984,\n",
       "  0.5082659932659932,\n",
       "  0.4692087542087542,\n",
       "  0.4677702702702703,\n",
       "  0.508097643097643,\n",
       "  0.5233277591973244,\n",
       "  0.5446366782006921,\n",
       "  0.4806208053691275,\n",
       "  0.49138047138047136,\n",
       "  0.46976588628762533,\n",
       "  0.5528222996515679,\n",
       "  0.4819425675675675,\n",
       "  0.49159395973154363,\n",
       "  0.5177397260273973,\n",
       "  0.5011734693877552,\n",
       "  0.5415993265993265,\n",
       "  0.5248129251700681,\n",
       "  0.5100340136054421,\n",
       "  0.529513422818792,\n",
       "  0.5071452702702703,\n",
       "  0.46226804123711346,\n",
       "  0.455593220338983,\n",
       "  0.4776351351351351,\n",
       "  0.5093367346938775,\n",
       "  0.4527133105802047,\n",
       "  0.5423166666666667,\n",
       "  0.5290033783783783,\n",
       "  0.5155201342281879,\n",
       "  ...]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_trainer._eval_save_prefix = OUTPUT_PREFIX + \"-test-pos-biased\"\n",
    "model_trainer._evaluate_partial(test_dataset_pos_biased)\n",
    "\n",
    "model_trainer._eval_save_prefix = OUTPUT_PREFIX +  \"-test-neg-biased\"\n",
    "model_trainer._evaluate_partial(test_dataset_neg_biased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unbiased Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2296/2296 [00:00<00:00, 2761.46it/s]\n",
      "100%|| 2296/2296 [00:01<00:00, 1528.65it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AUC': [0.46611111111111103,\n",
       "  0.5780000000000001,\n",
       "  0.5166666666666666,\n",
       "  0.5066666666666667,\n",
       "  0.49222222222222217,\n",
       "  0.4635714285714285,\n",
       "  0.38833333333333336,\n",
       "  0.5835714285714285,\n",
       "  0.454375,\n",
       "  0.46444444444444444,\n",
       "  0.369375,\n",
       "  0.43357142857142855,\n",
       "  0.5721428571428572,\n",
       "  0.45875,\n",
       "  0.465,\n",
       "  0.5116666666666667,\n",
       "  0.47388888888888886,\n",
       "  0.5149999999999999,\n",
       "  0.4992857142857144,\n",
       "  0.5127777777777778,\n",
       "  0.493125,\n",
       "  0.47,\n",
       "  0.32944444444444443,\n",
       "  0.45,\n",
       "  0.675,\n",
       "  0.33062499999999995,\n",
       "  0.79,\n",
       "  0.490625,\n",
       "  0.393125,\n",
       "  0.5744444444444443,\n",
       "  0.19714285714285712,\n",
       "  0.5222222222222223,\n",
       "  0.46499999999999997,\n",
       "  0.49833333333333335,\n",
       "  0.4600000000000001,\n",
       "  0.692857142857143,\n",
       "  0.6625,\n",
       "  0.3825,\n",
       "  0.51,\n",
       "  0.40785714285714286,\n",
       "  0.5125,\n",
       "  0.480625,\n",
       "  0.62125,\n",
       "  0.506,\n",
       "  0.41444444444444445,\n",
       "  0.50375,\n",
       "  0.49000000000000005,\n",
       "  0.2225,\n",
       "  0.4888888888888888,\n",
       "  0.4677777777777778,\n",
       "  0.5438888888888889,\n",
       "  0.5429999999999999,\n",
       "  0.4666666666666666,\n",
       "  0.39888888888888885,\n",
       "  0.41333333333333333,\n",
       "  0.5128571428571428,\n",
       "  0.570625,\n",
       "  0.7675000000000001,\n",
       "  0.5577777777777777,\n",
       "  0.41899999999999993,\n",
       "  0.5577777777777777,\n",
       "  0.745,\n",
       "  0.4088888888888889,\n",
       "  0.6021428571428571,\n",
       "  0.44111111111111106,\n",
       "  0.6699999999999999,\n",
       "  0.4755555555555557,\n",
       "  0.57875,\n",
       "  0.42166666666666663,\n",
       "  0.565,\n",
       "  0.5005555555555555,\n",
       "  0.423125,\n",
       "  0.39555555555555555,\n",
       "  0.2727777777777778,\n",
       "  0.33399999999999996,\n",
       "  0.4444444444444444,\n",
       "  0.44388888888888894,\n",
       "  0.5305555555555556,\n",
       "  0.5027777777777778,\n",
       "  0.475,\n",
       "  0.519375,\n",
       "  0.5611111111111111,\n",
       "  0.431875,\n",
       "  0.335625,\n",
       "  0.5,\n",
       "  0.4838888888888889,\n",
       "  0.4255555555555556,\n",
       "  0.44812500000000005,\n",
       "  0.545625,\n",
       "  0.431875,\n",
       "  0.6083333333333335,\n",
       "  0.5705555555555556,\n",
       "  0.449375,\n",
       "  0.5166666666666667,\n",
       "  0.5958333333333333,\n",
       "  0.41333333333333333,\n",
       "  0.5177777777777778,\n",
       "  0.5572222222222222,\n",
       "  0.41055555555555556,\n",
       "  0.51,\n",
       "  0.4625,\n",
       "  0.5022222222222221,\n",
       "  0.45222222222222214,\n",
       "  0.4788888888888889,\n",
       "  0.62,\n",
       "  0.6266666666666666,\n",
       "  0.6216666666666666,\n",
       "  0.385625,\n",
       "  0.6675,\n",
       "  0.4992857142857143,\n",
       "  0.4438888888888889,\n",
       "  0.41388888888888886,\n",
       "  0.3622222222222222,\n",
       "  0.41437500000000005,\n",
       "  0.483125,\n",
       "  0.48562500000000003,\n",
       "  0.365,\n",
       "  0.49,\n",
       "  0.48812500000000003,\n",
       "  0.36,\n",
       "  0.38277777777777783,\n",
       "  0.5466666666666665,\n",
       "  0.5311111111111111,\n",
       "  0.53625,\n",
       "  0.5194444444444445,\n",
       "  0.46444444444444444,\n",
       "  0.4307142857142857,\n",
       "  0.43666666666666665,\n",
       "  0.414375,\n",
       "  0.4957142857142857,\n",
       "  0.6138888888888889,\n",
       "  0.52125,\n",
       "  0.4033333333333333,\n",
       "  0.5081249999999999,\n",
       "  0.46375,\n",
       "  0.3622222222222222,\n",
       "  0.48888888888888893,\n",
       "  0.48687499999999995,\n",
       "  0.3977777777777778,\n",
       "  0.5661111111111112,\n",
       "  0.5505555555555556,\n",
       "  0.6305555555555555,\n",
       "  0.49000000000000005,\n",
       "  0.5283333333333333,\n",
       "  0.5311111111111111,\n",
       "  0.400625,\n",
       "  0.5855555555555555,\n",
       "  0.5277777777777778,\n",
       "  0.363125,\n",
       "  0.3838888888888889,\n",
       "  0.5322222222222223,\n",
       "  0.3927777777777777,\n",
       "  0.49625,\n",
       "  0.491875,\n",
       "  0.5544444444444445,\n",
       "  0.43571428571428567,\n",
       "  0.41055555555555556,\n",
       "  0.4216666666666667,\n",
       "  0.41375,\n",
       "  0.36777777777777776,\n",
       "  0.2805555555555556,\n",
       "  0.5266666666666667,\n",
       "  0.5766666666666667,\n",
       "  0.4227777777777778,\n",
       "  0.45222222222222214,\n",
       "  0.585625,\n",
       "  0.47214285714285714,\n",
       "  0.23,\n",
       "  0.5816666666666666,\n",
       "  0.26937500000000003,\n",
       "  0.44250000000000006,\n",
       "  0.5305555555555554,\n",
       "  0.405625,\n",
       "  0.3844444444444445,\n",
       "  0.7194444444444444,\n",
       "  0.4207142857142857,\n",
       "  0.393125,\n",
       "  0.50875,\n",
       "  0.5349999999999999,\n",
       "  0.528888888888889,\n",
       "  0.47375,\n",
       "  0.599375,\n",
       "  0.5121428571428571,\n",
       "  0.5935714285714286,\n",
       "  0.46499999999999997,\n",
       "  0.3238888888888889,\n",
       "  0.6077777777777779,\n",
       "  0.64125,\n",
       "  0.4422222222222222,\n",
       "  0.6005555555555556,\n",
       "  0.49124999999999996,\n",
       "  0.3705555555555555,\n",
       "  0.40499999999999997,\n",
       "  0.4122222222222222,\n",
       "  0.2871428571428571,\n",
       "  0.5675,\n",
       "  0.4191666666666667,\n",
       "  0.48000000000000004,\n",
       "  0.29500000000000004,\n",
       "  0.79375,\n",
       "  0.4466666666666667,\n",
       "  0.525,\n",
       "  0.4716666666666667,\n",
       "  0.7000000000000001,\n",
       "  0.43555555555555553,\n",
       "  0.4864285714285715,\n",
       "  0.17166666666666666,\n",
       "  0.53,\n",
       "  0.46222222222222226,\n",
       "  0.49,\n",
       "  0.505,\n",
       "  0.5344444444444445,\n",
       "  0.456875,\n",
       "  0.603125,\n",
       "  0.4772222222222222,\n",
       "  0.4575,\n",
       "  0.515,\n",
       "  0.4961111111111111,\n",
       "  0.535,\n",
       "  0.5383333333333332,\n",
       "  0.4872222222222222,\n",
       "  0.461111111111111,\n",
       "  0.4355555555555556,\n",
       "  0.633888888888889,\n",
       "  0.558888888888889,\n",
       "  0.3861111111111111,\n",
       "  0.6372222222222221,\n",
       "  0.5,\n",
       "  0.39357142857142857,\n",
       "  0.49750000000000005,\n",
       "  0.31,\n",
       "  0.2516666666666667,\n",
       "  0.43277777777777776,\n",
       "  0.5661111111111111,\n",
       "  0.43944444444444447,\n",
       "  0.6425,\n",
       "  0.28444444444444444,\n",
       "  0.3357142857142857,\n",
       "  0.5137499999999999,\n",
       "  0.5138888888888888,\n",
       "  0.49875,\n",
       "  0.415,\n",
       "  0.7791666666666667,\n",
       "  0.45625,\n",
       "  0.48666666666666664,\n",
       "  0.6914285714285715,\n",
       "  0.4388888888888889,\n",
       "  0.3821428571428572,\n",
       "  0.46124999999999994,\n",
       "  0.546,\n",
       "  0.48,\n",
       "  0.5733333333333333,\n",
       "  0.5211111111111112,\n",
       "  0.5011111111111112,\n",
       "  0.5761111111111111,\n",
       "  0.6377777777777778,\n",
       "  0.5488888888888889,\n",
       "  0.4066666666666667,\n",
       "  0.47000000000000003,\n",
       "  0.42750000000000005,\n",
       "  0.35944444444444446,\n",
       "  0.5542857142857143,\n",
       "  0.48062499999999997,\n",
       "  0.5111111111111112,\n",
       "  0.48944444444444446,\n",
       "  0.5277777777777777,\n",
       "  0.3938888888888889,\n",
       "  0.39111111111111113,\n",
       "  0.5611111111111111,\n",
       "  0.46111111111111114,\n",
       "  0.43875,\n",
       "  0.2716666666666667,\n",
       "  0.2755555555555556,\n",
       "  0.5171428571428572,\n",
       "  0.5111111111111111,\n",
       "  0.4042857142857143,\n",
       "  0.65,\n",
       "  0.6405555555555557,\n",
       "  0.4561111111111111,\n",
       "  0.6705555555555556,\n",
       "  0.576875,\n",
       "  0.425,\n",
       "  0.4766666666666667,\n",
       "  0.46499999999999997,\n",
       "  0.2742857142857143,\n",
       "  0.44611111111111107,\n",
       "  0.49874999999999997,\n",
       "  0.428125,\n",
       "  0.4583333333333333,\n",
       "  0.3894444444444444,\n",
       "  0.36875,\n",
       "  0.4927777777777778,\n",
       "  0.4255555555555556,\n",
       "  0.37833333333333335,\n",
       "  0.5406249999999999,\n",
       "  0.45499999999999996,\n",
       "  0.315625,\n",
       "  0.6221428571428571,\n",
       "  0.7066666666666667,\n",
       "  0.631875,\n",
       "  0.47285714285714286,\n",
       "  0.578125,\n",
       "  0.4841666666666666,\n",
       "  0.6616666666666666,\n",
       "  0.521875,\n",
       "  0.56,\n",
       "  0.49166666666666675,\n",
       "  0.5555555555555556,\n",
       "  0.459375,\n",
       "  0.365,\n",
       "  0.6027777777777779,\n",
       "  0.5750000000000001,\n",
       "  0.56,\n",
       "  0.603125,\n",
       "  0.33999999999999997,\n",
       "  0.30777777777777776,\n",
       "  0.33625,\n",
       "  0.615,\n",
       "  0.40388888888888885,\n",
       "  0.36944444444444446,\n",
       "  0.535,\n",
       "  0.46388888888888885,\n",
       "  0.225,\n",
       "  0.5221428571428571,\n",
       "  0.5349999999999999,\n",
       "  0.36833333333333335,\n",
       "  0.42333333333333334,\n",
       "  0.45125000000000004,\n",
       "  0.6962499999999999,\n",
       "  0.47611111111111115,\n",
       "  0.3833333333333333,\n",
       "  0.43833333333333335,\n",
       "  0.5616666666666668,\n",
       "  0.5883333333333334,\n",
       "  0.6211111111111111,\n",
       "  0.43000000000000005,\n",
       "  0.24900000000000003,\n",
       "  0.614375,\n",
       "  0.3561111111111111,\n",
       "  0.43833333333333335,\n",
       "  0.6325000000000001,\n",
       "  0.44875,\n",
       "  0.3457142857142857,\n",
       "  0.6508333333333333,\n",
       "  0.38277777777777783,\n",
       "  0.4864285714285715,\n",
       "  0.405,\n",
       "  0.5177777777777778,\n",
       "  0.3541666666666667,\n",
       "  0.5933333333333333,\n",
       "  0.646875,\n",
       "  0.5477777777777778,\n",
       "  0.5733333333333334,\n",
       "  0.33944444444444444,\n",
       "  0.6544444444444444,\n",
       "  0.52,\n",
       "  0.5022222222222221,\n",
       "  0.47555555555555556,\n",
       "  0.6072222222222222,\n",
       "  0.285625,\n",
       "  0.39375,\n",
       "  0.4,\n",
       "  0.728,\n",
       "  0.3683333333333334,\n",
       "  0.66,\n",
       "  0.49,\n",
       "  0.3572222222222222,\n",
       "  0.6483333333333334,\n",
       "  0.3242857142857143,\n",
       "  0.4628571428571429,\n",
       "  0.4194444444444444,\n",
       "  0.5488888888888889,\n",
       "  0.4566666666666666,\n",
       "  0.4794444444444444,\n",
       "  0.39285714285714285,\n",
       "  0.468125,\n",
       "  0.3744444444444444,\n",
       "  0.43699999999999994,\n",
       "  0.2644444444444444,\n",
       "  0.45333333333333337,\n",
       "  0.618125,\n",
       "  0.45500000000000007,\n",
       "  0.5194444444444444,\n",
       "  0.4672222222222222,\n",
       "  0.43833333333333335,\n",
       "  0.40750000000000003,\n",
       "  0.54,\n",
       "  0.5266666666666667,\n",
       "  0.32277777777777783,\n",
       "  0.67625,\n",
       "  0.29444444444444445,\n",
       "  0.5855555555555556,\n",
       "  0.524375,\n",
       "  0.5264285714285714,\n",
       "  0.4625,\n",
       "  0.6205555555555555,\n",
       "  0.44999999999999996,\n",
       "  0.3005555555555556,\n",
       "  0.6538888888888889,\n",
       "  0.6455555555555555,\n",
       "  0.42071428571428576,\n",
       "  0.465,\n",
       "  0.43388888888888894,\n",
       "  0.2588888888888889,\n",
       "  0.37312500000000004,\n",
       "  0.5633333333333334,\n",
       "  0.55,\n",
       "  0.515625,\n",
       "  0.45611111111111113,\n",
       "  0.598125,\n",
       "  0.351,\n",
       "  0.424375,\n",
       "  0.562142857142857,\n",
       "  0.5722222222222223,\n",
       "  0.5607142857142858,\n",
       "  0.3611111111111111,\n",
       "  0.605,\n",
       "  0.4816666666666667,\n",
       "  0.5299999999999999,\n",
       "  0.4911111111111111,\n",
       "  0.53,\n",
       "  0.6558333333333334,\n",
       "  0.5783333333333334,\n",
       "  0.41055555555555556,\n",
       "  0.43833333333333335,\n",
       "  0.3857142857142857,\n",
       "  0.5449999999999999,\n",
       "  0.3944444444444445,\n",
       "  0.36777777777777776,\n",
       "  0.66,\n",
       "  0.41357142857142853,\n",
       "  0.45333333333333337,\n",
       "  0.3677777777777778,\n",
       "  0.509375,\n",
       "  0.46111111111111114,\n",
       "  0.35277777777777775,\n",
       "  0.38,\n",
       "  0.4891666666666667,\n",
       "  0.37374999999999997,\n",
       "  0.5544444444444444,\n",
       "  0.44388888888888883,\n",
       "  0.39999999999999997,\n",
       "  0.571875,\n",
       "  0.3866666666666667,\n",
       "  0.34833333333333333,\n",
       "  0.475,\n",
       "  0.4175,\n",
       "  0.4857142857142857,\n",
       "  0.553125,\n",
       "  0.51,\n",
       "  0.45125,\n",
       "  0.453125,\n",
       "  0.38222222222222224,\n",
       "  0.47187499999999993,\n",
       "  0.42125,\n",
       "  0.3794444444444445,\n",
       "  0.40812499999999996,\n",
       "  0.4588888888888889,\n",
       "  0.45624999999999993,\n",
       "  0.4605555555555555,\n",
       "  0.5525,\n",
       "  0.45999999999999996,\n",
       "  0.4383333333333333,\n",
       "  0.49888888888888894,\n",
       "  0.5238888888888888,\n",
       "  0.4605555555555555,\n",
       "  0.45222222222222214,\n",
       "  0.56,\n",
       "  0.4494444444444444,\n",
       "  0.49166666666666664,\n",
       "  0.6022222222222223,\n",
       "  0.3741666666666667,\n",
       "  0.5077777777777778,\n",
       "  0.3244444444444444,\n",
       "  0.47611111111111115,\n",
       "  0.525,\n",
       "  0.540625,\n",
       "  0.39625,\n",
       "  0.4511111111111112,\n",
       "  0.5172222222222221,\n",
       "  0.40388888888888885,\n",
       "  0.5383333333333333,\n",
       "  0.546875,\n",
       "  0.39625000000000005,\n",
       "  0.405,\n",
       "  0.615,\n",
       "  0.47777777777777775,\n",
       "  0.17444444444444446,\n",
       "  0.5505555555555556,\n",
       "  0.298,\n",
       "  0.26833333333333337,\n",
       "  0.58125,\n",
       "  0.6208333333333333,\n",
       "  0.5744444444444444,\n",
       "  0.401875,\n",
       "  0.4921428571428571,\n",
       "  0.5018750000000001,\n",
       "  0.435,\n",
       "  0.353125,\n",
       "  0.58,\n",
       "  0.5944444444444444,\n",
       "  0.5393749999999999,\n",
       "  0.6183333333333333,\n",
       "  0.26416666666666666,\n",
       "  0.4466666666666666,\n",
       "  0.5944444444444446,\n",
       "  0.341875,\n",
       "  0.3355555555555555,\n",
       "  0.5385714285714286,\n",
       "  0.515,\n",
       "  0.625,\n",
       "  0.5372222222222222,\n",
       "  0.3858333333333333,\n",
       "  0.3964285714285714,\n",
       "  0.45222222222222225,\n",
       "  0.5405555555555556,\n",
       "  0.5133333333333333,\n",
       "  0.37166666666666665,\n",
       "  0.675,\n",
       "  0.36687499999999995,\n",
       "  0.4521428571428571,\n",
       "  0.5166666666666667,\n",
       "  0.364375,\n",
       "  0.51,\n",
       "  0.44,\n",
       "  0.724375,\n",
       "  0.5131249999999999,\n",
       "  0.515,\n",
       "  0.5338888888888889,\n",
       "  0.5094444444444445,\n",
       "  0.4033333333333333,\n",
       "  0.33375,\n",
       "  0.4577777777777777,\n",
       "  0.45500000000000007,\n",
       "  0.6107142857142858,\n",
       "  0.5466666666666666,\n",
       "  0.5575,\n",
       "  0.4375,\n",
       "  0.5344444444444444,\n",
       "  0.5116666666666666,\n",
       "  0.39099999999999996,\n",
       "  0.3561111111111111,\n",
       "  0.42611111111111116,\n",
       "  0.4728571428571428,\n",
       "  0.6072222222222222,\n",
       "  0.3988888888888889,\n",
       "  0.37777777777777777,\n",
       "  0.40722222222222215,\n",
       "  0.4961111111111111,\n",
       "  0.3385714285714286,\n",
       "  0.469375,\n",
       "  0.2857142857142857,\n",
       "  0.5593750000000001,\n",
       "  0.47111111111111104,\n",
       "  0.07166666666666667,\n",
       "  0.5856250000000001,\n",
       "  0.3472222222222222,\n",
       "  0.38571428571428573,\n",
       "  0.35555555555555557,\n",
       "  0.5188888888888888,\n",
       "  0.4677777777777778,\n",
       "  0.541111111111111,\n",
       "  0.69,\n",
       "  0.4558333333333333,\n",
       "  0.41444444444444445,\n",
       "  0.6022222222222222,\n",
       "  0.48944444444444435,\n",
       "  0.4657142857142857,\n",
       "  0.6449999999999999,\n",
       "  0.5790000000000001,\n",
       "  0.354,\n",
       "  0.5511111111111111,\n",
       "  0.5505555555555556,\n",
       "  0.435,\n",
       "  0.6642857142857144,\n",
       "  0.5393749999999999,\n",
       "  0.52875,\n",
       "  0.4041666666666666,\n",
       "  0.5438888888888889,\n",
       "  0.31916666666666665,\n",
       "  0.33111111111111113,\n",
       "  0.40611111111111114,\n",
       "  0.5,\n",
       "  0.45055555555555554,\n",
       "  0.463125,\n",
       "  0.32555555555555554,\n",
       "  0.5264285714285714,\n",
       "  0.5383333333333333,\n",
       "  0.6022222222222222,\n",
       "  0.5938888888888888,\n",
       "  0.49888888888888894,\n",
       "  0.6322222222222222,\n",
       "  0.6333333333333333,\n",
       "  0.57125,\n",
       "  0.47000000000000003,\n",
       "  0.5614285714285714,\n",
       "  0.5927777777777777,\n",
       "  0.38375,\n",
       "  0.42,\n",
       "  0.530625,\n",
       "  0.47166666666666657,\n",
       "  0.3378571428571428,\n",
       "  0.49666666666666665,\n",
       "  0.3992857142857143,\n",
       "  0.5188888888888888,\n",
       "  0.628125,\n",
       "  0.5822222222222222,\n",
       "  0.484375,\n",
       "  0.5561111111111111,\n",
       "  0.403,\n",
       "  0.3133333333333333,\n",
       "  0.6087499999999999,\n",
       "  0.4149999999999999,\n",
       "  0.45222222222222225,\n",
       "  0.38111111111111107,\n",
       "  0.5305555555555556,\n",
       "  0.48875,\n",
       "  0.5533333333333332,\n",
       "  0.5033333333333334,\n",
       "  0.37857142857142856,\n",
       "  0.5171428571428571,\n",
       "  0.48750000000000004,\n",
       "  0.343125,\n",
       "  0.5987500000000001,\n",
       "  0.5544444444444445,\n",
       "  0.5772222222222223,\n",
       "  0.404375,\n",
       "  0.43875000000000003,\n",
       "  0.25055555555555553,\n",
       "  0.5405555555555557,\n",
       "  0.49888888888888894,\n",
       "  0.304375,\n",
       "  0.41055555555555556,\n",
       "  0.470625,\n",
       "  0.4366666666666667,\n",
       "  0.5485714285714286,\n",
       "  0.4933333333333334,\n",
       "  0.3772222222222223,\n",
       "  0.458125,\n",
       "  0.457,\n",
       "  0.476875,\n",
       "  0.6177777777777779,\n",
       "  0.3522222222222222,\n",
       "  0.39444444444444443,\n",
       "  0.3116666666666667,\n",
       "  0.405,\n",
       "  0.406,\n",
       "  0.4116666666666667,\n",
       "  0.484375,\n",
       "  0.6325000000000001,\n",
       "  0.336111111111111,\n",
       "  0.425,\n",
       "  0.4675,\n",
       "  0.25833333333333336,\n",
       "  0.44555555555555554,\n",
       "  0.5355555555555556,\n",
       "  0.47625,\n",
       "  0.485625,\n",
       "  0.5049999999999999,\n",
       "  0.33666666666666667,\n",
       "  0.5727777777777778,\n",
       "  0.3764285714285714,\n",
       "  0.37214285714285705,\n",
       "  0.5816666666666667,\n",
       "  0.5088888888888888,\n",
       "  0.581875,\n",
       "  0.55,\n",
       "  0.48277777777777786,\n",
       "  0.4577777777777777,\n",
       "  0.670625,\n",
       "  0.5183333333333333,\n",
       "  0.5622222222222222,\n",
       "  0.4911111111111111,\n",
       "  0.415625,\n",
       "  0.3916666666666667,\n",
       "  0.4816666666666667,\n",
       "  0.34,\n",
       "  0.3222222222222222,\n",
       "  0.5075000000000001,\n",
       "  0.5583333333333333,\n",
       "  0.46611111111111114,\n",
       "  0.37888888888888883,\n",
       "  0.40125,\n",
       "  0.25142857142857145,\n",
       "  0.5933333333333333,\n",
       "  0.7125,\n",
       "  0.4605555555555556,\n",
       "  0.41250000000000003,\n",
       "  0.47333333333333333,\n",
       "  0.40944444444444444,\n",
       "  0.545,\n",
       "  0.3711111111111111,\n",
       "  0.47285714285714286,\n",
       "  0.4116666666666667,\n",
       "  0.35000000000000003,\n",
       "  0.45687500000000003,\n",
       "  0.41388888888888886,\n",
       "  0.6922222222222222,\n",
       "  0.6483333333333333,\n",
       "  0.4766666666666667,\n",
       "  0.385625,\n",
       "  0.21500000000000002,\n",
       "  0.506,\n",
       "  0.445,\n",
       "  0.4077777777777778,\n",
       "  0.54375,\n",
       "  0.5016666666666667,\n",
       "  0.544375,\n",
       "  0.425,\n",
       "  0.5261111111111111,\n",
       "  0.60375,\n",
       "  0.2842857142857143,\n",
       "  0.5599999999999999,\n",
       "  0.34611111111111115,\n",
       "  0.503125,\n",
       "  0.3622222222222222,\n",
       "  0.45,\n",
       "  0.528125,\n",
       "  0.363125,\n",
       "  0.5066666666666666,\n",
       "  0.520625,\n",
       "  0.5116666666666666,\n",
       "  0.5572222222222222,\n",
       "  0.4655555555555556,\n",
       "  0.42875,\n",
       "  0.5921428571428571,\n",
       "  0.6985714285714286,\n",
       "  0.40722222222222215,\n",
       "  0.364375,\n",
       "  0.26375,\n",
       "  0.525625,\n",
       "  0.35125,\n",
       "  0.4635714285714286,\n",
       "  0.5972222222222221,\n",
       "  0.39428571428571424,\n",
       "  0.5335714285714286,\n",
       "  0.518125,\n",
       "  0.6661111111111111,\n",
       "  0.41571428571428576,\n",
       "  0.4283333333333333,\n",
       "  0.5664285714285715,\n",
       "  0.39285714285714285,\n",
       "  0.435,\n",
       "  0.262,\n",
       "  0.5694444444444444,\n",
       "  0.5416666666666667,\n",
       "  0.44777777777777783,\n",
       "  0.6727777777777778,\n",
       "  0.3961111111111111,\n",
       "  0.5294444444444445,\n",
       "  0.378125,\n",
       "  0.325,\n",
       "  0.38187499999999996,\n",
       "  0.5572222222222223,\n",
       "  0.28388888888888886,\n",
       "  0.46187500000000004,\n",
       "  0.525,\n",
       "  0.5341666666666667,\n",
       "  0.4557142857142858,\n",
       "  0.3733333333333333,\n",
       "  0.5544444444444445,\n",
       "  0.46444444444444444,\n",
       "  0.37333333333333335,\n",
       "  0.365,\n",
       "  0.4444444444444444,\n",
       "  0.5016666666666666,\n",
       "  0.5433333333333334,\n",
       "  0.4078571428571429,\n",
       "  0.5288888888888889,\n",
       "  0.42428571428571427,\n",
       "  0.39437500000000003,\n",
       "  0.6277777777777778,\n",
       "  0.6708333333333334,\n",
       "  0.598125,\n",
       "  0.5349999999999999,\n",
       "  0.47714285714285715,\n",
       "  0.5038888888888889,\n",
       "  0.33999999999999997,\n",
       "  0.444375,\n",
       "  0.5299999999999999,\n",
       "  0.420625,\n",
       "  0.515,\n",
       "  0.4405555555555556,\n",
       "  0.39222222222222214,\n",
       "  0.36375,\n",
       "  0.343125,\n",
       "  0.45222222222222225,\n",
       "  0.49555555555555564,\n",
       "  0.285625,\n",
       "  0.4822222222222222,\n",
       "  0.6966666666666667,\n",
       "  0.5277777777777778,\n",
       "  0.4725,\n",
       "  0.45499999999999996,\n",
       "  0.58875,\n",
       "  0.41888888888888887,\n",
       "  0.5725,\n",
       "  0.5861111111111111,\n",
       "  0.358125,\n",
       "  0.38375,\n",
       "  0.5244444444444445,\n",
       "  0.28300000000000003,\n",
       "  0.5305555555555556,\n",
       "  0.43222222222222223,\n",
       "  0.35875,\n",
       "  0.5175,\n",
       "  0.314375,\n",
       "  0.385,\n",
       "  0.3238888888888889,\n",
       "  0.42666666666666664,\n",
       "  0.36000000000000004,\n",
       "  0.680625,\n",
       "  0.41571428571428576,\n",
       "  0.4977777777777777,\n",
       "  0.375,\n",
       "  0.45125000000000004,\n",
       "  0.5058333333333334,\n",
       "  0.5655555555555556,\n",
       "  0.5299999999999999,\n",
       "  0.37333333333333335,\n",
       "  0.43777777777777777,\n",
       "  0.49142857142857144,\n",
       "  0.28,\n",
       "  0.4114285714285714,\n",
       "  0.4892857142857143,\n",
       "  0.2825,\n",
       "  0.21857142857142858,\n",
       "  0.509375,\n",
       "  0.543888888888889,\n",
       "  0.365625,\n",
       "  0.3527777777777778,\n",
       "  0.5642857142857142,\n",
       "  0.590625,\n",
       "  0.3957142857142858,\n",
       "  0.5277777777777778,\n",
       "  0.5355555555555556,\n",
       "  0.371,\n",
       "  0.7216666666666667,\n",
       "  0.575,\n",
       "  0.5606249999999999,\n",
       "  0.410625,\n",
       "  0.445,\n",
       "  0.5405555555555556,\n",
       "  0.5438888888888889,\n",
       "  0.4388888888888889,\n",
       "  0.40800000000000003,\n",
       "  0.39625,\n",
       "  0.5361111111111111,\n",
       "  0.5044444444444445,\n",
       "  0.6793750000000001,\n",
       "  0.455,\n",
       "  0.315,\n",
       "  0.47750000000000004,\n",
       "  0.44111111111111106,\n",
       "  0.47111111111111115,\n",
       "  0.620625,\n",
       "  0.5866666666666667,\n",
       "  0.4772222222222223,\n",
       "  0.46666666666666656,\n",
       "  0.491875,\n",
       "  0.5516666666666666,\n",
       "  0.37333333333333335,\n",
       "  0.41312499999999996,\n",
       "  0.3694444444444444,\n",
       "  0.6450000000000001,\n",
       "  0.55,\n",
       "  0.5455555555555556,\n",
       "  0.33375,\n",
       "  0.4575,\n",
       "  0.28357142857142853,\n",
       "  0.545625,\n",
       "  0.6858333333333334,\n",
       "  0.46888888888888886,\n",
       "  0.62625,\n",
       "  0.6671428571428571,\n",
       "  0.5321428571428571,\n",
       "  0.5831250000000001,\n",
       "  0.33714285714285713,\n",
       "  0.5625000000000001,\n",
       "  0.43812499999999993,\n",
       "  0.6177777777777778,\n",
       "  0.42300000000000004,\n",
       "  0.48928571428571427,\n",
       "  0.6038888888888888,\n",
       "  0.588125,\n",
       "  0.39499999999999996,\n",
       "  0.5977777777777777,\n",
       "  0.4844444444444444,\n",
       "  0.5711111111111111,\n",
       "  0.5133333333333332,\n",
       "  0.569375,\n",
       "  0.44777777777777783,\n",
       "  0.5375,\n",
       "  0.5155555555555555,\n",
       "  0.5844444444444444,\n",
       "  0.4011111111111111,\n",
       "  0.4133333333333333,\n",
       "  0.43374999999999997,\n",
       "  0.3383333333333333,\n",
       "  0.3138888888888889,\n",
       "  0.31444444444444447,\n",
       "  0.5116666666666667,\n",
       "  0.5205555555555555,\n",
       "  0.5733333333333334,\n",
       "  0.28250000000000003,\n",
       "  0.3442857142857143,\n",
       "  0.38055555555555554,\n",
       "  0.653888888888889,\n",
       "  0.6194444444444445,\n",
       "  0.3728571428571429,\n",
       "  0.44625000000000004,\n",
       "  0.320625,\n",
       "  0.468125,\n",
       "  0.34142857142857136,\n",
       "  0.3544444444444445,\n",
       "  0.47250000000000003,\n",
       "  0.2911111111111111,\n",
       "  0.4833333333333334,\n",
       "  0.5905555555555555,\n",
       "  0.43437499999999996,\n",
       "  0.45333333333333337,\n",
       "  0.38083333333333336,\n",
       "  0.5466666666666666,\n",
       "  0.4816666666666667,\n",
       "  0.3861111111111111,\n",
       "  0.6042857142857142,\n",
       "  0.6605555555555555,\n",
       "  0.4294444444444444,\n",
       "  0.5055555555555555,\n",
       "  0.51625,\n",
       "  0.51625,\n",
       "  0.35562499999999997,\n",
       "  0.6633333333333334,\n",
       "  0.39111111111111113,\n",
       "  0.5233333333333333,\n",
       "  0.18555555555555558,\n",
       "  0.590625,\n",
       "  0.46666666666666673,\n",
       "  0.705,\n",
       "  0.5214285714285715,\n",
       "  0.495,\n",
       "  0.4144444444444445,\n",
       "  0.45499999999999996,\n",
       "  0.26071428571428573,\n",
       "  0.39375,\n",
       "  0.47750000000000004,\n",
       "  0.6827777777777778,\n",
       "  0.5411111111111111,\n",
       "  0.5233333333333333,\n",
       "  0.4811111111111111,\n",
       "  0.3575,\n",
       "  0.4494444444444444,\n",
       "  0.37777777777777777,\n",
       "  0.47333333333333333,\n",
       "  0.4275,\n",
       "  0.55,\n",
       "  0.30642857142857144,\n",
       "  0.35166666666666674,\n",
       "  0.423125,\n",
       "  0.5644444444444444,\n",
       "  0.585,\n",
       "  0.544375,\n",
       "  0.36875,\n",
       "  0.5166666666666667,\n",
       "  0.45200000000000007,\n",
       "  0.4928571428571429,\n",
       "  0.29875,\n",
       "  0.5361111111111111,\n",
       "  0.5875,\n",
       "  0.48000000000000004,\n",
       "  0.43083333333333335,\n",
       "  0.6149999999999999,\n",
       "  0.6764285714285714,\n",
       "  0.5711111111111111,\n",
       "  0.431875,\n",
       "  0.4564285714285714,\n",
       "  0.58875,\n",
       "  0.45999999999999996,\n",
       "  0.34555555555555556,\n",
       "  0.5105555555555555,\n",
       "  0.5921428571428572,\n",
       "  0.409375,\n",
       "  0.382,\n",
       "  0.31611111111111106,\n",
       "  0.45999999999999996,\n",
       "  0.5294444444444444,\n",
       "  0.5193749999999999,\n",
       "  0.5594444444444444,\n",
       "  0.44800000000000006,\n",
       "  0.45562499999999995,\n",
       "  0.5633333333333334,\n",
       "  0.4366666666666667,\n",
       "  0.46312500000000006,\n",
       "  0.5792857142857143,\n",
       "  0.43333333333333335,\n",
       "  0.6585714285714285,\n",
       "  0.38277777777777783,\n",
       "  0.6064285714285714,\n",
       "  0.410625,\n",
       "  0.5922222222222222,\n",
       "  ...]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_trainer._eval_save_prefix = OUTPUT_PREFIX + \"-test-pos-unbiased\"\n",
    "model_trainer._evaluate_partial(test_dataset_pos_unbiased)\n",
    "\n",
    "model_trainer._eval_save_prefix = OUTPUT_PREFIX +  \"-test-neg-unbiased\"\n",
    "model_trainer._evaluate_partial(test_dataset_neg_unbiased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Propensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "propensities = calculate_propensities(15401,1001, output_name+\"training_arr.npy\",normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1.5: array([[0.        , 0.29915924, 0.00843328, ..., 0.05913756, 0.01114638,\n",
       "         0.01399943],\n",
       "        [0.        , 0.29915924, 0.00843328, ..., 0.05913756, 0.01114638,\n",
       "         0.01399943],\n",
       "        [0.        , 0.29915924, 0.00843328, ..., 0.05913756, 0.01114638,\n",
       "         0.01399943],\n",
       "        ...,\n",
       "        [0.        , 0.29915924, 0.00843328, ..., 0.05913756, 0.01114638,\n",
       "         0.01399943],\n",
       "        [0.        , 0.29915924, 0.00843328, ..., 0.05913756, 0.01114638,\n",
       "         0.01399943],\n",
       "        [0.        , 0.29915924, 0.00843328, ..., 0.05913756, 0.01114638,\n",
       "         0.01399943]]),\n",
       " 2: array([[0.        , 0.23500814, 0.00324486, ..., 0.03359202, 0.00453483,\n",
       "         0.00596118],\n",
       "        [0.        , 0.23500814, 0.00324486, ..., 0.03359202, 0.00453483,\n",
       "         0.00596118],\n",
       "        [0.        , 0.23500814, 0.00324486, ..., 0.03359202, 0.00453483,\n",
       "         0.00596118],\n",
       "        ...,\n",
       "        [0.        , 0.23500814, 0.00324486, ..., 0.03359202, 0.00453483,\n",
       "         0.00596118],\n",
       "        [0.        , 0.23500814, 0.00324486, ..., 0.03359202, 0.00453483,\n",
       "         0.00596118],\n",
       "        [0.        , 0.23500814, 0.00324486, ..., 0.03359202, 0.00453483,\n",
       "         0.00596118]]),\n",
       " 2.5: array([[0.        , 0.18461347, 0.00124852, ..., 0.01908134, 0.00184496,\n",
       "         0.00253837],\n",
       "        [0.        , 0.18461347, 0.00124852, ..., 0.01908134, 0.00184496,\n",
       "         0.00253837],\n",
       "        [0.        , 0.18461347, 0.00124852, ..., 0.01908134, 0.00184496,\n",
       "         0.00253837],\n",
       "        ...,\n",
       "        [0.        , 0.18461347, 0.00124852, ..., 0.01908134, 0.00184496,\n",
       "         0.00253837],\n",
       "        [0.        , 0.18461347, 0.00124852, ..., 0.01908134, 0.00184496,\n",
       "         0.00253837],\n",
       "        [0.        , 0.18461347, 0.00124852, ..., 0.01908134, 0.00184496,\n",
       "         0.00253837]]),\n",
       " 3: array([[0.        , 0.14502533, 0.00048039, ..., 0.01083881, 0.00075061,\n",
       "         0.00108088],\n",
       "        [0.        , 0.14502533, 0.00048039, ..., 0.01083881, 0.00075061,\n",
       "         0.00108088],\n",
       "        [0.        , 0.14502533, 0.00048039, ..., 0.01083881, 0.00075061,\n",
       "         0.00108088],\n",
       "        ...,\n",
       "        [0.        , 0.14502533, 0.00048039, ..., 0.01083881, 0.00075061,\n",
       "         0.00108088],\n",
       "        [0.        , 0.14502533, 0.00048039, ..., 0.01083881, 0.00075061,\n",
       "         0.00108088],\n",
       "        [0.        , 0.14502533, 0.00048039, ..., 0.01083881, 0.00075061,\n",
       "         0.00108088]])}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "propensities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute AOA and unbiased evaluator metrics with biased testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "biased_results = dict()\n",
    "\n",
    "biased_results[\"AOA\"] = aoa(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", K=30)\n",
    "\n",
    "for gamma in GAMMAS:\n",
    "    key = \"UB_\" + str(gamma).replace(\".\",\"\")\n",
    "    biased_results[key] = eq(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", propensities[gamma], K=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute AOA and unbiased evaluator metrics with unbiased testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbiased_results = dict()\n",
    "\n",
    "# unbiased_results[\"STRATIFIED_15\"] = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=1.5, K=4, partition=100)\n",
    "unbiased_results[\"AOA\"] = aoa(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", K=1)\n",
    "for gamma in GAMMAS:\n",
    "    key = \"UB_\" + str(gamma).replace(\".\",\"\")\n",
    "    unbiased_results[key] = eq(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", propensities[gamma], K=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([240, 200, 743, 255, 657, 469, 808, 987, 568, 323,  69, 297, 647,\n",
       "       321, 352, 468, 455, 301, 359,  31, 969, 141, 803, 429, 903, 587,\n",
       "       128, 167,  71, 400, 378, 992, 144, 211, 800, 601, 608, 266, 941,\n",
       "       978, 327, 476, 232, 344, 258,  22, 801, 733, 696, 399, 388, 942,\n",
       "       946,  48, 124, 805, 100,  80,  15, 536, 404, 473, 521, 190, 736,\n",
       "        68, 363, 975, 626, 444, 692, 470, 826, 889, 847, 874, 483, 679,\n",
       "       302, 879, 244, 155, 210, 667, 174, 791, 933, 463, 988, 430, 305,\n",
       "       356, 514, 613, 607, 170,  55,  92, 782, 872, 558, 411, 495, 542,\n",
       "       490, 519, 387, 780, 422, 417, 753, 650, 768, 456, 900, 699, 453,\n",
       "       254, 382, 637, 390, 117, 685, 389, 802, 635,  34, 319, 783, 376,\n",
       "       734, 126, 575, 840, 775, 415, 620, 662,  11, 677, 219, 237, 764,\n",
       "       147, 569, 907, 932, 998, 909, 922, 492, 394, 668, 643, 578,  62,\n",
       "       711, 973, 482,  24, 206, 156, 749, 956, 994, 809, 518, 621,  46,\n",
       "       459, 143, 663, 947, 113,  57,  41, 794, 745, 163, 101, 656, 315,\n",
       "       625, 708, 714, 229, 917, 145, 566,  79, 340, 224, 691, 638,  20,\n",
       "       239, 790,  12, 333, 130, 618, 610, 870, 446, 466, 704, 761, 421,\n",
       "       524, 818, 682, 845, 910, 541, 372, 104, 262, 841, 661, 796, 510,\n",
       "       972, 153, 897, 467, 270, 504, 507, 209,   5, 303, 905, 827, 154,\n",
       "       557, 385, 218, 653, 612, 508, 409, 936, 326, 433, 615, 614, 334,\n",
       "       471, 313, 261, 631, 596,   9, 595, 248, 530, 628, 339, 835, 915,\n",
       "        96, 729, 586, 563, 288, 675, 846, 257, 205, 678,  70, 253, 739,\n",
       "       151, 496, 810, 160, 177, 408, 726, 179, 771, 431, 842, 592,  28,\n",
       "       354, 216, 537, 511, 916, 660, 480, 899, 477, 856, 720, 111, 564,\n",
       "       369])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get number of items\n",
    "num_items = max_item\n",
    "\n",
    "# Get the n_p partitions\n",
    "n_p = 300\n",
    "nums = np.arange(1, num_items+1)\n",
    "partitions = np.random.choice(nums, n_p, replace=False)\n",
    "\n",
    "# Visualize\n",
    "partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the partition which minimizes the sum of AUC and Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STRATIFIED_15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbddab59971e491ba6ee9dde348532c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best partition: 28 with combined score: 24855822.141171552\n",
      "Minimum score from history: 24855822.141171552\n"
     ]
    }
   ],
   "source": [
    "# Compute biased and unbiased results with stratified for each partition\n",
    "# and store biased and unbiased results such that the sum of AUC and Recall is minimized\n",
    "\n",
    "# Value of gamma to use for minimization\n",
    "gamma = 1.5\n",
    "\n",
    "# To print :)\n",
    "key = \"STRATIFIED_\" + str(gamma).replace(\".\",\"\")\n",
    "print(key)\n",
    "\n",
    "unbiased_results[key] = {}\n",
    "biased_results[key] = {}\n",
    "best_partition = np.random.choice(nums, 1)[0]\n",
    "best_score = float('inf')\n",
    "\n",
    "history = np.full(1000, np.inf)  # Adjusted to match the size of nums\n",
    "\n",
    "for p in tqdm(partitions):\n",
    "    # Fetch stratified results; these functions need to be defined or replaced with actual logic\n",
    "    temp_unbiased = stratified(OUTPUT_PREFIX + \"-test-pos-unbiased_evaluate_partial.pickle\",\n",
    "                               OUTPUT_PREFIX + \"-test-neg-unbiased_evaluate_partial.pickle\",\n",
    "                               output_name + \"training_arr.npy\", propensities[gamma], K=1, partition=p)\n",
    "    temp_biased = stratified(OUTPUT_PREFIX + \"-test-pos-biased_evaluate_partial.pickle\",\n",
    "                             OUTPUT_PREFIX + \"-test-neg-biased_evaluate_partial.pickle\",\n",
    "                             output_name + \"training_arr.npy\", propensities[gamma], K=30, partition=p)\n",
    "\n",
    "    # Calculate combined score\n",
    "    combined_score = temp_unbiased['bias'] + temp_unbiased['concentration'] + \\\n",
    "                     temp_biased['bias'] + temp_biased['concentration']\n",
    "\n",
    "    history[p-1] = combined_score  # Store the combined score\n",
    "\n",
    "    # Update the best_partition and best_score if the current partition's score is lower\n",
    "    if combined_score < best_score:\n",
    "        best_score = combined_score\n",
    "        best_partition = p\n",
    "\n",
    "print(f\"Best partition: {best_partition} with combined score: {best_score}\")\n",
    "print(f\"Minimum score from history: {np.min(history)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed scores from history: [              inf               inf               inf               inf\n",
      " 28389304.45856429               inf               inf               inf\n",
      " 27260497.93998905               inf 26372049.44067401 26224500.79289838\n",
      "               inf               inf 25698084.38735908               inf\n",
      "               inf               inf               inf 25110612.4680917\n",
      "               inf 24930641.53299935               inf 24901089.51418758\n",
      "               inf               inf               inf 24855822.14117155\n",
      "               inf               inf 24941183.23207924               inf\n",
      "               inf 24953716.55908788               inf               inf\n",
      "               inf               inf               inf               inf\n",
      " 25215895.65230725               inf               inf               inf\n",
      "               inf 25740810.14904466               inf 26084093.23593797\n",
      "               inf               inf               inf               inf\n",
      "               inf               inf 26234849.76174815               inf\n",
      " 26766463.56988775               inf               inf               inf\n",
      "               inf 27349164.66274075               inf               inf\n",
      "               inf               inf               inf 28048306.32254216\n",
      " 28027764.13021521 28028087.70303425 28025312.55205406               inf\n",
      "               inf               inf               inf               inf\n",
      "               inf               inf 29134811.89861587 29134571.76512741\n",
      "               inf               inf               inf               inf\n",
      "               inf               inf               inf               inf\n",
      "               inf               inf               inf 30403644.89733951\n",
      "               inf               inf               inf 32557816.44033692\n",
      "               inf               inf               inf 32548606.42725901\n",
      " 32549247.19157616               inf               inf 32535701.21826218\n",
      "               inf               inf               inf               inf\n",
      "               inf               inf 35103334.77130042               inf\n",
      " 35074573.43441021               inf               inf               inf\n",
      " 35067197.29970187               inf               inf               inf\n",
      "               inf               inf               inf 35042003.16747542\n",
      "               inf 35041056.35480151               inf 35037977.20568096\n",
      "               inf 38424553.55091478               inf               inf\n",
      "               inf               inf               inf               inf\n",
      "               inf               inf               inf               inf\n",
      " 38387451.87646642               inf 38379427.29800415 38378465.38676544\n",
      " 38378357.65100548               inf 38370771.62894171               inf\n",
      "               inf               inf 38348884.36212984               inf\n",
      " 38345293.38445895 42647256.24990448 42640791.57644056 42640495.20766699\n",
      "               inf               inf               inf 42638215.10133795\n",
      "               inf               inf 42636387.8396702                inf\n",
      "               inf               inf 42638892.83133984               inf\n",
      "               inf 42610360.35877158               inf               inf\n",
      "               inf 42608410.33255999               inf               inf\n",
      " 42599463.19646242               inf 42599414.72249792               inf\n",
      "               inf               inf               inf               inf\n",
      "               inf               inf               inf               inf\n",
      "               inf 42580616.9213585                inf               inf\n",
      "               inf               inf               inf               inf\n",
      "               inf               inf               inf 48196329.74168178\n",
      "               inf               inf               inf               inf\n",
      " 48184067.10452668 48184870.46610223               inf               inf\n",
      " 48181690.64104234 48182040.25497888 48181341.62538819               inf\n",
      "               inf               inf               inf 48179830.01249413\n",
      "               inf 48175854.39802887 48155035.48462867               inf\n",
      "               inf               inf               inf 48155211.26762901\n",
      "               inf               inf               inf               inf\n",
      " 48131277.71961024               inf               inf 48130185.52409575\n",
      "               inf               inf               inf               inf\n",
      " 48129581.88044161               inf 48124979.78147605 48126666.84615822\n",
      "               inf               inf               inf 48125046.69388532\n",
      "               inf               inf               inf 58890815.93969823\n",
      "               inf               inf               inf               inf\n",
      " 58885845.33034222 58882946.65358736 58882958.23953861               inf\n",
      " 58838959.7084631  58838108.23085253               inf               inf\n",
      " 58838033.73241283 58838186.07948351               inf               inf\n",
      "               inf 58834990.00245938               inf               inf\n",
      "               inf 58832534.40054843               inf               inf\n",
      "               inf               inf               inf               inf\n",
      "               inf               inf               inf               inf\n",
      "               inf               inf               inf               inf\n",
      "               inf               inf               inf 58821804.12465048\n",
      "               inf               inf               inf               inf\n",
      "               inf               inf               inf               inf\n",
      " 58815904.0143981                inf               inf               inf\n",
      " 58810907.32480185 58810433.40033607 58810390.70369664               inf\n",
      " 58808944.66394588               inf               inf               inf\n",
      "               inf               inf               inf               inf\n",
      " 58779864.11786146               inf 58780004.68101815               inf\n",
      "               inf               inf 58779089.29447052               inf\n",
      " 58777999.98858946               inf 58778407.92854711               inf\n",
      "               inf 58778120.01974569 58778922.399095                 inf\n",
      "               inf               inf               inf               inf\n",
      " 58771160.18169855 58771164.63273246               inf               inf\n",
      "               inf               inf 58758409.56642023 58759748.37618093\n",
      "               inf               inf               inf 75252958.17614034\n",
      "               inf               inf               inf               inf\n",
      "               inf               inf               inf 75249535.7742559\n",
      "               inf 75249811.61665368               inf 75250138.98437919\n",
      "               inf               inf 75248885.66581643               inf\n",
      "               inf               inf 75247923.40329006               inf\n",
      "               inf               inf               inf               inf\n",
      " 75246618.18429977               inf               inf 75244053.73866847\n",
      "               inf               inf               inf 75238337.36365213\n",
      "               inf 75236520.19750237               inf               inf\n",
      "               inf 75220689.34376363               inf               inf\n",
      " 75211981.77576263               inf 75210100.36412051 75208848.54266396\n",
      " 75208739.61262278 75208587.3842209                inf               inf\n",
      "               inf 75204104.9639214                inf               inf\n",
      "               inf               inf 75204622.36124453 75205393.49171379\n",
      "               inf               inf               inf 75202375.8118877\n",
      "               inf               inf               inf 75201224.54549046\n",
      " 75199225.40630439               inf 75197888.83697593               inf\n",
      "               inf               inf 75192248.5293981                inf\n",
      " 75192418.1718327                inf               inf               inf\n",
      " 75188578.57415591 75187759.98066486               inf               inf\n",
      "               inf               inf               inf               inf\n",
      " 75190455.35465379 75190006.1535613  75190006.1535613                inf\n",
      " 75189133.78604534               inf               inf               inf\n",
      "               inf               inf               inf               inf\n",
      "               inf               inf               inf 75185026.12086207\n",
      "               inf 75184386.84006159               inf               inf\n",
      "               inf               inf               inf               inf\n",
      " 75172560.59771788               inf 75170135.63056637 75169778.49382645\n",
      "               inf               inf 75170348.7537796                inf\n",
      "               inf               inf 75164098.98535821               inf\n",
      "               inf 75163014.04063703 75162712.96220788 75163449.5457049\n",
      " 75163806.62442106 75161503.70798257 75161614.46556239               inf\n",
      " 75159035.72589366               inf               inf 75154701.91551363\n",
      " 75154716.67659914               inf               inf 75152885.32718778\n",
      "               inf 75151363.44670078 75151363.44670078               inf\n",
      "               inf               inf               inf               inf\n",
      "               inf 75152162.05419485               inf 75152036.69698825\n",
      "               inf               inf 75133686.5270907  75133563.72381593\n",
      "               inf               inf               inf               inf\n",
      "               inf               inf               inf 75121348.89101072\n",
      "               inf               inf 75120395.87525794 75121166.13416494\n",
      "               inf 75122463.87695748 75121606.78405553               inf\n",
      "               inf 75115577.747576                 inf               inf\n",
      "               inf 75114833.56431028 75114833.56431028               inf\n",
      " 75116320.7073777                inf               inf 75113616.50254977\n",
      "               inf               inf               inf               inf\n",
      "               inf 75111029.27386421               inf               inf\n",
      "               inf               inf               inf 75110254.38082397\n",
      " 75107204.5985391                inf               inf               inf\n",
      " 92678090.89380965 92678090.89380965               inf               inf\n",
      "               inf               inf               inf               inf\n",
      "               inf               inf               inf               inf\n",
      "               inf               inf               inf               inf\n",
      " 92672117.53359997 92672117.53359997               inf               inf\n",
      "               inf               inf 92666946.75503296 92666946.75503296\n",
      "               inf 92666172.23154956               inf 92666172.23154956\n",
      " 92666172.23154956               inf               inf               inf\n",
      "               inf               inf 92649971.84821075               inf\n",
      "               inf 92646921.15669316               inf               inf\n",
      "               inf               inf               inf               inf\n",
      "               inf 92643390.45244485 92638961.86361673               inf\n",
      "               inf               inf               inf 92638961.86361673\n",
      "               inf               inf 92638961.86361673 92638961.86361673\n",
      "               inf               inf               inf               inf\n",
      " 92634781.59324494               inf               inf               inf\n",
      "               inf               inf 92631663.83940756 92631663.83940756\n",
      "               inf 92631663.83940756               inf 92631044.45574734\n",
      " 92631044.45574734 92631044.45574734 92620779.98870847               inf\n",
      "               inf 92620001.68558621               inf 92615323.4440529\n",
      " 92615323.4440529                inf               inf               inf\n",
      " 92615323.4440529  92615323.4440529                inf 92615323.4440529\n",
      "               inf               inf 92615323.4440529                inf\n",
      "               inf               inf 92615323.4440529                inf\n",
      " 92615323.4440529  92615323.4440529                inf               inf\n",
      "               inf               inf 92611643.10457413               inf\n",
      "               inf               inf 92611643.10457413               inf\n",
      "               inf 92611643.10457413               inf               inf\n",
      " 92611643.10457413               inf               inf 92609442.13748378\n",
      " 92609442.13748378               inf               inf 92609442.13748378\n",
      " 92609442.13748378 92609442.13748378 92609442.13748378               inf\n",
      "               inf               inf 92603056.52838874 92603056.52838874\n",
      "               inf               inf               inf               inf\n",
      "               inf               inf 92580088.72375566               inf\n",
      " 92580088.72375566 92580088.72375566 92580088.72375566               inf\n",
      "               inf 92580088.72375566               inf               inf\n",
      " 92586049.64264005               inf               inf               inf\n",
      "               inf               inf 92586049.64264005 92586049.64264005\n",
      "               inf               inf               inf 92586049.64264005\n",
      "               inf               inf 92586049.64264005               inf\n",
      "               inf               inf               inf 92586049.64264005\n",
      "               inf               inf               inf 92586049.64264005\n",
      "               inf               inf 92586049.64264005               inf\n",
      "               inf 92586049.64264005               inf               inf\n",
      "               inf               inf               inf 92586049.64264005\n",
      "               inf               inf               inf               inf\n",
      "               inf 92586049.64264005               inf               inf\n",
      " 92586049.64264005               inf               inf               inf\n",
      " 92586049.64264005 92586049.64264005               inf 92586049.64264005\n",
      "               inf               inf 92571507.74570562               inf\n",
      "               inf               inf 92571507.74570562               inf\n",
      " 92571507.74570562               inf               inf               inf\n",
      " 92571507.74570562               inf               inf               inf\n",
      " 92571507.74570562               inf               inf               inf\n",
      "               inf               inf               inf               inf\n",
      " 92559856.34453519               inf               inf 92559856.34453519\n",
      "               inf               inf               inf 92554740.42014094\n",
      "               inf               inf 92519501.00674656               inf\n",
      "               inf               inf 92519501.00674656               inf\n",
      "               inf               inf               inf 92519501.00674656\n",
      "               inf 92519501.00674656 92519501.00674656               inf\n",
      "               inf               inf               inf               inf\n",
      "               inf 92519501.00674656 92519501.00674656               inf\n",
      "               inf 92519501.00674656               inf 92519501.00674656\n",
      "               inf               inf               inf 92519501.00674656\n",
      " 92519501.00674656 92519501.00674656 92519501.00674656               inf\n",
      " 92519501.00674656               inf               inf 92519501.00674656\n",
      " 92519501.00674656 92519501.00674656               inf               inf\n",
      "               inf               inf               inf               inf\n",
      "               inf 92519501.00674656               inf               inf\n",
      "               inf               inf               inf               inf\n",
      "               inf 92519501.00674656 92519501.00674656               inf\n",
      "               inf               inf               inf               inf\n",
      "               inf               inf 92519501.00674656               inf\n",
      "               inf               inf               inf 92519501.00674656\n",
      " 92519501.00674656 92519501.00674656               inf               inf\n",
      " 92519501.00674656 92519501.00674656 92519501.00674656               inf\n",
      "               inf               inf               inf               inf\n",
      "               inf               inf               inf 92519501.00674656\n",
      "               inf               inf               inf               inf\n",
      "               inf               inf               inf               inf\n",
      "               inf               inf               inf               inf\n",
      "               inf 92519501.00674656               inf 92519501.00674656\n",
      "               inf 92519501.00674656               inf               inf\n",
      "               inf               inf 92519501.00674656               inf\n",
      "               inf               inf               inf               inf\n",
      "               inf               inf               inf               inf\n",
      " 92519501.00674656               inf               inf               inf\n",
      "               inf               inf               inf               inf\n",
      " 92519501.00674656               inf 92519501.00674656 92519501.00674656\n",
      "               inf               inf 92519501.00674656               inf\n",
      " 92519501.00674656               inf 92519501.00674656               inf\n",
      " 92519501.00674656 92519501.00674656               inf               inf\n",
      "               inf               inf 92519501.00674656 92519501.00674656\n",
      " 92519501.00674656               inf               inf               inf\n",
      "               inf 92519501.00674656               inf               inf\n",
      "               inf               inf               inf               inf\n",
      "               inf               inf               inf 92519501.00674656\n",
      " 92519501.00674656               inf               inf 92519501.00674656\n",
      "               inf               inf               inf               inf\n",
      " 92519501.00674656 92519501.00674656               inf               inf\n",
      "               inf 92519501.00674656 92519501.00674656               inf\n",
      "               inf               inf               inf               inf\n",
      "               inf               inf               inf 92519501.00674656\n",
      "               inf               inf               inf               inf\n",
      "               inf               inf               inf               inf\n",
      "               inf               inf               inf               inf\n",
      " 92519501.00674656               inf               inf 92519501.00674656\n",
      " 92519501.00674656               inf 92519501.00674656               inf\n",
      "               inf 92519501.00674656               inf               inf\n",
      "               inf               inf               inf               inf\n",
      "               inf               inf 92519501.00674656 92519501.00674656\n",
      "               inf               inf               inf 92519501.00674656\n",
      "               inf 92519501.00674656               inf               inf\n",
      "               inf 92519501.00674656               inf               inf]\n"
     ]
    }
   ],
   "source": [
    "# Additional outputs for verification\n",
    "print(\"Detailed scores from history:\", history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the histogram\n",
    "plt.hist(history[history != np.inf], bins=10, edgecolor='black')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Data')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, for the chosen value of gamma, the best partition is..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize\n",
    "best_partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute stratified metrics with unbiased testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gamma in GAMMAS:\n",
    "    key = \"STRATIFIED_\" + str(gamma).replace(\".\",\"\")\n",
    "    unbiased_results[key] = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", propensities[gamma], K=1, partition=best_partition)\n",
    "    biased_results[key] = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", propensities[gamma], K=30, partition=best_partition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version uses the linspace of items instead of linspace of propensities to make the partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gamma in GAMMAS:\n",
    "    key = \"STRATIFIED_v2_\" + str(gamma).replace(\".\",\"\")\n",
    "    unbiased_results[key] = stratified_2(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", propensities[gamma], K=1, partition=best_partition)\n",
    "    biased_results[key] = stratified_2(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", propensities[gamma], K=30, partition=best_partition)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare table for results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 13)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = 0\n",
    "columns = len(biased_results.keys())\n",
    "\n",
    "for key in biased_results.keys():\n",
    "    rows = max(rows, len(biased_results[key].keys()))\n",
    "\n",
    "for key in unbiased_results.keys():\n",
    "    rows = max(rows, len(biased_results[key].keys()))\n",
    "\n",
    "rows, columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init dictionary\n",
    "mae_results = dict()\n",
    "\n",
    "# Get the names of the rows\n",
    "list_biased_res = list(biased_results.keys())\n",
    "\n",
    "# Init results\n",
    "results_array = np.zeros((rows,columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each row\n",
    "for i in range(len(list_biased_res)):\n",
    "    key = list_biased_res[i]\n",
    "\n",
    "    # For each column\n",
    "    for j in range(len(list(biased_results[key].keys()))):\n",
    "        key_2 = list(biased_results[key].keys())[j]\n",
    "\n",
    "        # Compute MAE\n",
    "        results_array[j][i] = abs(biased_results[key][key_2] - unbiased_results[key][key_2])\n",
    "\n",
    "# Make it a DataFrame\n",
    "mae_df = pd.DataFrame(columns=list(biased_results.keys()), data=results_array)\n",
    "metric_values = list(biased_results[list(biased_results.keys())[0]].keys())\n",
    "mae_df.insert(0, \"metric\", metric_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the table with the MAE results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **RESULTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>AOA</th>\n",
       "      <th>UB_15</th>\n",
       "      <th>UB_2</th>\n",
       "      <th>UB_25</th>\n",
       "      <th>UB_3</th>\n",
       "      <th>STRATIFIED_15</th>\n",
       "      <th>STRATIFIED_2</th>\n",
       "      <th>STRATIFIED_25</th>\n",
       "      <th>STRATIFIED_3</th>\n",
       "      <th>STRATIFIED_v2_15</th>\n",
       "      <th>STRATIFIED_v2_2</th>\n",
       "      <th>STRATIFIED_v2_25</th>\n",
       "      <th>STRATIFIED_v2_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>auc</td>\n",
       "      <td>0.154493</td>\n",
       "      <td>0.128429</td>\n",
       "      <td>0.125081</td>\n",
       "      <td>0.122529</td>\n",
       "      <td>0.120581</td>\n",
       "      <td>6.079207e-02</td>\n",
       "      <td>1.036950e-01</td>\n",
       "      <td>4.504004e-01</td>\n",
       "      <td>2.416001e+00</td>\n",
       "      <td>1.673795e-01</td>\n",
       "      <td>1.727882e-01</td>\n",
       "      <td>1.810619e-01</td>\n",
       "      <td>1.925789e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>recall</td>\n",
       "      <td>0.377779</td>\n",
       "      <td>0.259048</td>\n",
       "      <td>0.247508</td>\n",
       "      <td>0.238948</td>\n",
       "      <td>0.232533</td>\n",
       "      <td>3.615133e-01</td>\n",
       "      <td>6.554661e-01</td>\n",
       "      <td>1.696810e+00</td>\n",
       "      <td>5.918754e+00</td>\n",
       "      <td>3.040895e-01</td>\n",
       "      <td>3.034154e-01</td>\n",
       "      <td>3.081898e-01</td>\n",
       "      <td>3.182208e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bias</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.707661e+05</td>\n",
       "      <td>1.534050e+06</td>\n",
       "      <td>3.903466e+06</td>\n",
       "      <td>1.081622e+07</td>\n",
       "      <td>8.330794e+03</td>\n",
       "      <td>9.956024e+03</td>\n",
       "      <td>1.151149e+04</td>\n",
       "      <td>1.303961e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>concentration</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.109038e+06</td>\n",
       "      <td>4.467708e+06</td>\n",
       "      <td>1.870334e+07</td>\n",
       "      <td>7.888749e+07</td>\n",
       "      <td>1.018654e+07</td>\n",
       "      <td>4.967987e+07</td>\n",
       "      <td>2.360621e+08</td>\n",
       "      <td>1.101246e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          metric       AOA     UB_15      UB_2     UB_25      UB_3  \\\n",
       "0            auc  0.154493  0.128429  0.125081  0.122529  0.120581   \n",
       "1         recall  0.377779  0.259048  0.247508  0.238948  0.232533   \n",
       "2           bias  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3  concentration  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "   STRATIFIED_15  STRATIFIED_2  STRATIFIED_25  STRATIFIED_3  STRATIFIED_v2_15  \\\n",
       "0   6.079207e-02  1.036950e-01   4.504004e-01  2.416001e+00      1.673795e-01   \n",
       "1   3.615133e-01  6.554661e-01   1.696810e+00  5.918754e+00      3.040895e-01   \n",
       "2   5.707661e+05  1.534050e+06   3.903466e+06  1.081622e+07      8.330794e+03   \n",
       "3   1.109038e+06  4.467708e+06   1.870334e+07  7.888749e+07      1.018654e+07   \n",
       "\n",
       "   STRATIFIED_v2_2  STRATIFIED_v2_25  STRATIFIED_v2_3  \n",
       "0     1.727882e-01      1.810619e-01     1.925789e-01  \n",
       "1     3.034154e-01      3.081898e-01     3.182208e-01  \n",
       "2     9.956024e+03      1.151149e+04     1.303961e+04  \n",
       "3     4.967987e+07      2.360621e+08     1.101246e+09  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize\n",
    "mae_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RecSysEvaluation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
