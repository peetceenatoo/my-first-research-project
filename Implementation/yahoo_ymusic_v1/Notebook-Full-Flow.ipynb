{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **IMPORT LIBS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from openrec.tf1.legacy import ImplicitModelTrainer\n",
    "from openrec.tf1.legacy.utils.evaluators import ImplicitEvalManager\n",
    "from openrec.tf1.legacy.utils import ImplicitDataset\n",
    "from openrec.tf1.legacy.recommenders import CML, BPR, PMF\n",
    "from openrec.tf1.legacy.utils.evaluators import AUC\n",
    "from openrec.tf1.legacy.utils.samplers import PairwiseSampler\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPEAT_TRAINING = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **GENERATE THE DATASET**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "seed = 2384795\n",
    "np.random.seed(seed=seed)\n",
    "\n",
    "# Preparing folder for output data\n",
    "output_name = f\"./generated_data/\"\n",
    "if os.path.exists(output_name) == False:\n",
    "    os.makedirs(output_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>SongID</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>83</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>153</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>170</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>184</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>194</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  SongID  Rating\n",
       "0       1      14       5\n",
       "1       1      35       1\n",
       "2       1      46       1\n",
       "3       1      83       1\n",
       "4       1      93       1\n",
       "5       1      94       1\n",
       "6       1     153       5\n",
       "7       1     170       4\n",
       "8       1     184       5\n",
       "9       1     194       5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Name of the dataset paths\n",
    "file_path = 'ydata-ymusic-rating-study-v1_0-train.txt'\n",
    "folder_name = f\"./original_files/\"\n",
    "\n",
    "# Load the training set into a DataFrame\n",
    "df_train = pd.read_csv(folder_name+file_path, sep='\\t',names=[\"UserID\",\"SongID\",\"Rating\"], header=None)  # sep='\\t' for tab-separated values\n",
    "\n",
    "# Visualize\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to implicit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"We treat items rated greater than or equal to 4 as relevant, and others as irrelevant, as suggested by prior literature.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>SongID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>ImplicitRating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>83</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>153</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>170</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>184</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>194</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  SongID  Rating  ImplicitRating\n",
       "0       1      14       5               1\n",
       "1       1      35       1               0\n",
       "2       1      46       1               0\n",
       "3       1      83       1               0\n",
       "4       1      93       1               0\n",
       "5       1      94       1               0\n",
       "6       1     153       5               1\n",
       "7       1     170       4               1\n",
       "8       1     184       5               1\n",
       "9       1     194       5               1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Suggested on paper\n",
    "POSITIVE_THRESHOLD = 4\n",
    "\n",
    "# Add column to the DataFrame\n",
    "df_train['ImplicitRating'] = np.where(df_train['Rating'] >= POSITIVE_THRESHOLD, 1, 0)\n",
    "\n",
    "# Visualize\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the number of users and items in the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The training set contains 300K ratings given by 15.4K users against 1K songs through natural interactions.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 15400)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store the range of ids for users\n",
    "min_user = df_train[\"UserID\"].min()\n",
    "max_user = df_train[\"UserID\"].max()\n",
    "\n",
    "# Store the range of items\n",
    "min_item = df_train[\"SongID\"].min()\n",
    "max_item = df_train[\"SongID\"].max()\n",
    "\n",
    "# Visualize the number of both\n",
    "max_item, max_user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **GET UNBIASED TESTSET**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the unbiased testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the dataset path\n",
    "file_path = folder_name + 'ydata-ymusic-rating-study-v1_0-test.txt'\n",
    "\n",
    "# Load the training set into a DataFrame\n",
    "df_test = pd.read_csv(file_path, sep='\\t',names=[\"UserID\",\"SongID\",\"Rating\"], header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to implicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>SongID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>ImplicitRating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>138</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>141</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>177</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>268</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>511</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>587</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>772</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>941</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  SongID  Rating  ImplicitRating\n",
       "0       1      49       1               0\n",
       "1       1     126       1               0\n",
       "2       1     138       1               0\n",
       "3       1     141       1               0\n",
       "4       1     177       1               0\n",
       "5       1     268       3               0\n",
       "6       1     511       1               0\n",
       "7       1     587       1               0\n",
       "8       1     772       5               1\n",
       "9       1     941       1               0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add column to the DataFrame\n",
    "df_test['ImplicitRating'] = np.where(df_test['Rating'] >= POSITIVE_THRESHOLD, 1, 0)\n",
    "\n",
    "# Visualize\n",
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the number of users and items in the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The testing set is collected by asking a subset of 5.4K users to rate 10 randomly selected songs.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5400, 1000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize\n",
    "df_test[\"UserID\"].max(), df_test[\"SongID\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter unbiased testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"We filter the testing set by retaining users who have at least a relevant and an irrelevant song in the testing set and two relevant songs in the training set.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select users with at least an irrelevant song in the unbiased testset\n",
    "usersWithNegativeInteractionInTest = df_test[df_test[\"ImplicitRating\"] == 0][\"UserID\"].unique()\n",
    "\n",
    "# Select UserID of users with at least a relevant song in testset\n",
    "usersWithPositiveInteractionInTest = df_test[df_test[\"ImplicitRating\"] == 1][\"UserID\"].unique()\n",
    "\n",
    "# Select UserID of users with at least two relevant song in trainset\n",
    "usersWithTwoPositiveInteractions = df_train[df_train[\"ImplicitRating\"] == 1].groupby(\"UserID\").filter(lambda x: len(x) >= 2)['UserID'].unique()\n",
    "\n",
    "# Compute the intersection\n",
    "set1 = set(usersWithNegativeInteractionInTest)\n",
    "set2 = set(usersWithPositiveInteractionInTest)\n",
    "set3 = set(usersWithTwoPositiveInteractions)\n",
    "valid_users_testset = set1 & set2 & set3\n",
    "\n",
    "# Filter the testset\n",
    "df_test_filtered = df_test[df_test[\"UserID\"].isin(valid_users_testset)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"2296 users satisfy these requirements.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2296"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize\n",
    "len(valid_users_testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape the unbiased test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the dataframe, for each row where ImplicitRating is 1, append [userID, itemID] to unbiased_pos_test_set\n",
    "# and for each row where ImplicitRating is 0, append [userID, itemID] to unbiased_neg_test_set\n",
    "unbiased_pos_test_set = df_test_filtered[df_test_filtered[\"ImplicitRating\"] == 1][[\"UserID\", \"SongID\"]].values\n",
    "unbiased_neg_test_set = df_test_filtered[df_test_filtered[\"ImplicitRating\"] == 0][[\"UserID\", \"SongID\"]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save unbiased test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to split pos and neg test set into two separate files\n",
    "\n",
    "# Get the dataframe\n",
    "unbiased_pos_test_set_df = pd.DataFrame(unbiased_pos_test_set)\n",
    "unbiased_neg_test_set_df = pd.DataFrame(unbiased_neg_test_set)\n",
    "\n",
    "# Get couples user-item\n",
    "unbiased_pos_test_set_df.columns = [\"user_id\",\"item_id\"]\n",
    "unbiased_neg_test_set_df.columns = [\"user_id\",\"item_id\"]\n",
    "\n",
    "# Turn into records\n",
    "structured_data_pos_test_set_unbiased = unbiased_pos_test_set_df.to_records(index=False)\n",
    "structured_data_neg_test_set_unbiased = unbiased_neg_test_set_df.to_records(index=False)\n",
    "\n",
    "# Save\n",
    "np.save(output_name + \"unbiased-test_arr_pos.npy\", structured_data_pos_test_set_unbiased)\n",
    "np.save(output_name + \"unbiased-test_arr_neg.npy\", structured_data_neg_test_set_unbiased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **GET BIASED TESTSET**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read again to reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the dataset path\n",
    "file_path = folder_name + 'ydata-ymusic-rating-study-v1_0-train.txt'\n",
    "\n",
    "# Load the training set into a DataFrame\n",
    "df_train = pd.read_csv(file_path, sep='\\t',names=[\"UserID\",\"SongID\",\"Rating\"], header=None)  # sep='\\t' for tab-separated values\n",
    "df_train['ImplicitRating'] = np.where(df_train['Rating'] >= POSITIVE_THRESHOLD, 1, 0)\n",
    "df_train = df_train[df_train[\"UserID\"].isin(valid_users_testset)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the biased test set and shape it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"We additionally held out a biased testing set (biased-testing) from the training set by randomly sampling 300 songs for each user.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompute, for each user, the list of songs with a relevant rating\n",
    "user_positive_ratings = df_train[df_train[\"ImplicitRating\"] == 1].groupby(\"UserID\")[\"SongID\"].apply(set)\n",
    "\n",
    "# Initialize the range of indexes for the items\n",
    "items_ids = np.arange(min_item, max_item + 1)\n",
    "\n",
    "# Set the number of songs for each user\n",
    "SONGS_FOR_BIASED_TEST = 300\n",
    "\n",
    "# Init empty\n",
    "pos_test_set = []\n",
    "neg_test_set = []\n",
    "\n",
    "# Extract the biased test set\n",
    "for user_id in valid_users_testset:\n",
    "\n",
    "    # Get SONGS_FOR_BIASED_TEST items\n",
    "    np.random.shuffle(items_ids)\n",
    "    test_items = set(items_ids[-SONGS_FOR_BIASED_TEST:])\n",
    "\n",
    "    # Get which are positive\n",
    "    pos_ids = user_positive_ratings.get(user_id, set()) & test_items\n",
    "\n",
    "    # Set the positive ones to 0 in the training set (extract)\n",
    "    df_train.loc[(df_train['SongID'].isin(pos_ids)) & (df_train['UserID'] == user_id), 'ImplicitRating'] = 0\n",
    "\n",
    "    # Append items\n",
    "    for id in test_items:\n",
    "        if id in pos_ids:\n",
    "            pos_test_set.append([user_id, id])\n",
    "        else:\n",
    "            neg_test_set.append([user_id, id])\n",
    "\n",
    "# Get np arrays\n",
    "pos_test_set = np.array(pos_test_set)\n",
    "neg_test_set = np.array(neg_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the biased test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to split pos and neg test set into two separate files\n",
    "\n",
    "# Get the dataframe\n",
    "pos_test_set_df = pd.DataFrame(pos_test_set)\n",
    "neg_test_set_df = pd.DataFrame(neg_test_set)\n",
    "\n",
    "# Get couples user-item\n",
    "pos_test_set_df.columns = [\"user_id\",\"item_id\"]\n",
    "neg_test_set_df.columns = [\"user_id\",\"item_id\"]\n",
    "\n",
    "# Turn into records\n",
    "structured_data_pos_test_set = pos_test_set_df.to_records(index=False)\n",
    "structured_data_neg_test_set = neg_test_set_df.to_records(index=False)\n",
    "\n",
    "# Save\n",
    "np.save(output_name + \"biased-test_arr_pos.npy\", structured_data_pos_test_set)\n",
    "np.save(output_name + \"biased-test_arr_neg.npy\", structured_data_neg_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **STORE TRAINSET**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter positive couples (user, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only take the couples (user, item) with relevant rating\n",
    "new_df = df_train[df_train['ImplicitRating'] != 0]\n",
    "new_df = new_df.drop(columns=['Rating', 'ImplicitRating'])\n",
    "\n",
    "# Define a dictionary for renaming columns\n",
    "rename_dict = {\n",
    "    'UserID': 'user_id',\n",
    "    'SongID': 'item_id'\n",
    "}\n",
    "\n",
    "# Rename the columns\n",
    "new_df = new_df.rename(columns=rename_dict)\n",
    "\n",
    "# Convert the DataFrame to a structured array\n",
    "train_data = new_df.to_records(index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "np.save(output_name + \"training_arr.npy\", train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MODEL CHOICE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I won't comment anything, we are just using the code provided by the authors of the paper\n",
    "\n",
    "raw_data = dict()\n",
    "raw_data['train_data'] = np.load(output_name + \"training_arr.npy\")\n",
    "raw_data['max_user'] = 15401\n",
    "raw_data['max_item'] = 1001\n",
    "batch_size = 8000\n",
    "test_batch_size = 1000\n",
    "display_itr = 1000\n",
    "\n",
    "train_dataset = ImplicitDataset(raw_data['train_data'], raw_data['max_user'], raw_data['max_item'], name='Train')\n",
    "\n",
    "MODEL_CLASS = CML\n",
    "MODEL_PREFIX = \"cml\"\n",
    "DATASET_NAME = \"yahoo\"\n",
    "OUTPUT_FOLDER = output_name\n",
    "OUTPUT_PATH = OUTPUT_FOLDER + MODEL_PREFIX + \"-\" + DATASET_NAME + \"/\"\n",
    "OUTPUT_PREFIX = str(OUTPUT_PATH) + str(MODEL_PREFIX) + \"-\" + str(DATASET_NAME)\n",
    "\n",
    "\n",
    "if os.path.exists(OUTPUT_PATH) == False:\n",
    "    os.makedirs(OUTPUT_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TRAIN THE MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_56965/3362066690.py:4: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prevent tensorflow from using cached embeddings\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.reset_default_graph()\n",
    "tf.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REPEAT_TRAINING:\n",
    "    # Define the model\n",
    "    model = MODEL_CLASS(batch_size=batch_size, max_user=train_dataset.max_user(), max_item=train_dataset.max_item(), dim_embed=50, l2_reg=0.001, opt='Adam', sess_config=None)\n",
    "    sampler = PairwiseSampler(batch_size=batch_size, dataset=train_dataset, num_process=4)\n",
    "    model_trainer = ImplicitModelTrainer(batch_size=batch_size, test_batch_size=test_batch_size, train_dataset=train_dataset, model=model, sampler=sampler, eval_save_prefix=OUTPUT_PATH + DATASET_NAME, item_serving_size=500)\n",
    "    auc_evaluator = AUC()\n",
    "\n",
    "    # Train the model\n",
    "    model_trainer.train(num_itr=10001, display_itr=display_itr)\n",
    "\n",
    "    # Save in the output folder\n",
    "    model.save(OUTPUT_PATH,None)\n",
    "\n",
    "    # Delete the model from the memory\n",
    "    del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DEFINING FUNCTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eq(infilename, infilename_neg, trainfilename, gamma=-1.0, K=1):\n",
    "\n",
    "    # Read pickles\n",
    "    infile = open(infilename, 'rb')\n",
    "    infile_neg = open(infilename_neg, 'rb')\n",
    "    P = pickle.load(infile)\n",
    "    infile.close()\n",
    "    P_neg = pickle.load(infile_neg)\n",
    "    infile_neg.close()\n",
    "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
    "    \n",
    "    # Merge P and P_neg\n",
    "    for theuser in P[\"users\"]:\n",
    "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "    \n",
    "    Zui = dict()\n",
    "    Ni = dict()\n",
    "    \n",
    "    # Compute frequencies of items in the training set\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "    del trainset\n",
    "\n",
    "    # Count #users with non-zero item frequencies\n",
    "    nonzero_user_count = 0\n",
    "    for theuser in P[\"users\"]:\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for pos_item in pos_items:\n",
    "            if pos_item in Ni:\n",
    "                nonzero_user_count += 1\n",
    "                break\n",
    "    \n",
    "    # Compute recommendations for each user\n",
    "    for theuser in P[\"users\"]:\n",
    "        all_scores = np.array(P[\"results\"][theuser])\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for i, pos_item in enumerate(pos_items):\n",
    "            pos_score = pos_scores[i]\n",
    "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
    "\n",
    "    \n",
    "    # Calculate per-user scores\n",
    "    sum_user_auc = 0.0\n",
    "    sum_user_recall = 0.0\n",
    "    for theuser in P[\"users\"]:\n",
    "        numerator_auc = 0.0\n",
    "        numerator_recall = 0.0\n",
    "        denominator = 0.0\n",
    "\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            # Skip items with null frequency\n",
    "            if theitem not in Ni:\n",
    "                continue\n",
    "            \n",
    "            # Compute propensity score\n",
    "            pui = np.power(Ni[theitem], (gamma + 1) / 2.0)\n",
    "\n",
    "            # Add things to be summed for each item\n",
    "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser])) / pui\n",
    "\n",
    "            # Add things for recall\n",
    "            if Zui[(theuser, theitem)] < K:\n",
    "                numerator_recall += 1.0 / pui\n",
    "\n",
    "            # Increment denominator that the sum must be divided by\n",
    "            denominator += 1 / pui\n",
    "                \n",
    "        # If there was at least one item for the user, count the user and sum the results\n",
    "        if denominator > 0:\n",
    "            sum_user_auc += numerator_auc / denominator\n",
    "            sum_user_recall += numerator_recall / denominator \n",
    "\n",
    "    # Return\n",
    "    return {\n",
    "        \"auc\"       : sum_user_auc / nonzero_user_count,\n",
    "        \"recall\"    : sum_user_recall / nonzero_user_count,\n",
    "        \"bias\"      : -1,\n",
    "        \"concentration\" : -1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aoa(infilename, infilename_neg, trainfilename, K=1):\n",
    "\n",
    "    # Read pickles\n",
    "    infile = open(infilename, 'rb')\n",
    "    infile_neg = open(infilename_neg, 'rb')\n",
    "    P = pickle.load(infile)\n",
    "    infile.close()\n",
    "    P_neg = pickle.load(infile_neg)\n",
    "    infile_neg.close()\n",
    "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
    "    \n",
    "    # Merge P and P_neg\n",
    "    for theuser in P[\"users\"]:\n",
    "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "    \n",
    "    Zui = dict()\n",
    "    Ni = dict()\n",
    "\n",
    "    # Compute frequencies of items in the training set\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "    del trainset\n",
    "    \n",
    "    # Count #users with non-zero item frequencies\n",
    "    nonzero_user_count = 0\n",
    "    for theuser in P[\"users\"]:\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for pos_item in pos_items:\n",
    "            if pos_item in Ni:\n",
    "                nonzero_user_count += 1\n",
    "                break\n",
    "    \n",
    "    # Compute recommendations for each user\n",
    "    for theuser in P[\"users\"]:\n",
    "        all_scores = np.array(P[\"results\"][theuser])\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for i, pos_item in enumerate(pos_items):\n",
    "            pos_score = pos_scores[i]\n",
    "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
    "\n",
    "    # Calculate per-user scores\n",
    "    sum_user_auc = 0.0\n",
    "    sum_user_recall = 0.0\n",
    "    for theuser in P[\"users\"]:\n",
    "        numerator_auc = 0.0\n",
    "        numerator_recall = 0.0\n",
    "        denominator = 0.0\n",
    "\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            # Skip items with null frequency\n",
    "            if theitem not in Ni:\n",
    "                continue\n",
    "\n",
    "            # Add things to be summed for each item\n",
    "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser]))\n",
    "\n",
    "            # Add things for recall\n",
    "            if Zui[(theuser, theitem)] < K:\n",
    "                numerator_recall += 1.0\n",
    "                \n",
    "            # Increment denominator that the sum must be divided by\n",
    "            denominator += 1 \n",
    "\n",
    "        # If there was at least one item for the user, count the user and sum the results\n",
    "        if denominator > 0:\n",
    "            sum_user_auc += numerator_auc / denominator\n",
    "            sum_user_recall += numerator_recall / denominator\n",
    "\n",
    "    # Return\n",
    "    return {\n",
    "        \"auc\"       : sum_user_auc / nonzero_user_count,\n",
    "        \"recall\"    : sum_user_recall / nonzero_user_count,\n",
    "        \"bias\"      : -1,\n",
    "        \"concentration\" : -1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified(infilename, infilename_neg, trainfilename, gamma=1.0, K=30, partition=10, delta=0.1):\n",
    "\n",
    "    # Read pickles\n",
    "    infile = open(infilename, 'rb')\n",
    "    infile_neg = open(infilename_neg, 'rb')\n",
    "    P = pickle.load(infile)\n",
    "    infile.close()\n",
    "    P_neg = pickle.load(infile_neg)\n",
    "    infile_neg.close()\n",
    "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
    "\n",
    "    # Merge P and P_neg\n",
    "    for theuser in P[\"users\"]:\n",
    "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "\n",
    "    Zui = dict()\n",
    "    Ni = dict()\n",
    "\n",
    "    # Compute frequencies of items in the training set\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "    #del trainset\n",
    "\n",
    "    # Compute recommendations for each user\n",
    "    for theuser in P[\"users\"]:\n",
    "        all_scores = np.array(P[\"results\"][theuser])\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for i, pos_item in enumerate(pos_items):\n",
    "            pos_score = pos_scores[i]\n",
    "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
    "\n",
    "    pui = dict()\n",
    "    w = dict()\n",
    "\n",
    "    # Compute dictionary of propensity scores\n",
    "    for theuser in P[\"users\"]:\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            if theitem not in Ni:\n",
    "                continue\n",
    "            if theitem in pui:\n",
    "                continue\n",
    "            pui[theitem] = np.power(Ni[theitem], (gamma + 1) / 2.0)\n",
    "\n",
    "    # Take the list of items (not tuples) in pui sorted by value\n",
    "    items_sorted_by_value = sorted(pui, key=pui.get, reverse=True)\n",
    "\n",
    "    # Compute linspace between the pui[0] and pui[-1] \n",
    "    linspace = np.linspace(pui[items_sorted_by_value[0]], pui[items_sorted_by_value[-1]], partition+1)\n",
    "   \n",
    "    # Compute dictionary w, that is, for each item, assigns the average of the puis in the partition it belongs to\n",
    "    i=0\n",
    "    j = 0\n",
    "    while i < len(items_sorted_by_value):\n",
    "                            \n",
    "        avg = 0\n",
    "        start = i\n",
    "        end = i\n",
    "    \n",
    "        while i < len(items_sorted_by_value) and pui[items_sorted_by_value[i]] >= linspace[j+1]:\n",
    "            avg += 1.0 / pui[items_sorted_by_value[i]]\n",
    "            end = i\n",
    "            i += 1\n",
    "        avg = avg / (end - start + 1)\n",
    "\n",
    "        for k in range(start, end+1):\n",
    "            w[items_sorted_by_value[k]] = avg\n",
    "\n",
    "        j += 1\n",
    "\n",
    "    # Compute bias' numerator\n",
    "    bias = 0.0\n",
    "    for k in pui.keys():\n",
    "        # add |pui*w - 1!|\n",
    "        bias += abs(pui[k] * w[k] - 1)\n",
    "    # Multiply by number of users\n",
    "    bias *= len(P[\"users\"])\n",
    "\n",
    "    # Compute concentrations numerator (for each user)\n",
    "    concentrations = {}\n",
    "    max_w = max(w.values())\n",
    "    # ... by computing the sum of squares of w for each user\n",
    "    for user, item in zip(trainset['user_id'], trainset['item_id']):\n",
    "        # Iterate over the trainset to compute the sum of squares for each user\n",
    "        if item in w:\n",
    "            if user not in concentrations:\n",
    "                concentrations[user] = 0\n",
    "            concentrations[user] += w[item] ** 2\n",
    "    # ... and then applying the formula\n",
    "    for user in concentrations:\n",
    "        concentrations[user] = math.sqrt(concentrations[user] * 2 * math.log(2/delta)) + max_w * 7 * math.log(2/delta)\n",
    "    # Now sum all the concentrations\n",
    "    concentration = sum(concentrations.values())\n",
    "\n",
    "    # Calculate per-user scores\n",
    "    nonzero_user_count = 0\n",
    "    sum_user_auc = 0.0\n",
    "    sum_user_recall = 0.0\n",
    "    for theuser in P[\"users\"]:\n",
    "        numerator_auc = 0.0\n",
    "        numerator_recall = 0.0\n",
    "        denominator = 0.0\n",
    "        \n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            # Skip items with null frequency\n",
    "            if  theitem not in Ni:\n",
    "                continue\n",
    "\n",
    "            # Add things to be summed for each item\n",
    "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser])) * w[theitem]\n",
    "\n",
    "            # Add things for recall\n",
    "            if Zui[(theuser, theitem)] < K:\n",
    "                numerator_recall += 1.0 * w[theitem]\n",
    "\n",
    "            # Increment denominator that the sum must be divided by \n",
    "            denominator += 1 / pui[theitem]\n",
    "\n",
    "        # If there was at least one item for the user, count the user and sum the results\n",
    "        if denominator > 0:\n",
    "            nonzero_user_count += 1\n",
    "            sum_user_auc += numerator_auc / denominator\n",
    "            sum_user_recall += numerator_recall / denominator \n",
    "\n",
    "    # Return\n",
    "    return {\n",
    "        \"auc\"       : sum_user_auc / nonzero_user_count, \n",
    "        \"recall\"    : sum_user_recall / nonzero_user_count,\n",
    "        \"bias\"      : bias,\n",
    "        \"concentration\" : concentration\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_logspace(infilename, infilename_neg, trainfilename, gamma=1.0, K=30, partition=10, delta=0.1):\n",
    "\n",
    "    # Read pickles\n",
    "    infile = open(infilename, 'rb')\n",
    "    infile_neg = open(infilename_neg, 'rb')\n",
    "    P = pickle.load(infile)\n",
    "    infile.close()\n",
    "    P_neg = pickle.load(infile_neg)\n",
    "    infile_neg.close()\n",
    "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
    "\n",
    "    # Merge P and P_neg\n",
    "    for theuser in P[\"users\"]:\n",
    "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "\n",
    "    Zui = dict()\n",
    "    Ni = dict()\n",
    "\n",
    "    # Compute frequencies of items in the training set\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "    #del trainset\n",
    "\n",
    "    # Compute recommendations for each user\n",
    "    for theuser in P[\"users\"]:\n",
    "        all_scores = np.array(P[\"results\"][theuser])\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for i, pos_item in enumerate(pos_items):\n",
    "            pos_score = pos_scores[i]\n",
    "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
    "\n",
    "    pui = dict()\n",
    "    w = dict()\n",
    "\n",
    "    # Compute dictionary of propensity scores\n",
    "    for theuser in P[\"users\"]:\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            if theitem not in Ni:\n",
    "                continue\n",
    "            if theitem in pui:\n",
    "                continue\n",
    "            pui[theitem] = np.power(Ni[theitem], (gamma + 1) / 2.0)\n",
    "\n",
    "    # Take the list of items (not tuples) in pui sorted by value\n",
    "    items_sorted_by_value = sorted(pui, key=pui.get, reverse=True)\n",
    "\n",
    "    # Compute linspace between the pui[0] and pui[-1] \n",
    "\n",
    "    # Maybe try to split the logspace instead of the linspace?\n",
    "    logspace = np.logspace(pui[items_sorted_by_value[0]], pui[items_sorted_by_value[-1]], partition+1)\n",
    "   \n",
    "    # Compute dictionary w, that is, for each item, assigns the average of the puis in the partition it belongs to\n",
    "    i=0\n",
    "    j = 0\n",
    "    while i < len(items_sorted_by_value):\n",
    "                            \n",
    "        avg = 0\n",
    "        start = i\n",
    "        end = i\n",
    "    \n",
    "        while i < len(items_sorted_by_value) and pui[items_sorted_by_value[i]] >= logspace[j+1]:\n",
    "            avg += 1.0 / pui[items_sorted_by_value[i]]\n",
    "            end = i\n",
    "            i += 1\n",
    "        \n",
    "        # Is the average the only good choice? even with the log space split?\n",
    "        avg = avg / (end - start + 1)\n",
    "\n",
    "        for k in range(start, end+1):\n",
    "            w[items_sorted_by_value[k]] = avg\n",
    "\n",
    "        j += 1\n",
    "\n",
    "        # Compute bias' numerator\n",
    "        bias = 0.0\n",
    "        for k in pui.keys():\n",
    "            # add |pui*w - 1!|\n",
    "            bias += abs(pui[k] * w[k] - 1)\n",
    "        # Multiply by number of users\n",
    "        bias *= len(P[\"users\"])\n",
    "\n",
    "        # Compute concentrations numerator (for each user)\n",
    "        concentrations = {}\n",
    "        max_w = max(w.values())\n",
    "        # ... by computing the sum of squares of w for each user\n",
    "        for user, item in zip(trainset['user_id'], trainset['item_id']):\n",
    "            # Iterate over the trainset to compute the sum of squares for each user\n",
    "            if item in w:\n",
    "                if user not in concentrations:\n",
    "                    concentrations[user] = 0\n",
    "                concentrations[user] += w[item] ** 2\n",
    "        # ... and then applying the formula\n",
    "        for user in concentrations:\n",
    "            concentrations[user] = math.sqrt(concentrations[user] * 2 * math.log(2/delta)) + max_w * 7 * math.log(2/delta)\n",
    "        # Now sum all the concentrations\n",
    "        concentration = sum(concentrations.values())\n",
    "\n",
    "    # Calculate per-user scores\n",
    "    nonzero_user_count = 0\n",
    "    sum_user_auc = 0.0\n",
    "    sum_user_recall = 0.0\n",
    "    for theuser in P[\"users\"]:\n",
    "        numerator_auc = 0.0\n",
    "        numerator_recall = 0.0\n",
    "        denominator = 0.0\n",
    "        \n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            # Skip items with null frequency\n",
    "            if  theitem not in Ni:\n",
    "                continue\n",
    "\n",
    "            # Add things to be summed for each item\n",
    "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser])) * w[theitem]\n",
    "\n",
    "            # Add things for recall\n",
    "            if Zui[(theuser, theitem)] < K:\n",
    "                numerator_recall += 1.0 * w[theitem]\n",
    "\n",
    "            # Increment denominator that the sum must be divided by \n",
    "            denominator += 1 / pui[theitem]\n",
    "\n",
    "        # If there was at least one item for the user, count the user and sum the results\n",
    "        if denominator > 0:\n",
    "            nonzero_user_count += 1\n",
    "            sum_user_auc += numerator_auc / denominator\n",
    "            sum_user_recall += numerator_recall / denominator  \n",
    "\n",
    "    # Return\n",
    "    return {\n",
    "        \"auc\"       : sum_user_auc / nonzero_user_count, \n",
    "        \"recall\"    : sum_user_recall / nonzero_user_count,\n",
    "        \"bias\"      : bias,\n",
    "        \"concentration\" : concentration\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This version uses the linspace of the number of number of items used for evaluation, not of the propensities\n",
    "def stratified_2(infilename, infilename_neg, trainfilename, gamma=1.0, K=30, partition=10, delta=0.1):\n",
    "\n",
    "    # Read pickles\n",
    "    infile = open(infilename, 'rb')\n",
    "    infile_neg = open(infilename_neg, 'rb')\n",
    "    P = pickle.load(infile)\n",
    "    infile.close()\n",
    "    P_neg = pickle.load(infile_neg)\n",
    "    infile_neg.close()\n",
    "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
    "\n",
    "    # Merge P and P_neg\n",
    "    for theuser in P[\"users\"]:\n",
    "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "\n",
    "    Zui = dict()\n",
    "    Ni = dict()\n",
    "\n",
    "    # Compute frequencies of items in the training set\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "    #del trainset\n",
    "\n",
    "    # Compute recommendations for each user\n",
    "    for theuser in P[\"users\"]:\n",
    "        all_scores = np.array(P[\"results\"][theuser])\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for i, pos_item in enumerate(pos_items):\n",
    "            pos_score = pos_scores[i]\n",
    "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
    "\n",
    "    pui = dict()\n",
    "    w = dict()\n",
    "\n",
    "    # Compute dictionary of propensity scores\n",
    "    for theuser in P[\"users\"]:\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            if theitem not in Ni:\n",
    "                continue\n",
    "            if theitem in pui:\n",
    "                continue\n",
    "            pui[theitem] = np.power(Ni[theitem], (gamma + 1) / 2.0)\n",
    "\n",
    "    # Take the list of items (not tuples) in pui sorted by value\n",
    "    items_sorted_by_value = sorted(pui, key=pui.get, reverse=True)\n",
    "\n",
    "    # Compute linspace between the 0 to len(item_sorted...)\n",
    "    linspace = np.linspace(0, len(items_sorted_by_value), partition+1)\n",
    "   \n",
    "    # Compute dictionary w, that is, for each item, assigns the average of the puis in the partition it belongs to\n",
    "    i=0\n",
    "    j = 0\n",
    "    while i < len(items_sorted_by_value):\n",
    "                            \n",
    "        avg = 0\n",
    "        start = i\n",
    "        end = i\n",
    "    \n",
    "        while i < len(items_sorted_by_value) and i < linspace[j+1]:\n",
    "            avg += 1.0 / pui[items_sorted_by_value[i]]\n",
    "            end = i\n",
    "            i += 1\n",
    "        \n",
    "        avg = avg / (end - start + 1)\n",
    "\n",
    "        for k in range(start, end+1):\n",
    "            w[items_sorted_by_value[k]] = avg\n",
    "\n",
    "        j += 1\n",
    "\n",
    "    # Compute bias' numerator\n",
    "    bias = 0.0\n",
    "    for k in pui.keys():\n",
    "        # add |pui*w - 1!|\n",
    "        bias += abs(pui[k] * w[k] - 1)\n",
    "    # Multiply by number of users\n",
    "    bias *= len(P[\"users\"])\n",
    "\n",
    "    # Compute concentrations numerator (for each user)\n",
    "    concentrations = {}\n",
    "    max_w = max(w.values())\n",
    "    # ... by computing the sum of squares of w for each user\n",
    "    for user, item in zip(trainset['user_id'], trainset['item_id']):\n",
    "        # Iterate over the trainset to compute the sum of squares for each user\n",
    "        if item in w:\n",
    "            if user not in concentrations:\n",
    "                concentrations[user] = 0\n",
    "            concentrations[user] += w[item] ** 2\n",
    "    # ... and then applying the formula\n",
    "    for user in concentrations:\n",
    "        concentrations[user] = math.sqrt(concentrations[user] * 2 * math.log(2/delta)) + max_w * 7 * math.log(2/delta)\n",
    "    # Now sum all the concentrations\n",
    "    concentration = sum(concentrations.values())\n",
    "\n",
    "    # Calculate per-user scores\n",
    "    nonzero_user_count = 0\n",
    "    sum_user_auc = 0.0\n",
    "    sum_user_recall = 0.0\n",
    "    for theuser in P[\"users\"]:\n",
    "        numerator_auc = 0.0\n",
    "        numerator_recall = 0.0\n",
    "        denominator = 0.0\n",
    "        \n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            # Skip items with null frequency\n",
    "            if  theitem not in Ni:\n",
    "                continue\n",
    "\n",
    "            # Add things to be summed for each item\n",
    "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser])) * w[theitem]\n",
    "\n",
    "            # Add things for recall\n",
    "            if Zui[(theuser, theitem)] < K:\n",
    "                numerator_recall += 1.0 * w[theitem]\n",
    "\n",
    "            # Increment denominator that the sum must be divided by \n",
    "            denominator += 1 / pui[theitem]\n",
    "\n",
    "        # If there was at least one item for the user, count the user and sum the results\n",
    "        if denominator > 0:\n",
    "            nonzero_user_count += 1\n",
    "            sum_user_auc += numerator_auc / denominator\n",
    "            sum_user_recall += numerator_recall / denominator \n",
    "\n",
    "    # Return\n",
    "    return {\n",
    "        \"auc\"       : sum_user_auc / nonzero_user_count, \n",
    "        \"recall\"    : sum_user_recall / nonzero_user_count,\n",
    "        \"bias\"      : bias,\n",
    "        \"concentration\" : concentration\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **EVALUATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "raw_data = dict()\n",
    "raw_data['train_data'] = np.load(output_name + \"training_arr.npy\")\n",
    "raw_data['test_data_pos_biased'] = np.load(output_name + \"biased-test_arr_pos.npy\")\n",
    "raw_data['test_data_neg_biased'] = np.load(output_name + \"biased-test_arr_neg.npy\")\n",
    "raw_data['test_data_pos_unbiased'] = np.load(output_name + \"unbiased-test_arr_pos.npy\")\n",
    "raw_data['test_data_neg_unbiased'] = np.load(output_name + \"unbiased-test_arr_neg.npy\")\n",
    "raw_data['max_user'] = 15401\n",
    "raw_data['max_item'] = 1001\n",
    "batch_size = 8000\n",
    "test_batch_size = 1000\n",
    "display_itr = 1000\n",
    "\n",
    "# Load data\n",
    "train_dataset = ImplicitDataset(raw_data['train_data'], raw_data['max_user'], raw_data['max_item'], name='Train')\n",
    "test_dataset_pos_biased = ImplicitDataset(raw_data['test_data_pos_biased'], raw_data['max_user'], raw_data['max_item'])\n",
    "test_dataset_neg_biased = ImplicitDataset(raw_data['test_data_neg_biased'], raw_data['max_user'], raw_data['max_item'])\n",
    "test_dataset_pos_unbiased = ImplicitDataset(raw_data['test_data_pos_unbiased'], raw_data['max_user'], raw_data['max_item'])\n",
    "test_dataset_neg_unbiased = ImplicitDataset(raw_data['test_data_neg_unbiased'], raw_data['max_user'], raw_data['max_item'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:391: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:31: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:41: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:43: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:33: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/interactions/pairwise_eu_dist.py:71: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:596: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:75: The name tf.scatter_update is deprecated. Please use tf.compat.v1.scatter_update instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:144: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:365: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:148: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-20 11:24:24.949715: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
      "2024-08-20 11:24:24.952426: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 4192055000 Hz\n",
      "2024-08-20 11:24:24.952994: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55cb8b9de750 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2024-08-20 11:24:24.953010: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./generated_data/cml-yahoo/\n",
      "[Subsampling negative items]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = MODEL_CLASS(batch_size=batch_size, max_user=train_dataset.max_user(), max_item=train_dataset.max_item(), dim_embed=50, l2_reg=0.001, opt='Adam', sess_config=None)\n",
    "sampler = PairwiseSampler(batch_size=batch_size, dataset=train_dataset, num_process=4)\n",
    "model_trainer = ImplicitModelTrainer(batch_size=batch_size, test_batch_size=test_batch_size, train_dataset=train_dataset, model=model, sampler=sampler, eval_save_prefix=OUTPUT_PATH + DATASET_NAME, item_serving_size=500)\n",
    "auc_evaluator = AUC()\n",
    "\n",
    "# Load model\n",
    "model.load(OUTPUT_PATH)\n",
    "\n",
    "# Set parameters\n",
    "model_trainer._eval_manager = ImplicitEvalManager(evaluators=[auc_evaluator])\n",
    "model_trainer._num_negatives = 200\n",
    "model_trainer._exclude_positives([train_dataset, test_dataset_pos_biased, test_dataset_neg_biased])\n",
    "model_trainer._sample_negatives(seed=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biased Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2070/2070 [00:00<00:00, 2517.51it/s]\n",
      "100%|| 2296/2296 [00:25<00:00, 89.93it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AUC': [0.5039491525423728,\n",
       "  0.5153220338983051,\n",
       "  0.5071644295302014,\n",
       "  0.5041442953020134,\n",
       "  0.4916666666666667,\n",
       "  0.4769661016949153,\n",
       "  0.5358892617449664,\n",
       "  0.5279222972972972,\n",
       "  0.5011577181208055,\n",
       "  0.4937878787878789,\n",
       "  0.517864406779661,\n",
       "  0.5050675675675675,\n",
       "  0.4627638190954774,\n",
       "  0.4978,\n",
       "  0.5228472222222221,\n",
       "  0.4881879194630873,\n",
       "  0.5278595317725752,\n",
       "  0.48707482993197276,\n",
       "  0.5023322147651007,\n",
       "  0.5379797979797981,\n",
       "  0.5238127090301004,\n",
       "  0.5319349315068492,\n",
       "  0.486571906354515,\n",
       "  0.4471644295302013,\n",
       "  0.4835451505016722,\n",
       "  0.4724295774647888,\n",
       "  0.49429553264604814,\n",
       "  0.48894648829431436,\n",
       "  0.4587074829931973,\n",
       "  0.5340909090909091,\n",
       "  0.4631711409395972,\n",
       "  0.5076094276094276,\n",
       "  0.47067708333333336,\n",
       "  0.5034782608695653,\n",
       "  0.47471476510067107,\n",
       "  0.49164429530201337,\n",
       "  0.5371070234113712,\n",
       "  0.46685374149659864,\n",
       "  0.4741442953020134,\n",
       "  0.5084966216216217,\n",
       "  0.5023986486486486,\n",
       "  0.43272413793103454,\n",
       "  0.552098976109215,\n",
       "  0.47006734006734013,\n",
       "  0.4715604026845638,\n",
       "  0.49927609427609426,\n",
       "  0.5302525252525253,\n",
       "  0.5354166666666667,\n",
       "  0.501510067114094,\n",
       "  0.4929765886287625,\n",
       "  0.5082833333333333,\n",
       "  0.4901010101010101,\n",
       "  0.4989322033898305,\n",
       "  0.48835570469798656,\n",
       "  0.505222602739726,\n",
       "  0.5098154362416107,\n",
       "  0.4819191919191919,\n",
       "  0.49674137931034484,\n",
       "  0.46536912751677856,\n",
       "  0.5564067796610169,\n",
       "  0.501077441077441,\n",
       "  0.4992114093959731,\n",
       "  0.4784523809523809,\n",
       "  0.4560606060606061,\n",
       "  0.4482775919732441,\n",
       "  0.513758389261745,\n",
       "  0.4868518518518519,\n",
       "  0.48255033557046983,\n",
       "  0.4803666666666667,\n",
       "  0.4899163879598662,\n",
       "  0.48789297658862874,\n",
       "  0.5199665551839466,\n",
       "  0.43978114478114483,\n",
       "  0.5009246575342465,\n",
       "  0.48510135135135135,\n",
       "  0.4910338983050847,\n",
       "  0.4663255033557047,\n",
       "  0.4834333333333332,\n",
       "  0.49291946308724827,\n",
       "  0.4860437710437711,\n",
       "  0.4717676767676768,\n",
       "  0.4663265306122449,\n",
       "  0.5138422818791947,\n",
       "  0.5229530201342281,\n",
       "  0.4950505050505051,\n",
       "  0.502104377104377,\n",
       "  0.45449324324324325,\n",
       "  0.49917525773195875,\n",
       "  0.5685304054054054,\n",
       "  0.5117905405405405,\n",
       "  0.5203703703703704,\n",
       "  0.5236912751677852,\n",
       "  0.4340878378378379,\n",
       "  0.48471476510067113,\n",
       "  0.5192087542087541,\n",
       "  0.468167808219178,\n",
       "  0.5025335570469799,\n",
       "  0.48755852842809366,\n",
       "  0.478125,\n",
       "  0.5375838926174498,\n",
       "  0.48533783783783785,\n",
       "  0.49273972602739724,\n",
       "  0.5229761904761905,\n",
       "  0.5329545454545455,\n",
       "  0.5148833333333334,\n",
       "  0.5271186440677966,\n",
       "  0.49339464882943135,\n",
       "  0.498238255033557,\n",
       "  0.5212333333333333,\n",
       "  0.4839057239057239,\n",
       "  0.48159395973154356,\n",
       "  0.4978619528619529,\n",
       "  0.4844612794612794,\n",
       "  0.48930272108843537,\n",
       "  0.5136363636363637,\n",
       "  0.5284500000000001,\n",
       "  0.46464765100671135,\n",
       "  0.4962289562289562,\n",
       "  0.512813559322034,\n",
       "  0.48421666666666663,\n",
       "  0.4463804713804713,\n",
       "  0.4777272727272727,\n",
       "  0.5531270903010033,\n",
       "  0.4930536912751678,\n",
       "  0.48300000000000004,\n",
       "  0.5166329966329967,\n",
       "  0.5321548821548822,\n",
       "  0.5159259259259259,\n",
       "  0.5340604026845638,\n",
       "  0.5417114093959732,\n",
       "  0.4976006711409396,\n",
       "  0.47068728522336767,\n",
       "  0.5149830508474577,\n",
       "  0.5024564459930314,\n",
       "  0.5533724832214765,\n",
       "  0.48598993288590603,\n",
       "  0.5111486486486486,\n",
       "  0.498003355704698,\n",
       "  0.4818288590604026,\n",
       "  0.4687244897959184,\n",
       "  0.5204391891891892,\n",
       "  0.5375,\n",
       "  0.5426779661016949,\n",
       "  0.5323333333333333,\n",
       "  0.5080821917808219,\n",
       "  0.4807357859531773,\n",
       "  0.5149999999999999,\n",
       "  0.5062331081081081,\n",
       "  0.529261744966443,\n",
       "  0.4736073825503355,\n",
       "  0.48456597222222225,\n",
       "  0.49928093645484956,\n",
       "  0.5474328859060402,\n",
       "  0.5032705479452054,\n",
       "  0.5009,\n",
       "  0.52965,\n",
       "  0.5114932885906041,\n",
       "  0.5182154882154882,\n",
       "  0.5104545454545455,\n",
       "  0.4814864864864864,\n",
       "  0.5309427609427609,\n",
       "  0.4904898648648649,\n",
       "  0.5223310810810812,\n",
       "  0.4693288590604026,\n",
       "  0.48721428571428577,\n",
       "  0.547962962962963,\n",
       "  0.5206879194630871,\n",
       "  0.5005084745762713,\n",
       "  0.5179765886287625,\n",
       "  0.5209899328859061,\n",
       "  0.4728716216216216,\n",
       "  0.4871088435374149,\n",
       "  0.5215476190476189,\n",
       "  0.4805405405405406,\n",
       "  0.5123986486486486,\n",
       "  0.4905518394648829,\n",
       "  0.49280201342281876,\n",
       "  0.5124161073825503,\n",
       "  0.5400335570469799,\n",
       "  0.4808528428093646,\n",
       "  0.43393220338983046,\n",
       "  0.48708754208754207,\n",
       "  0.5307457627118645,\n",
       "  0.503973063973064,\n",
       "  0.4994798657718121,\n",
       "  0.4761204013377927,\n",
       "  0.5135906040268456,\n",
       "  0.5107718120805368,\n",
       "  0.4634833333333334,\n",
       "  0.4558999999999999,\n",
       "  0.5477272727272727,\n",
       "  0.5680267558528428,\n",
       "  0.4792929292929294,\n",
       "  0.516864406779661,\n",
       "  0.4476027397260274,\n",
       "  0.5520033670033669,\n",
       "  0.4759726962457338,\n",
       "  0.5128523489932886,\n",
       "  0.5026588628762542,\n",
       "  0.5293074324324325,\n",
       "  0.5235570469798657,\n",
       "  0.5602181208053691,\n",
       "  0.5028187919463087,\n",
       "  0.5020333333333333,\n",
       "  0.4922203389830508,\n",
       "  0.4955555555555556,\n",
       "  0.5040033783783784,\n",
       "  0.5060101010101011,\n",
       "  0.46699999999999997,\n",
       "  0.5175,\n",
       "  0.4676351351351352,\n",
       "  0.4892976588628763,\n",
       "  0.45558528428093653,\n",
       "  0.4696464646464647,\n",
       "  0.5021717171717172,\n",
       "  0.46511705685618726,\n",
       "  0.4873569023569024,\n",
       "  0.5255218855218855,\n",
       "  0.5202027027027026,\n",
       "  0.5198657718120805,\n",
       "  0.4885810810810811,\n",
       "  0.4880936454849498,\n",
       "  0.47459999999999997,\n",
       "  0.5049665551839465,\n",
       "  0.556761744966443,\n",
       "  0.5139527027027027,\n",
       "  0.5177181208053692,\n",
       "  0.5114765100671141,\n",
       "  0.4706802721088435,\n",
       "  0.49824915824915833,\n",
       "  0.4585324232081912,\n",
       "  0.4998657718120805,\n",
       "  0.4962709030100334,\n",
       "  0.5072147651006712,\n",
       "  0.4952006688963211,\n",
       "  0.43776845637583894,\n",
       "  0.508695652173913,\n",
       "  0.4955326460481099,\n",
       "  0.49692953020134223,\n",
       "  0.5042424242424242,\n",
       "  0.49229865771812076,\n",
       "  0.49566666666666664,\n",
       "  0.5019666666666668,\n",
       "  0.5097483221476509,\n",
       "  0.5029833333333333,\n",
       "  0.5340301003344481,\n",
       "  0.45427586206896553,\n",
       "  0.4616442953020134,\n",
       "  0.457768456375839,\n",
       "  0.5180976430976431,\n",
       "  0.5188813559322034,\n",
       "  0.5221794871794873,\n",
       "  0.5348322147651007,\n",
       "  0.4983277591973244,\n",
       "  0.5294636678200692,\n",
       "  0.5026333333333333,\n",
       "  0.47085,\n",
       "  0.4974414715719064,\n",
       "  0.48731418918918923,\n",
       "  0.4833779264214046,\n",
       "  0.47308333333333336,\n",
       "  0.48870748299319733,\n",
       "  0.48124579124579125,\n",
       "  0.5017340067340067,\n",
       "  0.4906879194630872,\n",
       "  0.5333221476510067,\n",
       "  0.5085166666666667,\n",
       "  0.5069295302013422,\n",
       "  0.5365762711864408,\n",
       "  0.5447651006711409,\n",
       "  0.505016835016835,\n",
       "  0.5068620689655172,\n",
       "  0.5263028169014085,\n",
       "  0.49847972972972976,\n",
       "  0.516593220338983,\n",
       "  0.5236195286195287,\n",
       "  0.516996644295302,\n",
       "  0.5083053691275168,\n",
       "  0.4901546391752577,\n",
       "  0.5471477663230241,\n",
       "  0.4791836734693877,\n",
       "  0.4698,\n",
       "  0.52245,\n",
       "  0.46481605351170563,\n",
       "  0.48752508361204006,\n",
       "  0.4587625418060201,\n",
       "  0.5376767676767676,\n",
       "  0.4496296296296296,\n",
       "  0.49961279461279456,\n",
       "  0.5163175675675675,\n",
       "  0.44933110367892987,\n",
       "  0.48034129692832767,\n",
       "  0.5517500000000001,\n",
       "  0.4795945945945945,\n",
       "  0.4551839464882943,\n",
       "  0.4662207357859532,\n",
       "  0.48835570469798656,\n",
       "  0.514113712374582,\n",
       "  0.49450511945392495,\n",
       "  0.5335953177257524,\n",
       "  0.5212372881355932,\n",
       "  0.5463422818791946,\n",
       "  0.514206081081081,\n",
       "  0.5274832214765101,\n",
       "  0.4501689189189189,\n",
       "  0.5060034013605442,\n",
       "  0.5294773519163763,\n",
       "  0.5142307692307693,\n",
       "  0.46547297297297296,\n",
       "  0.46843537414965986,\n",
       "  0.4865833333333332,\n",
       "  0.5041442953020133,\n",
       "  0.5270066889632107,\n",
       "  0.4949665551839465,\n",
       "  0.5170637583892618,\n",
       "  0.444748322147651,\n",
       "  0.5229720279720279,\n",
       "  0.4885521885521885,\n",
       "  0.4820000000000001,\n",
       "  0.5010034013605442,\n",
       "  0.5086610169491526,\n",
       "  0.49687290969899667,\n",
       "  0.4773639455782313,\n",
       "  0.4761705685618729,\n",
       "  0.5182491582491583,\n",
       "  0.5245101351351351,\n",
       "  0.5236531986531986,\n",
       "  0.49834448160535116,\n",
       "  0.5354666666666666,\n",
       "  0.5026013513513513,\n",
       "  0.5170469798657717,\n",
       "  0.47272575250836113,\n",
       "  0.5327551020408163,\n",
       "  0.4980134680134679,\n",
       "  0.544404761904762,\n",
       "  0.48158075601374567,\n",
       "  0.4664393939393939,\n",
       "  0.5208417508417509,\n",
       "  0.5539799331103679,\n",
       "  0.4806228956228956,\n",
       "  0.49545762711864405,\n",
       "  0.47611301369863024,\n",
       "  0.4625084175084174,\n",
       "  0.4419938650306748,\n",
       "  0.4866220735785953,\n",
       "  0.48070469798657717,\n",
       "  0.4409060402684563,\n",
       "  0.4772666666666667,\n",
       "  0.5150503355704699,\n",
       "  0.49377516778523495,\n",
       "  0.5116047297297297,\n",
       "  0.49609999999999993,\n",
       "  0.5277931034482759,\n",
       "  0.5024916387959867,\n",
       "  0.509765100671141,\n",
       "  0.5477586206896552,\n",
       "  0.512125850340136,\n",
       "  0.46603040540540536,\n",
       "  0.5169230769230769,\n",
       "  0.4949,\n",
       "  0.47645484949832767,\n",
       "  0.5344295302013422,\n",
       "  0.5483838383838384,\n",
       "  0.5249498327759198,\n",
       "  0.5190169491525424,\n",
       "  0.49781355932203386,\n",
       "  0.4817892976588629,\n",
       "  0.4717114093959731,\n",
       "  0.4864965986394559,\n",
       "  0.5052341137123746,\n",
       "  0.4739393939393939,\n",
       "  0.4836789297658863,\n",
       "  0.5103833333333334,\n",
       "  0.5212457912457913,\n",
       "  0.5182718120805369,\n",
       "  0.5386531986531987,\n",
       "  0.5098662207357859,\n",
       "  0.5060702341137123,\n",
       "  0.4984589041095891,\n",
       "  0.4993311036789298,\n",
       "  0.5062367491166078,\n",
       "  0.5854119850187266,\n",
       "  0.521986531986532,\n",
       "  0.52006734006734,\n",
       "  0.5258724832214764,\n",
       "  0.486048951048951,\n",
       "  0.5220101351351351,\n",
       "  0.4736655405405406,\n",
       "  0.5076870748299319,\n",
       "  0.4756565656565656,\n",
       "  0.44719594594594597,\n",
       "  0.47978333333333334,\n",
       "  0.5131666666666667,\n",
       "  0.5506996587030717,\n",
       "  0.4710535117056856,\n",
       "  0.48010033444816047,\n",
       "  0.4857770270270271,\n",
       "  0.5075919732441471,\n",
       "  0.516722972972973,\n",
       "  0.5068060200668897,\n",
       "  0.5105067567567568,\n",
       "  0.4967844522968197,\n",
       "  0.5209731543624161,\n",
       "  0.48429999999999995,\n",
       "  0.48713804713804715,\n",
       "  0.5288590604026846,\n",
       "  0.5366555183946489,\n",
       "  0.5046153846153846,\n",
       "  0.4903535353535353,\n",
       "  0.49616554054054046,\n",
       "  0.4827796610169492,\n",
       "  0.47077966101694924,\n",
       "  0.4557457627118644,\n",
       "  0.4282608695652174,\n",
       "  0.5166000000000001,\n",
       "  0.5182711864406779,\n",
       "  0.5022651006711409,\n",
       "  0.45803448275862063,\n",
       "  0.5120066889632107,\n",
       "  0.49745762711864405,\n",
       "  0.504845890410959,\n",
       "  0.5155555555555557,\n",
       "  0.5534511784511784,\n",
       "  0.5141778523489933,\n",
       "  0.49645270270270264,\n",
       "  0.5064548494983278,\n",
       "  0.4978020134228187,\n",
       "  0.5041778523489933,\n",
       "  0.48348993288590597,\n",
       "  0.5604745762711866,\n",
       "  0.5117142857142856,\n",
       "  0.5138795986622073,\n",
       "  0.4871548821548822,\n",
       "  0.5136363636363637,\n",
       "  0.4702372881355931,\n",
       "  0.44501742160278746,\n",
       "  0.48424749163879605,\n",
       "  0.5433035714285714,\n",
       "  0.4663389830508474,\n",
       "  0.5298316498316499,\n",
       "  0.5222569444444445,\n",
       "  0.4950170068027211,\n",
       "  0.549543918918919,\n",
       "  0.5032996632996634,\n",
       "  0.4836287625418061,\n",
       "  0.510738255033557,\n",
       "  0.443597972972973,\n",
       "  0.5244127516778524,\n",
       "  0.45299663299663295,\n",
       "  0.520847750865052,\n",
       "  0.5330369127516777,\n",
       "  0.4768729096989967,\n",
       "  0.47568791946308725,\n",
       "  0.4879761904761905,\n",
       "  0.4855574324324325,\n",
       "  0.49722033898305085,\n",
       "  0.5172666666666667,\n",
       "  0.4588775510204082,\n",
       "  0.525271186440678,\n",
       "  0.487962962962963,\n",
       "  0.48925,\n",
       "  0.4595205479452055,\n",
       "  0.519554794520548,\n",
       "  0.5165436241610738,\n",
       "  0.4869565217391305,\n",
       "  0.4990100671140939,\n",
       "  0.5043559322033898,\n",
       "  0.5251178451178451,\n",
       "  0.47528619528619526,\n",
       "  0.5441047297297298,\n",
       "  0.5376845637583894,\n",
       "  0.4899290780141845,\n",
       "  0.5087354085603113,\n",
       "  0.5175418060200669,\n",
       "  0.49663299663299665,\n",
       "  0.4928938356164384,\n",
       "  0.5427852348993288,\n",
       "  0.47801694915254234,\n",
       "  0.49279264214046825,\n",
       "  0.5214166666666666,\n",
       "  0.5197474747474747,\n",
       "  0.5060535117056856,\n",
       "  0.4745317725752508,\n",
       "  0.5489057239057239,\n",
       "  0.5161734693877551,\n",
       "  0.48935593220338985,\n",
       "  0.5177380952380952,\n",
       "  0.4994763513513513,\n",
       "  0.5454,\n",
       "  0.48784280936454855,\n",
       "  0.5091471571906355,\n",
       "  0.5071061643835616,\n",
       "  0.5102525252525252,\n",
       "  0.543238255033557,\n",
       "  0.49153846153846154,\n",
       "  0.4629251700680272,\n",
       "  0.4936166666666667,\n",
       "  0.5113299663299664,\n",
       "  0.5168394648829431,\n",
       "  0.50395,\n",
       "  0.49142140468227424,\n",
       "  0.5658833333333333,\n",
       "  0.515304054054054,\n",
       "  0.524714765100671,\n",
       "  0.4985284280936454,\n",
       "  0.5114093959731545,\n",
       "  0.533956228956229,\n",
       "  0.4821160409556314,\n",
       "  0.49553511705685627,\n",
       "  0.5201677852348994,\n",
       "  0.4733838383838384,\n",
       "  0.5245221843003413,\n",
       "  0.5178093645484949,\n",
       "  0.508614864864865,\n",
       "  0.4945469798657718,\n",
       "  0.4872986577181208,\n",
       "  0.5076712328767123,\n",
       "  0.5078355704697985,\n",
       "  0.5016498316498317,\n",
       "  0.4919896193771626,\n",
       "  0.4861833333333333,\n",
       "  0.5112,\n",
       "  0.49276666666666663,\n",
       "  0.46846666666666664,\n",
       "  0.5024324324324325,\n",
       "  0.5208193979933111,\n",
       "  0.5411577181208054,\n",
       "  0.5066722408026756,\n",
       "  0.48661073825503354,\n",
       "  0.5266442953020135,\n",
       "  0.49031141868512107,\n",
       "  0.5348979591836734,\n",
       "  0.4704333333333333,\n",
       "  0.449625850340136,\n",
       "  0.5113804713804714,\n",
       "  0.4929530201342282,\n",
       "  0.5101174496644296,\n",
       "  0.4553645833333333,\n",
       "  0.46534129692832765,\n",
       "  0.5184563758389261,\n",
       "  0.4710641891891892,\n",
       "  0.5004026845637584,\n",
       "  0.4911744966442953,\n",
       "  0.5463299663299663,\n",
       "  0.5194127516778524,\n",
       "  0.5152203389830508,\n",
       "  0.48919732441471575,\n",
       "  0.4667118644067797,\n",
       "  0.4713377926421405,\n",
       "  0.5503344481605351,\n",
       "  0.5176421404682273,\n",
       "  0.4577533783783784,\n",
       "  0.49565436241610733,\n",
       "  0.5009030100334448,\n",
       "  0.49201342281879196,\n",
       "  0.5084511784511784,\n",
       "  0.48635135135135127,\n",
       "  0.49415824915824913,\n",
       "  0.4652356902356903,\n",
       "  0.4970648464163823,\n",
       "  0.4632432432432433,\n",
       "  0.4913095238095238,\n",
       "  0.5013636363636363,\n",
       "  0.46749999999999997,\n",
       "  0.4734020618556701,\n",
       "  0.5260535117056857,\n",
       "  0.5331879194630873,\n",
       "  0.5327181208053692,\n",
       "  0.5092662116040957,\n",
       "  0.4890771812080537,\n",
       "  0.48685810810810803,\n",
       "  0.5165136054421768,\n",
       "  0.4662626262626263,\n",
       "  0.4865604026845638,\n",
       "  0.47559523809523807,\n",
       "  0.5091778523489934,\n",
       "  0.4771501706484642,\n",
       "  0.4630555555555556,\n",
       "  0.4847128378378378,\n",
       "  0.5512627986348122,\n",
       "  0.44998333333333335,\n",
       "  0.5209866220735785,\n",
       "  0.524933110367893,\n",
       "  0.5058695652173912,\n",
       "  0.5018197278911565,\n",
       "  0.5274831081081081,\n",
       "  0.4900167224080268,\n",
       "  0.4452533783783784,\n",
       "  0.5150501672240803,\n",
       "  0.5532943143812709,\n",
       "  0.492295918367347,\n",
       "  0.4820875420875421,\n",
       "  0.47704391891891895,\n",
       "  0.5214184397163121,\n",
       "  0.5547147651006712,\n",
       "  0.5239527027027027,\n",
       "  0.48359060402684567,\n",
       "  0.49854515050167225,\n",
       "  0.5421917808219178,\n",
       "  0.5226262626262627,\n",
       "  0.4936166666666667,\n",
       "  0.4794006849315069,\n",
       "  0.5053703703703702,\n",
       "  0.4658026755852842,\n",
       "  0.4923400673400673,\n",
       "  0.5096655518394648,\n",
       "  0.4807666666666666,\n",
       "  0.4556040268456376,\n",
       "  0.49377516778523484,\n",
       "  0.5033108108108107,\n",
       "  0.5071212121212121,\n",
       "  0.5250836120401338,\n",
       "  0.5209333333333334,\n",
       "  0.4270903010033445,\n",
       "  0.5075250836120402,\n",
       "  0.5024916387959867,\n",
       "  0.5215551839464883,\n",
       "  0.5029194630872483,\n",
       "  0.46328231292517,\n",
       "  0.5040925266903915,\n",
       "  0.4779797979797979,\n",
       "  0.5334006734006734,\n",
       "  0.5190771812080537,\n",
       "  0.5014358108108108,\n",
       "  0.5049832214765101,\n",
       "  0.49506688963210693,\n",
       "  0.49722033898305085,\n",
       "  0.5178260869565217,\n",
       "  0.4944666666666666,\n",
       "  0.5718959731543625,\n",
       "  0.5001342281879194,\n",
       "  0.5314695945945946,\n",
       "  0.5392760942760944,\n",
       "  0.45909395973154365,\n",
       "  0.4900349650349651,\n",
       "  0.5485953177257527,\n",
       "  0.4830666666666667,\n",
       "  0.5336734693877551,\n",
       "  0.5157859531772575,\n",
       "  0.5142087542087542,\n",
       "  0.5092424242424243,\n",
       "  0.4798801369863014,\n",
       "  0.4935451505016722,\n",
       "  0.4949655172413794,\n",
       "  0.4898494983277591,\n",
       "  0.44673333333333337,\n",
       "  0.4985714285714286,\n",
       "  0.5038644067796609,\n",
       "  0.4925762711864407,\n",
       "  0.5225000000000001,\n",
       "  0.46190635451505013,\n",
       "  0.4952881355932204,\n",
       "  0.5027609427609427,\n",
       "  0.484057239057239,\n",
       "  0.5236317567567568,\n",
       "  0.5130993150684932,\n",
       "  0.4864478114478114,\n",
       "  0.5003496503496503,\n",
       "  0.5565384615384615,\n",
       "  0.49506779661016953,\n",
       "  0.4908020477815699,\n",
       "  0.480561224489796,\n",
       "  0.5334006734006734,\n",
       "  0.5013087248322147,\n",
       "  0.5092006802721089,\n",
       "  0.5029280821917809,\n",
       "  0.4990202702702703,\n",
       "  0.4981438127090301,\n",
       "  0.47540677966101696,\n",
       "  0.5167171717171717,\n",
       "  0.4895666666666667,\n",
       "  0.46954081632653066,\n",
       "  0.5374579124579124,\n",
       "  0.47515306122448975,\n",
       "  0.4595986622073579,\n",
       "  0.5420066889632107,\n",
       "  0.5178260869565218,\n",
       "  0.5101190476190476,\n",
       "  0.5522635135135135,\n",
       "  0.48538461538461536,\n",
       "  0.46697986577181216,\n",
       "  0.5058333333333334,\n",
       "  0.5114932885906041,\n",
       "  0.4987414965986394,\n",
       "  0.4926254180602006,\n",
       "  0.5186486486486486,\n",
       "  0.5344314381270903,\n",
       "  0.5080968858131488,\n",
       "  0.5259060402684564,\n",
       "  0.5205033557046981,\n",
       "  0.49036666666666673,\n",
       "  0.5016778523489933,\n",
       "  0.5366666666666666,\n",
       "  0.5090666666666667,\n",
       "  0.5156734006734007,\n",
       "  0.5330405405405405,\n",
       "  0.46086666666666665,\n",
       "  0.5203666666666668,\n",
       "  0.5043311036789299,\n",
       "  0.5178885135135135,\n",
       "  0.5010068259385666,\n",
       "  0.47632550335570467,\n",
       "  0.5083559322033898,\n",
       "  0.5023979591836735,\n",
       "  0.5171134020618556,\n",
       "  0.4610641891891893,\n",
       "  0.4976086956521739,\n",
       "  0.48363945578231293,\n",
       "  0.52875,\n",
       "  0.4960234899328859,\n",
       "  0.4963255033557047,\n",
       "  0.4276286764705882,\n",
       "  0.5073244147157191,\n",
       "  0.49747474747474746,\n",
       "  0.47725589225589227,\n",
       "  0.4981103678929766,\n",
       "  0.4639830508474576,\n",
       "  0.5040033783783784,\n",
       "  0.4639833333333333,\n",
       "  0.4945333333333334,\n",
       "  0.45681818181818185,\n",
       "  0.5044237288135593,\n",
       "  0.5139003436426116,\n",
       "  0.5017391304347826,\n",
       "  0.5271621621621622,\n",
       "  0.5036271186440678,\n",
       "  0.49998310810810814,\n",
       "  0.5137,\n",
       "  0.4956440677966102,\n",
       "  0.4747431506849315,\n",
       "  0.4942021276595745,\n",
       "  0.4572241992882562,\n",
       "  0.4288620689655172,\n",
       "  0.5037416107382551,\n",
       "  0.5443478260869565,\n",
       "  0.5198976109215017,\n",
       "  0.5201672240802676,\n",
       "  0.5744915254237288,\n",
       "  0.5441186440677965,\n",
       "  0.4986271186440679,\n",
       "  0.5160906040268457,\n",
       "  0.5324916387959866,\n",
       "  0.5153198653198654,\n",
       "  0.4709491525423729,\n",
       "  0.4444666666666667,\n",
       "  0.47790969899665553,\n",
       "  0.499513422818792,\n",
       "  0.5162203389830509,\n",
       "  0.511728813559322,\n",
       "  0.4964358108108108,\n",
       "  0.461160409556314,\n",
       "  0.4863573883161511,\n",
       "  0.5121404109589042,\n",
       "  0.5020033670033671,\n",
       "  0.44578859060402687,\n",
       "  0.5218537414965986,\n",
       "  0.5153412969283276,\n",
       "  0.5398464163822525,\n",
       "  0.43706081081081083,\n",
       "  0.5076421404682275,\n",
       "  0.4982333333333333,\n",
       "  0.5208710801393729,\n",
       "  0.48115646258503403,\n",
       "  0.5098116438356164,\n",
       "  0.5027257525083612,\n",
       "  0.4597457627118644,\n",
       "  0.4909563758389262,\n",
       "  0.4775783972125435,\n",
       "  0.47449999999999987,\n",
       "  0.5079151943462897,\n",
       "  0.5015901060070671,\n",
       "  0.5111525423728813,\n",
       "  0.5277257525083613,\n",
       "  0.49608695652173906,\n",
       "  0.5270439189189189,\n",
       "  0.5068561872909699,\n",
       "  0.5142642140468227,\n",
       "  0.5523648648648649,\n",
       "  0.5075506756756757,\n",
       "  0.4688813559322035,\n",
       "  0.4942424242424243,\n",
       "  0.48903114186851204,\n",
       "  0.49833333333333335,\n",
       "  0.44852842809364546,\n",
       "  0.4825850340136054,\n",
       "  0.5182931034482758,\n",
       "  0.48,\n",
       "  0.48300000000000004,\n",
       "  0.4875589225589226,\n",
       "  0.4939966555183946,\n",
       "  0.535685618729097,\n",
       "  0.5536744966442954,\n",
       "  0.48734899328859066,\n",
       "  0.5413050847457627,\n",
       "  0.5018456375838927,\n",
       "  0.5383389261744966,\n",
       "  0.5093288590604027,\n",
       "  0.542190635451505,\n",
       "  0.5028787878787879,\n",
       "  0.4830639730639731,\n",
       "  0.5227104377104377,\n",
       "  0.5312627986348124,\n",
       "  0.5167056856187291,\n",
       "  0.528561872909699,\n",
       "  0.5000526315789473,\n",
       "  0.5165371621621621,\n",
       "  0.47162408759124086,\n",
       "  0.46481605351170574,\n",
       "  0.47790268456375845,\n",
       "  0.45591973244147155,\n",
       "  0.5230204778156997,\n",
       "  0.5487331081081082,\n",
       "  0.483628762541806,\n",
       "  0.4997089041095889,\n",
       "  0.4856856187290969,\n",
       "  0.5294387755102041,\n",
       "  0.5184429065743944,\n",
       "  0.4730677966101695,\n",
       "  0.4832601351351352,\n",
       "  0.48913333333333336,\n",
       "  0.5219425675675676,\n",
       "  0.5093624161073825,\n",
       "  0.5100836120401337,\n",
       "  0.5015656565656564,\n",
       "  0.5129026845637584,\n",
       "  0.46793918918918914,\n",
       "  0.48672818791946304,\n",
       "  0.49445392491467577,\n",
       "  0.510945945945946,\n",
       "  0.4636195286195286,\n",
       "  0.452876254180602,\n",
       "  0.5176599326599327,\n",
       "  0.49969491525423726,\n",
       "  0.5076086956521739,\n",
       "  0.48075174825174816,\n",
       "  0.4553716216216217,\n",
       "  0.5139597315436242,\n",
       "  0.4563833333333334,\n",
       "  0.4884121621621622,\n",
       "  0.49107744107744117,\n",
       "  0.5176936026936027,\n",
       "  0.5186622073578595,\n",
       "  0.4808026755852843,\n",
       "  0.5301178451178451,\n",
       "  0.5002356902356903,\n",
       "  0.5032033898305085,\n",
       "  0.5454222972972973,\n",
       "  0.46718333333333334,\n",
       "  0.48998161764705883,\n",
       "  0.4738758389261745,\n",
       "  0.4804347826086956,\n",
       "  0.5443050847457627,\n",
       "  0.47930976430976435,\n",
       "  0.4683500000000001,\n",
       "  0.5372635135135135,\n",
       "  0.5007457627118643,\n",
       "  0.5072,\n",
       "  0.5084899328859059,\n",
       "  0.4846153846153846,\n",
       "  0.5176816608996541,\n",
       "  0.5446724137931035,\n",
       "  0.5189464882943144,\n",
       "  0.5015719063545151,\n",
       "  0.543922558922559,\n",
       "  0.46791095890410955,\n",
       "  0.5322147651006712,\n",
       "  0.5413389830508475,\n",
       "  0.4817966101694915,\n",
       "  0.5249486301369863,\n",
       "  0.47115771812080537,\n",
       "  0.47379999999999994,\n",
       "  0.5074745762711864,\n",
       "  0.4633164983164983,\n",
       "  0.5441095890410959,\n",
       "  0.5003166666666666,\n",
       "  0.5034899328859062,\n",
       "  0.5234310344827586,\n",
       "  0.5081081081081081,\n",
       "  0.4627551020408164,\n",
       "  0.5071140939597315,\n",
       "  0.5184228187919463,\n",
       "  0.48281786941580757,\n",
       "  0.5421428571428571,\n",
       "  0.49348484848484847,\n",
       "  0.5035117056856188,\n",
       "  0.5154882154882154,\n",
       "  0.49091216216216216,\n",
       "  0.4862374581939799,\n",
       "  0.5176297577854672,\n",
       "  0.507864406779661,\n",
       "  0.48951666666666666,\n",
       "  0.4968561872909698,\n",
       "  0.5393771043771044,\n",
       "  0.513078231292517,\n",
       "  0.47577966101694913,\n",
       "  0.5231569965870307,\n",
       "  0.4841778523489933,\n",
       "  0.569438775510204,\n",
       "  0.507820945945946,\n",
       "  0.47878424657534246,\n",
       "  0.47313356164383574,\n",
       "  0.5216021126760563,\n",
       "  0.5065833333333334,\n",
       "  0.5242567567567568,\n",
       "  0.5029124579124579,\n",
       "  0.5251202749140893,\n",
       "  0.47711666666666663,\n",
       "  0.5383166666666667,\n",
       "  0.4962668918918918,\n",
       "  0.5027777777777778,\n",
       "  0.5105236486486486,\n",
       "  0.5580134680134681,\n",
       "  0.49623728813559315,\n",
       "  0.5081184668989547,\n",
       "  0.45476510067114095,\n",
       "  0.5199496644295302,\n",
       "  0.49861301369863004,\n",
       "  0.4580729166666667,\n",
       "  0.4983164983164983,\n",
       "  0.5135690235690236,\n",
       "  0.5229530201342281,\n",
       "  0.5117905405405405,\n",
       "  0.47703020134228197,\n",
       "  0.505135593220339,\n",
       "  0.5260750853242321,\n",
       "  0.5175420875420875,\n",
       "  0.4840499999999999,\n",
       "  0.5028082191780822,\n",
       "  0.504542372881356,\n",
       "  0.4554377104377104,\n",
       "  0.49102836879432626,\n",
       "  0.4949831081081081,\n",
       "  0.5449333333333333,\n",
       "  0.49363333333333337,\n",
       "  0.5542976588628763,\n",
       "  0.46426174496644296,\n",
       "  0.4711666666666666,\n",
       "  0.48785958904109594,\n",
       "  0.5094983277591973,\n",
       "  0.4784129692832765,\n",
       "  0.5519630872483222,\n",
       "  0.482195945945946,\n",
       "  0.49703333333333327,\n",
       "  0.44860068259385666,\n",
       "  0.4632214765100672,\n",
       "  0.4372542372881356,\n",
       "  0.5046938775510204,\n",
       "  0.5289464882943143,\n",
       "  0.5157575757575757,\n",
       "  0.4878956228956229,\n",
       "  0.5050838926174496,\n",
       "  0.5415604026845637,\n",
       "  0.5246949152542373,\n",
       "  0.495959595959596,\n",
       "  0.4446724137931035,\n",
       "  0.48127516778523494,\n",
       "  0.5355574324324325,\n",
       "  0.4975666666666666,\n",
       "  0.47184121621621616,\n",
       "  0.5044666666666667,\n",
       "  0.46586642599277983,\n",
       "  0.49546075085324237,\n",
       "  0.45945762711864413,\n",
       "  0.4607744107744108,\n",
       "  0.47988333333333333,\n",
       "  0.5069565217391305,\n",
       "  0.499983164983165,\n",
       "  0.5228040540540541,\n",
       "  0.4886271186440677,\n",
       "  0.4968918918918919,\n",
       "  0.5145563139931741,\n",
       "  0.5250499999999999,\n",
       "  0.4909364548494984,\n",
       "  0.5082659932659932,\n",
       "  0.4692087542087542,\n",
       "  0.4677702702702703,\n",
       "  0.508097643097643,\n",
       "  0.5233277591973244,\n",
       "  0.5446366782006921,\n",
       "  0.4806208053691275,\n",
       "  0.49138047138047136,\n",
       "  0.46976588628762533,\n",
       "  0.5528222996515679,\n",
       "  0.4819425675675675,\n",
       "  0.49159395973154363,\n",
       "  0.5177397260273973,\n",
       "  0.5011734693877552,\n",
       "  0.5415993265993265,\n",
       "  0.5248129251700681,\n",
       "  0.5100340136054421,\n",
       "  0.529513422818792,\n",
       "  0.5071452702702703,\n",
       "  0.46226804123711346,\n",
       "  0.455593220338983,\n",
       "  0.4776351351351351,\n",
       "  0.5093367346938775,\n",
       "  0.4527133105802047,\n",
       "  0.5423166666666667,\n",
       "  0.5290033783783783,\n",
       "  0.5155201342281879,\n",
       "  ...]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_trainer._eval_save_prefix = OUTPUT_PREFIX + \"-test-pos-biased\"\n",
    "model_trainer._evaluate_partial(test_dataset_pos_biased)\n",
    "\n",
    "model_trainer._eval_save_prefix = OUTPUT_PREFIX +  \"-test-neg-biased\"\n",
    "model_trainer._evaluate_partial(test_dataset_neg_biased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unbiased Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2296/2296 [00:00<00:00, 2872.41it/s]\n",
      "100%|| 2296/2296 [00:01<00:00, 1560.29it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AUC': [0.46611111111111103,\n",
       "  0.5780000000000001,\n",
       "  0.5166666666666666,\n",
       "  0.5066666666666667,\n",
       "  0.49222222222222217,\n",
       "  0.4635714285714285,\n",
       "  0.38833333333333336,\n",
       "  0.5835714285714285,\n",
       "  0.454375,\n",
       "  0.46444444444444444,\n",
       "  0.369375,\n",
       "  0.43357142857142855,\n",
       "  0.5721428571428572,\n",
       "  0.45875,\n",
       "  0.465,\n",
       "  0.5116666666666667,\n",
       "  0.47388888888888886,\n",
       "  0.5149999999999999,\n",
       "  0.4992857142857144,\n",
       "  0.5127777777777778,\n",
       "  0.493125,\n",
       "  0.47,\n",
       "  0.32944444444444443,\n",
       "  0.45,\n",
       "  0.675,\n",
       "  0.33062499999999995,\n",
       "  0.79,\n",
       "  0.490625,\n",
       "  0.393125,\n",
       "  0.5744444444444443,\n",
       "  0.19714285714285712,\n",
       "  0.5222222222222223,\n",
       "  0.46499999999999997,\n",
       "  0.49833333333333335,\n",
       "  0.4600000000000001,\n",
       "  0.692857142857143,\n",
       "  0.6625,\n",
       "  0.3825,\n",
       "  0.51,\n",
       "  0.40785714285714286,\n",
       "  0.5125,\n",
       "  0.480625,\n",
       "  0.62125,\n",
       "  0.506,\n",
       "  0.41444444444444445,\n",
       "  0.50375,\n",
       "  0.49000000000000005,\n",
       "  0.2225,\n",
       "  0.4888888888888888,\n",
       "  0.4677777777777778,\n",
       "  0.5438888888888889,\n",
       "  0.5429999999999999,\n",
       "  0.4666666666666666,\n",
       "  0.39888888888888885,\n",
       "  0.41333333333333333,\n",
       "  0.5128571428571428,\n",
       "  0.570625,\n",
       "  0.7675000000000001,\n",
       "  0.5577777777777777,\n",
       "  0.41899999999999993,\n",
       "  0.5577777777777777,\n",
       "  0.745,\n",
       "  0.4088888888888889,\n",
       "  0.6021428571428571,\n",
       "  0.44111111111111106,\n",
       "  0.6699999999999999,\n",
       "  0.4755555555555557,\n",
       "  0.57875,\n",
       "  0.42166666666666663,\n",
       "  0.565,\n",
       "  0.5005555555555555,\n",
       "  0.423125,\n",
       "  0.39555555555555555,\n",
       "  0.2727777777777778,\n",
       "  0.33399999999999996,\n",
       "  0.4444444444444444,\n",
       "  0.44388888888888894,\n",
       "  0.5305555555555556,\n",
       "  0.5027777777777778,\n",
       "  0.475,\n",
       "  0.519375,\n",
       "  0.5611111111111111,\n",
       "  0.431875,\n",
       "  0.335625,\n",
       "  0.5,\n",
       "  0.4838888888888889,\n",
       "  0.4255555555555556,\n",
       "  0.44812500000000005,\n",
       "  0.545625,\n",
       "  0.431875,\n",
       "  0.6083333333333335,\n",
       "  0.5705555555555556,\n",
       "  0.449375,\n",
       "  0.5166666666666667,\n",
       "  0.5958333333333333,\n",
       "  0.41333333333333333,\n",
       "  0.5177777777777778,\n",
       "  0.5572222222222222,\n",
       "  0.41055555555555556,\n",
       "  0.51,\n",
       "  0.4625,\n",
       "  0.5022222222222221,\n",
       "  0.45222222222222214,\n",
       "  0.4788888888888889,\n",
       "  0.62,\n",
       "  0.6266666666666666,\n",
       "  0.6216666666666666,\n",
       "  0.385625,\n",
       "  0.6675,\n",
       "  0.4992857142857143,\n",
       "  0.4438888888888889,\n",
       "  0.41388888888888886,\n",
       "  0.3622222222222222,\n",
       "  0.41437500000000005,\n",
       "  0.483125,\n",
       "  0.48562500000000003,\n",
       "  0.365,\n",
       "  0.49,\n",
       "  0.48812500000000003,\n",
       "  0.36,\n",
       "  0.38277777777777783,\n",
       "  0.5466666666666665,\n",
       "  0.5311111111111111,\n",
       "  0.53625,\n",
       "  0.5194444444444445,\n",
       "  0.46444444444444444,\n",
       "  0.4307142857142857,\n",
       "  0.43666666666666665,\n",
       "  0.414375,\n",
       "  0.4957142857142857,\n",
       "  0.6138888888888889,\n",
       "  0.52125,\n",
       "  0.4033333333333333,\n",
       "  0.5081249999999999,\n",
       "  0.46375,\n",
       "  0.3622222222222222,\n",
       "  0.48888888888888893,\n",
       "  0.48687499999999995,\n",
       "  0.3977777777777778,\n",
       "  0.5661111111111112,\n",
       "  0.5505555555555556,\n",
       "  0.6305555555555555,\n",
       "  0.49000000000000005,\n",
       "  0.5283333333333333,\n",
       "  0.5311111111111111,\n",
       "  0.400625,\n",
       "  0.5855555555555555,\n",
       "  0.5277777777777778,\n",
       "  0.363125,\n",
       "  0.3838888888888889,\n",
       "  0.5322222222222223,\n",
       "  0.3927777777777777,\n",
       "  0.49625,\n",
       "  0.491875,\n",
       "  0.5544444444444445,\n",
       "  0.43571428571428567,\n",
       "  0.41055555555555556,\n",
       "  0.4216666666666667,\n",
       "  0.41375,\n",
       "  0.36777777777777776,\n",
       "  0.2805555555555556,\n",
       "  0.5266666666666667,\n",
       "  0.5766666666666667,\n",
       "  0.4227777777777778,\n",
       "  0.45222222222222214,\n",
       "  0.585625,\n",
       "  0.47214285714285714,\n",
       "  0.23,\n",
       "  0.5816666666666666,\n",
       "  0.26937500000000003,\n",
       "  0.44250000000000006,\n",
       "  0.5305555555555554,\n",
       "  0.405625,\n",
       "  0.3844444444444445,\n",
       "  0.7194444444444444,\n",
       "  0.4207142857142857,\n",
       "  0.393125,\n",
       "  0.50875,\n",
       "  0.5349999999999999,\n",
       "  0.528888888888889,\n",
       "  0.47375,\n",
       "  0.599375,\n",
       "  0.5121428571428571,\n",
       "  0.5935714285714286,\n",
       "  0.46499999999999997,\n",
       "  0.3238888888888889,\n",
       "  0.6077777777777779,\n",
       "  0.64125,\n",
       "  0.4422222222222222,\n",
       "  0.6005555555555556,\n",
       "  0.49124999999999996,\n",
       "  0.3705555555555555,\n",
       "  0.40499999999999997,\n",
       "  0.4122222222222222,\n",
       "  0.2871428571428571,\n",
       "  0.5675,\n",
       "  0.4191666666666667,\n",
       "  0.48000000000000004,\n",
       "  0.29500000000000004,\n",
       "  0.79375,\n",
       "  0.4466666666666667,\n",
       "  0.525,\n",
       "  0.4716666666666667,\n",
       "  0.7000000000000001,\n",
       "  0.43555555555555553,\n",
       "  0.4864285714285715,\n",
       "  0.17166666666666666,\n",
       "  0.53,\n",
       "  0.46222222222222226,\n",
       "  0.49,\n",
       "  0.505,\n",
       "  0.5344444444444445,\n",
       "  0.456875,\n",
       "  0.603125,\n",
       "  0.4772222222222222,\n",
       "  0.4575,\n",
       "  0.515,\n",
       "  0.4961111111111111,\n",
       "  0.535,\n",
       "  0.5383333333333332,\n",
       "  0.4872222222222222,\n",
       "  0.461111111111111,\n",
       "  0.4355555555555556,\n",
       "  0.633888888888889,\n",
       "  0.558888888888889,\n",
       "  0.3861111111111111,\n",
       "  0.6372222222222221,\n",
       "  0.5,\n",
       "  0.39357142857142857,\n",
       "  0.49750000000000005,\n",
       "  0.31,\n",
       "  0.2516666666666667,\n",
       "  0.43277777777777776,\n",
       "  0.5661111111111111,\n",
       "  0.43944444444444447,\n",
       "  0.6425,\n",
       "  0.28444444444444444,\n",
       "  0.3357142857142857,\n",
       "  0.5137499999999999,\n",
       "  0.5138888888888888,\n",
       "  0.49875,\n",
       "  0.415,\n",
       "  0.7791666666666667,\n",
       "  0.45625,\n",
       "  0.48666666666666664,\n",
       "  0.6914285714285715,\n",
       "  0.4388888888888889,\n",
       "  0.3821428571428572,\n",
       "  0.46124999999999994,\n",
       "  0.546,\n",
       "  0.48,\n",
       "  0.5733333333333333,\n",
       "  0.5211111111111112,\n",
       "  0.5011111111111112,\n",
       "  0.5761111111111111,\n",
       "  0.6377777777777778,\n",
       "  0.5488888888888889,\n",
       "  0.4066666666666667,\n",
       "  0.47000000000000003,\n",
       "  0.42750000000000005,\n",
       "  0.35944444444444446,\n",
       "  0.5542857142857143,\n",
       "  0.48062499999999997,\n",
       "  0.5111111111111112,\n",
       "  0.48944444444444446,\n",
       "  0.5277777777777777,\n",
       "  0.3938888888888889,\n",
       "  0.39111111111111113,\n",
       "  0.5611111111111111,\n",
       "  0.46111111111111114,\n",
       "  0.43875,\n",
       "  0.2716666666666667,\n",
       "  0.2755555555555556,\n",
       "  0.5171428571428572,\n",
       "  0.5111111111111111,\n",
       "  0.4042857142857143,\n",
       "  0.65,\n",
       "  0.6405555555555557,\n",
       "  0.4561111111111111,\n",
       "  0.6705555555555556,\n",
       "  0.576875,\n",
       "  0.425,\n",
       "  0.4766666666666667,\n",
       "  0.46499999999999997,\n",
       "  0.2742857142857143,\n",
       "  0.44611111111111107,\n",
       "  0.49874999999999997,\n",
       "  0.428125,\n",
       "  0.4583333333333333,\n",
       "  0.3894444444444444,\n",
       "  0.36875,\n",
       "  0.4927777777777778,\n",
       "  0.4255555555555556,\n",
       "  0.37833333333333335,\n",
       "  0.5406249999999999,\n",
       "  0.45499999999999996,\n",
       "  0.315625,\n",
       "  0.6221428571428571,\n",
       "  0.7066666666666667,\n",
       "  0.631875,\n",
       "  0.47285714285714286,\n",
       "  0.578125,\n",
       "  0.4841666666666666,\n",
       "  0.6616666666666666,\n",
       "  0.521875,\n",
       "  0.56,\n",
       "  0.49166666666666675,\n",
       "  0.5555555555555556,\n",
       "  0.459375,\n",
       "  0.365,\n",
       "  0.6027777777777779,\n",
       "  0.5750000000000001,\n",
       "  0.56,\n",
       "  0.603125,\n",
       "  0.33999999999999997,\n",
       "  0.30777777777777776,\n",
       "  0.33625,\n",
       "  0.615,\n",
       "  0.40388888888888885,\n",
       "  0.36944444444444446,\n",
       "  0.535,\n",
       "  0.46388888888888885,\n",
       "  0.225,\n",
       "  0.5221428571428571,\n",
       "  0.5349999999999999,\n",
       "  0.36833333333333335,\n",
       "  0.42333333333333334,\n",
       "  0.45125000000000004,\n",
       "  0.6962499999999999,\n",
       "  0.47611111111111115,\n",
       "  0.3833333333333333,\n",
       "  0.43833333333333335,\n",
       "  0.5616666666666668,\n",
       "  0.5883333333333334,\n",
       "  0.6211111111111111,\n",
       "  0.43000000000000005,\n",
       "  0.24900000000000003,\n",
       "  0.614375,\n",
       "  0.3561111111111111,\n",
       "  0.43833333333333335,\n",
       "  0.6325000000000001,\n",
       "  0.44875,\n",
       "  0.3457142857142857,\n",
       "  0.6508333333333333,\n",
       "  0.38277777777777783,\n",
       "  0.4864285714285715,\n",
       "  0.405,\n",
       "  0.5177777777777778,\n",
       "  0.3541666666666667,\n",
       "  0.5933333333333333,\n",
       "  0.646875,\n",
       "  0.5477777777777778,\n",
       "  0.5733333333333334,\n",
       "  0.33944444444444444,\n",
       "  0.6544444444444444,\n",
       "  0.52,\n",
       "  0.5022222222222221,\n",
       "  0.47555555555555556,\n",
       "  0.6072222222222222,\n",
       "  0.285625,\n",
       "  0.39375,\n",
       "  0.4,\n",
       "  0.728,\n",
       "  0.3683333333333334,\n",
       "  0.66,\n",
       "  0.49,\n",
       "  0.3572222222222222,\n",
       "  0.6483333333333334,\n",
       "  0.3242857142857143,\n",
       "  0.4628571428571429,\n",
       "  0.4194444444444444,\n",
       "  0.5488888888888889,\n",
       "  0.4566666666666666,\n",
       "  0.4794444444444444,\n",
       "  0.39285714285714285,\n",
       "  0.468125,\n",
       "  0.3744444444444444,\n",
       "  0.43699999999999994,\n",
       "  0.2644444444444444,\n",
       "  0.45333333333333337,\n",
       "  0.618125,\n",
       "  0.45500000000000007,\n",
       "  0.5194444444444444,\n",
       "  0.4672222222222222,\n",
       "  0.43833333333333335,\n",
       "  0.40750000000000003,\n",
       "  0.54,\n",
       "  0.5266666666666667,\n",
       "  0.32277777777777783,\n",
       "  0.67625,\n",
       "  0.29444444444444445,\n",
       "  0.5855555555555556,\n",
       "  0.524375,\n",
       "  0.5264285714285714,\n",
       "  0.4625,\n",
       "  0.6205555555555555,\n",
       "  0.44999999999999996,\n",
       "  0.3005555555555556,\n",
       "  0.6538888888888889,\n",
       "  0.6455555555555555,\n",
       "  0.42071428571428576,\n",
       "  0.465,\n",
       "  0.43388888888888894,\n",
       "  0.2588888888888889,\n",
       "  0.37312500000000004,\n",
       "  0.5633333333333334,\n",
       "  0.55,\n",
       "  0.515625,\n",
       "  0.45611111111111113,\n",
       "  0.598125,\n",
       "  0.351,\n",
       "  0.424375,\n",
       "  0.562142857142857,\n",
       "  0.5722222222222223,\n",
       "  0.5607142857142858,\n",
       "  0.3611111111111111,\n",
       "  0.605,\n",
       "  0.4816666666666667,\n",
       "  0.5299999999999999,\n",
       "  0.4911111111111111,\n",
       "  0.53,\n",
       "  0.6558333333333334,\n",
       "  0.5783333333333334,\n",
       "  0.41055555555555556,\n",
       "  0.43833333333333335,\n",
       "  0.3857142857142857,\n",
       "  0.5449999999999999,\n",
       "  0.3944444444444445,\n",
       "  0.36777777777777776,\n",
       "  0.66,\n",
       "  0.41357142857142853,\n",
       "  0.45333333333333337,\n",
       "  0.3677777777777778,\n",
       "  0.509375,\n",
       "  0.46111111111111114,\n",
       "  0.35277777777777775,\n",
       "  0.38,\n",
       "  0.4891666666666667,\n",
       "  0.37374999999999997,\n",
       "  0.5544444444444444,\n",
       "  0.44388888888888883,\n",
       "  0.39999999999999997,\n",
       "  0.571875,\n",
       "  0.3866666666666667,\n",
       "  0.34833333333333333,\n",
       "  0.475,\n",
       "  0.4175,\n",
       "  0.4857142857142857,\n",
       "  0.553125,\n",
       "  0.51,\n",
       "  0.45125,\n",
       "  0.453125,\n",
       "  0.38222222222222224,\n",
       "  0.47187499999999993,\n",
       "  0.42125,\n",
       "  0.3794444444444445,\n",
       "  0.40812499999999996,\n",
       "  0.4588888888888889,\n",
       "  0.45624999999999993,\n",
       "  0.4605555555555555,\n",
       "  0.5525,\n",
       "  0.45999999999999996,\n",
       "  0.4383333333333333,\n",
       "  0.49888888888888894,\n",
       "  0.5238888888888888,\n",
       "  0.4605555555555555,\n",
       "  0.45222222222222214,\n",
       "  0.56,\n",
       "  0.4494444444444444,\n",
       "  0.49166666666666664,\n",
       "  0.6022222222222223,\n",
       "  0.3741666666666667,\n",
       "  0.5077777777777778,\n",
       "  0.3244444444444444,\n",
       "  0.47611111111111115,\n",
       "  0.525,\n",
       "  0.540625,\n",
       "  0.39625,\n",
       "  0.4511111111111112,\n",
       "  0.5172222222222221,\n",
       "  0.40388888888888885,\n",
       "  0.5383333333333333,\n",
       "  0.546875,\n",
       "  0.39625000000000005,\n",
       "  0.405,\n",
       "  0.615,\n",
       "  0.47777777777777775,\n",
       "  0.17444444444444446,\n",
       "  0.5505555555555556,\n",
       "  0.298,\n",
       "  0.26833333333333337,\n",
       "  0.58125,\n",
       "  0.6208333333333333,\n",
       "  0.5744444444444444,\n",
       "  0.401875,\n",
       "  0.4921428571428571,\n",
       "  0.5018750000000001,\n",
       "  0.435,\n",
       "  0.353125,\n",
       "  0.58,\n",
       "  0.5944444444444444,\n",
       "  0.5393749999999999,\n",
       "  0.6183333333333333,\n",
       "  0.26416666666666666,\n",
       "  0.4466666666666666,\n",
       "  0.5944444444444446,\n",
       "  0.341875,\n",
       "  0.3355555555555555,\n",
       "  0.5385714285714286,\n",
       "  0.515,\n",
       "  0.625,\n",
       "  0.5372222222222222,\n",
       "  0.3858333333333333,\n",
       "  0.3964285714285714,\n",
       "  0.45222222222222225,\n",
       "  0.5405555555555556,\n",
       "  0.5133333333333333,\n",
       "  0.37166666666666665,\n",
       "  0.675,\n",
       "  0.36687499999999995,\n",
       "  0.4521428571428571,\n",
       "  0.5166666666666667,\n",
       "  0.364375,\n",
       "  0.51,\n",
       "  0.44,\n",
       "  0.724375,\n",
       "  0.5131249999999999,\n",
       "  0.515,\n",
       "  0.5338888888888889,\n",
       "  0.5094444444444445,\n",
       "  0.4033333333333333,\n",
       "  0.33375,\n",
       "  0.4577777777777777,\n",
       "  0.45500000000000007,\n",
       "  0.6107142857142858,\n",
       "  0.5466666666666666,\n",
       "  0.5575,\n",
       "  0.4375,\n",
       "  0.5344444444444444,\n",
       "  0.5116666666666666,\n",
       "  0.39099999999999996,\n",
       "  0.3561111111111111,\n",
       "  0.42611111111111116,\n",
       "  0.4728571428571428,\n",
       "  0.6072222222222222,\n",
       "  0.3988888888888889,\n",
       "  0.37777777777777777,\n",
       "  0.40722222222222215,\n",
       "  0.4961111111111111,\n",
       "  0.3385714285714286,\n",
       "  0.469375,\n",
       "  0.2857142857142857,\n",
       "  0.5593750000000001,\n",
       "  0.47111111111111104,\n",
       "  0.07166666666666667,\n",
       "  0.5856250000000001,\n",
       "  0.3472222222222222,\n",
       "  0.38571428571428573,\n",
       "  0.35555555555555557,\n",
       "  0.5188888888888888,\n",
       "  0.4677777777777778,\n",
       "  0.541111111111111,\n",
       "  0.69,\n",
       "  0.4558333333333333,\n",
       "  0.41444444444444445,\n",
       "  0.6022222222222222,\n",
       "  0.48944444444444435,\n",
       "  0.4657142857142857,\n",
       "  0.6449999999999999,\n",
       "  0.5790000000000001,\n",
       "  0.354,\n",
       "  0.5511111111111111,\n",
       "  0.5505555555555556,\n",
       "  0.435,\n",
       "  0.6642857142857144,\n",
       "  0.5393749999999999,\n",
       "  0.52875,\n",
       "  0.4041666666666666,\n",
       "  0.5438888888888889,\n",
       "  0.31916666666666665,\n",
       "  0.33111111111111113,\n",
       "  0.40611111111111114,\n",
       "  0.5,\n",
       "  0.45055555555555554,\n",
       "  0.463125,\n",
       "  0.32555555555555554,\n",
       "  0.5264285714285714,\n",
       "  0.5383333333333333,\n",
       "  0.6022222222222222,\n",
       "  0.5938888888888888,\n",
       "  0.49888888888888894,\n",
       "  0.6322222222222222,\n",
       "  0.6333333333333333,\n",
       "  0.57125,\n",
       "  0.47000000000000003,\n",
       "  0.5614285714285714,\n",
       "  0.5927777777777777,\n",
       "  0.38375,\n",
       "  0.42,\n",
       "  0.530625,\n",
       "  0.47166666666666657,\n",
       "  0.3378571428571428,\n",
       "  0.49666666666666665,\n",
       "  0.3992857142857143,\n",
       "  0.5188888888888888,\n",
       "  0.628125,\n",
       "  0.5822222222222222,\n",
       "  0.484375,\n",
       "  0.5561111111111111,\n",
       "  0.403,\n",
       "  0.3133333333333333,\n",
       "  0.6087499999999999,\n",
       "  0.4149999999999999,\n",
       "  0.45222222222222225,\n",
       "  0.38111111111111107,\n",
       "  0.5305555555555556,\n",
       "  0.48875,\n",
       "  0.5533333333333332,\n",
       "  0.5033333333333334,\n",
       "  0.37857142857142856,\n",
       "  0.5171428571428571,\n",
       "  0.48750000000000004,\n",
       "  0.343125,\n",
       "  0.5987500000000001,\n",
       "  0.5544444444444445,\n",
       "  0.5772222222222223,\n",
       "  0.404375,\n",
       "  0.43875000000000003,\n",
       "  0.25055555555555553,\n",
       "  0.5405555555555557,\n",
       "  0.49888888888888894,\n",
       "  0.304375,\n",
       "  0.41055555555555556,\n",
       "  0.470625,\n",
       "  0.4366666666666667,\n",
       "  0.5485714285714286,\n",
       "  0.4933333333333334,\n",
       "  0.3772222222222223,\n",
       "  0.458125,\n",
       "  0.457,\n",
       "  0.476875,\n",
       "  0.6177777777777779,\n",
       "  0.3522222222222222,\n",
       "  0.39444444444444443,\n",
       "  0.3116666666666667,\n",
       "  0.405,\n",
       "  0.406,\n",
       "  0.4116666666666667,\n",
       "  0.484375,\n",
       "  0.6325000000000001,\n",
       "  0.336111111111111,\n",
       "  0.425,\n",
       "  0.4675,\n",
       "  0.25833333333333336,\n",
       "  0.44555555555555554,\n",
       "  0.5355555555555556,\n",
       "  0.47625,\n",
       "  0.485625,\n",
       "  0.5049999999999999,\n",
       "  0.33666666666666667,\n",
       "  0.5727777777777778,\n",
       "  0.3764285714285714,\n",
       "  0.37214285714285705,\n",
       "  0.5816666666666667,\n",
       "  0.5088888888888888,\n",
       "  0.581875,\n",
       "  0.55,\n",
       "  0.48277777777777786,\n",
       "  0.4577777777777777,\n",
       "  0.670625,\n",
       "  0.5183333333333333,\n",
       "  0.5622222222222222,\n",
       "  0.4911111111111111,\n",
       "  0.415625,\n",
       "  0.3916666666666667,\n",
       "  0.4816666666666667,\n",
       "  0.34,\n",
       "  0.3222222222222222,\n",
       "  0.5075000000000001,\n",
       "  0.5583333333333333,\n",
       "  0.46611111111111114,\n",
       "  0.37888888888888883,\n",
       "  0.40125,\n",
       "  0.25142857142857145,\n",
       "  0.5933333333333333,\n",
       "  0.7125,\n",
       "  0.4605555555555556,\n",
       "  0.41250000000000003,\n",
       "  0.47333333333333333,\n",
       "  0.40944444444444444,\n",
       "  0.545,\n",
       "  0.3711111111111111,\n",
       "  0.47285714285714286,\n",
       "  0.4116666666666667,\n",
       "  0.35000000000000003,\n",
       "  0.45687500000000003,\n",
       "  0.41388888888888886,\n",
       "  0.6922222222222222,\n",
       "  0.6483333333333333,\n",
       "  0.4766666666666667,\n",
       "  0.385625,\n",
       "  0.21500000000000002,\n",
       "  0.506,\n",
       "  0.445,\n",
       "  0.4077777777777778,\n",
       "  0.54375,\n",
       "  0.5016666666666667,\n",
       "  0.544375,\n",
       "  0.425,\n",
       "  0.5261111111111111,\n",
       "  0.60375,\n",
       "  0.2842857142857143,\n",
       "  0.5599999999999999,\n",
       "  0.34611111111111115,\n",
       "  0.503125,\n",
       "  0.3622222222222222,\n",
       "  0.45,\n",
       "  0.528125,\n",
       "  0.363125,\n",
       "  0.5066666666666666,\n",
       "  0.520625,\n",
       "  0.5116666666666666,\n",
       "  0.5572222222222222,\n",
       "  0.4655555555555556,\n",
       "  0.42875,\n",
       "  0.5921428571428571,\n",
       "  0.6985714285714286,\n",
       "  0.40722222222222215,\n",
       "  0.364375,\n",
       "  0.26375,\n",
       "  0.525625,\n",
       "  0.35125,\n",
       "  0.4635714285714286,\n",
       "  0.5972222222222221,\n",
       "  0.39428571428571424,\n",
       "  0.5335714285714286,\n",
       "  0.518125,\n",
       "  0.6661111111111111,\n",
       "  0.41571428571428576,\n",
       "  0.4283333333333333,\n",
       "  0.5664285714285715,\n",
       "  0.39285714285714285,\n",
       "  0.435,\n",
       "  0.262,\n",
       "  0.5694444444444444,\n",
       "  0.5416666666666667,\n",
       "  0.44777777777777783,\n",
       "  0.6727777777777778,\n",
       "  0.3961111111111111,\n",
       "  0.5294444444444445,\n",
       "  0.378125,\n",
       "  0.325,\n",
       "  0.38187499999999996,\n",
       "  0.5572222222222223,\n",
       "  0.28388888888888886,\n",
       "  0.46187500000000004,\n",
       "  0.525,\n",
       "  0.5341666666666667,\n",
       "  0.4557142857142858,\n",
       "  0.3733333333333333,\n",
       "  0.5544444444444445,\n",
       "  0.46444444444444444,\n",
       "  0.37333333333333335,\n",
       "  0.365,\n",
       "  0.4444444444444444,\n",
       "  0.5016666666666666,\n",
       "  0.5433333333333334,\n",
       "  0.4078571428571429,\n",
       "  0.5288888888888889,\n",
       "  0.42428571428571427,\n",
       "  0.39437500000000003,\n",
       "  0.6277777777777778,\n",
       "  0.6708333333333334,\n",
       "  0.598125,\n",
       "  0.5349999999999999,\n",
       "  0.47714285714285715,\n",
       "  0.5038888888888889,\n",
       "  0.33999999999999997,\n",
       "  0.444375,\n",
       "  0.5299999999999999,\n",
       "  0.420625,\n",
       "  0.515,\n",
       "  0.4405555555555556,\n",
       "  0.39222222222222214,\n",
       "  0.36375,\n",
       "  0.343125,\n",
       "  0.45222222222222225,\n",
       "  0.49555555555555564,\n",
       "  0.285625,\n",
       "  0.4822222222222222,\n",
       "  0.6966666666666667,\n",
       "  0.5277777777777778,\n",
       "  0.4725,\n",
       "  0.45499999999999996,\n",
       "  0.58875,\n",
       "  0.41888888888888887,\n",
       "  0.5725,\n",
       "  0.5861111111111111,\n",
       "  0.358125,\n",
       "  0.38375,\n",
       "  0.5244444444444445,\n",
       "  0.28300000000000003,\n",
       "  0.5305555555555556,\n",
       "  0.43222222222222223,\n",
       "  0.35875,\n",
       "  0.5175,\n",
       "  0.314375,\n",
       "  0.385,\n",
       "  0.3238888888888889,\n",
       "  0.42666666666666664,\n",
       "  0.36000000000000004,\n",
       "  0.680625,\n",
       "  0.41571428571428576,\n",
       "  0.4977777777777777,\n",
       "  0.375,\n",
       "  0.45125000000000004,\n",
       "  0.5058333333333334,\n",
       "  0.5655555555555556,\n",
       "  0.5299999999999999,\n",
       "  0.37333333333333335,\n",
       "  0.43777777777777777,\n",
       "  0.49142857142857144,\n",
       "  0.28,\n",
       "  0.4114285714285714,\n",
       "  0.4892857142857143,\n",
       "  0.2825,\n",
       "  0.21857142857142858,\n",
       "  0.509375,\n",
       "  0.543888888888889,\n",
       "  0.365625,\n",
       "  0.3527777777777778,\n",
       "  0.5642857142857142,\n",
       "  0.590625,\n",
       "  0.3957142857142858,\n",
       "  0.5277777777777778,\n",
       "  0.5355555555555556,\n",
       "  0.371,\n",
       "  0.7216666666666667,\n",
       "  0.575,\n",
       "  0.5606249999999999,\n",
       "  0.410625,\n",
       "  0.445,\n",
       "  0.5405555555555556,\n",
       "  0.5438888888888889,\n",
       "  0.4388888888888889,\n",
       "  0.40800000000000003,\n",
       "  0.39625,\n",
       "  0.5361111111111111,\n",
       "  0.5044444444444445,\n",
       "  0.6793750000000001,\n",
       "  0.455,\n",
       "  0.315,\n",
       "  0.47750000000000004,\n",
       "  0.44111111111111106,\n",
       "  0.47111111111111115,\n",
       "  0.620625,\n",
       "  0.5866666666666667,\n",
       "  0.4772222222222223,\n",
       "  0.46666666666666656,\n",
       "  0.491875,\n",
       "  0.5516666666666666,\n",
       "  0.37333333333333335,\n",
       "  0.41312499999999996,\n",
       "  0.3694444444444444,\n",
       "  0.6450000000000001,\n",
       "  0.55,\n",
       "  0.5455555555555556,\n",
       "  0.33375,\n",
       "  0.4575,\n",
       "  0.28357142857142853,\n",
       "  0.545625,\n",
       "  0.6858333333333334,\n",
       "  0.46888888888888886,\n",
       "  0.62625,\n",
       "  0.6671428571428571,\n",
       "  0.5321428571428571,\n",
       "  0.5831250000000001,\n",
       "  0.33714285714285713,\n",
       "  0.5625000000000001,\n",
       "  0.43812499999999993,\n",
       "  0.6177777777777778,\n",
       "  0.42300000000000004,\n",
       "  0.48928571428571427,\n",
       "  0.6038888888888888,\n",
       "  0.588125,\n",
       "  0.39499999999999996,\n",
       "  0.5977777777777777,\n",
       "  0.4844444444444444,\n",
       "  0.5711111111111111,\n",
       "  0.5133333333333332,\n",
       "  0.569375,\n",
       "  0.44777777777777783,\n",
       "  0.5375,\n",
       "  0.5155555555555555,\n",
       "  0.5844444444444444,\n",
       "  0.4011111111111111,\n",
       "  0.4133333333333333,\n",
       "  0.43374999999999997,\n",
       "  0.3383333333333333,\n",
       "  0.3138888888888889,\n",
       "  0.31444444444444447,\n",
       "  0.5116666666666667,\n",
       "  0.5205555555555555,\n",
       "  0.5733333333333334,\n",
       "  0.28250000000000003,\n",
       "  0.3442857142857143,\n",
       "  0.38055555555555554,\n",
       "  0.653888888888889,\n",
       "  0.6194444444444445,\n",
       "  0.3728571428571429,\n",
       "  0.44625000000000004,\n",
       "  0.320625,\n",
       "  0.468125,\n",
       "  0.34142857142857136,\n",
       "  0.3544444444444445,\n",
       "  0.47250000000000003,\n",
       "  0.2911111111111111,\n",
       "  0.4833333333333334,\n",
       "  0.5905555555555555,\n",
       "  0.43437499999999996,\n",
       "  0.45333333333333337,\n",
       "  0.38083333333333336,\n",
       "  0.5466666666666666,\n",
       "  0.4816666666666667,\n",
       "  0.3861111111111111,\n",
       "  0.6042857142857142,\n",
       "  0.6605555555555555,\n",
       "  0.4294444444444444,\n",
       "  0.5055555555555555,\n",
       "  0.51625,\n",
       "  0.51625,\n",
       "  0.35562499999999997,\n",
       "  0.6633333333333334,\n",
       "  0.39111111111111113,\n",
       "  0.5233333333333333,\n",
       "  0.18555555555555558,\n",
       "  0.590625,\n",
       "  0.46666666666666673,\n",
       "  0.705,\n",
       "  0.5214285714285715,\n",
       "  0.495,\n",
       "  0.4144444444444445,\n",
       "  0.45499999999999996,\n",
       "  0.26071428571428573,\n",
       "  0.39375,\n",
       "  0.47750000000000004,\n",
       "  0.6827777777777778,\n",
       "  0.5411111111111111,\n",
       "  0.5233333333333333,\n",
       "  0.4811111111111111,\n",
       "  0.3575,\n",
       "  0.4494444444444444,\n",
       "  0.37777777777777777,\n",
       "  0.47333333333333333,\n",
       "  0.4275,\n",
       "  0.55,\n",
       "  0.30642857142857144,\n",
       "  0.35166666666666674,\n",
       "  0.423125,\n",
       "  0.5644444444444444,\n",
       "  0.585,\n",
       "  0.544375,\n",
       "  0.36875,\n",
       "  0.5166666666666667,\n",
       "  0.45200000000000007,\n",
       "  0.4928571428571429,\n",
       "  0.29875,\n",
       "  0.5361111111111111,\n",
       "  0.5875,\n",
       "  0.48000000000000004,\n",
       "  0.43083333333333335,\n",
       "  0.6149999999999999,\n",
       "  0.6764285714285714,\n",
       "  0.5711111111111111,\n",
       "  0.431875,\n",
       "  0.4564285714285714,\n",
       "  0.58875,\n",
       "  0.45999999999999996,\n",
       "  0.34555555555555556,\n",
       "  0.5105555555555555,\n",
       "  0.5921428571428572,\n",
       "  0.409375,\n",
       "  0.382,\n",
       "  0.31611111111111106,\n",
       "  0.45999999999999996,\n",
       "  0.5294444444444444,\n",
       "  0.5193749999999999,\n",
       "  0.5594444444444444,\n",
       "  0.44800000000000006,\n",
       "  0.45562499999999995,\n",
       "  0.5633333333333334,\n",
       "  0.4366666666666667,\n",
       "  0.46312500000000006,\n",
       "  0.5792857142857143,\n",
       "  0.43333333333333335,\n",
       "  0.6585714285714285,\n",
       "  0.38277777777777783,\n",
       "  0.6064285714285714,\n",
       "  0.410625,\n",
       "  0.5922222222222222,\n",
       "  ...]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_trainer._eval_save_prefix = OUTPUT_PREFIX + \"-test-pos-unbiased\"\n",
    "model_trainer._evaluate_partial(test_dataset_pos_unbiased)\n",
    "\n",
    "model_trainer._eval_save_prefix = OUTPUT_PREFIX +  \"-test-neg-unbiased\"\n",
    "model_trainer._evaluate_partial(test_dataset_neg_unbiased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute AOA and unbiased evaluator metrics with biased testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "biased_results = dict()\n",
    "\n",
    "# biased_results[\"STRATIFIED_15\"] = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=1.5, K=30, partition=100)\n",
    "biased_results[\"AOA\"] = aoa(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", K=30)\n",
    "biased_results[\"UB_15\"] = eq(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=1.5, K=30)\n",
    "biased_results[\"UB_2\"] =  eq(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2, K=30)\n",
    "biased_results[\"UB_25\"] =  eq(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2.5, K=30)\n",
    "biased_results[\"UB_3\"] =  eq(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=3, K=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute AOA and unbiased evaluator metrics with unbiased testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbiased_results = dict()\n",
    "\n",
    "# unbiased_results[\"STRATIFIED_15\"] = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=1.5, K=1, partition=100)\n",
    "unbiased_results[\"AOA\"] = aoa(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", K=1)\n",
    "unbiased_results[\"UB_15\"] = eq(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=1.5, K=1)\n",
    "unbiased_results[\"UB_2\"] =  eq(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2, K=1)\n",
    "unbiased_results[\"UB_25\"] =  eq(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2.5, K=1)\n",
    "unbiased_results[\"UB_3\"] =  eq(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=3, K=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 742,  967,  141,  707,  751,   24,  273,  958,  193,  200, 1000,\n",
       "        206,  783,  662,  295,  384,  228,  974,  340,  122,  146,  740,\n",
       "        818,  747,  346,  852,  849,  579,   20,  284,  197,  727,  910,\n",
       "        332,  251,  599,  529,  463,  192,  873,  502,  309,  731,   11,\n",
       "        366,  581,  108,  400,  294,  524,  486,  265,  530,  445,  883,\n",
       "        476,  168,  672,   38,  666,  983,  848,    7,  874,  356,    2,\n",
       "        490,  545,  954,  365,  189,  701,  992,   99,  823,  885,  128,\n",
       "        832,  680,  804,  654,  329,  521,  746,  691,  318,   55,  936,\n",
       "        715,  429,  768,   40,   51,  506,  458,   18,  244,  648,  409,\n",
       "        548,  694,  567,  730,  411,  864,  172,  272,   15,  227,  427,\n",
       "        606,  917,  809,  646,  515,  838,  651,  511,  266,  420,  861,\n",
       "         62,   46,  290,  522,  372,   93,   19,  773,  793,  302,  886,\n",
       "         26,  705,  781,  395,  456,  957,  785,  483,  334,  644,  536,\n",
       "        937,  156,  577,  297,  704,  561,  617,  813,  836,  215,  584,\n",
       "        538,  230,  117,  510,  650,  303,  786,  754,  956,  489,  630,\n",
       "        758,  393,  216,  824,  620,  299,    5,  360,   22,    3,  353,\n",
       "        593,  323,  494,  424,  981,  656,  512,  941,  975,  955,  573,\n",
       "        177,  321,   33,  422,  847,  412,  794,  713,  822,  869,  428,\n",
       "        776,  513,  167,  635,  571,  225,  853,  383,  300,  878,  798,\n",
       "        653,  286,  276,  324,  724,  390,  600,  540,  460,  526,  980,\n",
       "        118,  922,  622,  113,  639,  663,   43,  363,  222,  152,  433,\n",
       "        993,  597,  891,  898,   96,  854,   30,  771,  947,  909,  929,\n",
       "        962,  948,  739,  110,  440,  238,  734,  721,  382,  188,  357,\n",
       "        661,  142,  839,  634,  178,  461,  765,  888,  399,  166,  815,\n",
       "        528,  313,  638,  418,  518,  692,  209,  889,  665,  190,   67,\n",
       "        223,   97,  718,  689,   12,  902,  415,  392,  467,  289,  601,\n",
       "        348,  441,  271,  444,  492,  471,  884,  780,   63,    9,  155,\n",
       "        589,  554,  173])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get number of items\n",
    "num_items = max_item\n",
    "\n",
    "# Get the n_p partitions\n",
    "n_p = 300\n",
    "nums = np.arange(1, num_items+1)\n",
    "partitions = np.random.choice(nums, n_p, replace=False)\n",
    "\n",
    "# Visualize\n",
    "partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the partition which minimizes the sum of AUC and Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STRATIFIED_15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "091ddcc60bf2462cb7c7c2d63a667ed2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute biased and unbiased results with stratified for each partition\n",
    "# and store biased and unbiased results such that the sum of AUC and Recall is minimized\n",
    "\n",
    "# Value of gamma to use for minimization\n",
    "gamma = 1.5\n",
    "\n",
    "# To print :)\n",
    "key = \"STRATIFIED_\" + str(gamma).replace(\".\",\"\")\n",
    "print(key)\n",
    "\n",
    "unbiased_results[key] = {}\n",
    "biased_results[key] = {}\n",
    "best_partition = -1\n",
    "best_score = float('inf')\n",
    "\n",
    "history = np.full(15400, np.inf)  # Adjusted to match the size of nums\n",
    "\n",
    "for p in tqdm(partitions):\n",
    "    # Fetch stratified results; these functions need to be defined or replaced with actual logic\n",
    "    temp_unbiased = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=gamma, K=1, partition=p)\n",
    "    temp_biased = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=gamma, K=30, partition=p)\n",
    "\n",
    "    # Calculate combined score\n",
    "    combined_score = temp_unbiased['bias'] + temp_unbiased['concentration'] + \\\n",
    "                     temp_biased['bias'] + temp_biased['concentration']\n",
    "\n",
    "    history[p-1] = combined_score  # Store the combined score\n",
    "\n",
    "    # Update the best_partition and best_score if the current partition's score is lower\n",
    "    if combined_score < best_score:\n",
    "        best_score = combined_score\n",
    "        best_partition = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best partition: 967 with combined score: 156160.02161393766\n",
      "Minimum score from history: 156160.02161393766\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best partition: {best_partition} with combined score: {best_score}\")\n",
    "print(f\"Minimum score from history: {np.min(history)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the histogram\n",
    "plt.hist(history[history != np.inf], bins=10, edgecolor='black')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Data')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, for the chosen value of gamma, the best partition is..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "967"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize\n",
    "best_partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute stratified metrics with unbiased testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbiased_results[\"STRATIFIED_15\"] = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=1.5, K=1, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_15\"] = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=1.5, K=30, partition=best_partition)\n",
    "\n",
    "unbiased_results[\"STRATIFIED_2\"] = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2, K=1, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_2\"] = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2, K=30, partition=best_partition)\n",
    "\n",
    "unbiased_results[\"STRATIFIED_25\"] = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2.5, K=1, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_25\"] = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2.5, K=30, partition=best_partition)\n",
    "\n",
    "unbiased_results[\"STRATIFIED_3\"] = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=3, K=1, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_3\"] = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=3, K=30, partition=best_partition)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version uses the linspace of items instead of linspace of propensities to make the partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbiased_results[\"STRATIFIED_v2_15\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=1.5, K=1, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_v2_15\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=1.5, K=30, partition=best_partition)\n",
    "\n",
    "unbiased_results[\"STRATIFIED_v2_2\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2, K=1, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_v2_2\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2, K=30, partition=best_partition)\n",
    "\n",
    "unbiased_results[\"STRATIFIED_v2_25\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2.5, K=1, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_v2_25\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2.5, K=30, partition=best_partition)\n",
    "\n",
    "unbiased_results[\"STRATIFIED_v2_3\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=3, K=1, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_v2_3\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=3, K=30, partition=best_partition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare table for results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 13)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = 0\n",
    "columns = len(biased_results.keys())\n",
    "\n",
    "for key in biased_results.keys():\n",
    "    rows = max(rows, len(biased_results[key].keys()))\n",
    "\n",
    "for key in unbiased_results.keys():\n",
    "    rows = max(rows, len(biased_results[key].keys()))\n",
    "\n",
    "rows, columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init dictionary\n",
    "mae_results = dict()\n",
    "\n",
    "# Get the names of the rows\n",
    "list_biased_res = list(biased_results.keys())\n",
    "\n",
    "# Init results\n",
    "results_array = np.zeros((rows,columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each row\n",
    "for i in range(len(list_biased_res)):\n",
    "    key = list_biased_res[i]\n",
    "\n",
    "    # For each column\n",
    "    for j in range(len(list(biased_results[key].keys()))):\n",
    "        key_2 = list(biased_results[key].keys())[j]\n",
    "\n",
    "        # Compute MAE\n",
    "        results_array[j][i] = abs(biased_results[key][key_2] - unbiased_results[key][key_2])\n",
    "\n",
    "# Make it a DataFrame\n",
    "mae_df = pd.DataFrame(columns=list(biased_results.keys()), data=results_array)\n",
    "metric_values = list(biased_results[list(biased_results.keys())[0]].keys())\n",
    "mae_df.insert(0, \"metric\", metric_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the table with the MAE results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **RESULTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>AOA</th>\n",
       "      <th>UB_15</th>\n",
       "      <th>UB_2</th>\n",
       "      <th>UB_25</th>\n",
       "      <th>UB_3</th>\n",
       "      <th>STRATIFIED_15</th>\n",
       "      <th>STRATIFIED_2</th>\n",
       "      <th>STRATIFIED_25</th>\n",
       "      <th>STRATIFIED_3</th>\n",
       "      <th>STRATIFIED_v2_15</th>\n",
       "      <th>STRATIFIED_v2_2</th>\n",
       "      <th>STRATIFIED_v2_25</th>\n",
       "      <th>STRATIFIED_v2_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>auc</td>\n",
       "      <td>0.154493</td>\n",
       "      <td>0.128429</td>\n",
       "      <td>0.125081</td>\n",
       "      <td>0.122529</td>\n",
       "      <td>0.120581</td>\n",
       "      <td>0.124765</td>\n",
       "      <td>0.103931</td>\n",
       "      <td>0.050041</td>\n",
       "      <td>2.136094e-01</td>\n",
       "      <td>1.284291e-01</td>\n",
       "      <td>1.250808e-01</td>\n",
       "      <td>1.225293e-01</td>\n",
       "      <td>1.205810e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>recall</td>\n",
       "      <td>0.377779</td>\n",
       "      <td>0.259048</td>\n",
       "      <td>0.247508</td>\n",
       "      <td>0.238948</td>\n",
       "      <td>0.232533</td>\n",
       "      <td>0.258495</td>\n",
       "      <td>0.252065</td>\n",
       "      <td>0.247736</td>\n",
       "      <td>2.199377e-01</td>\n",
       "      <td>2.590477e-01</td>\n",
       "      <td>2.475084e-01</td>\n",
       "      <td>2.389476e-01</td>\n",
       "      <td>2.325327e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bias</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22632.642818</td>\n",
       "      <td>66767.452460</td>\n",
       "      <td>329021.602207</td>\n",
       "      <td>1.193921e+06</td>\n",
       "      <td>3.296030e-12</td>\n",
       "      <td>3.496758e-12</td>\n",
       "      <td>4.046541e-12</td>\n",
       "      <td>9.465762e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>concentration</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1264.088294</td>\n",
       "      <td>2243.314702</td>\n",
       "      <td>1368.105797</td>\n",
       "      <td>9.060283e+02</td>\n",
       "      <td>1.036990e+01</td>\n",
       "      <td>1.423835e+01</td>\n",
       "      <td>1.641256e+01</td>\n",
       "      <td>1.759407e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          metric       AOA     UB_15      UB_2     UB_25      UB_3  \\\n",
       "0            auc  0.154493  0.128429  0.125081  0.122529  0.120581   \n",
       "1         recall  0.377779  0.259048  0.247508  0.238948  0.232533   \n",
       "2           bias  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3  concentration  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "   STRATIFIED_15  STRATIFIED_2  STRATIFIED_25  STRATIFIED_3  STRATIFIED_v2_15  \\\n",
       "0       0.124765      0.103931       0.050041  2.136094e-01      1.284291e-01   \n",
       "1       0.258495      0.252065       0.247736  2.199377e-01      2.590477e-01   \n",
       "2   22632.642818  66767.452460  329021.602207  1.193921e+06      3.296030e-12   \n",
       "3    1264.088294   2243.314702    1368.105797  9.060283e+02      1.036990e+01   \n",
       "\n",
       "   STRATIFIED_v2_2  STRATIFIED_v2_25  STRATIFIED_v2_3  \n",
       "0     1.250808e-01      1.225293e-01     1.205810e-01  \n",
       "1     2.475084e-01      2.389476e-01     2.325327e-01  \n",
       "2     3.496758e-12      4.046541e-12     9.465762e-13  \n",
       "3     1.423835e+01      1.641256e+01     1.759407e+01  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize\n",
    "mae_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RecSysEvaluation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
