{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **IMPORT LIBS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import standard libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from openrec.tf1.legacy import ImplicitModelTrainer\n",
    "from openrec.tf1.legacy.utils.evaluators import ImplicitEvalManager\n",
    "from openrec.tf1.legacy.utils import ImplicitDataset\n",
    "from openrec.tf1.legacy.recommenders import CML\n",
    "from openrec.tf1.legacy.utils.evaluators import AUC\n",
    "from openrec.tf1.legacy.utils.samplers import PairwiseSampler\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt  \n",
    "import pandas as pd\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import our functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusts path to include the utilities.py file\n",
    "sys.path.append('../')\n",
    "# Imports it\n",
    "from helper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set training flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag to retrain the model (in this notebook should always be False)\n",
    "REPEAT_TRAINING = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **INITIALIZATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize notebook parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "seed = 2384795\n",
    "np.random.seed(seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing folder for output data\n",
    "output_name = f\"./generated_data/\"\n",
    "if os.path.exists(output_name) == False:\n",
    "    os.makedirs(output_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LOAD MODEL AND DATA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will just be using the code provided by the authors of the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare names and paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Initialize metadata and paths\n",
    "MODEL_CLASS = CML\n",
    "MODEL_PREFIX = \"cml\"\n",
    "DATASET_NAME = \"yahoo\"\n",
    "OUTPUT_FOLDER = output_name\n",
    "OUTPUT_PATH = OUTPUT_FOLDER + MODEL_PREFIX + \"-\" + DATASET_NAME + \"/\"\n",
    "OUTPUT_PREFIX = str(OUTPUT_PATH) + str(MODEL_PREFIX) + \"-\" + str(DATASET_NAME)\n",
    "if os.path.exists(OUTPUT_PATH) == False:\n",
    "    os.makedirs(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset and model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data from output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Init dictionary for \n",
    "raw_data = dict()\n",
    "raw_data['max_user'] = 15401\n",
    "raw_data['max_item'] = 1001\n",
    "\n",
    "# Load train and test data from files\n",
    "raw_data['train_data'] = np.load(output_name + \"training_arr.npy\")\n",
    "raw_data['test_data_pos_biased'] = np.load(output_name + \"biased-test_arr_pos.npy\")\n",
    "raw_data['test_data_neg_biased'] = np.load(output_name + \"biased-test_arr_neg.npy\")\n",
    "raw_data['test_data_pos_unbiased'] = np.load(output_name + \"unbiased-test_arr_pos.npy\")\n",
    "raw_data['test_data_neg_unbiased'] = np.load(output_name + \"unbiased-test_arr_neg.npy\")\n",
    "\n",
    "# Form the datasets\n",
    "train_dataset = ImplicitDataset(raw_data['train_data'], raw_data['max_user'], raw_data['max_item'], name='Train')\n",
    "test_dataset_pos_biased = ImplicitDataset(raw_data['test_data_pos_biased'], raw_data['max_user'], raw_data['max_item'])\n",
    "test_dataset_neg_biased = ImplicitDataset(raw_data['test_data_neg_biased'], raw_data['max_user'], raw_data['max_item'])\n",
    "test_dataset_pos_unbiased = ImplicitDataset(raw_data['test_data_pos_unbiased'], raw_data['max_user'], raw_data['max_item'])\n",
    "test_dataset_neg_unbiased = ImplicitDataset(raw_data['test_data_neg_unbiased'], raw_data['max_user'], raw_data['max_item'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Prevent tensorflow from using cached embeddings and set seed\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.reset_default_graph()\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "# Set parameters to define the model\n",
    "batch_size = 8000\n",
    "test_batch_size = 1000\n",
    "\n",
    "# Define the model\n",
    "model = MODEL_CLASS(batch_size=batch_size, max_user=train_dataset.max_user(), max_item=train_dataset.max_item(), dim_embed=50, l2_reg=0.001, opt='Adam', sess_config=None)\n",
    "sampler = PairwiseSampler(batch_size=batch_size, dataset=train_dataset, num_process=4)\n",
    "model_trainer = ImplicitModelTrainer(batch_size=batch_size, test_batch_size=test_batch_size, train_dataset=train_dataset, model=model, sampler=sampler, eval_save_prefix=OUTPUT_PATH + DATASET_NAME, item_serving_size=500)\n",
    "auc_evaluator = AUC()\n",
    "\n",
    "# Load the model\n",
    "model.load(OUTPUT_PATH)\n",
    "\n",
    "# Set further parameters\n",
    "model_trainer._eval_manager = ImplicitEvalManager(evaluators=[auc_evaluator])\n",
    "model_trainer._num_negatives = 200\n",
    "model_trainer._exclude_positives([train_dataset, test_dataset_pos_biased, test_dataset_neg_biased])\n",
    "model_trainer._sample_negatives(seed=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **EVALUATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will just be using the code provided by the authors of the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biased Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute recommendations with biased testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Set output path for pickles files\n",
    "model_trainer._eval_save_prefix = OUTPUT_PREFIX + \"-test-pos-biased\"\n",
    "# Evaluate\n",
    "model_trainer._evaluate_partial(test_dataset_pos_biased)\n",
    "\n",
    "# Set output path for pickles files\n",
    "model_trainer._eval_save_prefix = OUTPUT_PREFIX +  \"-test-neg-biased\"\n",
    "# Evaluate\n",
    "model_trainer._evaluate_partial(test_dataset_neg_biased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unbiased Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute recommendations with biased testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Set output path for pickles files\n",
    "model_trainer._eval_save_prefix = OUTPUT_PREFIX + \"-test-pos-unbiased\"\n",
    "# Evaluate\n",
    "model_trainer._evaluate_partial(test_dataset_pos_unbiased)\n",
    "\n",
    "# Set output path for pickles files\n",
    "model_trainer._eval_save_prefix = OUTPUT_PREFIX +  \"-test-neg-unbiased\"\n",
    "# Evaluate\n",
    "model_trainer._evaluate_partial(test_dataset_neg_unbiased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **COMPUTE RESULTS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we first preprocess the propensities from the training set data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the propensities\n",
    "propensities = calculate_propensities(raw_data['max_user'], raw_data['max_item'], output_name+\"training_arr.npy\", normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute rivals metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now set the gamma values to be considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose values for gamma\n",
    "GAMMAS = [1.5, 2, 2.5, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute AOA and IPS metrics for both biased and unbiased datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Init biased and unbiased results dictionaries\n",
    "biased_results = dict()\n",
    "unbiased_results = dict()\n",
    "\n",
    "# Compute AOA results\n",
    "biased_results[\"AOA\"] = aoa(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", K=30)\n",
    "unbiased_results[\"AOA\"] = aoa(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", K=1)\n",
    "\n",
    "# Compute IPS results\n",
    "for gamma in GAMMAS:\n",
    "    key = \"UB_\" + str(gamma).replace(\".\",\"\")\n",
    "    biased_results[key] = eq(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", propensities[gamma], K=30)\n",
    "    unbiased_results[key] = eq(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", propensities[gamma], K=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Stratified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get array of possible values for the number of subsets in the partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Get number of items\n",
    "num_items = raw_data['max_item'] - 1\n",
    "\n",
    "# Set the number of possible values to be considered\n",
    "n_p = 300\n",
    "\n",
    "# Get array with the first num_items natural numbers \n",
    "nums = np.arange(1, num_items+1)\n",
    "\n",
    "# Choose n_p random values\n",
    "partitions = np.random.choice(nums, n_p, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the partition p which minimizes the sum of Bias and Conc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value of gamma to use for minimization\n",
    "opt_gamma = 1.5\n",
    "\n",
    "# Keys prefix in the dictionaries + gamma value\n",
    "key = \"STRATIFIED_\" + str(opt_gamma).replace(\".\",\"\")\n",
    "\n",
    "# Init dictionaries to store results for biased and unbiased testset\n",
    "unbiased_results[key] = {}\n",
    "biased_results[key] = {}\n",
    "\n",
    "# Random init for best partition\n",
    "best_partition = np.random.choice(nums, 1)[0]\n",
    "# Init infinity as min score\n",
    "best_score = float('inf')\n",
    "\n",
    "# To plot the results init histories\n",
    "history_objective = np.full(num_items, np.inf)\n",
    "history_mae_auc = np.full(num_items, np.inf)\n",
    "history_mae_recall = np.full(num_items, np.inf)\n",
    "history_bound = np.full(num_items, np.inf)\n",
    "\n",
    "# For each value in the partitions array (n_p random values from 1 to num_items)\n",
    "for p in tqdm(partitions):\n",
    "\n",
    "    # Fetch stratified results\n",
    "    temp_unbiased = stratified(OUTPUT_PREFIX + \"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX + \"-test-neg-unbiased_evaluate_partial.pickle\", output_name + \"training_arr.npy\", propensities[opt_gamma], K=1, partition=p)\n",
    "    temp_biased = stratified(OUTPUT_PREFIX + \"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX + \"-test-neg-biased_evaluate_partial.pickle\", output_name + \"training_arr.npy\", propensities[opt_gamma], K=30, partition=p)\n",
    "      \n",
    "    # Computing bound score\n",
    "    bound_score = temp_unbiased['bias'] + temp_unbiased['concentration'] + temp_biased['bias'] + temp_biased['concentration']\n",
    "    # Compute maes to plot the history\n",
    "    mae_score_auc = abs(temp_unbiased['auc'] - temp_biased['auc'])   \n",
    "    mae_score_recall = abs(temp_unbiased['recall'] - temp_biased['recall'])\n",
    "    \n",
    "    # Set objective (to switch between the bound and the MAE to check whether the bound is a good metric)\n",
    "    objective = bound_score \n",
    "    \n",
    "    # Storing historical values\n",
    "    history_mae_auc[p-1] = mae_score_auc  # Store the mae using auc\n",
    "    history_mae_recall[p-1] = mae_score_recall  # Store the mae using recall\n",
    "    history_bound[p-1] = bound_score  # Store the bound score\n",
    "\n",
    "    # Update the best_partition and best_score if the current partition's bound is lower\n",
    "    if objective < best_score:\n",
    "        best_score = objective\n",
    "        best_partition = p\n",
    "\n",
    "# Print the best partition and the combined score\n",
    "print(f\"Best partition: {best_partition} with combined score: {best_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot MAE with AUC over each considered partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get p values (only the ones which were considered)\n",
    "x = np.where(history_mae_auc != np.inf)\n",
    "# Get MAE values with AUC\n",
    "y = history_mae_auc[history_mae_auc != np.inf]\n",
    " \n",
    "# Plot\n",
    "plt.title(\"MAE using auc\") \n",
    "plt.xlabel(\"NUM PARTITIONS\") \n",
    "plt.ylabel(\"MAE using auc\") \n",
    "plt.scatter(x, y, color =\"red\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot MAE with Recall over each considered partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get p values (only the ones which were considered)\n",
    "x = np.where(history_mae_recall != np.inf)\n",
    "# Get MAE values with recall\n",
    "y = history_mae_recall[history_mae_recall != np.inf]\n",
    " \n",
    "# Plot\n",
    "plt.title(\"MAE using recall\") \n",
    "plt.xlabel(\"NUM PARTITIONS\") \n",
    "plt.ylabel(\"MAE using recall\") \n",
    "plt.scatter(x, y, color =\"red\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the bound over each considered partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get p values (only the ones which were considered)\n",
    "x = np.where(history_bound != np.inf)\n",
    "# Get the bound values\n",
    "y = history_bound[history_bound != np.inf]\n",
    " \n",
    "# Plot\n",
    "plt.title(\"Bound values\") \n",
    "plt.xlabel(\"NUM PARTITIONS\") \n",
    "plt.ylabel(\"Bound value\") \n",
    "plt.scatter(x, y, color =\"red\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now finally compute the evaluation with the Stratified estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Compute Stratified results\n",
    "for gamma in GAMMAS:\n",
    "    key = \"STRATIFIED_\" + str(gamma).replace(\".\",\"\")\n",
    "    unbiased_results[key] = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", propensities[gamma], K=1, partition=best_partition)\n",
    "    biased_results[key] = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", propensities[gamma], K=30, partition=best_partition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version builds a partition evenly distributing the items instead of the propensities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Compute Stratified results with the second version\n",
    "for gamma in GAMMAS:\n",
    "    key = \"STRATIFIED_v2_\" + str(gamma).replace(\".\",\"\")\n",
    "    unbiased_results[key] = stratified_2(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", propensities[gamma], K=1, partition=best_partition)\n",
    "    biased_results[key] = stratified_2(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", propensities[gamma], K=30, partition=best_partition)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare table for results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Set number of columns as number of estimators which were used\n",
    "columns = len(biased_results.keys())\n",
    "\n",
    "# Determine the maximum number (number of used metrics)\n",
    "rows = max(max(len(biased_results[key].keys()) for key in biased_results.keys()), max(len(unbiased_results[key].keys()) for key in unbiased_results.keys()))\n",
    "\n",
    "# Init matrix to store results\n",
    "results_array = np.zeros((rows,columns))\n",
    "\n",
    "# Get the names of the rows\n",
    "list_biased_res = list(biased_results.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the table with the MAE results and get the DataFrame object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# For each evaluator\n",
    "for i in range(len(list_biased_res)):\n",
    "    key = list_biased_res[i]\n",
    "\n",
    "    # For each metric\n",
    "    for j in range(len(list(biased_results[key].keys()))):\n",
    "        key_2 = list(biased_results[key].keys())[j]\n",
    "\n",
    "        # Compute MAE between biased and unbiased results\n",
    "        results_array[j][i] = abs(biased_results[key][key_2] - unbiased_results[key][key_2])\n",
    "\n",
    "# Make it a DataFrame\n",
    "mae_df = pd.DataFrame(columns=list(biased_results.keys()), data=results_array)\n",
    "metric_values = list(biased_results[list(biased_results.keys())[0]].keys())\n",
    "mae_df.insert(0, \"metric\", metric_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **RESULTS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remembering that..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the results\n",
    "print(\"Minimization was done for gamma = \", opt_gamma, \". The best number of partitions: \", best_partition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Visualize\n",
    "mae_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RecSys-Evaluation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
