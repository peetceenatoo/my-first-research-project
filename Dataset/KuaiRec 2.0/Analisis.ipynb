{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **IMPORT LIBS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from openrec.tf1.legacy import ImplicitModelTrainer\n",
    "from openrec.tf1.legacy.utils.evaluators import ImplicitEvalManager\n",
    "from openrec.tf1.legacy.utils import ImplicitDataset\n",
    "from openrec.tf1.legacy.recommenders import CML, BPR, PMF\n",
    "from openrec.tf1.legacy.utils.evaluators import AUC\n",
    "from openrec.tf1.legacy.utils.samplers import PairwiseSampler\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **GENERATE THE DATASET**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "seed = 2384795\n",
    "np.random.seed(seed=seed)\n",
    "\n",
    "# Preparing folder for output data\n",
    "output_name = f\"./generated_data/\"\n",
    "if os.path.exists(output_name) == False:\n",
    "    os.makedirs(output_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MODEL CHOICE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I won't comment anything, we are just using the code provided by the authors of the paper\n",
    "\n",
    "raw_data = dict()\n",
    "raw_data['train_data'] = np.load(output_name + \"training_arr.npy\")\n",
    "raw_data['max_user'] = 15401\n",
    "raw_data['max_item'] = 1001\n",
    "batch_size = 8000\n",
    "test_batch_size = 1000\n",
    "display_itr = 1000\n",
    "\n",
    "train_dataset = ImplicitDataset(raw_data['train_data'], raw_data['max_user'], raw_data['max_item'], name='Train')\n",
    "\n",
    "MODEL_CLASS = CML\n",
    "MODEL_PREFIX = \"cml\"\n",
    "DATASET_NAME = \"KuaiRec\"\n",
    "OUTPUT_FOLDER = output_name\n",
    "OUTPUT_PATH = OUTPUT_FOLDER + MODEL_PREFIX + \"-\" + DATASET_NAME + \"/\"\n",
    "OUTPUT_PREFIX = str(OUTPUT_PATH) + str(MODEL_PREFIX) + \"-\" + str(DATASET_NAME)\n",
    "\n",
    "\n",
    "if os.path.exists(OUTPUT_PATH) == False:\n",
    "    os.makedirs(OUTPUT_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TRAIN THE MODEL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DEFINING FUNCTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eq(infilename, infilename_neg, trainfilename, gamma=-1.0, K=1):\n",
    "\n",
    "    # Read pickles\n",
    "    infile = open(infilename, 'rb')\n",
    "    infile_neg = open(infilename_neg, 'rb')\n",
    "    P = pickle.load(infile)\n",
    "    infile.close()\n",
    "    P_neg = pickle.load(infile_neg)\n",
    "    infile_neg.close()\n",
    "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
    "    \n",
    "    # Merge P and P_neg\n",
    "    for theuser in P[\"users\"]:\n",
    "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "    \n",
    "    Zui = dict()\n",
    "    Ni = dict()\n",
    "    \n",
    "    # Compute frequencies of items in the training set\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "    del trainset\n",
    "\n",
    "    # Count #users with non-zero item frequencies\n",
    "    nonzero_user_count = 0\n",
    "    for theuser in P[\"users\"]:\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for pos_item in pos_items:\n",
    "            if pos_item in Ni:\n",
    "                nonzero_user_count += 1\n",
    "                break\n",
    "    \n",
    "    # Compute recommendations for each user\n",
    "    for theuser in P[\"users\"]:\n",
    "        all_scores = np.array(P[\"results\"][theuser])\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for i, pos_item in enumerate(pos_items):\n",
    "            pos_score = pos_scores[i]\n",
    "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
    "\n",
    "    \n",
    "    # Calculate per-user scores\n",
    "    sum_user_auc = 0.0\n",
    "    sum_user_recall = 0.0\n",
    "    for theuser in P[\"users\"]:\n",
    "        numerator_auc = 0.0\n",
    "        numerator_recall = 0.0\n",
    "        denominator = 0.0\n",
    "\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            if theitem not in Ni:\n",
    "                continue\n",
    "            pui = np.power(Ni[theitem], (gamma + 1) / 2.0)\n",
    "\n",
    "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser])) / pui\n",
    "            \n",
    "            if Zui[(theuser, theitem)] < K:\n",
    "                numerator_recall += 1.0 / pui\n",
    "            denominator += 1 / pui\n",
    "                \n",
    "        if denominator > 0:\n",
    "            sum_user_auc += numerator_auc / denominator\n",
    "            sum_user_recall += numerator_recall / denominator \n",
    "\n",
    "    # Return\n",
    "    return {\n",
    "        \"auc\"       : sum_user_auc / nonzero_user_count,\n",
    "        \"recall\"    : sum_user_recall / nonzero_user_count\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aoa(infilename, infilename_neg, trainfilename, K=1):\n",
    "\n",
    "    # Read pickles\n",
    "    infile = open(infilename, 'rb')\n",
    "    infile_neg = open(infilename_neg, 'rb')\n",
    "    P = pickle.load(infile)\n",
    "    infile.close()\n",
    "    P_neg = pickle.load(infile_neg)\n",
    "    infile_neg.close()\n",
    "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
    "    \n",
    "    # Merge P and P_neg\n",
    "    for theuser in P[\"users\"]:\n",
    "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "    \n",
    "    Zui = dict()\n",
    "    Ni = dict()\n",
    "\n",
    "    # Compute frequencies of items in the training set\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "    del trainset\n",
    "    \n",
    "    # Count #users with non-zero item frequencies\n",
    "    nonzero_user_count = 0\n",
    "    for theuser in P[\"users\"]:\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for pos_item in pos_items:\n",
    "            if pos_item in Ni:\n",
    "                nonzero_user_count += 1\n",
    "                break\n",
    "    \n",
    "    # Compute recommendations for each user\n",
    "    for theuser in P[\"users\"]:\n",
    "        all_scores = np.array(P[\"results\"][theuser])\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for i, pos_item in enumerate(pos_items):\n",
    "            pos_score = pos_scores[i]\n",
    "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
    "\n",
    "    # Calculate per-user scores\n",
    "    sum_user_auc = 0.0\n",
    "    sum_user_recall = 0.0\n",
    "    for theuser in P[\"users\"]:\n",
    "        numerator_auc = 0.0\n",
    "        numerator_recall = 0.0\n",
    "        denominator = 0.0\n",
    "\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            if theitem not in Ni:\n",
    "                continue\n",
    "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser]))\n",
    "            # Calcolo il Recall a 30, vedi nota 6 paper\n",
    "            if Zui[(theuser, theitem)] < K:\n",
    "                numerator_recall += 1.0\n",
    "            denominator += 1 \n",
    "\n",
    "        if denominator > 0:\n",
    "            sum_user_auc += numerator_auc / denominator\n",
    "            sum_user_recall += numerator_recall / denominator\n",
    "\n",
    "    # Return\n",
    "    return {\n",
    "        \"auc\"       : sum_user_auc / nonzero_user_count,\n",
    "        \"recall\"    : sum_user_recall / nonzero_user_count\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified(infilename, infilename_neg, trainfilename, gamma=1.0, K=30, partition=10, delta=0.1):\n",
    "\n",
    "    # Read pickles\n",
    "    infile = open(infilename, 'rb')\n",
    "    infile_neg = open(infilename_neg, 'rb')\n",
    "    P = pickle.load(infile)\n",
    "    infile.close()\n",
    "    P_neg = pickle.load(infile_neg)\n",
    "    infile_neg.close()\n",
    "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
    "\n",
    "    # Merge P and P_neg\n",
    "    for theuser in P[\"users\"]:\n",
    "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "\n",
    "    Zui = dict()\n",
    "    Ni = dict()\n",
    "\n",
    "    # Compute frequencies of items in the training set\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "    #del trainset\n",
    "\n",
    "    # Compute recommendations for each user\n",
    "    for theuser in P[\"users\"]:\n",
    "        all_scores = np.array(P[\"results\"][theuser])\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for i, pos_item in enumerate(pos_items):\n",
    "            pos_score = pos_scores[i]\n",
    "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
    "\n",
    "    pui = dict()\n",
    "    w = dict()\n",
    "\n",
    "    # Compute dictionary of propensity scores\n",
    "    for theuser in P[\"users\"]:\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            if theitem not in Ni:\n",
    "                continue\n",
    "            if theitem in pui:\n",
    "                continue\n",
    "            pui[theitem] = np.power(Ni[theitem], (gamma + 1) / 2.0)\n",
    "\n",
    "    # Take the list of items (not tuples) in pui sorted by value\n",
    "    items_sorted_by_value = sorted(pui, key=pui.get, reverse=True)\n",
    "\n",
    "    # Compute linspace between the pui[0] and pui[-1] \n",
    "    linspace = np.linspace(pui[items_sorted_by_value[0]], pui[items_sorted_by_value[-1]], partition+1)\n",
    "   \n",
    "    # Compute dictionary w, that is, for each item, assigns the average of the puis in the partition it belongs to\n",
    "    i=0\n",
    "    j = 0\n",
    "    while i < len(items_sorted_by_value):\n",
    "                            \n",
    "        avg = 0\n",
    "        start = i\n",
    "        end = i\n",
    "    \n",
    "        while i < len(items_sorted_by_value) and pui[items_sorted_by_value[i]] >= linspace[j+1]:\n",
    "            avg += 1.0 / pui[items_sorted_by_value[i]]\n",
    "            end = i\n",
    "            i += 1\n",
    "        avg = avg / (end - start + 1)\n",
    "\n",
    "        for k in range(start, end+1):\n",
    "            w[items_sorted_by_value[k]] = avg\n",
    "\n",
    "        j += 1\n",
    "\n",
    "    # Compute bias' numerator\n",
    "    bias = 0.0\n",
    "    for k in pui.keys():\n",
    "        # add |pui*w - 1!|\n",
    "        bias += abs(pui[k] * w[k] - 1)\n",
    "    # Multiply by number of users\n",
    "    bias *= len(P[\"users\"])\n",
    "\n",
    "    # Compute concentrations numerator (for each user)\n",
    "    concentrations = {}\n",
    "    max_w = max(w.values())\n",
    "    # ... by computing the sum of squares of w for each user\n",
    "    for user, item in zip(trainset['user_id'], trainset['item_id']):\n",
    "        # Iterate over the trainset to compute the sum of squares for each user\n",
    "        if item in w:\n",
    "            if user not in concentrations:\n",
    "                concentrations[user] = 0\n",
    "            concentrations[user] += w[item] ** 2\n",
    "    # ... and then applying the formula\n",
    "    for user in concentrations:\n",
    "        concentrations[user] = math.sqrt(concentrations[user] * 2 * math.log(2/delta)) + max_w * 7 * math.log(2/delta)\n",
    "    # Now sum all the concentrations\n",
    "    concentration = sum(concentrations.values())\n",
    "\n",
    "    # Calculate per-user scores\n",
    "    nonzero_user_count = 0\n",
    "    sum_user_auc = 0.0\n",
    "    sum_user_recall = 0.0\n",
    "    for theuser in P[\"users\"]:\n",
    "        numerator_auc = 0.0\n",
    "        numerator_recall = 0.0\n",
    "        denominator = 0.0\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            # Skip items with null frequency\n",
    "            if  theitem not in Ni:\n",
    "                continue\n",
    "            # Add things to be summed for each item\n",
    "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser])) * w[theitem]\n",
    "            # Add things for recall\n",
    "            if Zui[(theuser, theitem)] < K:\n",
    "                numerator_recall += 1.0 * w[theitem] # spetta\n",
    "            # Increment denominator that the sum must be divided by \n",
    "            denominator += 1 / pui[theitem]\n",
    "\n",
    "\n",
    "        # If there was at least one item for the user, count the user and sum the results\n",
    "        if denominator > 0:\n",
    "            nonzero_user_count += 1\n",
    "            sum_user_auc += numerator_auc / denominator\n",
    "            sum_user_recall += numerator_recall / denominator \n",
    "\n",
    "    #del trainset\n",
    "\n",
    "    # Return\n",
    "    return {\n",
    "        \"auc\"       : sum_user_auc / nonzero_user_count, \n",
    "        \"recall\"    : sum_user_recall / nonzero_user_count,\n",
    "        \"bias\"      : bias,\n",
    "        \"concentration\" : concentration\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_logspace(infilename, infilename_neg, trainfilename, gamma=1.0, K=30, partition=10, delta=0.1):\n",
    "\n",
    "    # Read pickles\n",
    "    infile = open(infilename, 'rb')\n",
    "    infile_neg = open(infilename_neg, 'rb')\n",
    "    P = pickle.load(infile)\n",
    "    infile.close()\n",
    "    P_neg = pickle.load(infile_neg)\n",
    "    infile_neg.close()\n",
    "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
    "\n",
    "    # Merge P and P_neg\n",
    "    for theuser in P[\"users\"]:\n",
    "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "\n",
    "    Zui = dict()\n",
    "    Ni = dict()\n",
    "\n",
    "    # Compute frequencies of items in the training set\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "    del trainset\n",
    "\n",
    "    # Compute recommendations for each user\n",
    "    for theuser in P[\"users\"]:\n",
    "        all_scores = np.array(P[\"results\"][theuser])\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for i, pos_item in enumerate(pos_items):\n",
    "            pos_score = pos_scores[i]\n",
    "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
    "\n",
    "    pui = dict()\n",
    "    w = dict()\n",
    "\n",
    "    # Compute dictionary of propensity scores\n",
    "    for theuser in P[\"users\"]:\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            if theitem not in Ni:\n",
    "                continue\n",
    "            if theitem in pui:\n",
    "                continue\n",
    "            pui[theitem] = np.power(Ni[theitem], (gamma + 1) / 2.0)\n",
    "\n",
    "    # Take the list of items (not tuples) in pui sorted by value\n",
    "    items_sorted_by_value = sorted(pui, key=pui.get, reverse=True)\n",
    "\n",
    "    # Compute linspace between the pui[0] and pui[-1] \n",
    "\n",
    "    # Maybe try to split the logspace instead of the linspace?\n",
    "    logspace = np.logspace(pui[items_sorted_by_value[0]], pui[items_sorted_by_value[-1]], partition+1)\n",
    "   \n",
    "    # Compute dictionary w, that is, for each item, assigns the average of the puis in the partition it belongs to\n",
    "    i=0\n",
    "    j = 0\n",
    "    while i < len(items_sorted_by_value):\n",
    "                            \n",
    "        avg = 0\n",
    "        start = i\n",
    "        end = i\n",
    "    \n",
    "        while i < len(items_sorted_by_value) and pui[items_sorted_by_value[i]] >= logspace[j+1]:\n",
    "            avg += 1.0 / pui[items_sorted_by_value[i]]\n",
    "            end = i\n",
    "            i += 1\n",
    "        \n",
    "        # Is the average the only good choice? even with the log space split?\n",
    "        avg = avg / (end - start + 1)\n",
    "\n",
    "        for k in range(start, end+1):\n",
    "            w[items_sorted_by_value[k]] = avg\n",
    "\n",
    "        j += 1\n",
    "\n",
    "        # Compute bias' numerator\n",
    "        bias = 0.0\n",
    "        for k in pui.keys():\n",
    "            # add |pui*w - 1!|\n",
    "            bias += abs(pui[k] * w[k] - 1)\n",
    "        # Multiply by number of users\n",
    "        bias *= len(P[\"users\"])\n",
    "\n",
    "        # Compute concentrations numerator (for each user)\n",
    "        concentrations = {}\n",
    "        max_w = max(w.values())\n",
    "        # ... by computing the sum of squares of w for each user\n",
    "        for user, item in zip(trainset['user_id'], trainset['item_id']):\n",
    "            # Iterate over the trainset to compute the sum of squares for each user\n",
    "            if item in w:\n",
    "                if user not in concentrations:\n",
    "                    concentrations[user] = 0\n",
    "                concentrations[user] += w[item] ** 2\n",
    "        # ... and then applying the formula\n",
    "        for user in concentrations:\n",
    "            concentrations[user] = math.sqrt(concentrations[user] * 2 * math.log(2/delta)) + max_w * 7 * math.log(2/delta)\n",
    "        # Now sum all the concentrations\n",
    "        concentration = sum(concentrations.values())\n",
    "\n",
    "    # Compute score with AUC and compute Recall\n",
    "    nonzero_user_count = 0\n",
    "    sum_user_auc = 0.0\n",
    "    sum_user_recall = 0.0\n",
    "    for theuser in P[\"users\"]:\n",
    "        numerator_auc = 0.0\n",
    "        numerator_recall = 0.0\n",
    "        denominator = 0.0\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            # Skip items with null frequency\n",
    "            if  theitem not in Ni:\n",
    "                continue\n",
    "            # Add things to be summed for each item\n",
    "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser])) * w[theitem]\n",
    "            # Add things for recall\n",
    "            if Zui[(theuser, theitem)] < K:\n",
    "                numerator_recall += 1.0 * w[theitem] # spetta\n",
    "            # Increment denominator that the sum must be divided by \n",
    "            denominator += 1 / pui[theitem]\n",
    "\n",
    "\n",
    "        # If there was at least one item for the user, count the user and sum the results\n",
    "        if denominator > 0:\n",
    "            nonzero_user_count += 1\n",
    "            sum_user_auc += numerator_auc / denominator\n",
    "            sum_user_recall += numerator_recall / denominator \n",
    "\n",
    "    # Return\n",
    "    return {\n",
    "        \"auc\"       : sum_user_auc / nonzero_user_count, \n",
    "        \"recall\"    : sum_user_recall / nonzero_user_count,\n",
    "        \"bias\"      : bias,\n",
    "        \"concentration\" : concentration\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This version uses the linspace of the number of number of items used for evaluation, not of the propensities\n",
    "def stratified_2(infilename, infilename_neg, trainfilename, gamma=1.0, K=30, partition=10, delta=0.1):\n",
    "\n",
    "    # Read pickles\n",
    "    infile = open(infilename, 'rb')\n",
    "    infile_neg = open(infilename_neg, 'rb')\n",
    "    P = pickle.load(infile)\n",
    "    infile.close()\n",
    "    P_neg = pickle.load(infile_neg)\n",
    "    infile_neg.close()\n",
    "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
    "\n",
    "    # Merge P and P_neg\n",
    "    for theuser in P[\"users\"]:\n",
    "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "\n",
    "    Zui = dict()\n",
    "    Ni = dict()\n",
    "\n",
    "    # Compute frequencies of items in the training set\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "    #del trainset\n",
    "\n",
    "    # Compute recommendations for each user\n",
    "    for theuser in P[\"users\"]:\n",
    "        all_scores = np.array(P[\"results\"][theuser])\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for i, pos_item in enumerate(pos_items):\n",
    "            pos_score = pos_scores[i]\n",
    "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
    "\n",
    "    pui = dict()\n",
    "    w = dict()\n",
    "\n",
    "    # Compute dictionary of propensity scores\n",
    "    for theuser in P[\"users\"]:\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            if theitem not in Ni:\n",
    "                continue\n",
    "            if theitem in pui:\n",
    "                continue\n",
    "            pui[theitem] = np.power(Ni[theitem], (gamma + 1) / 2.0)\n",
    "\n",
    "    # Take the list of items (not tuples) in pui sorted by value\n",
    "    items_sorted_by_value = sorted(pui, key=pui.get, reverse=True)\n",
    "\n",
    "    # Compute linspace between the 0 to len(item_sorted...)\n",
    "    linspace = np.linspace(0, len(items_sorted_by_value), partition+1)\n",
    "   \n",
    "    # Compute dictionary w, that is, for each item, assigns the average of the puis in the partition it belongs to\n",
    "    i=0\n",
    "    j = 0\n",
    "    while i < len(items_sorted_by_value):\n",
    "                            \n",
    "        avg = 0\n",
    "        start = i\n",
    "        end = i\n",
    "    \n",
    "        while i < len(items_sorted_by_value) and i < linspace[j+1]:\n",
    "            avg += 1.0 / pui[items_sorted_by_value[i]]\n",
    "            end = i\n",
    "            i += 1\n",
    "        \n",
    "        avg = avg / (end - start + 1)\n",
    "\n",
    "        for k in range(start, end+1):\n",
    "            w[items_sorted_by_value[k]] = avg\n",
    "\n",
    "        j += 1\n",
    "\n",
    "    # Compute bias' numerator\n",
    "    bias = 0.0\n",
    "    for k in pui.keys():\n",
    "        # add |pui*w - 1!|\n",
    "        bias += abs(pui[k] * w[k] - 1)\n",
    "    # Multiply by number of users\n",
    "    bias *= len(P[\"users\"])\n",
    "\n",
    "    # Compute concentrations numerator (for each user)\n",
    "    concentrations = {}\n",
    "    max_w = max(w.values())\n",
    "    # ... by computing the sum of squares of w for each user\n",
    "    for user, item in zip(trainset['user_id'], trainset['item_id']):\n",
    "        # Iterate over the trainset to compute the sum of squares for each user\n",
    "        if item in w:\n",
    "            if user not in concentrations:\n",
    "                concentrations[user] = 0\n",
    "            concentrations[user] += w[item] ** 2\n",
    "    # ... and then applying the formula\n",
    "    for user in concentrations:\n",
    "        concentrations[user] = math.sqrt(concentrations[user] * 2 * math.log(2/delta)) + max_w * 7 * math.log(2/delta)\n",
    "    # Now sum all the concentrations\n",
    "    concentration = sum(concentrations.values())\n",
    "\n",
    "    # Compute score with AUC and compute Recall\n",
    "    nonzero_user_count = 0\n",
    "    sum_user_auc = 0.0\n",
    "    sum_user_recall = 0.0\n",
    "    for theuser in P[\"users\"]:\n",
    "        numerator_auc = 0.0\n",
    "        numerator_recall = 0.0\n",
    "        denominator = 0.0\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            # Skip items with null frequency\n",
    "            if  theitem not in Ni:\n",
    "                continue\n",
    "            # Add things to be summed for each item\n",
    "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser])) * w[theitem]\n",
    "            # Add things for recall\n",
    "            if Zui[(theuser, theitem)] < K:\n",
    "                numerator_recall += 1.0 * w[theitem] # spetta\n",
    "            # Increment denominator that the sum must be divided by \n",
    "            denominator += 1 / pui[theitem]\n",
    "\n",
    "\n",
    "        # If there was at least one item for the user, count the user and sum the results\n",
    "        if denominator > 0:\n",
    "            nonzero_user_count += 1\n",
    "            sum_user_auc += numerator_auc / denominator\n",
    "            sum_user_recall += numerator_recall / denominator \n",
    "\n",
    "    return {\n",
    "        \"auc\"       : sum_user_auc / nonzero_user_count, \n",
    "        \"recall\"    : sum_user_recall / nonzero_user_count,\n",
    "        \"bias\"      : bias,\n",
    "        \"concentration\" : concentration\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **EVALUATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "raw_data = dict()\n",
    "raw_data['train_data'] = np.load(output_name + \"training_arr.npy\")\n",
    "raw_data['test_data_pos_biased'] = np.load(output_name + \"biased-test_arr_pos.npy\")\n",
    "raw_data['test_data_neg_biased'] = np.load(output_name + \"biased-test_arr_neg.npy\")\n",
    "raw_data['test_data_pos_unbiased'] = np.load(output_name + \"unbiased-test_arr_pos.npy\")\n",
    "raw_data['test_data_neg_unbiased'] = np.load(output_name + \"unbiased-test_arr_neg.npy\")\n",
    "raw_data['max_user'] = 15401\n",
    "raw_data['max_item'] = 1001\n",
    "batch_size = 8000\n",
    "test_batch_size = 1000\n",
    "display_itr = 1000\n",
    "\n",
    "# Load data\n",
    "train_dataset = ImplicitDataset(raw_data['train_data'], raw_data['max_user'], raw_data['max_item'], name='Train')\n",
    "test_dataset_pos_biased = ImplicitDataset(raw_data['test_data_pos_biased'], raw_data['max_user'], raw_data['max_item'])\n",
    "test_dataset_neg_biased = ImplicitDataset(raw_data['test_data_neg_biased'], raw_data['max_user'], raw_data['max_item'])\n",
    "test_dataset_pos_unbiased = ImplicitDataset(raw_data['test_data_pos_unbiased'], raw_data['max_user'], raw_data['max_item'])\n",
    "test_dataset_neg_unbiased = ImplicitDataset(raw_data['test_data_neg_unbiased'], raw_data['max_user'], raw_data['max_item'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:391: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:31: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:41: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:43: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:33: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/interactions/pairwise_eu_dist.py:71: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:596: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:75: The name tf.scatter_update is deprecated. Please use tf.compat.v1.scatter_update instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:144: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:365: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:148: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 23:39:47.429963: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
      "2024-06-06 23:39:47.432277: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 4192050000 Hz\n",
      "2024-06-06 23:39:47.432855: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5640603ee910 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-06 23:39:47.432870: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./generated_data/cml-yahoo/\n",
      "[Subsampling negative items]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    }
   ],
   "source": [
    "# Prevent tensorflow from using cached embeddings\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "# Define the model\n",
    "model = MODEL_CLASS(batch_size=batch_size, max_user=train_dataset.max_user(), max_item=train_dataset.max_item(), dim_embed=50, l2_reg=0.001, opt='Adam', sess_config=None)\n",
    "sampler = PairwiseSampler(batch_size=batch_size, dataset=train_dataset, num_process=4)\n",
    "model_trainer = ImplicitModelTrainer(batch_size=batch_size, test_batch_size=test_batch_size, train_dataset=train_dataset, model=model, sampler=sampler, eval_save_prefix=OUTPUT_PATH + DATASET_NAME, item_serving_size=500)\n",
    "auc_evaluator = AUC()\n",
    "\n",
    "# Load model\n",
    "model.load(OUTPUT_PATH)\n",
    "\n",
    "# Set parameters\n",
    "model_trainer._eval_manager = ImplicitEvalManager(evaluators=[auc_evaluator])\n",
    "model_trainer._num_negatives = 200\n",
    "model_trainer._exclude_positives([train_dataset, test_dataset_pos_biased, test_dataset_neg_biased])\n",
    "model_trainer._sample_negatives(seed=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biased Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2070/2070 [00:00<00:00, 2661.78it/s]\n",
      "100%|██████████| 2296/2296 [00:24<00:00, 92.95it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AUC': [0.5110847457627118,\n",
       "  0.5187966101694916,\n",
       "  0.4932046979865772,\n",
       "  0.5002181208053691,\n",
       "  0.49484848484848487,\n",
       "  0.4901525423728815,\n",
       "  0.5253691275167786,\n",
       "  0.5481587837837838,\n",
       "  0.5156208053691275,\n",
       "  0.5096632996632997,\n",
       "  0.5333728813559322,\n",
       "  0.5027702702702702,\n",
       "  0.46683417085427137,\n",
       "  0.4884833333333334,\n",
       "  0.5181250000000001,\n",
       "  0.491258389261745,\n",
       "  0.5062040133779264,\n",
       "  0.46937074829931963,\n",
       "  0.49236577181208063,\n",
       "  0.5365151515151516,\n",
       "  0.535685618729097,\n",
       "  0.5333219178082191,\n",
       "  0.5132441471571907,\n",
       "  0.44110738255033555,\n",
       "  0.48285953177257523,\n",
       "  0.4842781690140845,\n",
       "  0.5057903780068729,\n",
       "  0.49501672240802674,\n",
       "  0.4794557823129251,\n",
       "  0.5300505050505051,\n",
       "  0.4695302013422819,\n",
       "  0.4973905723905725,\n",
       "  0.48343749999999996,\n",
       "  0.5170401337792643,\n",
       "  0.46093959731543627,\n",
       "  0.5019463087248321,\n",
       "  0.5332107023411371,\n",
       "  0.47493197278911564,\n",
       "  0.5011241610738254,\n",
       "  0.4914527027027027,\n",
       "  0.48986486486486486,\n",
       "  0.4477413793103448,\n",
       "  0.5466040955631399,\n",
       "  0.5118181818181818,\n",
       "  0.48672818791946315,\n",
       "  0.5012457912457913,\n",
       "  0.5525084175084176,\n",
       "  0.52175,\n",
       "  0.5153187919463087,\n",
       "  0.4693311036789297,\n",
       "  0.5060333333333333,\n",
       "  0.5128451178451179,\n",
       "  0.5073728813559323,\n",
       "  0.5075838926174496,\n",
       "  0.5109417808219178,\n",
       "  0.47981543624161077,\n",
       "  0.4890572390572391,\n",
       "  0.4689655172413793,\n",
       "  0.4974161073825503,\n",
       "  0.5609322033898305,\n",
       "  0.5022558922558923,\n",
       "  0.4906879194630872,\n",
       "  0.48916666666666664,\n",
       "  0.4819191919191919,\n",
       "  0.4690802675585284,\n",
       "  0.5195637583892617,\n",
       "  0.4841077441077441,\n",
       "  0.48298657718120797,\n",
       "  0.4565166666666667,\n",
       "  0.5099665551839465,\n",
       "  0.5167725752508361,\n",
       "  0.5437792642140469,\n",
       "  0.4409259259259259,\n",
       "  0.5153253424657535,\n",
       "  0.4839695945945946,\n",
       "  0.48716949152542366,\n",
       "  0.4560067114093959,\n",
       "  0.46785,\n",
       "  0.49926174496644293,\n",
       "  0.4784006734006733,\n",
       "  0.44324915824915817,\n",
       "  0.4710884353741497,\n",
       "  0.5121979865771812,\n",
       "  0.5089597315436242,\n",
       "  0.49643097643097645,\n",
       "  0.5048821548821548,\n",
       "  0.4597128378378378,\n",
       "  0.5078350515463917,\n",
       "  0.5584290540540541,\n",
       "  0.5141722972972973,\n",
       "  0.5061616161616163,\n",
       "  0.5049664429530201,\n",
       "  0.4420439189189189,\n",
       "  0.4887080536912751,\n",
       "  0.5390909090909092,\n",
       "  0.46577054794520545,\n",
       "  0.5123489932885906,\n",
       "  0.5105852842809364,\n",
       "  0.47675675675675677,\n",
       "  0.5414429530201342,\n",
       "  0.4933445945945946,\n",
       "  0.504708904109589,\n",
       "  0.5067006802721088,\n",
       "  0.5156468531468531,\n",
       "  0.5088,\n",
       "  0.4983898305084745,\n",
       "  0.4880602006688963,\n",
       "  0.49219798657718117,\n",
       "  0.5303166666666668,\n",
       "  0.5127777777777778,\n",
       "  0.474244966442953,\n",
       "  0.48478114478114487,\n",
       "  0.4797474747474748,\n",
       "  0.5094557823129251,\n",
       "  0.5196296296296296,\n",
       "  0.5182166666666665,\n",
       "  0.5159395973154363,\n",
       "  0.49469696969696975,\n",
       "  0.513271186440678,\n",
       "  0.47665,\n",
       "  0.4605387205387205,\n",
       "  0.47966329966329957,\n",
       "  0.5672408026755853,\n",
       "  0.5020637583892618,\n",
       "  0.4946101694915253,\n",
       "  0.5120875420875421,\n",
       "  0.5422558922558922,\n",
       "  0.5184343434343435,\n",
       "  0.5417953020134227,\n",
       "  0.5074664429530202,\n",
       "  0.47617449664429523,\n",
       "  0.47853951890034363,\n",
       "  0.4892033898305085,\n",
       "  0.5062717770034842,\n",
       "  0.5388926174496644,\n",
       "  0.47224832214765106,\n",
       "  0.495929054054054,\n",
       "  0.5198322147651007,\n",
       "  0.48583892617449653,\n",
       "  0.46518707482993193,\n",
       "  0.519543918918919,\n",
       "  0.52921768707483,\n",
       "  0.5327457627118644,\n",
       "  0.52425,\n",
       "  0.5127054794520548,\n",
       "  0.4828428093645485,\n",
       "  0.5133110367892976,\n",
       "  0.5186993243243243,\n",
       "  0.531996644295302,\n",
       "  0.48604026845637577,\n",
       "  0.4950173611111111,\n",
       "  0.5010367892976588,\n",
       "  0.5195973154362417,\n",
       "  0.5068493150684932,\n",
       "  0.5021166666666667,\n",
       "  0.5619500000000001,\n",
       "  0.5019630872483221,\n",
       "  0.4954882154882155,\n",
       "  0.49691919191919187,\n",
       "  0.514679054054054,\n",
       "  0.5177946127946128,\n",
       "  0.4880743243243243,\n",
       "  0.49949324324324323,\n",
       "  0.4626677852348993,\n",
       "  0.4887857142857143,\n",
       "  0.5610101010101011,\n",
       "  0.5323489932885905,\n",
       "  0.4926101694915254,\n",
       "  0.5061204013377927,\n",
       "  0.5341610738255034,\n",
       "  0.46393581081081076,\n",
       "  0.4720578231292516,\n",
       "  0.5155102040816326,\n",
       "  0.49244932432432426,\n",
       "  0.47829391891891887,\n",
       "  0.48165551839464876,\n",
       "  0.4771476510067114,\n",
       "  0.5217953020134228,\n",
       "  0.5333389261744966,\n",
       "  0.48608695652173917,\n",
       "  0.4419830508474576,\n",
       "  0.4877104377104377,\n",
       "  0.5356779661016948,\n",
       "  0.5189898989898989,\n",
       "  0.4963087248322148,\n",
       "  0.4673578595317726,\n",
       "  0.5154026845637584,\n",
       "  0.5151174496644295,\n",
       "  0.44580000000000003,\n",
       "  0.43955000000000005,\n",
       "  0.5385690235690235,\n",
       "  0.550133779264214,\n",
       "  0.46247474747474754,\n",
       "  0.5124745762711864,\n",
       "  0.4574828767123287,\n",
       "  0.552070707070707,\n",
       "  0.4947952218430034,\n",
       "  0.49857382550335566,\n",
       "  0.5224749163879597,\n",
       "  0.5212668918918919,\n",
       "  0.5104026845637584,\n",
       "  0.5620973154362416,\n",
       "  0.5179362416107384,\n",
       "  0.5007833333333334,\n",
       "  0.4685593220338984,\n",
       "  0.47895622895622897,\n",
       "  0.5059290540540541,\n",
       "  0.49309764309764303,\n",
       "  0.4829833333333335,\n",
       "  0.5155067567567567,\n",
       "  0.4520777027027027,\n",
       "  0.5106354515050167,\n",
       "  0.44615384615384607,\n",
       "  0.4792592592592593,\n",
       "  0.5122222222222222,\n",
       "  0.47028428093645486,\n",
       "  0.5154882154882156,\n",
       "  0.5156565656565657,\n",
       "  0.508277027027027,\n",
       "  0.5181879194630873,\n",
       "  0.48103040540540537,\n",
       "  0.5030936454849498,\n",
       "  0.46541666666666665,\n",
       "  0.5009698996655517,\n",
       "  0.5565604026845639,\n",
       "  0.4982263513513514,\n",
       "  0.5146812080536913,\n",
       "  0.511275167785235,\n",
       "  0.48680272108843536,\n",
       "  0.47996632996633,\n",
       "  0.4498805460750853,\n",
       "  0.49441275167785226,\n",
       "  0.4892140468227424,\n",
       "  0.47838926174496643,\n",
       "  0.5111872909698997,\n",
       "  0.42317114093959735,\n",
       "  0.5275250836120401,\n",
       "  0.485704467353952,\n",
       "  0.49758389261744956,\n",
       "  0.5136026936026936,\n",
       "  0.5050503355704697,\n",
       "  0.4915833333333334,\n",
       "  0.5113499999999999,\n",
       "  0.5078187919463086,\n",
       "  0.50385,\n",
       "  0.5094147157190635,\n",
       "  0.4736206896551724,\n",
       "  0.45177852348993286,\n",
       "  0.468271812080537,\n",
       "  0.5138215488215488,\n",
       "  0.5353050847457627,\n",
       "  0.5095238095238095,\n",
       "  0.5386577181208054,\n",
       "  0.5004180602006689,\n",
       "  0.536159169550173,\n",
       "  0.48511666666666664,\n",
       "  0.45,\n",
       "  0.4798494983277592,\n",
       "  0.4954391891891892,\n",
       "  0.4799999999999999,\n",
       "  0.47041666666666665,\n",
       "  0.46486394557823124,\n",
       "  0.4935690235690236,\n",
       "  0.49526936026936025,\n",
       "  0.4713926174496643,\n",
       "  0.5390100671140939,\n",
       "  0.5016333333333334,\n",
       "  0.49369127516778527,\n",
       "  0.5412881355932203,\n",
       "  0.566006711409396,\n",
       "  0.48934343434343425,\n",
       "  0.5046551724137932,\n",
       "  0.5172535211267606,\n",
       "  0.496706081081081,\n",
       "  0.5136779661016949,\n",
       "  0.5368855218855219,\n",
       "  0.5319798657718121,\n",
       "  0.5024832214765101,\n",
       "  0.5018384879725086,\n",
       "  0.5483848797250859,\n",
       "  0.4784013605442176,\n",
       "  0.4986166666666667,\n",
       "  0.5098166666666667,\n",
       "  0.46795986622073577,\n",
       "  0.4760535117056856,\n",
       "  0.46563545150501673,\n",
       "  0.5314478114478115,\n",
       "  0.4755050505050505,\n",
       "  0.49767676767676766,\n",
       "  0.522820945945946,\n",
       "  0.4512541806020067,\n",
       "  0.4777986348122867,\n",
       "  0.5248833333333334,\n",
       "  0.49560810810810807,\n",
       "  0.4539966555183947,\n",
       "  0.43951505016722414,\n",
       "  0.5029362416107382,\n",
       "  0.5113043478260869,\n",
       "  0.5107849829351536,\n",
       "  0.5529264214046823,\n",
       "  0.5048813559322034,\n",
       "  0.5440100671140939,\n",
       "  0.4936993243243243,\n",
       "  0.5202852348993289,\n",
       "  0.441097972972973,\n",
       "  0.5132482993197279,\n",
       "  0.5308885017421603,\n",
       "  0.5326254180602007,\n",
       "  0.46954391891891895,\n",
       "  0.4832312925170068,\n",
       "  0.4816166666666667,\n",
       "  0.5239597315436241,\n",
       "  0.523628762541806,\n",
       "  0.5228260869565217,\n",
       "  0.49615771812080545,\n",
       "  0.44156040268456376,\n",
       "  0.5241258741258742,\n",
       "  0.49090909090909096,\n",
       "  0.49434482758620685,\n",
       "  0.5178231292517007,\n",
       "  0.5049661016949153,\n",
       "  0.5069397993311037,\n",
       "  0.4887585034013605,\n",
       "  0.49045150501672236,\n",
       "  0.5154713804713804,\n",
       "  0.5221114864864865,\n",
       "  0.5224915824915826,\n",
       "  0.4874247491638796,\n",
       "  0.5259166666666666,\n",
       "  0.4996283783783783,\n",
       "  0.5201006711409396,\n",
       "  0.45953177257525074,\n",
       "  0.5432653061224489,\n",
       "  0.5272895622895624,\n",
       "  0.5412755102040816,\n",
       "  0.5155498281786941,\n",
       "  0.46933712121212123,\n",
       "  0.5132491582491583,\n",
       "  0.5269397993311037,\n",
       "  0.4616835016835017,\n",
       "  0.5123389830508475,\n",
       "  0.4767465753424658,\n",
       "  0.4772053872053872,\n",
       "  0.4412883435582822,\n",
       "  0.47110367892976596,\n",
       "  0.4686744966442954,\n",
       "  0.44974832214765104,\n",
       "  0.4686166666666667,\n",
       "  0.5061577181208053,\n",
       "  0.5204194630872484,\n",
       "  0.5280236486486486,\n",
       "  0.5161666666666667,\n",
       "  0.5179137931034482,\n",
       "  0.48842809364548495,\n",
       "  0.5176174496644296,\n",
       "  0.5548103448275862,\n",
       "  0.5071428571428572,\n",
       "  0.47577702702702696,\n",
       "  0.5123411371237458,\n",
       "  0.46521666666666667,\n",
       "  0.4943645484949833,\n",
       "  0.5484228187919463,\n",
       "  0.5903367003367004,\n",
       "  0.5241806020066889,\n",
       "  0.5088813559322034,\n",
       "  0.5129152542372881,\n",
       "  0.49217391304347813,\n",
       "  0.4722818791946309,\n",
       "  0.5052380952380953,\n",
       "  0.5195986622073578,\n",
       "  0.4792760942760943,\n",
       "  0.48998327759197324,\n",
       "  0.49543333333333334,\n",
       "  0.5074242424242424,\n",
       "  0.5170805369127516,\n",
       "  0.5567171717171718,\n",
       "  0.5117391304347826,\n",
       "  0.4960367892976589,\n",
       "  0.48897260273972604,\n",
       "  0.4932943143812709,\n",
       "  0.5121378091872791,\n",
       "  0.6099812734082397,\n",
       "  0.5317676767676768,\n",
       "  0.5248316498316499,\n",
       "  0.5130536912751678,\n",
       "  0.49601398601398605,\n",
       "  0.5259797297297297,\n",
       "  0.4768918918918919,\n",
       "  0.5002210884353742,\n",
       "  0.47885521885521887,\n",
       "  0.4705236486486486,\n",
       "  0.49123333333333336,\n",
       "  0.4991333333333334,\n",
       "  0.5479180887372014,\n",
       "  0.46108695652173914,\n",
       "  0.47698996655518394,\n",
       "  0.4786824324324324,\n",
       "  0.5145652173913045,\n",
       "  0.5225844594594594,\n",
       "  0.498494983277592,\n",
       "  0.48202702702702704,\n",
       "  0.51839222614841,\n",
       "  0.5297315436241611,\n",
       "  0.4827999999999999,\n",
       "  0.5278956228956228,\n",
       "  0.5301174496644296,\n",
       "  0.5488127090301003,\n",
       "  0.5138461538461538,\n",
       "  0.4738888888888889,\n",
       "  0.5024662162162162,\n",
       "  0.46991525423728814,\n",
       "  0.4693050847457627,\n",
       "  0.44223728813559315,\n",
       "  0.4069565217391304,\n",
       "  0.5058333333333334,\n",
       "  0.5142542372881356,\n",
       "  0.5133221476510067,\n",
       "  0.46510344827586203,\n",
       "  0.5256354515050167,\n",
       "  0.5098813559322034,\n",
       "  0.4911643835616439,\n",
       "  0.49692708333333324,\n",
       "  0.5311111111111111,\n",
       "  0.5291778523489933,\n",
       "  0.4904054054054054,\n",
       "  0.5217056856187291,\n",
       "  0.5044798657718121,\n",
       "  0.5015939597315437,\n",
       "  0.4944463087248322,\n",
       "  0.5491525423728814,\n",
       "  0.4953214285714286,\n",
       "  0.5124247491638796,\n",
       "  0.4805892255892256,\n",
       "  0.5163804713804713,\n",
       "  0.4683898305084746,\n",
       "  0.46961672473867594,\n",
       "  0.46745819397993316,\n",
       "  0.5360357142857142,\n",
       "  0.48527118644067796,\n",
       "  0.5212289562289563,\n",
       "  0.5345659722222221,\n",
       "  0.499251700680272,\n",
       "  0.5317736486486486,\n",
       "  0.5019191919191919,\n",
       "  0.48110367892976585,\n",
       "  0.496493288590604,\n",
       "  0.4420608108108108,\n",
       "  0.5301677852348994,\n",
       "  0.48164983164983166,\n",
       "  0.516280276816609,\n",
       "  0.5220302013422818,\n",
       "  0.4695484949832776,\n",
       "  0.49078859060402685,\n",
       "  0.48047619047619045,\n",
       "  0.4915371621621622,\n",
       "  0.46950847457627115,\n",
       "  0.4996333333333334,\n",
       "  0.45775510204081626,\n",
       "  0.517135593220339,\n",
       "  0.4841077441077441,\n",
       "  0.5039666666666667,\n",
       "  0.481541095890411,\n",
       "  0.540291095890411,\n",
       "  0.5075671140939597,\n",
       "  0.4904013377926421,\n",
       "  0.47273489932885904,\n",
       "  0.5064067796610169,\n",
       "  0.5117508417508417,\n",
       "  0.46060606060606063,\n",
       "  0.5297804054054054,\n",
       "  0.5383221476510067,\n",
       "  0.4783156028368794,\n",
       "  0.5093774319066148,\n",
       "  0.538628762541806,\n",
       "  0.5002861952861952,\n",
       "  0.50736301369863,\n",
       "  0.5440268456375839,\n",
       "  0.4566101694915255,\n",
       "  0.484933110367893,\n",
       "  0.5103500000000001,\n",
       "  0.5281818181818182,\n",
       "  0.4872909698996655,\n",
       "  0.45762541806020063,\n",
       "  0.5495117845117845,\n",
       "  0.5197789115646259,\n",
       "  0.49586440677966104,\n",
       "  0.5155952380952381,\n",
       "  0.4866047297297298,\n",
       "  0.5372333333333333,\n",
       "  0.4629933110367893,\n",
       "  0.538628762541806,\n",
       "  0.4923801369863013,\n",
       "  0.4878787878787878,\n",
       "  0.5421140939597315,\n",
       "  0.4874080267558529,\n",
       "  0.4752721088435375,\n",
       "  0.4891666666666667,\n",
       "  0.5167676767676768,\n",
       "  0.5291973244147156,\n",
       "  0.49026666666666663,\n",
       "  0.49397993311036786,\n",
       "  0.5648833333333333,\n",
       "  0.4672128378378379,\n",
       "  0.5427348993288591,\n",
       "  0.4932943143812709,\n",
       "  0.5353187919463087,\n",
       "  0.5261279461279461,\n",
       "  0.4779010238907849,\n",
       "  0.4963210702341137,\n",
       "  0.5441778523489934,\n",
       "  0.5123400673400673,\n",
       "  0.5367747440273039,\n",
       "  0.5148327759197323,\n",
       "  0.5115202702702704,\n",
       "  0.4943288590604027,\n",
       "  0.4979865771812081,\n",
       "  0.49366438356164377,\n",
       "  0.4910738255033558,\n",
       "  0.5281481481481481,\n",
       "  0.481089965397924,\n",
       "  0.47806666666666664,\n",
       "  0.5052666666666666,\n",
       "  0.4982166666666667,\n",
       "  0.4943666666666667,\n",
       "  0.4782770270270271,\n",
       "  0.5209197324414716,\n",
       "  0.5187248322147651,\n",
       "  0.5102842809364548,\n",
       "  0.4805369127516778,\n",
       "  0.5150503355704699,\n",
       "  0.5079065743944636,\n",
       "  0.5287244897959184,\n",
       "  0.4834166666666666,\n",
       "  0.44692176870748296,\n",
       "  0.5140740740740741,\n",
       "  0.4806375838926174,\n",
       "  0.5139597315436243,\n",
       "  0.47312499999999996,\n",
       "  0.47890784982935153,\n",
       "  0.5109731543624161,\n",
       "  0.5025337837837838,\n",
       "  0.507281879194631,\n",
       "  0.4890939597315436,\n",
       "  0.5511447811447812,\n",
       "  0.5128859060402685,\n",
       "  0.4983728813559322,\n",
       "  0.49954849498327764,\n",
       "  0.4623559322033898,\n",
       "  0.47978260869565215,\n",
       "  0.5543645484949834,\n",
       "  0.5268729096989966,\n",
       "  0.43819256756756764,\n",
       "  0.5035570469798658,\n",
       "  0.5319230769230769,\n",
       "  0.4905872483221476,\n",
       "  0.49476430976430974,\n",
       "  0.4650844594594594,\n",
       "  0.4951851851851852,\n",
       "  0.46644781144781144,\n",
       "  0.4799829351535836,\n",
       "  0.4508108108108108,\n",
       "  0.4931632653061225,\n",
       "  0.5035016835016835,\n",
       "  0.4695637583892618,\n",
       "  0.47130584192439856,\n",
       "  0.5117892976588629,\n",
       "  0.5285906040268455,\n",
       "  0.5324328859060402,\n",
       "  0.4931399317406144,\n",
       "  0.49578859060402686,\n",
       "  0.46310810810810804,\n",
       "  0.5391666666666668,\n",
       "  0.4824242424242423,\n",
       "  0.4763590604026846,\n",
       "  0.48127551020408166,\n",
       "  0.5208892617449665,\n",
       "  0.47433447098976117,\n",
       "  0.4747222222222222,\n",
       "  0.49226351351351344,\n",
       "  0.5592491467576792,\n",
       "  0.4538833333333333,\n",
       "  0.5191638795986623,\n",
       "  0.5109030100334447,\n",
       "  0.5260535117056856,\n",
       "  0.4938095238095238,\n",
       "  0.5256587837837838,\n",
       "  0.48772575250836114,\n",
       "  0.4299662162162162,\n",
       "  0.5052173913043478,\n",
       "  0.5332775919732441,\n",
       "  0.4984183673469388,\n",
       "  0.5002356902356903,\n",
       "  0.4955405405405406,\n",
       "  0.5090957446808511,\n",
       "  0.5714597315436242,\n",
       "  0.5373648648648649,\n",
       "  0.4965939597315437,\n",
       "  0.4749665551839464,\n",
       "  0.5376883561643836,\n",
       "  0.5087373737373737,\n",
       "  0.4888833333333334,\n",
       "  0.474041095890411,\n",
       "  0.4937710437710437,\n",
       "  0.4768729096989966,\n",
       "  0.4672727272727273,\n",
       "  0.5082107023411371,\n",
       "  0.48843333333333333,\n",
       "  0.4578691275167785,\n",
       "  0.4884228187919463,\n",
       "  0.508108108108108,\n",
       "  0.5181313131313131,\n",
       "  0.5418060200668896,\n",
       "  0.5324166666666666,\n",
       "  0.41931438127090304,\n",
       "  0.5145819397993312,\n",
       "  0.5083946488294314,\n",
       "  0.5163210702341137,\n",
       "  0.5035067114093961,\n",
       "  0.4756972789115646,\n",
       "  0.5016725978647687,\n",
       "  0.47234006734006734,\n",
       "  0.520016835016835,\n",
       "  0.5142281879194631,\n",
       "  0.48819256756756757,\n",
       "  0.5028523489932886,\n",
       "  0.4955183946488295,\n",
       "  0.4862372881355932,\n",
       "  0.5207859531772574,\n",
       "  0.49251666666666666,\n",
       "  0.5458724832214765,\n",
       "  0.5132214765100671,\n",
       "  0.5317567567567567,\n",
       "  0.5409427609427611,\n",
       "  0.4689261744966443,\n",
       "  0.4869405594405594,\n",
       "  0.541571906354515,\n",
       "  0.4966666666666666,\n",
       "  0.5367517006802721,\n",
       "  0.5402842809364548,\n",
       "  0.511936026936027,\n",
       "  0.5063131313131313,\n",
       "  0.4613527397260274,\n",
       "  0.4692307692307692,\n",
       "  0.4995344827586207,\n",
       "  0.47357859531772584,\n",
       "  0.4487333333333334,\n",
       "  0.512312925170068,\n",
       "  0.501813559322034,\n",
       "  0.5166271186440679,\n",
       "  0.5136993243243243,\n",
       "  0.4802842809364549,\n",
       "  0.5055593220338983,\n",
       "  0.5121380471380472,\n",
       "  0.48164983164983166,\n",
       "  0.5206418918918918,\n",
       "  0.5155821917808219,\n",
       "  0.48511784511784517,\n",
       "  0.5145454545454545,\n",
       "  0.5462876254180602,\n",
       "  0.4744067796610169,\n",
       "  0.490580204778157,\n",
       "  0.4971598639455783,\n",
       "  0.5217845117845119,\n",
       "  0.5138422818791947,\n",
       "  0.49964285714285717,\n",
       "  0.4755479452054795,\n",
       "  0.5186655405405405,\n",
       "  0.5058528428093646,\n",
       "  0.48759322033898317,\n",
       "  0.5105387205387205,\n",
       "  0.4871,\n",
       "  0.503843537414966,\n",
       "  0.5395454545454546,\n",
       "  0.4669557823129253,\n",
       "  0.46712374581939803,\n",
       "  0.5182943143812709,\n",
       "  0.5136622073578595,\n",
       "  0.4955612244897959,\n",
       "  0.5435979729729729,\n",
       "  0.4977257525083612,\n",
       "  0.46645973154362413,\n",
       "  0.4744833333333333,\n",
       "  0.5010738255033557,\n",
       "  0.5178571428571429,\n",
       "  0.4953846153846154,\n",
       "  0.5148817567567568,\n",
       "  0.5147993311036789,\n",
       "  0.5172145328719724,\n",
       "  0.5283724832214766,\n",
       "  0.5208389261744966,\n",
       "  0.49929999999999997,\n",
       "  0.509496644295302,\n",
       "  0.5384175084175085,\n",
       "  0.5408000000000001,\n",
       "  0.5067676767676768,\n",
       "  0.5303378378378378,\n",
       "  0.44656666666666656,\n",
       "  0.5145500000000001,\n",
       "  0.503561872909699,\n",
       "  0.5220608108108108,\n",
       "  0.5034641638225256,\n",
       "  0.4676006711409396,\n",
       "  0.5201016949152543,\n",
       "  0.4568027210884354,\n",
       "  0.5317697594501718,\n",
       "  0.4758952702702703,\n",
       "  0.5140635451505016,\n",
       "  0.4936394557823129,\n",
       "  0.522431506849315,\n",
       "  0.501728187919463,\n",
       "  0.5127348993288591,\n",
       "  0.42189338235294116,\n",
       "  0.5475752508361205,\n",
       "  0.5001851851851852,\n",
       "  0.4949158249158249,\n",
       "  0.5023244147157191,\n",
       "  0.44671186440677974,\n",
       "  0.5044256756756756,\n",
       "  0.46606666666666663,\n",
       "  0.4956999999999999,\n",
       "  0.4518350168350168,\n",
       "  0.48172881355932207,\n",
       "  0.5261340206185566,\n",
       "  0.49018394648829433,\n",
       "  0.5261993243243243,\n",
       "  0.4909491525423729,\n",
       "  0.5097635135135135,\n",
       "  0.5058333333333334,\n",
       "  0.49059322033898295,\n",
       "  0.4923801369863013,\n",
       "  0.4950354609929078,\n",
       "  0.4472241992882562,\n",
       "  0.44374137931034485,\n",
       "  0.4808389261744967,\n",
       "  0.5176588628762541,\n",
       "  0.506962457337884,\n",
       "  0.5139464882943144,\n",
       "  0.5594406779661016,\n",
       "  0.5450847457627119,\n",
       "  0.4913898305084745,\n",
       "  0.5031040268456376,\n",
       "  0.5267892976588628,\n",
       "  0.5177104377104378,\n",
       "  0.48047457627118645,\n",
       "  0.438,\n",
       "  0.4891304347826087,\n",
       "  0.45731543624161075,\n",
       "  0.5234237288135594,\n",
       "  0.500457627118644,\n",
       "  0.503277027027027,\n",
       "  0.4616723549488055,\n",
       "  0.5278694158075602,\n",
       "  0.5110787671232877,\n",
       "  0.4920538720538721,\n",
       "  0.43674496644295296,\n",
       "  0.5177891156462585,\n",
       "  0.5222013651877133,\n",
       "  0.5340102389078497,\n",
       "  0.44089527027027026,\n",
       "  0.5242976588628762,\n",
       "  0.5083166666666666,\n",
       "  0.5213240418118467,\n",
       "  0.45908163265306134,\n",
       "  0.5268835616438357,\n",
       "  0.5003678929765887,\n",
       "  0.4613389830508475,\n",
       "  0.4850335570469798,\n",
       "  0.5005574912891986,\n",
       "  0.4632666666666667,\n",
       "  0.4976678445229682,\n",
       "  0.5043992932862191,\n",
       "  0.5023898305084745,\n",
       "  0.5240301003344482,\n",
       "  0.4713377926421405,\n",
       "  0.5188175675675676,\n",
       "  0.5132775919732442,\n",
       "  0.5256521739130435,\n",
       "  0.5505912162162162,\n",
       "  0.5270608108108108,\n",
       "  0.47271186440677965,\n",
       "  0.5139057239057239,\n",
       "  0.45650519031141873,\n",
       "  0.49885000000000007,\n",
       "  0.45571906354515046,\n",
       "  0.503078231292517,\n",
       "  0.5282931034482758,\n",
       "  0.49094276094276096,\n",
       "  0.48538983050847456,\n",
       "  0.5215656565656566,\n",
       "  0.46603678929765885,\n",
       "  0.540066889632107,\n",
       "  0.5349161073825505,\n",
       "  0.4871812080536913,\n",
       "  0.5224237288135594,\n",
       "  0.5104026845637584,\n",
       "  0.5217785234899329,\n",
       "  0.49149328859060404,\n",
       "  0.5215384615384615,\n",
       "  0.5230808080808081,\n",
       "  0.49717171717171715,\n",
       "  0.5217171717171717,\n",
       "  0.5314163822525597,\n",
       "  0.5145819397993312,\n",
       "  0.540819397993311,\n",
       "  0.49705263157894736,\n",
       "  0.5165033783783785,\n",
       "  0.4690693430656935,\n",
       "  0.47876254180602007,\n",
       "  0.4994127516778523,\n",
       "  0.45675585284280934,\n",
       "  0.5151194539249146,\n",
       "  0.5448141891891891,\n",
       "  0.48474916387959865,\n",
       "  0.4907705479452055,\n",
       "  0.48499999999999993,\n",
       "  0.5391326530612245,\n",
       "  0.5031487889273356,\n",
       "  0.4855423728813559,\n",
       "  0.4767398648648649,\n",
       "  0.4669666666666666,\n",
       "  0.5157770270270271,\n",
       "  0.49327181208053694,\n",
       "  0.5244816053511705,\n",
       "  0.5086026936026936,\n",
       "  0.5026006711409396,\n",
       "  0.4688682432432432,\n",
       "  0.4786241610738255,\n",
       "  0.4962627986348124,\n",
       "  0.5015371621621622,\n",
       "  0.4554208754208754,\n",
       "  0.44996655518394657,\n",
       "  0.5188215488215489,\n",
       "  0.46708474576271186,\n",
       "  0.4893311036789298,\n",
       "  0.4754720279720279,\n",
       "  0.46900337837837835,\n",
       "  0.5003355704697987,\n",
       "  0.4777,\n",
       "  0.4844932432432432,\n",
       "  0.4812626262626263,\n",
       "  0.535925925925926,\n",
       "  0.5038461538461537,\n",
       "  0.47031772575250835,\n",
       "  0.5172222222222222,\n",
       "  0.4993939393939394,\n",
       "  0.5075932203389831,\n",
       "  0.5238344594594595,\n",
       "  0.4809333333333333,\n",
       "  0.4969852941176471,\n",
       "  0.4973154362416107,\n",
       "  0.45377926421404685,\n",
       "  0.5231864406779662,\n",
       "  0.48481481481481487,\n",
       "  0.46983333333333327,\n",
       "  0.5244087837837837,\n",
       "  0.48362711864406777,\n",
       "  0.49551666666666666,\n",
       "  0.48734899328859055,\n",
       "  0.498210702341137,\n",
       "  0.49515570934256053,\n",
       "  0.5250344827586206,\n",
       "  0.5276588628762542,\n",
       "  0.49347826086956526,\n",
       "  0.5270033670033669,\n",
       "  0.4670719178082192,\n",
       "  0.5317785234899329,\n",
       "  0.5369491525423729,\n",
       "  0.4878474576271186,\n",
       "  0.5132705479452054,\n",
       "  0.4811073825503356,\n",
       "  0.46703333333333336,\n",
       "  0.5222033898305085,\n",
       "  0.4660942760942761,\n",
       "  0.5322602739726028,\n",
       "  0.4829833333333334,\n",
       "  0.5036744966442953,\n",
       "  0.506,\n",
       "  0.523429054054054,\n",
       "  0.4594557823129251,\n",
       "  0.5112919463087249,\n",
       "  0.5226510067114094,\n",
       "  0.5191924398625429,\n",
       "  0.5308163265306123,\n",
       "  0.4782491582491582,\n",
       "  0.5038628762541806,\n",
       "  0.5019865319865321,\n",
       "  0.4872297297297297,\n",
       "  0.48424749163879605,\n",
       "  0.5229757785467127,\n",
       "  0.5199322033898305,\n",
       "  0.5071833333333334,\n",
       "  0.4918561872909699,\n",
       "  0.5258080808080808,\n",
       "  0.49377551020408167,\n",
       "  0.47859322033898305,\n",
       "  0.48994880546075087,\n",
       "  0.4433724832214765,\n",
       "  0.5652380952380952,\n",
       "  0.5117736486486487,\n",
       "  0.5017123287671232,\n",
       "  0.4692465753424658,\n",
       "  0.5456866197183098,\n",
       "  0.5070166666666668,\n",
       "  0.5237162162162162,\n",
       "  0.5035353535353536,\n",
       "  0.5204295532646048,\n",
       "  0.4971,\n",
       "  0.5395,\n",
       "  0.4969256756756757,\n",
       "  0.49895622895622893,\n",
       "  0.48994932432432425,\n",
       "  0.5406060606060606,\n",
       "  0.5151525423728812,\n",
       "  0.5183972125435539,\n",
       "  0.4508389261744966,\n",
       "  0.49266778523489935,\n",
       "  0.4880308219178082,\n",
       "  0.48180555555555554,\n",
       "  0.5019528619528619,\n",
       "  0.5032154882154881,\n",
       "  0.5284395973154362,\n",
       "  0.524222972972973,\n",
       "  0.4715604026845637,\n",
       "  0.49511864406779654,\n",
       "  0.5284641638225256,\n",
       "  0.5140740740740741,\n",
       "  0.4821499999999999,\n",
       "  0.4870205479452055,\n",
       "  0.49077966101694925,\n",
       "  0.44656565656565655,\n",
       "  0.4668262411347518,\n",
       "  0.5038851351351351,\n",
       "  0.5287999999999999,\n",
       "  0.5219833333333334,\n",
       "  0.5558361204013378,\n",
       "  0.463741610738255,\n",
       "  0.4654333333333333,\n",
       "  0.4931164383561644,\n",
       "  0.48269230769230764,\n",
       "  0.4914505119453925,\n",
       "  0.5682046979865771,\n",
       "  0.48643581081081083,\n",
       "  0.5253666666666668,\n",
       "  0.42636518771331056,\n",
       "  0.42949664429530204,\n",
       "  0.44416949152542373,\n",
       "  0.5089115646258504,\n",
       "  0.5115384615384615,\n",
       "  0.5129461279461279,\n",
       "  0.509006734006734,\n",
       "  0.5017617449664429,\n",
       "  0.5397147651006712,\n",
       "  0.5084237288135594,\n",
       "  0.5032491582491582,\n",
       "  0.4565172413793103,\n",
       "  0.481761744966443,\n",
       "  0.527820945945946,\n",
       "  0.4879,\n",
       "  0.46576013513513514,\n",
       "  0.5162666666666668,\n",
       "  0.47070397111913354,\n",
       "  0.501023890784983,\n",
       "  0.46066101694915257,\n",
       "  0.4831649831649832,\n",
       "  0.4593166666666667,\n",
       "  0.4797324414715719,\n",
       "  0.48540404040404045,\n",
       "  0.49344594594594593,\n",
       "  0.5032203389830509,\n",
       "  0.4904898648648649,\n",
       "  0.5001023890784982,\n",
       "  0.5352833333333333,\n",
       "  0.49770903010033446,\n",
       "  0.5170538720538721,\n",
       "  0.4717508417508418,\n",
       "  0.4929898648648649,\n",
       "  0.5195791245791246,\n",
       "  0.531571906354515,\n",
       "  0.5124221453287197,\n",
       "  0.4955033557046979,\n",
       "  0.49861952861952863,\n",
       "  0.48961538461538456,\n",
       "  0.5697735191637631,\n",
       "  0.5014020270270271,\n",
       "  0.45251677852349,\n",
       "  0.5196061643835617,\n",
       "  0.5033503401360545,\n",
       "  0.5503198653198653,\n",
       "  0.5351700680272109,\n",
       "  0.49738095238095237,\n",
       "  0.5311744966442953,\n",
       "  0.516418918918919,\n",
       "  0.4607560137457044,\n",
       "  0.45398305084745766,\n",
       "  0.4554898648648648,\n",
       "  0.5088775510204081,\n",
       "  0.45003412969283274,\n",
       "  0.5386,\n",
       "  0.5540878378378379,\n",
       "  0.5079697986577181,\n",
       "  ...]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_trainer._eval_save_prefix = OUTPUT_PREFIX + \"-test-pos-biased\"\n",
    "model_trainer._evaluate_partial(test_dataset_pos_biased)\n",
    "\n",
    "model_trainer._eval_save_prefix = OUTPUT_PREFIX +  \"-test-neg-biased\"\n",
    "model_trainer._evaluate_partial(test_dataset_neg_biased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unbiased Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2296/2296 [00:00<00:00, 3167.91it/s]\n",
      "100%|██████████| 2296/2296 [00:01<00:00, 1672.94it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AUC': [0.578888888888889,\n",
       "  0.533,\n",
       "  0.5658333333333333,\n",
       "  0.5166666666666666,\n",
       "  0.4444444444444444,\n",
       "  0.5314285714285715,\n",
       "  0.48583333333333334,\n",
       "  0.5485714285714286,\n",
       "  0.4475,\n",
       "  0.4822222222222222,\n",
       "  0.530625,\n",
       "  0.4342857142857142,\n",
       "  0.5364285714285714,\n",
       "  0.423125,\n",
       "  0.459375,\n",
       "  0.5261111111111112,\n",
       "  0.4211111111111111,\n",
       "  0.548125,\n",
       "  0.5285714285714286,\n",
       "  0.5422222222222223,\n",
       "  0.46499999999999997,\n",
       "  0.4641666666666666,\n",
       "  0.4083333333333334,\n",
       "  0.489375,\n",
       "  0.6581250000000001,\n",
       "  0.37625,\n",
       "  0.66,\n",
       "  0.44062500000000004,\n",
       "  0.349375,\n",
       "  0.5822222222222222,\n",
       "  0.3392857142857143,\n",
       "  0.5233333333333333,\n",
       "  0.47777777777777775,\n",
       "  0.5327777777777778,\n",
       "  0.42500000000000004,\n",
       "  0.6450000000000001,\n",
       "  0.7324999999999999,\n",
       "  0.45500000000000007,\n",
       "  0.4527777777777778,\n",
       "  0.3664285714285715,\n",
       "  0.5175,\n",
       "  0.38749999999999996,\n",
       "  0.599375,\n",
       "  0.627,\n",
       "  0.4361111111111111,\n",
       "  0.48625,\n",
       "  0.6478571428571428,\n",
       "  0.3116666666666667,\n",
       "  0.5027777777777778,\n",
       "  0.4211111111111111,\n",
       "  0.5188888888888888,\n",
       "  0.548,\n",
       "  0.3916666666666666,\n",
       "  0.38000000000000006,\n",
       "  0.4,\n",
       "  0.5121428571428571,\n",
       "  0.60125,\n",
       "  0.6975,\n",
       "  0.5755555555555556,\n",
       "  0.44799999999999995,\n",
       "  0.5716666666666667,\n",
       "  0.6572222222222224,\n",
       "  0.4600000000000001,\n",
       "  0.5135714285714286,\n",
       "  0.4861111111111111,\n",
       "  0.6922222222222223,\n",
       "  0.5105555555555555,\n",
       "  0.5631249999999999,\n",
       "  0.39944444444444444,\n",
       "  0.48416666666666663,\n",
       "  0.5405555555555555,\n",
       "  0.44937499999999997,\n",
       "  0.36833333333333335,\n",
       "  0.295,\n",
       "  0.371,\n",
       "  0.4116666666666667,\n",
       "  0.38000000000000006,\n",
       "  0.47333333333333333,\n",
       "  0.4822222222222222,\n",
       "  0.4325,\n",
       "  0.483125,\n",
       "  0.6255555555555555,\n",
       "  0.391875,\n",
       "  0.34624999999999995,\n",
       "  0.5694444444444444,\n",
       "  0.49444444444444446,\n",
       "  0.47500000000000003,\n",
       "  0.453125,\n",
       "  0.536875,\n",
       "  0.46125,\n",
       "  0.5366666666666666,\n",
       "  0.5544444444444445,\n",
       "  0.46312500000000006,\n",
       "  0.47277777777777774,\n",
       "  0.7016666666666667,\n",
       "  0.375,\n",
       "  0.5238888888888888,\n",
       "  0.6544444444444444,\n",
       "  0.42000000000000004,\n",
       "  0.445,\n",
       "  0.41312499999999996,\n",
       "  0.5105555555555555,\n",
       "  0.46722222222222215,\n",
       "  0.43833333333333335,\n",
       "  0.5561111111111112,\n",
       "  0.5983333333333333,\n",
       "  0.635,\n",
       "  0.316875,\n",
       "  0.610625,\n",
       "  0.44,\n",
       "  0.49222222222222217,\n",
       "  0.40444444444444444,\n",
       "  0.3572222222222222,\n",
       "  0.395625,\n",
       "  0.56,\n",
       "  0.5231250000000001,\n",
       "  0.436875,\n",
       "  0.5366666666666666,\n",
       "  0.5137499999999999,\n",
       "  0.3685714285714286,\n",
       "  0.32611111111111113,\n",
       "  0.5172222222222222,\n",
       "  0.5094444444444445,\n",
       "  0.5812499999999999,\n",
       "  0.4694444444444444,\n",
       "  0.43,\n",
       "  0.47214285714285714,\n",
       "  0.4372222222222222,\n",
       "  0.42687499999999995,\n",
       "  0.47285714285714286,\n",
       "  0.6211111111111111,\n",
       "  0.48375,\n",
       "  0.4322222222222223,\n",
       "  0.536875,\n",
       "  0.480625,\n",
       "  0.33166666666666667,\n",
       "  0.425,\n",
       "  0.5475,\n",
       "  0.4516666666666667,\n",
       "  0.588888888888889,\n",
       "  0.59,\n",
       "  0.5794444444444444,\n",
       "  0.645,\n",
       "  0.5449999999999999,\n",
       "  0.5394444444444444,\n",
       "  0.345,\n",
       "  0.5616666666666666,\n",
       "  0.4822222222222222,\n",
       "  0.35187500000000005,\n",
       "  0.3977777777777778,\n",
       "  0.6016666666666667,\n",
       "  0.5283333333333333,\n",
       "  0.5118750000000001,\n",
       "  0.499375,\n",
       "  0.47111111111111115,\n",
       "  0.5078571428571428,\n",
       "  0.435,\n",
       "  0.41277777777777774,\n",
       "  0.4225,\n",
       "  0.47000000000000003,\n",
       "  0.27722222222222226,\n",
       "  0.48416666666666663,\n",
       "  0.5427777777777778,\n",
       "  0.355,\n",
       "  0.4799999999999999,\n",
       "  0.64,\n",
       "  0.5014285714285714,\n",
       "  0.21750000000000003,\n",
       "  0.5966666666666666,\n",
       "  0.339375,\n",
       "  0.43250000000000005,\n",
       "  0.4494444444444444,\n",
       "  0.374375,\n",
       "  0.3638888888888889,\n",
       "  0.633888888888889,\n",
       "  0.3557142857142857,\n",
       "  0.35187500000000005,\n",
       "  0.5449999999999999,\n",
       "  0.46222222222222226,\n",
       "  0.5366666666666666,\n",
       "  0.49125,\n",
       "  0.540625,\n",
       "  0.42714285714285716,\n",
       "  0.5892857142857143,\n",
       "  0.4692857142857143,\n",
       "  0.32944444444444443,\n",
       "  0.5205555555555557,\n",
       "  0.55625,\n",
       "  0.4766666666666667,\n",
       "  0.5866666666666666,\n",
       "  0.45312500000000006,\n",
       "  0.3911111111111112,\n",
       "  0.311875,\n",
       "  0.37666666666666665,\n",
       "  0.3442857142857143,\n",
       "  0.4625,\n",
       "  0.4658333333333333,\n",
       "  0.463125,\n",
       "  0.35833333333333334,\n",
       "  0.7268749999999999,\n",
       "  0.48888888888888893,\n",
       "  0.50875,\n",
       "  0.43111111111111117,\n",
       "  0.6661111111111112,\n",
       "  0.4694444444444444,\n",
       "  0.43,\n",
       "  0.25166666666666665,\n",
       "  0.49666666666666665,\n",
       "  0.49,\n",
       "  0.385,\n",
       "  0.5016666666666667,\n",
       "  0.4555555555555555,\n",
       "  0.405,\n",
       "  0.6118750000000001,\n",
       "  0.4555555555555555,\n",
       "  0.465625,\n",
       "  0.5205555555555557,\n",
       "  0.48000000000000004,\n",
       "  0.5594444444444444,\n",
       "  0.48500000000000004,\n",
       "  0.41222222222222227,\n",
       "  0.49666666666666676,\n",
       "  0.48555555555555546,\n",
       "  0.6083333333333334,\n",
       "  0.4727777777777777,\n",
       "  0.34333333333333327,\n",
       "  0.6672222222222222,\n",
       "  0.4811111111111111,\n",
       "  0.35142857142857137,\n",
       "  0.44,\n",
       "  0.28,\n",
       "  0.31722222222222224,\n",
       "  0.41888888888888887,\n",
       "  0.6172222222222222,\n",
       "  0.38555555555555554,\n",
       "  0.6033333333333333,\n",
       "  0.32111111111111107,\n",
       "  0.37214285714285716,\n",
       "  0.5306249999999999,\n",
       "  0.5605555555555556,\n",
       "  0.4575,\n",
       "  0.3794444444444445,\n",
       "  0.7991666666666667,\n",
       "  0.43125,\n",
       "  0.44333333333333336,\n",
       "  0.7078571428571429,\n",
       "  0.49,\n",
       "  0.32642857142857146,\n",
       "  0.510625,\n",
       "  0.541,\n",
       "  0.46187500000000004,\n",
       "  0.6177777777777779,\n",
       "  0.6205555555555556,\n",
       "  0.4999999999999999,\n",
       "  0.47111111111111115,\n",
       "  0.5838888888888889,\n",
       "  0.5205555555555555,\n",
       "  0.4588888888888889,\n",
       "  0.47928571428571437,\n",
       "  0.36000000000000004,\n",
       "  0.3933333333333333,\n",
       "  0.49214285714285727,\n",
       "  0.48375,\n",
       "  0.5211111111111111,\n",
       "  0.4338888888888889,\n",
       "  0.5522222222222223,\n",
       "  0.46444444444444444,\n",
       "  0.3738888888888889,\n",
       "  0.5116666666666666,\n",
       "  0.5044444444444445,\n",
       "  0.3725,\n",
       "  0.37777777777777777,\n",
       "  0.29388888888888887,\n",
       "  0.47928571428571426,\n",
       "  0.5227777777777779,\n",
       "  0.4264285714285715,\n",
       "  0.655,\n",
       "  0.6961111111111111,\n",
       "  0.49388888888888893,\n",
       "  0.685,\n",
       "  0.6031249999999999,\n",
       "  0.5166666666666667,\n",
       "  0.36666666666666664,\n",
       "  0.4575,\n",
       "  0.2842857142857143,\n",
       "  0.48000000000000004,\n",
       "  0.515625,\n",
       "  0.45125000000000004,\n",
       "  0.40611111111111114,\n",
       "  0.40055555555555555,\n",
       "  0.36375,\n",
       "  0.4527777777777778,\n",
       "  0.4511111111111111,\n",
       "  0.40722222222222226,\n",
       "  0.61125,\n",
       "  0.32555555555555554,\n",
       "  0.330625,\n",
       "  0.7128571428571429,\n",
       "  0.6666666666666665,\n",
       "  0.54625,\n",
       "  0.4007142857142857,\n",
       "  0.5625,\n",
       "  0.435,\n",
       "  0.715,\n",
       "  0.538125,\n",
       "  0.5862499999999999,\n",
       "  0.40888888888888886,\n",
       "  0.5388888888888889,\n",
       "  0.44125,\n",
       "  0.4633333333333333,\n",
       "  0.5255555555555556,\n",
       "  0.6692857142857143,\n",
       "  0.595,\n",
       "  0.5756249999999999,\n",
       "  0.42055555555555557,\n",
       "  0.29555555555555557,\n",
       "  0.35250000000000004,\n",
       "  0.6655555555555556,\n",
       "  0.4744444444444444,\n",
       "  0.36444444444444446,\n",
       "  0.52375,\n",
       "  0.461111111111111,\n",
       "  0.2758333333333333,\n",
       "  0.43857142857142856,\n",
       "  0.48357142857142854,\n",
       "  0.49833333333333335,\n",
       "  0.405,\n",
       "  0.37250000000000005,\n",
       "  0.7075,\n",
       "  0.41333333333333333,\n",
       "  0.3977777777777778,\n",
       "  0.4222222222222222,\n",
       "  0.49555555555555564,\n",
       "  0.5605555555555556,\n",
       "  0.6238888888888888,\n",
       "  0.42857142857142855,\n",
       "  0.24600000000000005,\n",
       "  0.499375,\n",
       "  0.4338888888888889,\n",
       "  0.3838888888888889,\n",
       "  0.68375,\n",
       "  0.54,\n",
       "  0.39,\n",
       "  0.63,\n",
       "  0.3716666666666667,\n",
       "  0.47928571428571426,\n",
       "  0.4694444444444444,\n",
       "  0.5244444444444444,\n",
       "  0.4383333333333333,\n",
       "  0.5811111111111111,\n",
       "  0.629375,\n",
       "  0.5733333333333334,\n",
       "  0.5308333333333333,\n",
       "  0.3622222222222222,\n",
       "  0.5638888888888889,\n",
       "  0.54375,\n",
       "  0.405,\n",
       "  0.5294444444444444,\n",
       "  0.5422222222222222,\n",
       "  0.30500000000000005,\n",
       "  0.30687499999999995,\n",
       "  0.43222222222222223,\n",
       "  0.548,\n",
       "  0.3061111111111111,\n",
       "  0.6294444444444445,\n",
       "  0.5175000000000001,\n",
       "  0.3688888888888889,\n",
       "  0.6255555555555555,\n",
       "  0.2714285714285714,\n",
       "  0.4935714285714286,\n",
       "  0.4127777777777778,\n",
       "  0.551111111111111,\n",
       "  0.3655555555555556,\n",
       "  0.5022222222222221,\n",
       "  0.40428571428571425,\n",
       "  0.380625,\n",
       "  0.3344444444444445,\n",
       "  0.373,\n",
       "  0.3433333333333334,\n",
       "  0.41388888888888886,\n",
       "  0.556875,\n",
       "  0.38499999999999995,\n",
       "  0.5794444444444444,\n",
       "  0.47388888888888897,\n",
       "  0.4338888888888889,\n",
       "  0.4875,\n",
       "  0.5683333333333334,\n",
       "  0.5222222222222221,\n",
       "  0.37777777777777777,\n",
       "  0.68875,\n",
       "  0.35111111111111115,\n",
       "  0.6105555555555555,\n",
       "  0.48,\n",
       "  0.49214285714285716,\n",
       "  0.426875,\n",
       "  0.6227777777777779,\n",
       "  0.43,\n",
       "  0.39722222222222225,\n",
       "  0.6988888888888888,\n",
       "  0.6555555555555556,\n",
       "  0.44357142857142856,\n",
       "  0.715,\n",
       "  0.45444444444444443,\n",
       "  0.3933333333333333,\n",
       "  0.43437499999999996,\n",
       "  0.5077777777777777,\n",
       "  0.615625,\n",
       "  0.47500000000000003,\n",
       "  0.4083333333333333,\n",
       "  0.5774999999999999,\n",
       "  0.378,\n",
       "  0.390625,\n",
       "  0.5278571428571429,\n",
       "  0.5416666666666666,\n",
       "  0.5864285714285715,\n",
       "  0.37722222222222224,\n",
       "  0.5327777777777777,\n",
       "  0.4933333333333334,\n",
       "  0.6094444444444445,\n",
       "  0.4650000000000001,\n",
       "  0.5607142857142857,\n",
       "  0.5425,\n",
       "  0.5122222222222222,\n",
       "  0.38,\n",
       "  0.5022222222222221,\n",
       "  0.43357142857142866,\n",
       "  0.5750000000000001,\n",
       "  0.40277777777777785,\n",
       "  0.4272222222222222,\n",
       "  0.5207142857142857,\n",
       "  0.39357142857142857,\n",
       "  0.4988888888888888,\n",
       "  0.40055555555555555,\n",
       "  0.669375,\n",
       "  0.37555555555555553,\n",
       "  0.4011111111111111,\n",
       "  0.5314285714285715,\n",
       "  0.4316666666666667,\n",
       "  0.35124999999999995,\n",
       "  0.5511111111111111,\n",
       "  0.34777777777777774,\n",
       "  0.4685714285714285,\n",
       "  0.48375,\n",
       "  0.2908333333333333,\n",
       "  0.36083333333333334,\n",
       "  0.42937499999999995,\n",
       "  0.3433333333333333,\n",
       "  0.5671428571428571,\n",
       "  0.54375,\n",
       "  0.47555555555555556,\n",
       "  0.52625,\n",
       "  0.39375000000000004,\n",
       "  0.39444444444444443,\n",
       "  0.5106249999999999,\n",
       "  0.366875,\n",
       "  0.32277777777777783,\n",
       "  0.518125,\n",
       "  0.5155555555555557,\n",
       "  0.443125,\n",
       "  0.5005555555555556,\n",
       "  0.48624999999999996,\n",
       "  0.445,\n",
       "  0.44777777777777783,\n",
       "  0.4461111111111112,\n",
       "  0.5111111111111111,\n",
       "  0.4166666666666667,\n",
       "  0.5033333333333334,\n",
       "  0.613125,\n",
       "  0.4705555555555555,\n",
       "  0.3933333333333333,\n",
       "  0.5661111111111111,\n",
       "  0.3666666666666667,\n",
       "  0.5583333333333333,\n",
       "  0.3322222222222222,\n",
       "  0.5,\n",
       "  0.55,\n",
       "  0.40375000000000005,\n",
       "  0.379375,\n",
       "  0.3811111111111111,\n",
       "  0.5394444444444445,\n",
       "  0.5422222222222222,\n",
       "  0.5205555555555557,\n",
       "  0.6124999999999999,\n",
       "  0.41437500000000005,\n",
       "  0.389375,\n",
       "  0.6156250000000001,\n",
       "  0.45888888888888896,\n",
       "  0.2338888888888889,\n",
       "  0.515,\n",
       "  0.35600000000000004,\n",
       "  0.25166666666666665,\n",
       "  0.59125,\n",
       "  0.5625,\n",
       "  0.6305555555555556,\n",
       "  0.433125,\n",
       "  0.4878571428571429,\n",
       "  0.5631250000000001,\n",
       "  0.36888888888888893,\n",
       "  0.4225,\n",
       "  0.48777777777777787,\n",
       "  0.5722222222222222,\n",
       "  0.52,\n",
       "  0.6825,\n",
       "  0.26916666666666667,\n",
       "  0.3988888888888889,\n",
       "  0.6411111111111111,\n",
       "  0.28375,\n",
       "  0.33,\n",
       "  0.5371428571428571,\n",
       "  0.6244444444444444,\n",
       "  0.5543750000000001,\n",
       "  0.5305555555555556,\n",
       "  0.3866666666666667,\n",
       "  0.4342857142857143,\n",
       "  0.43722222222222223,\n",
       "  0.5777777777777778,\n",
       "  0.5305555555555556,\n",
       "  0.4011111111111111,\n",
       "  0.6805555555555556,\n",
       "  0.38812499999999994,\n",
       "  0.41571428571428576,\n",
       "  0.5066666666666666,\n",
       "  0.37312500000000004,\n",
       "  0.5705555555555556,\n",
       "  0.41888888888888887,\n",
       "  0.751875,\n",
       "  0.51625,\n",
       "  0.5575,\n",
       "  0.5299999999999999,\n",
       "  0.5494444444444444,\n",
       "  0.36000000000000004,\n",
       "  0.51875,\n",
       "  0.5116666666666667,\n",
       "  0.4105555555555555,\n",
       "  0.6849999999999999,\n",
       "  0.5116666666666667,\n",
       "  0.49062500000000003,\n",
       "  0.39000000000000007,\n",
       "  0.5716666666666667,\n",
       "  0.5016666666666667,\n",
       "  0.426,\n",
       "  0.41888888888888887,\n",
       "  0.36000000000000004,\n",
       "  0.3814285714285714,\n",
       "  0.663888888888889,\n",
       "  0.4083333333333333,\n",
       "  0.42888888888888893,\n",
       "  0.43944444444444447,\n",
       "  0.5461111111111111,\n",
       "  0.30428571428571427,\n",
       "  0.56875,\n",
       "  0.3442857142857143,\n",
       "  0.5675,\n",
       "  0.5527777777777777,\n",
       "  0.043333333333333335,\n",
       "  0.535,\n",
       "  0.26611111111111113,\n",
       "  0.48999999999999994,\n",
       "  0.34500000000000003,\n",
       "  0.5305555555555556,\n",
       "  0.4672222222222222,\n",
       "  0.4438888888888889,\n",
       "  0.6744444444444445,\n",
       "  0.4716666666666666,\n",
       "  0.4361111111111111,\n",
       "  0.5261111111111111,\n",
       "  0.5172222222222222,\n",
       "  0.4371428571428571,\n",
       "  0.6678571428571428,\n",
       "  0.577,\n",
       "  0.426,\n",
       "  0.5961111111111111,\n",
       "  0.54,\n",
       "  0.5205555555555555,\n",
       "  0.6507142857142857,\n",
       "  0.65375,\n",
       "  0.611875,\n",
       "  0.41583333333333333,\n",
       "  0.6422222222222221,\n",
       "  0.315,\n",
       "  0.3877777777777778,\n",
       "  0.49666666666666665,\n",
       "  0.49624999999999997,\n",
       "  0.5216666666666667,\n",
       "  0.44875,\n",
       "  0.4588888888888889,\n",
       "  0.4735714285714286,\n",
       "  0.44555555555555554,\n",
       "  0.5800000000000001,\n",
       "  0.5777777777777778,\n",
       "  0.4961111111111111,\n",
       "  0.611111111111111,\n",
       "  0.5222222222222223,\n",
       "  0.5449999999999999,\n",
       "  0.49833333333333335,\n",
       "  0.5321428571428571,\n",
       "  0.6244444444444445,\n",
       "  0.38875,\n",
       "  0.4183333333333333,\n",
       "  0.568125,\n",
       "  0.5055555555555555,\n",
       "  0.21428571428571427,\n",
       "  0.48666666666666664,\n",
       "  0.5064285714285715,\n",
       "  0.4872222222222222,\n",
       "  0.733125,\n",
       "  0.4383333333333333,\n",
       "  0.44375,\n",
       "  0.5766666666666667,\n",
       "  0.39899999999999997,\n",
       "  0.36666666666666664,\n",
       "  0.57625,\n",
       "  0.45944444444444443,\n",
       "  0.4766666666666667,\n",
       "  0.4683333333333333,\n",
       "  0.5005555555555555,\n",
       "  0.49375,\n",
       "  0.5027777777777778,\n",
       "  0.46888888888888886,\n",
       "  0.3578571428571428,\n",
       "  0.48714285714285716,\n",
       "  0.54375,\n",
       "  0.306875,\n",
       "  0.666875,\n",
       "  0.5283333333333333,\n",
       "  0.5577777777777778,\n",
       "  0.38437499999999997,\n",
       "  0.364375,\n",
       "  0.2827777777777778,\n",
       "  0.5599999999999999,\n",
       "  0.4905555555555556,\n",
       "  0.36812500000000004,\n",
       "  0.4744444444444444,\n",
       "  0.5025000000000001,\n",
       "  0.345,\n",
       "  0.66,\n",
       "  0.5777777777777777,\n",
       "  0.4516666666666666,\n",
       "  0.458125,\n",
       "  0.438,\n",
       "  0.506875,\n",
       "  0.6,\n",
       "  0.4127777777777777,\n",
       "  0.4655555555555555,\n",
       "  0.4511111111111111,\n",
       "  0.41222222222222216,\n",
       "  0.369,\n",
       "  0.4244444444444444,\n",
       "  0.5900000000000001,\n",
       "  0.60875,\n",
       "  0.3877777777777778,\n",
       "  0.5306249999999999,\n",
       "  0.475,\n",
       "  0.4075,\n",
       "  0.49833333333333335,\n",
       "  0.581111111111111,\n",
       "  0.39937500000000004,\n",
       "  0.436875,\n",
       "  0.4355555555555556,\n",
       "  0.32,\n",
       "  0.4955555555555555,\n",
       "  0.32428571428571434,\n",
       "  0.3814285714285714,\n",
       "  0.4511111111111112,\n",
       "  0.46111111111111114,\n",
       "  0.608125,\n",
       "  0.545,\n",
       "  0.4066666666666667,\n",
       "  0.35555555555555557,\n",
       "  0.593125,\n",
       "  0.47888888888888886,\n",
       "  0.596111111111111,\n",
       "  0.4527777777777777,\n",
       "  0.436875,\n",
       "  0.3255555555555556,\n",
       "  0.49166666666666675,\n",
       "  0.33749999999999997,\n",
       "  0.31666666666666665,\n",
       "  0.51,\n",
       "  0.5894444444444444,\n",
       "  0.3466666666666666,\n",
       "  0.4055555555555555,\n",
       "  0.43312500000000004,\n",
       "  0.2285714285714286,\n",
       "  0.5383333333333333,\n",
       "  0.6599999999999999,\n",
       "  0.35555555555555557,\n",
       "  0.38812499999999994,\n",
       "  0.4044444444444444,\n",
       "  0.4122222222222222,\n",
       "  0.5738888888888888,\n",
       "  0.3794444444444445,\n",
       "  0.49857142857142855,\n",
       "  0.4655555555555555,\n",
       "  0.33611111111111114,\n",
       "  0.521875,\n",
       "  0.48000000000000004,\n",
       "  0.6477777777777778,\n",
       "  0.6394444444444445,\n",
       "  0.45333333333333337,\n",
       "  0.37375,\n",
       "  0.22937500000000002,\n",
       "  0.525,\n",
       "  0.4044444444444444,\n",
       "  0.405,\n",
       "  0.510625,\n",
       "  0.5277777777777778,\n",
       "  0.51,\n",
       "  0.42875,\n",
       "  0.44833333333333336,\n",
       "  0.7418750000000001,\n",
       "  0.3742857142857142,\n",
       "  0.655,\n",
       "  0.35944444444444446,\n",
       "  0.534375,\n",
       "  0.29277777777777775,\n",
       "  0.525625,\n",
       "  0.5231250000000001,\n",
       "  0.3825,\n",
       "  0.48111111111111104,\n",
       "  0.519375,\n",
       "  0.5261111111111111,\n",
       "  0.5716666666666667,\n",
       "  0.4911111111111111,\n",
       "  0.5075,\n",
       "  0.6871428571428572,\n",
       "  0.7257142857142858,\n",
       "  0.44055555555555553,\n",
       "  0.3175,\n",
       "  0.25125,\n",
       "  0.5306249999999999,\n",
       "  0.42,\n",
       "  0.47214285714285714,\n",
       "  0.576111111111111,\n",
       "  0.3735714285714286,\n",
       "  0.5685714285714285,\n",
       "  0.535625,\n",
       "  0.5316666666666666,\n",
       "  0.4064285714285714,\n",
       "  0.45333333333333337,\n",
       "  0.5542857142857144,\n",
       "  0.37857142857142856,\n",
       "  0.4272222222222222,\n",
       "  0.47800000000000004,\n",
       "  0.5883333333333334,\n",
       "  0.5488888888888889,\n",
       "  0.56,\n",
       "  0.6977777777777777,\n",
       "  0.38388888888888884,\n",
       "  0.57,\n",
       "  0.44687499999999997,\n",
       "  0.3792857142857143,\n",
       "  0.463125,\n",
       "  0.4983333333333333,\n",
       "  0.24444444444444446,\n",
       "  0.451875,\n",
       "  0.5000000000000001,\n",
       "  0.4566666666666666,\n",
       "  0.49571428571428583,\n",
       "  0.42,\n",
       "  0.57,\n",
       "  0.45055555555555554,\n",
       "  0.3955555555555556,\n",
       "  0.391875,\n",
       "  0.4305555555555555,\n",
       "  0.4672222222222222,\n",
       "  0.5261111111111112,\n",
       "  0.34714285714285714,\n",
       "  0.5222222222222223,\n",
       "  0.43857142857142856,\n",
       "  0.42,\n",
       "  0.5672222222222223,\n",
       "  0.6,\n",
       "  0.546875,\n",
       "  0.5414285714285715,\n",
       "  0.46,\n",
       "  0.5322222222222222,\n",
       "  0.37142857142857144,\n",
       "  0.5075000000000001,\n",
       "  0.5183333333333334,\n",
       "  0.34875,\n",
       "  0.5094444444444445,\n",
       "  0.3544444444444444,\n",
       "  0.36277777777777775,\n",
       "  0.358125,\n",
       "  0.3275,\n",
       "  0.45444444444444454,\n",
       "  0.4955555555555555,\n",
       "  0.26875,\n",
       "  0.5461111111111112,\n",
       "  0.6561111111111111,\n",
       "  0.5066666666666666,\n",
       "  0.47875,\n",
       "  0.4977777777777778,\n",
       "  0.58375,\n",
       "  0.38666666666666666,\n",
       "  0.42125,\n",
       "  0.5433333333333334,\n",
       "  0.451875,\n",
       "  0.389375,\n",
       "  0.4911111111111111,\n",
       "  0.363,\n",
       "  0.5188888888888888,\n",
       "  0.4988888888888888,\n",
       "  0.53,\n",
       "  0.48374999999999996,\n",
       "  0.3925,\n",
       "  0.41000000000000003,\n",
       "  0.375,\n",
       "  0.4277777777777778,\n",
       "  0.3941666666666666,\n",
       "  0.6318750000000001,\n",
       "  0.41285714285714287,\n",
       "  0.4772222222222222,\n",
       "  0.2564285714285714,\n",
       "  0.415,\n",
       "  0.5099999999999999,\n",
       "  0.5850000000000001,\n",
       "  0.52,\n",
       "  0.40222222222222226,\n",
       "  0.43388888888888894,\n",
       "  0.48857142857142855,\n",
       "  0.30499999999999994,\n",
       "  0.33642857142857135,\n",
       "  0.5249999999999999,\n",
       "  0.3466666666666667,\n",
       "  0.16142857142857142,\n",
       "  0.50875,\n",
       "  0.5744444444444444,\n",
       "  0.41374999999999995,\n",
       "  0.38833333333333336,\n",
       "  0.5614285714285715,\n",
       "  0.475,\n",
       "  0.3792857142857143,\n",
       "  0.536111111111111,\n",
       "  0.42444444444444446,\n",
       "  0.54,\n",
       "  0.765,\n",
       "  0.5237499999999999,\n",
       "  0.599375,\n",
       "  0.4225,\n",
       "  0.4725,\n",
       "  0.5777777777777778,\n",
       "  0.5622222222222222,\n",
       "  0.49055555555555547,\n",
       "  0.34800000000000003,\n",
       "  0.38312499999999994,\n",
       "  0.5394444444444445,\n",
       "  0.4605555555555555,\n",
       "  0.669375,\n",
       "  0.45142857142857146,\n",
       "  0.39285714285714285,\n",
       "  0.4466666666666667,\n",
       "  0.44722222222222224,\n",
       "  0.4588888888888889,\n",
       "  0.59625,\n",
       "  0.54,\n",
       "  0.47888888888888886,\n",
       "  0.5388888888888889,\n",
       "  0.506875,\n",
       "  0.5133333333333333,\n",
       "  0.38333333333333336,\n",
       "  0.41375,\n",
       "  0.30944444444444447,\n",
       "  0.5633333333333334,\n",
       "  0.47,\n",
       "  0.5388888888888889,\n",
       "  0.275,\n",
       "  0.5325,\n",
       "  0.20142857142857146,\n",
       "  0.5925,\n",
       "  0.6725,\n",
       "  0.435,\n",
       "  0.585,\n",
       "  0.6021428571428572,\n",
       "  0.5757142857142856,\n",
       "  0.48,\n",
       "  0.37285714285714283,\n",
       "  0.4633333333333334,\n",
       "  0.5506249999999999,\n",
       "  0.6894444444444444,\n",
       "  0.44299999999999995,\n",
       "  0.5071428571428572,\n",
       "  0.6661111111111111,\n",
       "  0.585,\n",
       "  0.415,\n",
       "  0.5455555555555556,\n",
       "  0.4377777777777777,\n",
       "  0.5883333333333334,\n",
       "  0.5622222222222223,\n",
       "  0.57375,\n",
       "  0.3055555555555556,\n",
       "  0.4375,\n",
       "  0.405,\n",
       "  0.5761111111111111,\n",
       "  0.35000000000000003,\n",
       "  0.45499999999999996,\n",
       "  0.52,\n",
       "  0.3827777777777778,\n",
       "  0.39055555555555554,\n",
       "  0.2977777777777778,\n",
       "  0.4311111111111111,\n",
       "  0.44555555555555554,\n",
       "  0.5622222222222223,\n",
       "  0.37916666666666665,\n",
       "  0.32928571428571424,\n",
       "  0.4933333333333334,\n",
       "  0.4744444444444444,\n",
       "  0.535,\n",
       "  0.4342857142857143,\n",
       "  0.496875,\n",
       "  0.35874999999999996,\n",
       "  0.5293749999999999,\n",
       "  0.2864285714285714,\n",
       "  0.3938888888888889,\n",
       "  0.4275,\n",
       "  0.27666666666666667,\n",
       "  0.46222222222222226,\n",
       "  0.6088888888888888,\n",
       "  0.435,\n",
       "  0.4633333333333333,\n",
       "  0.43249999999999994,\n",
       "  0.4927777777777778,\n",
       "  0.42000000000000004,\n",
       "  0.38555555555555554,\n",
       "  0.5592857142857143,\n",
       "  0.6266666666666666,\n",
       "  0.48888888888888893,\n",
       "  0.49388888888888893,\n",
       "  0.48,\n",
       "  0.5725,\n",
       "  0.343125,\n",
       "  0.605,\n",
       "  0.3494444444444445,\n",
       "  0.47277777777777774,\n",
       "  0.18555555555555556,\n",
       "  0.5431250000000001,\n",
       "  0.4708333333333334,\n",
       "  0.6633333333333333,\n",
       "  0.5057142857142857,\n",
       "  0.5544444444444445,\n",
       "  0.5211111111111112,\n",
       "  0.39555555555555555,\n",
       "  0.24642857142857144,\n",
       "  0.403125,\n",
       "  0.5366666666666666,\n",
       "  0.6105555555555555,\n",
       "  0.48444444444444446,\n",
       "  0.505,\n",
       "  0.568888888888889,\n",
       "  0.32666666666666666,\n",
       "  0.4155555555555555,\n",
       "  0.3988888888888889,\n",
       "  0.48277777777777775,\n",
       "  0.53375,\n",
       "  0.5072222222222221,\n",
       "  0.25,\n",
       "  0.3555555555555555,\n",
       "  0.39,\n",
       "  0.5783333333333334,\n",
       "  0.525,\n",
       "  0.544375,\n",
       "  0.38437499999999997,\n",
       "  0.5627777777777777,\n",
       "  0.441,\n",
       "  0.45928571428571424,\n",
       "  0.270625,\n",
       "  0.4577777777777778,\n",
       "  0.655,\n",
       "  0.47875,\n",
       "  0.335,\n",
       "  0.49666666666666676,\n",
       "  0.535,\n",
       "  0.5922222222222222,\n",
       "  0.430625,\n",
       "  0.4992857142857143,\n",
       "  0.57625,\n",
       "  0.5183333333333333,\n",
       "  0.38611111111111107,\n",
       "  0.6066666666666668,\n",
       "  0.48571428571428577,\n",
       "  0.37625,\n",
       "  0.48200000000000004,\n",
       "  0.275,\n",
       "  0.5071428571428571,\n",
       "  0.5116666666666666,\n",
       "  0.49750000000000005,\n",
       "  0.583888888888889,\n",
       "  0.458,\n",
       "  0.40374999999999994,\n",
       "  0.56,\n",
       "  0.4658333333333334,\n",
       "  0.520625,\n",
       "  0.595,\n",
       "  0.4344444444444444,\n",
       "  0.6435714285714286,\n",
       "  0.3305555555555555,\n",
       "  0.6664285714285715,\n",
       "  0.538125,\n",
       "  0.6472222222222223,\n",
       "  ...]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_trainer._eval_save_prefix = OUTPUT_PREFIX + \"-test-pos-unbiased\"\n",
    "model_trainer._evaluate_partial(test_dataset_pos_unbiased)\n",
    "\n",
    "model_trainer._eval_save_prefix = OUTPUT_PREFIX +  \"-test-neg-unbiased\"\n",
    "model_trainer._evaluate_partial(test_dataset_neg_unbiased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute AOA and unbiased evaluator metrics with biased testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "biased_results = dict()\n",
    "\n",
    "# biased_results[\"STRATIFIED_15\"] = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=1.5, K=30, partition=100)\n",
    "biased_results[\"AOA\"] = aoa(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", K=30)\n",
    "biased_results[\"UB_15\"] = eq(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=1.5, K=30)\n",
    "biased_results[\"UB_2\"] =  eq(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2, K=30)\n",
    "biased_results[\"UB_25\"] =  eq(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2.5, K=30)\n",
    "biased_results[\"UB_3\"] =  eq(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=3, K=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute AOA and unbiased evaluator metrics with unbiased testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbiased_results = dict()\n",
    "\n",
    "# unbiased_results[\"STRATIFIED_15\"] = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=1.5, K=1, partition=100)\n",
    "unbiased_results[\"AOA\"] = aoa(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", K=1)\n",
    "unbiased_results[\"UB_15\"] = eq(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=1.5, K=1)\n",
    "unbiased_results[\"UB_2\"] =  eq(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2, K=1)\n",
    "unbiased_results[\"UB_25\"] =  eq(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2.5, K=1)\n",
    "unbiased_results[\"UB_3\"] =  eq(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=3, K=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 742,  977,  141,  707,  751,   24,  273,  959,  193,  200, 1001,\n",
       "        206,  783,  662,  295,  384,  228,  975,  340,  122,  146,  740,\n",
       "        818,  747,  346,  852,  849,  579,   20,  284,  197,  727,  910,\n",
       "        332,  251,  599,  529,  463,  192,  873,  502,  309,  731,   11,\n",
       "        366,  581,  108,  400,  294,  524,  486,  265,  530,  445,  883,\n",
       "        476,  168,  672,   38,  666,  984,  848,    7,  874,  356,    2,\n",
       "        490,  545,  955,  365,  189,  701,  993,   99,  823,  885,  128,\n",
       "        832,  680,  804,  654,  329,  521,  746,  691,  318,   55,  936,\n",
       "        715,  429,  768,   40,   51,  506,  458,   18,  244,  648,  409,\n",
       "        548,  694,  567,  730,  411,  864,  172,  272,   15,  227,  427,\n",
       "        606,  917,  809,  646,  515,  838,  651,  511,  266,  420,  861,\n",
       "         62,   46,  290,  522,  372,   93,   19,  773,  793,  302,  886,\n",
       "         26,  705,  781,  395,  456,  958,  785,  483,  334,  644,  536,\n",
       "        937,  156,  577,  297,  704,  561,  617,  813,  836,  215,  584,\n",
       "        538,  230,  117,  510,  650,  303,  786,  754,  957,  489,  630,\n",
       "        758,  393,  216,  824,  620,  299,    5,  360,   22,    3,  353,\n",
       "        593,  323,  494,  424,  982,  656,  512,  941,  976,  956,  573,\n",
       "        177,  321,   33,  422,  847,  412,  794,  713,  822,  869,  428,\n",
       "        776,  513,  167,  635,  571,  225,  853,  383,  300,  878,  798,\n",
       "        653,  286,  276,  324,  724,  390,  600,  540,  460,  526,  981,\n",
       "        118,  922,  622,  113,  639,  663,   43,  363,  222,  152,  433,\n",
       "        994,  597,  891,  898,   96,  854,   30,  771,  948,  909,  929,\n",
       "        963,  979,  739,  110,  440,  238,  734,  721,  382,  188,  357,\n",
       "        661,  142,  839,  634,  178,  461,  765,  888,  399,  166,  815,\n",
       "        528,  313,  638,  418,  518,  692,  209,  889,  665,  190,   67,\n",
       "        223,   97,  718,  689,   12,  902,  415,  392,  467,  289,  601,\n",
       "        348,  441,  271,  444,  492,  471,  884,  780,   63,    9,  155,\n",
       "        589,  554,  173])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get number of items\n",
    "num_items = raw_data['max_item']\n",
    "\n",
    "# Get the n_p partitions\n",
    "n_p = 300\n",
    "nums = np.arange(1, num_items+1)\n",
    "partitions = np.random.choice(nums, n_p, replace=False)\n",
    "\n",
    "# Visualize\n",
    "partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the partition which minimizes the sum of AUC and Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b47ec46385f467b8132ef9a4485c445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2714/649049226.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Compute the results (AUC and Recall) for both biased and unbiased test sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mtemp_unbiased\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstratified\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUTPUT_PREFIX\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"-test-pos-unbiased_evaluate_partial.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUTPUT_PREFIX\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"-test-neg-unbiased_evaluate_partial.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"training_arr.npy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mtemp_biased\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstratified\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUTPUT_PREFIX\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"-test-pos-biased_evaluate_partial.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUTPUT_PREFIX\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"-test-neg-biased_evaluate_partial.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"training_arr.npy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;31m# If first iteration...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0munbiased_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2714/3970068827.py\u001b[0m in \u001b[0;36mstratified\u001b[0;34m(infilename, infilename_neg, trainfilename, gamma, K, partition, delta)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtheuser\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"users\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mall_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"results\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtheuser\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mpos_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"user_items\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtheuser\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP_neg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"user_items\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtheuser\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNUM_NEGATIVES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mpos_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"results\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtheuser\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP_neg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"results\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtheuser\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNUM_NEGATIVES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_item\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_items\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Compute biased and unbiased results with stratified for each partition\n",
    "# and store biased and unbiased results such that the sum of AUC and Recall is minimized\n",
    "\n",
    "# Value of gamma to use for minimization\n",
    "gamma = 15\n",
    "\n",
    "# To print :)\n",
    "key = \"STRATIFIED_\" + str(gamma).replace(\".\",\"\")\n",
    "\n",
    "# Initialize results\n",
    "unbiased_results[key] = dict()\n",
    "biased_results[key] = dict()\n",
    "best_partition = np.random.choice(nums, 1)[0]\n",
    "\n",
    "# For each partition\n",
    "for p in tqdm(partitions):\n",
    "    # Compute the results (AUC and Recall) for both biased and unbiased test sets\n",
    "    temp_unbiased = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=gamma, K=1, partition=p)\n",
    "    temp_biased = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=gamma, K=30, partition=p)\n",
    "    # If first iteration...\n",
    "    if not unbiased_results[key]:\n",
    "        unbiased_results[key] = temp_unbiased\n",
    "    if not biased_results[key]:\n",
    "        biased_results[key] = temp_biased\n",
    "    # Else if a better partition was found, update the results\n",
    "    elif temp_unbiased['bias'] + temp_unbiased['concentration'] + temp_biased['bias'] + temp_biased['concentration'] < biased_results[key]['bias'] + biased_results[key]['concentration'] + unbiased_results[key]['bias'] + unbiased_results[key]['concentration']:\n",
    "        biased_results[key]['auc'] = temp_biased['auc']\n",
    "        biased_results[key]['recall'] = temp_biased['recall']\n",
    "        biased_results[key]['bias'] = temp_biased['bias']\n",
    "        biased_results[key]['concentration'] = temp_biased['concentration']\n",
    "        unbiased_results[key]['auc'] = temp_unbiased['auc']\n",
    "        unbiased_results[key]['recall'] = temp_unbiased['recall']\n",
    "        biased_results[key]['bias'] = temp_biased['bias']\n",
    "        biased_results[key]['concentration'] = temp_biased['concentration']\n",
    "        best_partition = p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, for the chosen value of gamma, the best partition is..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "977"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize\n",
    "best_partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute stratified metrics with unbiased testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbiased_results[\"STRATIFIED_15\"] = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=1.5, K=1, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_15\"] = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=1.5, K=30, partition=best_partition)\n",
    "\n",
    "unbiased_results[\"STRATIFIED_2\"] = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2, K=1, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_2\"] = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2, K=30, partition=best_partition)\n",
    "\n",
    "unbiased_results[\"STRATIFIED_25\"] = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2.5, K=1, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_25\"] = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2.5, K=30, partition=best_partition)\n",
    "\n",
    "unbiased_results[\"STRATIFIED_3\"] = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=3, K=1, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_3\"] = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=3, K=30, partition=best_partition)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version uses the linspace of items instead of linspace of propensities to make the partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbiased_results[\"STRATIFIED_v2_15\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=1.5, K=1, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_v2_15\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=1.5, K=30, partition=best_partition)\n",
    "\n",
    "unbiased_results[\"STRATIFIED_v2_2\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2, K=1, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_v2_2\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2, K=30, partition=best_partition)\n",
    "\n",
    "unbiased_results[\"STRATIFIED_v2_25\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2.5, K=1, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_v2_25\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2.5, K=30, partition=best_partition)\n",
    "\n",
    "unbiased_results[\"STRATIFIED_v2_3\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=3, K=1, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_v2_3\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=3, K=30, partition=best_partition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare table for results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(list(value.keys()))\n",
    "rows = 4\n",
    "#len(list(biased_results.items()))\n",
    "columns = 13\n",
    "\n",
    "# Init results\n",
    "results_array = np.zeros((rows,columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the table with the MAE results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init dictionary\n",
    "mae_results = dict()\n",
    "\n",
    "# Get the names of the rows\n",
    "list_biased_res = list(biased_results.keys())\n",
    "\n",
    "# For each row\n",
    "for i in range(len(list_biased_res)):\n",
    "    key = list_biased_res[i]\n",
    "\n",
    "    # For each column\n",
    "    for j in range(len(list(biased_results[key].keys()))):\n",
    "        key_2 = list(biased_results[key].keys())[j]\n",
    "\n",
    "        # Compute MAE\n",
    "        results_array[j][i] = abs(biased_results[key][key_2] - unbiased_results[key][key_2])\n",
    "\n",
    "# Make it a DataFrame\n",
    "mae_df = pd.DataFrame(columns=list(biased_results.keys()), data=results_array)\n",
    "metric_values = list(biased_results[list(biased_results.keys())[12]].keys())\n",
    "mae_df.insert(0, \"metric\", metric_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **RESULTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>AOA</th>\n",
       "      <th>UB_15</th>\n",
       "      <th>UB_2</th>\n",
       "      <th>UB_25</th>\n",
       "      <th>UB_3</th>\n",
       "      <th>STRATIFIED_15</th>\n",
       "      <th>STRATIFIED_2</th>\n",
       "      <th>STRATIFIED_25</th>\n",
       "      <th>STRATIFIED_3</th>\n",
       "      <th>STRATIFIED_v2_15</th>\n",
       "      <th>STRATIFIED_v2_2</th>\n",
       "      <th>STRATIFIED_v2_25</th>\n",
       "      <th>STRATIFIED_v2_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>auc</td>\n",
       "      <td>0.155585</td>\n",
       "      <td>0.126802</td>\n",
       "      <td>0.123134</td>\n",
       "      <td>0.120334</td>\n",
       "      <td>0.118195</td>\n",
       "      <td>0.123360</td>\n",
       "      <td>0.104660</td>\n",
       "      <td>0.038030</td>\n",
       "      <td>2.515272e-01</td>\n",
       "      <td>1.268023e-01</td>\n",
       "      <td>1.231336e-01</td>\n",
       "      <td>1.203343e-01</td>\n",
       "      <td>1.181951e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>recall</td>\n",
       "      <td>0.383764</td>\n",
       "      <td>0.264533</td>\n",
       "      <td>0.252967</td>\n",
       "      <td>0.244393</td>\n",
       "      <td>0.237973</td>\n",
       "      <td>0.264362</td>\n",
       "      <td>0.256718</td>\n",
       "      <td>0.247001</td>\n",
       "      <td>2.256460e-01</td>\n",
       "      <td>2.645330e-01</td>\n",
       "      <td>2.529666e-01</td>\n",
       "      <td>2.443934e-01</td>\n",
       "      <td>2.379733e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bias</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22632.642818</td>\n",
       "      <td>67532.210117</td>\n",
       "      <td>329182.766649</td>\n",
       "      <td>1.193920e+06</td>\n",
       "      <td>3.296030e-12</td>\n",
       "      <td>3.496758e-12</td>\n",
       "      <td>4.046541e-12</td>\n",
       "      <td>9.465762e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>concentration</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1264.088294</td>\n",
       "      <td>2243.273739</td>\n",
       "      <td>1368.115226</td>\n",
       "      <td>9.060278e+02</td>\n",
       "      <td>1.036990e+01</td>\n",
       "      <td>1.423835e+01</td>\n",
       "      <td>1.641256e+01</td>\n",
       "      <td>1.759407e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          metric       AOA     UB_15      UB_2     UB_25      UB_3  \\\n",
       "0            auc  0.155585  0.126802  0.123134  0.120334  0.118195   \n",
       "1         recall  0.383764  0.264533  0.252967  0.244393  0.237973   \n",
       "2           bias  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3  concentration  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "   STRATIFIED_15  STRATIFIED_2  STRATIFIED_25  STRATIFIED_3  STRATIFIED_v2_15  \\\n",
       "0       0.123360      0.104660       0.038030  2.515272e-01      1.268023e-01   \n",
       "1       0.264362      0.256718       0.247001  2.256460e-01      2.645330e-01   \n",
       "2   22632.642818  67532.210117  329182.766649  1.193920e+06      3.296030e-12   \n",
       "3    1264.088294   2243.273739    1368.115226  9.060278e+02      1.036990e+01   \n",
       "\n",
       "   STRATIFIED_v2_2  STRATIFIED_v2_25  STRATIFIED_v2_3  \n",
       "0     1.231336e-01      1.203343e-01     1.181951e-01  \n",
       "1     2.529666e-01      2.443934e-01     2.379733e-01  \n",
       "2     3.496758e-12      4.046541e-12     9.465762e-13  \n",
       "3     1.423835e+01      1.641256e+01     1.759407e+01  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize\n",
    "mae_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RecSys-Evaluation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
