{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **IMPORT LIBS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from openrec.tf1.legacy import ImplicitModelTrainer\n",
    "from openrec.tf1.legacy.utils.evaluators import ImplicitEvalManager\n",
    "from openrec.tf1.legacy.utils import ImplicitDataset\n",
    "from openrec.tf1.legacy.recommenders import CML, BPR, PMF\n",
    "from openrec.tf1.legacy.utils.evaluators import AUC\n",
    "from openrec.tf1.legacy.utils.samplers import PairwiseSampler\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **GENERATE THE DATASET**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "seed = 2384795\n",
    "np.random.seed(seed=seed)\n",
    "\n",
    "# Preparing folder for output data\n",
    "output_name = f\"./generated_data/\"\n",
    "if os.path.exists(output_name) == False:\n",
    "    os.makedirs(output_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MODEL CHOICE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Here I won't comment anything, we are just using the code provided by the authors of the paper\n",
    "\n",
    "raw_data = dict()\n",
    "raw_data['train_data'] = np.load(output_name + \"training_arr.npy\")\n",
    "raw_data['max_user'] = 15401\n",
    "raw_data['max_item'] = 1001\n",
    "batch_size = 8000\n",
    "test_batch_size = 1000\n",
    "display_itr = 1000\n",
    "\n",
    "train_dataset = ImplicitDataset(raw_data['train_data'], raw_data['max_user'], raw_data['max_item'], name='Train')\n",
    "\n",
    "MODEL_CLASS = CML\n",
    "MODEL_PREFIX = \"cml\"\n",
    "DATASET_NAME = \"yahoo\"\n",
    "OUTPUT_FOLDER = output_name\n",
    "OUTPUT_PATH = OUTPUT_FOLDER + MODEL_PREFIX + \"-\" + DATASET_NAME + \"/\"\n",
    "OUTPUT_PREFIX = str(OUTPUT_PATH) + str(MODEL_PREFIX) + \"-\" + str(DATASET_NAME)\n",
    "\n",
    "\n",
    "if os.path.exists(OUTPUT_PATH) == False:\n",
    "    os.makedirs(OUTPUT_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **EVALUATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "raw_data = dict()\n",
    "raw_data['train_data'] = np.load(output_name + \"training_arr.npy\")\n",
    "raw_data['test_data_pos_biased'] = np.load(output_name + \"biased-test_arr_pos.npy\")\n",
    "raw_data['test_data_neg_biased'] = np.load(output_name + \"biased-test_arr_neg.npy\")\n",
    "raw_data['test_data_pos_unbiased'] = np.load(output_name + \"unbiased-test_arr_pos.npy\")\n",
    "raw_data['test_data_neg_unbiased'] = np.load(output_name + \"unbiased-test_arr_neg.npy\")\n",
    "raw_data['max_user'] = 15401\n",
    "raw_data['max_item'] = 1001\n",
    "batch_size = 8000\n",
    "test_batch_size = 1000\n",
    "display_itr = 1000\n",
    "\n",
    "# Load data\n",
    "train_dataset = ImplicitDataset(raw_data['train_data'], raw_data['max_user'], raw_data['max_item'], name='Train')\n",
    "test_dataset_pos_biased = ImplicitDataset(raw_data['test_data_pos_biased'], raw_data['max_user'], raw_data['max_item'])\n",
    "test_dataset_neg_biased = ImplicitDataset(raw_data['test_data_neg_biased'], raw_data['max_user'], raw_data['max_item'])\n",
    "test_dataset_pos_unbiased = ImplicitDataset(raw_data['test_data_pos_unbiased'], raw_data['max_user'], raw_data['max_item'])\n",
    "test_dataset_neg_unbiased = ImplicitDataset(raw_data['test_data_neg_unbiased'], raw_data['max_user'], raw_data['max_item'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/piccinatomattia/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:391: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/piccinatomattia/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:31: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/piccinatomattia/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:41: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/piccinatomattia/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:43: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/piccinatomattia/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:33: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/piccinatomattia/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/interactions/pairwise_eu_dist.py:71: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/piccinatomattia/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:596: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/piccinatomattia/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/piccinatomattia/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:75: The name tf.scatter_update is deprecated. Please use tf.compat.v1.scatter_update instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/piccinatomattia/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:144: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/piccinatomattia/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:365: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-10 19:58:55.226790: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
      "2024-06-10 19:58:55.275329: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2419200000 Hz\n",
      "2024-06-10 19:58:55.280478: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5654e0851660 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-10 19:58:55.280536: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/piccinatomattia/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:148: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from ./generated_data/cml-yahoo/\n",
      "[Subsampling negative items]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    }
   ],
   "source": [
    "# Prevent tensorflow from using cached embeddings\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "# Define the model\n",
    "model = MODEL_CLASS(batch_size=batch_size, max_user=train_dataset.max_user(), max_item=train_dataset.max_item(), dim_embed=50, l2_reg=0.001, opt='Adam', sess_config=None)\n",
    "sampler = PairwiseSampler(batch_size=batch_size, dataset=train_dataset, num_process=4)\n",
    "model_trainer = ImplicitModelTrainer(batch_size=batch_size, test_batch_size=test_batch_size, train_dataset=train_dataset, model=model, sampler=sampler, eval_save_prefix=OUTPUT_PATH + DATASET_NAME, item_serving_size=500)\n",
    "auc_evaluator = AUC()\n",
    "\n",
    "# Load model\n",
    "model.load(OUTPUT_PATH)\n",
    "\n",
    "# Set parameters\n",
    "model_trainer._eval_manager = ImplicitEvalManager(evaluators=[auc_evaluator])\n",
    "model_trainer._num_negatives = 200\n",
    "model_trainer._exclude_positives([train_dataset, test_dataset_pos_biased, test_dataset_neg_biased])\n",
    "model_trainer._sample_negatives(seed=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biased Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2070/2070 [00:01<00:00, 1053.50it/s]\n",
      "100%|██████████| 2296/2296 [00:44<00:00, 52.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AUC': [0.49967796610169485,\n",
       "  0.5082203389830507,\n",
       "  0.5008221476510067,\n",
       "  0.5002181208053691,\n",
       "  0.49372053872053867,\n",
       "  0.4746610169491526,\n",
       "  0.518506711409396,\n",
       "  0.5479560810810811,\n",
       "  0.49776845637583883,\n",
       "  0.5143265993265993,\n",
       "  0.5179152542372881,\n",
       "  0.4952364864864865,\n",
       "  0.48668341708542723,\n",
       "  0.4925333333333333,\n",
       "  0.5158159722222222,\n",
       "  0.4890939597315436,\n",
       "  0.5106521739130435,\n",
       "  0.4566836734693877,\n",
       "  0.5097315436241611,\n",
       "  0.5299494949494948,\n",
       "  0.5154180602006688,\n",
       "  0.5342123287671233,\n",
       "  0.506989966555184,\n",
       "  0.4591610738255033,\n",
       "  0.4782608695652174,\n",
       "  0.47272887323943663,\n",
       "  0.48060137457044677,\n",
       "  0.4904013377926421,\n",
       "  0.4699829931972789,\n",
       "  0.5305050505050505,\n",
       "  0.4823154362416108,\n",
       "  0.48531986531986526,\n",
       "  0.48807291666666663,\n",
       "  0.5047826086956522,\n",
       "  0.46753355704697985,\n",
       "  0.49755033557046985,\n",
       "  0.5465384615384616,\n",
       "  0.4745238095238095,\n",
       "  0.49320469798657707,\n",
       "  0.4888006756756757,\n",
       "  0.4819932432432432,\n",
       "  0.43794827586206897,\n",
       "  0.5505972696245733,\n",
       "  0.49168350168350167,\n",
       "  0.48315436241610743,\n",
       "  0.4865488215488215,\n",
       "  0.5175420875420875,\n",
       "  0.5198833333333333,\n",
       "  0.5103187919463087,\n",
       "  0.4645986622073579,\n",
       "  0.5373,\n",
       "  0.5141077441077441,\n",
       "  0.5109661016949153,\n",
       "  0.4913255033557047,\n",
       "  0.4995890410958904,\n",
       "  0.4855369127516778,\n",
       "  0.4925925925925925,\n",
       "  0.47096551724137925,\n",
       "  0.48483221476510074,\n",
       "  0.5666440677966101,\n",
       "  0.5098484848484849,\n",
       "  0.5070302013422819,\n",
       "  0.4765476190476191,\n",
       "  0.48077441077441074,\n",
       "  0.4686120401337793,\n",
       "  0.529463087248322,\n",
       "  0.4613804713804714,\n",
       "  0.4910402684563758,\n",
       "  0.46321666666666667,\n",
       "  0.5035451505016723,\n",
       "  0.4783779264214047,\n",
       "  0.5440969899665552,\n",
       "  0.4461447811447811,\n",
       "  0.5016780821917809,\n",
       "  0.48599662162162166,\n",
       "  0.5149322033898306,\n",
       "  0.47531879194630877,\n",
       "  0.4923666666666666,\n",
       "  0.4705033557046979,\n",
       "  0.49469696969696975,\n",
       "  0.4635690235690236,\n",
       "  0.4803571428571429,\n",
       "  0.5037416107382551,\n",
       "  0.5050503355704697,\n",
       "  0.4915319865319865,\n",
       "  0.5027272727272727,\n",
       "  0.4493412162162162,\n",
       "  0.505979381443299,\n",
       "  0.5539358108108108,\n",
       "  0.524527027027027,\n",
       "  0.5005050505050506,\n",
       "  0.5243624161073825,\n",
       "  0.434070945945946,\n",
       "  0.499010067114094,\n",
       "  0.5288720538720538,\n",
       "  0.4585273972602739,\n",
       "  0.5042617449664429,\n",
       "  0.4921571906354515,\n",
       "  0.4769425675675676,\n",
       "  0.5580369127516778,\n",
       "  0.5094256756756756,\n",
       "  0.5161986301369863,\n",
       "  0.5202551020408164,\n",
       "  0.5266258741258741,\n",
       "  0.5118666666666667,\n",
       "  0.5043559322033897,\n",
       "  0.503628762541806,\n",
       "  0.4788590604026845,\n",
       "  0.5327000000000001,\n",
       "  0.5031144781144782,\n",
       "  0.48271812080536913,\n",
       "  0.5067845117845118,\n",
       "  0.49646464646464644,\n",
       "  0.4830612244897959,\n",
       "  0.5031986531986532,\n",
       "  0.4832333333333334,\n",
       "  0.4886577181208054,\n",
       "  0.4918855218855218,\n",
       "  0.49601694915254235,\n",
       "  0.4565333333333333,\n",
       "  0.45774410774410773,\n",
       "  0.48843434343434344,\n",
       "  0.5857190635451505,\n",
       "  0.5081711409395974,\n",
       "  0.48962711864406777,\n",
       "  0.5194781144781144,\n",
       "  0.5469696969696969,\n",
       "  0.5201683501683502,\n",
       "  0.5416946308724833,\n",
       "  0.5088590604026845,\n",
       "  0.47798657718120807,\n",
       "  0.4559450171821306,\n",
       "  0.5213050847457628,\n",
       "  0.5049128919860627,\n",
       "  0.5483724832214766,\n",
       "  0.4914429530201342,\n",
       "  0.48945945945945946,\n",
       "  0.5220134228187919,\n",
       "  0.48978187919463084,\n",
       "  0.4788265306122449,\n",
       "  0.53,\n",
       "  0.535986394557823,\n",
       "  0.5250169491525424,\n",
       "  0.5112166666666667,\n",
       "  0.5064041095890411,\n",
       "  0.510284280936455,\n",
       "  0.5351170568561873,\n",
       "  0.4877871621621621,\n",
       "  0.5238926174496644,\n",
       "  0.47151006711409404,\n",
       "  0.49166666666666664,\n",
       "  0.5000501672240802,\n",
       "  0.5271644295302014,\n",
       "  0.5056849315068492,\n",
       "  0.5020000000000001,\n",
       "  0.5319833333333334,\n",
       "  0.5071812080536913,\n",
       "  0.4779292929292929,\n",
       "  0.5156734006734007,\n",
       "  0.5027533783783784,\n",
       "  0.5165993265993266,\n",
       "  0.48552364864864866,\n",
       "  0.513293918918919,\n",
       "  0.47290268456375845,\n",
       "  0.48233928571428575,\n",
       "  0.5655555555555557,\n",
       "  0.5246308724832214,\n",
       "  0.5150847457627118,\n",
       "  0.5121571906354515,\n",
       "  0.5071812080536913,\n",
       "  0.460016891891892,\n",
       "  0.49122448979591843,\n",
       "  0.5138435374149659,\n",
       "  0.47386824324324317,\n",
       "  0.4775844594594595,\n",
       "  0.5171070234113713,\n",
       "  0.46395973154362413,\n",
       "  0.49377516778523495,\n",
       "  0.5366275167785235,\n",
       "  0.4729264214046823,\n",
       "  0.44379661016949157,\n",
       "  0.47545454545454546,\n",
       "  0.5168135593220339,\n",
       "  0.5159090909090909,\n",
       "  0.4983053691275168,\n",
       "  0.5053511705685619,\n",
       "  0.49328859060402686,\n",
       "  0.4746476510067114,\n",
       "  0.46359999999999996,\n",
       "  0.4563833333333333,\n",
       "  0.5484006734006734,\n",
       "  0.5455183946488293,\n",
       "  0.45279461279461286,\n",
       "  0.5228813559322034,\n",
       "  0.4311643835616438,\n",
       "  0.5220707070707071,\n",
       "  0.4861262798634812,\n",
       "  0.4939597315436242,\n",
       "  0.5166722408026756,\n",
       "  0.5170101351351352,\n",
       "  0.5113255033557047,\n",
       "  0.5598154362416107,\n",
       "  0.5085570469798658,\n",
       "  0.4833833333333333,\n",
       "  0.48854237288135594,\n",
       "  0.5105555555555555,\n",
       "  0.5066891891891893,\n",
       "  0.4966835016835016,\n",
       "  0.5018333333333334,\n",
       "  0.523918918918919,\n",
       "  0.44809121621621617,\n",
       "  0.48876254180602,\n",
       "  0.43978260869565217,\n",
       "  0.47099326599326596,\n",
       "  0.5062626262626263,\n",
       "  0.4688628762541806,\n",
       "  0.5025589225589225,\n",
       "  0.5243265993265994,\n",
       "  0.5119932432432432,\n",
       "  0.5241107382550335,\n",
       "  0.49506756756756753,\n",
       "  0.49653846153846143,\n",
       "  0.4810166666666667,\n",
       "  0.4893311036789298,\n",
       "  0.5476510067114093,\n",
       "  0.5151689189189189,\n",
       "  0.5012248322147651,\n",
       "  0.5077181208053692,\n",
       "  0.4942687074829932,\n",
       "  0.4843602693602693,\n",
       "  0.46361774744027295,\n",
       "  0.49427852348993284,\n",
       "  0.47804347826086957,\n",
       "  0.4975335570469798,\n",
       "  0.5085284280936455,\n",
       "  0.4456711409395973,\n",
       "  0.5305685618729098,\n",
       "  0.49039518900343637,\n",
       "  0.4932214765100672,\n",
       "  0.5263299663299663,\n",
       "  0.5066442953020135,\n",
       "  0.5169666666666667,\n",
       "  0.5187166666666667,\n",
       "  0.5135906040268455,\n",
       "  0.5017500000000001,\n",
       "  0.5267391304347826,\n",
       "  0.4672758620689655,\n",
       "  0.441996644295302,\n",
       "  0.45343959731543626,\n",
       "  0.507996632996633,\n",
       "  0.5417796610169491,\n",
       "  0.5043589743589744,\n",
       "  0.5207046979865771,\n",
       "  0.49973244147157175,\n",
       "  0.5439965397923876,\n",
       "  0.51195,\n",
       "  0.46278333333333327,\n",
       "  0.495267558528428,\n",
       "  0.4887837837837838,\n",
       "  0.4894147157190636,\n",
       "  0.4703833333333334,\n",
       "  0.458656462585034,\n",
       "  0.4865488215488215,\n",
       "  0.4865151515151515,\n",
       "  0.48697986577181207,\n",
       "  0.5293624161073825,\n",
       "  0.47808333333333336,\n",
       "  0.48974832214765096,\n",
       "  0.5407457627118644,\n",
       "  0.5500838926174496,\n",
       "  0.4802861952861952,\n",
       "  0.4971896551724138,\n",
       "  0.5290492957746479,\n",
       "  0.5006587837837838,\n",
       "  0.5273050847457628,\n",
       "  0.5273063973063973,\n",
       "  0.5167449664429531,\n",
       "  0.5172818791946308,\n",
       "  0.500807560137457,\n",
       "  0.5467182130584193,\n",
       "  0.4913095238095238,\n",
       "  0.4942,\n",
       "  0.5059166666666666,\n",
       "  0.4840969899665552,\n",
       "  0.49764214046822747,\n",
       "  0.4466722408026756,\n",
       "  0.5084175084175084,\n",
       "  0.4827946127946128,\n",
       "  0.4974579124579125,\n",
       "  0.5363344594594595,\n",
       "  0.45239130434782604,\n",
       "  0.47728668941979524,\n",
       "  0.5317833333333333,\n",
       "  0.4759459459459459,\n",
       "  0.46056856187290973,\n",
       "  0.44904682274247487,\n",
       "  0.4806543624161073,\n",
       "  0.5033110367892976,\n",
       "  0.5047098976109214,\n",
       "  0.5346822742474917,\n",
       "  0.5256949152542373,\n",
       "  0.5594630872483223,\n",
       "  0.49952702702702706,\n",
       "  0.5266107382550336,\n",
       "  0.45636824324324315,\n",
       "  0.5017857142857143,\n",
       "  0.5189372822299652,\n",
       "  0.5319063545150502,\n",
       "  0.45967905405405407,\n",
       "  0.4758333333333334,\n",
       "  0.48501666666666665,\n",
       "  0.5003523489932885,\n",
       "  0.5170401337792642,\n",
       "  0.504113712374582,\n",
       "  0.508238255033557,\n",
       "  0.4398657718120806,\n",
       "  0.5301398601398601,\n",
       "  0.49693602693602684,\n",
       "  0.4836206896551724,\n",
       "  0.49195578231292525,\n",
       "  0.5100847457627119,\n",
       "  0.4981939799331103,\n",
       "  0.4716836734693877,\n",
       "  0.4757692307692308,\n",
       "  0.5164983164983166,\n",
       "  0.5153547297297297,\n",
       "  0.53496632996633,\n",
       "  0.49687290969899667,\n",
       "  0.5302333333333333,\n",
       "  0.4845608108108108,\n",
       "  0.5183557046979865,\n",
       "  0.48102006688963206,\n",
       "  0.5290816326530613,\n",
       "  0.5077609427609427,\n",
       "  0.5376870748299319,\n",
       "  0.5050859106529211,\n",
       "  0.46691287878787885,\n",
       "  0.5140909090909092,\n",
       "  0.5358361204013377,\n",
       "  0.47481481481481475,\n",
       "  0.5000677966101694,\n",
       "  0.4756849315068493,\n",
       "  0.4824579124579124,\n",
       "  0.4672699386503067,\n",
       "  0.46635451505016723,\n",
       "  0.46130872483221474,\n",
       "  0.45248322147651,\n",
       "  0.4691666666666667,\n",
       "  0.5060570469798658,\n",
       "  0.5072315436241611,\n",
       "  0.5033108108108109,\n",
       "  0.5015333333333334,\n",
       "  0.5314827586206896,\n",
       "  0.49759197324414717,\n",
       "  0.5166946308724834,\n",
       "  0.5557586206896552,\n",
       "  0.5018537414965987,\n",
       "  0.47282094594594587,\n",
       "  0.5040802675585285,\n",
       "  0.46540000000000004,\n",
       "  0.510685618729097,\n",
       "  0.5266442953020134,\n",
       "  0.5618518518518518,\n",
       "  0.5138628762541806,\n",
       "  0.5183220338983051,\n",
       "  0.5100338983050847,\n",
       "  0.48618729096989965,\n",
       "  0.4725671140939597,\n",
       "  0.4839455782312924,\n",
       "  0.49770903010033446,\n",
       "  0.4906228956228956,\n",
       "  0.4995986622073578,\n",
       "  0.5080333333333333,\n",
       "  0.5122895622895624,\n",
       "  0.5230201342281879,\n",
       "  0.5344781144781146,\n",
       "  0.499247491638796,\n",
       "  0.4961371237458194,\n",
       "  0.5001712328767124,\n",
       "  0.48861204013377935,\n",
       "  0.529911660777385,\n",
       "  0.5797378277153558,\n",
       "  0.5041750841750842,\n",
       "  0.5207407407407407,\n",
       "  0.5282046979865772,\n",
       "  0.4871503496503496,\n",
       "  0.5263682432432433,\n",
       "  0.4830236486486486,\n",
       "  0.5149659863945577,\n",
       "  0.4695454545454546,\n",
       "  0.4567060810810811,\n",
       "  0.4754333333333333,\n",
       "  0.5072833333333333,\n",
       "  0.5498464163822526,\n",
       "  0.47944816053511713,\n",
       "  0.4906688963210702,\n",
       "  0.48826013513513516,\n",
       "  0.47282608695652173,\n",
       "  0.5336655405405405,\n",
       "  0.5135953177257525,\n",
       "  0.4922128378378378,\n",
       "  0.49088339222614846,\n",
       "  0.5244630872483221,\n",
       "  0.4909833333333334,\n",
       "  0.5047306397306397,\n",
       "  0.5325,\n",
       "  0.5394816053511706,\n",
       "  0.5171571906354515,\n",
       "  0.46841750841750845,\n",
       "  0.4944932432432433,\n",
       "  0.4716440677966101,\n",
       "  0.4674067796610169,\n",
       "  0.48571186440677966,\n",
       "  0.4123745819397993,\n",
       "  0.5183,\n",
       "  0.5105593220338983,\n",
       "  0.48998322147651,\n",
       "  0.45032758620689656,\n",
       "  0.5105685618729098,\n",
       "  0.5066610169491526,\n",
       "  0.5167294520547945,\n",
       "  0.49939236111111107,\n",
       "  0.5463973063973064,\n",
       "  0.5363758389261745,\n",
       "  0.4928378378378378,\n",
       "  0.5065551839464884,\n",
       "  0.49703020134228193,\n",
       "  0.5005536912751678,\n",
       "  0.4879194630872483,\n",
       "  0.5562542372881356,\n",
       "  0.4960714285714285,\n",
       "  0.535752508361204,\n",
       "  0.48734006734006735,\n",
       "  0.5366161616161617,\n",
       "  0.4567966101694915,\n",
       "  0.47459930313588844,\n",
       "  0.4678428093645486,\n",
       "  0.5279642857142858,\n",
       "  0.476406779661017,\n",
       "  0.5166666666666666,\n",
       "  0.5283506944444445,\n",
       "  0.5005782312925171,\n",
       "  0.5298648648648648,\n",
       "  0.4943265993265994,\n",
       "  0.4976923076923077,\n",
       "  0.5194463087248322,\n",
       "  0.44621621621621615,\n",
       "  0.5328355704697987,\n",
       "  0.47942760942760937,\n",
       "  0.526955017301038,\n",
       "  0.5197315436241611,\n",
       "  0.4692140468227424,\n",
       "  0.49666107382550334,\n",
       "  0.4877380952380952,\n",
       "  0.5018074324324324,\n",
       "  0.5018813559322034,\n",
       "  0.5158666666666666,\n",
       "  0.443656462585034,\n",
       "  0.5242881355932204,\n",
       "  0.4693771043771044,\n",
       "  0.49761666666666665,\n",
       "  0.46993150684931506,\n",
       "  0.5056164383561643,\n",
       "  0.4980033557046979,\n",
       "  0.4683946488294315,\n",
       "  0.49704697986577184,\n",
       "  0.5123050847457628,\n",
       "  0.5394444444444444,\n",
       "  0.4548821548821549,\n",
       "  0.5293749999999999,\n",
       "  0.5486577181208053,\n",
       "  0.47656028368794323,\n",
       "  0.5071400778210116,\n",
       "  0.4944983277591973,\n",
       "  0.5114814814814814,\n",
       "  0.5111130136986302,\n",
       "  0.5341442953020135,\n",
       "  0.45018644067796615,\n",
       "  0.4925250836120401,\n",
       "  0.5056666666666666,\n",
       "  0.5226094276094276,\n",
       "  0.4699665551839464,\n",
       "  0.49259197324414716,\n",
       "  0.5526599326599326,\n",
       "  0.5140816326530612,\n",
       "  0.5001864406779661,\n",
       "  0.5378741496598639,\n",
       "  0.4985304054054054,\n",
       "  0.5328666666666666,\n",
       "  0.46297658862876245,\n",
       "  0.555819397993311,\n",
       "  0.4975513698630137,\n",
       "  0.48117845117845115,\n",
       "  0.5400335570469799,\n",
       "  0.49986622073578596,\n",
       "  0.4523129251700681,\n",
       "  0.48178333333333323,\n",
       "  0.5073905723905724,\n",
       "  0.5145317725752508,\n",
       "  0.4736166666666666,\n",
       "  0.4924916387959866,\n",
       "  0.5481,\n",
       "  0.47810810810810805,\n",
       "  0.5452516778523491,\n",
       "  0.5171404682274248,\n",
       "  0.528758389261745,\n",
       "  0.5353198653198653,\n",
       "  0.4654095563139932,\n",
       "  0.49795986622073574,\n",
       "  0.5393120805369127,\n",
       "  0.4961111111111111,\n",
       "  0.5351877133105802,\n",
       "  0.5276421404682273,\n",
       "  0.5035135135135136,\n",
       "  0.5030704697986577,\n",
       "  0.5063087248322148,\n",
       "  0.5090410958904109,\n",
       "  0.5042953020134229,\n",
       "  0.5060437710437711,\n",
       "  0.5060207612456747,\n",
       "  0.4599,\n",
       "  0.4940833333333333,\n",
       "  0.49186666666666656,\n",
       "  0.5075,\n",
       "  0.4775506756756757,\n",
       "  0.5290969899665552,\n",
       "  0.5388422818791946,\n",
       "  0.4756521739130434,\n",
       "  0.49442953020134217,\n",
       "  0.5066610738255033,\n",
       "  0.5006574394463668,\n",
       "  0.5387585034013604,\n",
       "  0.4897333333333334,\n",
       "  0.4546938775510204,\n",
       "  0.5045622895622897,\n",
       "  0.4798657718120805,\n",
       "  0.49679530201342287,\n",
       "  0.45470486111111114,\n",
       "  0.4589249146757679,\n",
       "  0.4944463087248322,\n",
       "  0.48383445945945946,\n",
       "  0.4941275167785235,\n",
       "  0.47627516778523493,\n",
       "  0.5445117845117845,\n",
       "  0.5338758389261745,\n",
       "  0.48216949152542377,\n",
       "  0.5158862876254181,\n",
       "  0.469728813559322,\n",
       "  0.48269230769230764,\n",
       "  0.5360535117056856,\n",
       "  0.5240133779264214,\n",
       "  0.4555743243243243,\n",
       "  0.49843959731543624,\n",
       "  0.5311371237458195,\n",
       "  0.5069630872483221,\n",
       "  0.500909090909091,\n",
       "  0.48976351351351344,\n",
       "  0.48872053872053867,\n",
       "  0.45912457912457916,\n",
       "  0.46890784982935146,\n",
       "  0.45692567567567566,\n",
       "  0.4915306122448979,\n",
       "  0.49902356902356904,\n",
       "  0.46666107382550337,\n",
       "  0.4708419243986254,\n",
       "  0.5011872909698997,\n",
       "  0.5305872483221477,\n",
       "  0.5230872483221476,\n",
       "  0.505358361774744,\n",
       "  0.4978187919463087,\n",
       "  0.47873310810810815,\n",
       "  0.5406462585034013,\n",
       "  0.48658249158249156,\n",
       "  0.4602852348993288,\n",
       "  0.49903061224489786,\n",
       "  0.5247147651006712,\n",
       "  0.4922354948805461,\n",
       "  0.4661979166666666,\n",
       "  0.4730743243243242,\n",
       "  0.5531740614334472,\n",
       "  0.4541166666666666,\n",
       "  0.5167725752508361,\n",
       "  0.5045484949832776,\n",
       "  0.495133779264214,\n",
       "  0.5152210884353742,\n",
       "  0.5069763513513513,\n",
       "  0.4826755852842809,\n",
       "  0.4423817567567567,\n",
       "  0.5331103678929766,\n",
       "  0.5685785953177257,\n",
       "  0.489625850340136,\n",
       "  0.5007239057239057,\n",
       "  0.4820608108108108,\n",
       "  0.5039716312056738,\n",
       "  0.5684228187919462,\n",
       "  0.5489864864864865,\n",
       "  0.48911073825503354,\n",
       "  0.48269230769230764,\n",
       "  0.5438698630136987,\n",
       "  0.5119023569023569,\n",
       "  0.4885333333333333,\n",
       "  0.480958904109589,\n",
       "  0.498013468013468,\n",
       "  0.4616889632107023,\n",
       "  0.4512962962962963,\n",
       "  0.5055685618729098,\n",
       "  0.5036833333333334,\n",
       "  0.44135906040268447,\n",
       "  0.494244966442953,\n",
       "  0.4932094594594595,\n",
       "  0.5106565656565657,\n",
       "  0.5272240802675586,\n",
       "  0.5252,\n",
       "  0.42759197324414716,\n",
       "  0.5048494983277592,\n",
       "  0.5137123745819397,\n",
       "  0.5179933110367893,\n",
       "  0.5198154362416108,\n",
       "  0.4791836734693877,\n",
       "  0.487473309608541,\n",
       "  0.4690404040404041,\n",
       "  0.5364814814814816,\n",
       "  0.5004194630872483,\n",
       "  0.4989527027027027,\n",
       "  0.528221476510067,\n",
       "  0.4994147157190635,\n",
       "  0.4645084745762712,\n",
       "  0.5141638795986623,\n",
       "  0.4861833333333334,\n",
       "  0.5608557046979865,\n",
       "  0.4958389261744966,\n",
       "  0.5268412162162162,\n",
       "  0.533063973063973,\n",
       "  0.4922315436241611,\n",
       "  0.4907167832167832,\n",
       "  0.5356187290969899,\n",
       "  0.4779333333333333,\n",
       "  0.5435374149659864,\n",
       "  0.5492642140468228,\n",
       "  0.5113131313131314,\n",
       "  0.5003703703703705,\n",
       "  0.48351027397260277,\n",
       "  0.4809197324414715,\n",
       "  0.5025862068965518,\n",
       "  0.4604682274247492,\n",
       "  0.4492166666666667,\n",
       "  0.48855442176870745,\n",
       "  0.5059661016949152,\n",
       "  0.5030677966101695,\n",
       "  0.5280574324324324,\n",
       "  0.48538461538461536,\n",
       "  0.5003898305084746,\n",
       "  0.5043602693602693,\n",
       "  0.4842592592592592,\n",
       "  0.5099999999999999,\n",
       "  0.5259075342465753,\n",
       "  0.5043265993265993,\n",
       "  0.5038986013986014,\n",
       "  0.5428595317725753,\n",
       "  0.4966101694915254,\n",
       "  0.475938566552901,\n",
       "  0.5332312925170068,\n",
       "  0.5128787878787878,\n",
       "  0.5173657718120805,\n",
       "  0.520017006802721,\n",
       "  0.4843835616438356,\n",
       "  0.512195945945946,\n",
       "  0.5120401337792642,\n",
       "  0.46167796610169487,\n",
       "  0.5137205387205387,\n",
       "  0.49235000000000007,\n",
       "  0.46937074829931974,\n",
       "  0.5396801346801346,\n",
       "  0.5037414965986394,\n",
       "  0.4554013377926422,\n",
       "  0.5137123745819397,\n",
       "  0.5187959866220736,\n",
       "  0.49273809523809525,\n",
       "  0.5465033783783785,\n",
       "  0.4852341137123745,\n",
       "  0.4638087248322148,\n",
       "  0.4925999999999999,\n",
       "  0.5218624161073825,\n",
       "  0.5102210884353742,\n",
       "  0.5035953177257526,\n",
       "  0.5272635135135135,\n",
       "  0.5160535117056857,\n",
       "  0.5172318339100346,\n",
       "  0.5257550335570469,\n",
       "  0.5253187919463088,\n",
       "  0.48585,\n",
       "  0.47840604026845635,\n",
       "  0.5230639730639731,\n",
       "  0.5256333333333333,\n",
       "  0.5207407407407408,\n",
       "  0.5159628378378378,\n",
       "  0.4512833333333334,\n",
       "  0.5223833333333332,\n",
       "  0.5004180602006688,\n",
       "  0.526081081081081,\n",
       "  0.4953924914675768,\n",
       "  0.5052181208053691,\n",
       "  0.5069491525423729,\n",
       "  0.4866156462585034,\n",
       "  0.5370446735395188,\n",
       "  0.47402027027027027,\n",
       "  0.5019565217391304,\n",
       "  0.4997619047619048,\n",
       "  0.5488356164383561,\n",
       "  0.4836744966442953,\n",
       "  0.5070134228187919,\n",
       "  0.4222977941176471,\n",
       "  0.5241973244147157,\n",
       "  0.5013131313131313,\n",
       "  0.4863804713804714,\n",
       "  0.4891806020066889,\n",
       "  0.47340677966101685,\n",
       "  0.5040371621621621,\n",
       "  0.48975,\n",
       "  0.5010333333333333,\n",
       "  0.45654882154882154,\n",
       "  0.4991694915254237,\n",
       "  0.49202749140893476,\n",
       "  0.482257525083612,\n",
       "  0.5496283783783784,\n",
       "  0.5146271186440677,\n",
       "  0.5038344594594595,\n",
       "  0.5180333333333335,\n",
       "  0.49342372881355934,\n",
       "  0.4841609589041096,\n",
       "  0.4779964539007093,\n",
       "  0.452491103202847,\n",
       "  0.4239655172413793,\n",
       "  0.5092114093959732,\n",
       "  0.5139130434782608,\n",
       "  0.5305460750853241,\n",
       "  0.5170234113712374,\n",
       "  0.5575762711864406,\n",
       "  0.5381525423728813,\n",
       "  0.4766440677966102,\n",
       "  0.5018791946308725,\n",
       "  0.5277591973244148,\n",
       "  0.49909090909090903,\n",
       "  0.4676101694915254,\n",
       "  0.42266666666666663,\n",
       "  0.5000501672240802,\n",
       "  0.47817114093959734,\n",
       "  0.5228305084745762,\n",
       "  0.505271186440678,\n",
       "  0.511097972972973,\n",
       "  0.47923208191126276,\n",
       "  0.5144501718213058,\n",
       "  0.5045890410958904,\n",
       "  0.5002020202020202,\n",
       "  0.44483221476510065,\n",
       "  0.5303231292517007,\n",
       "  0.5094027303754266,\n",
       "  0.5200853242320819,\n",
       "  0.4550337837837838,\n",
       "  0.5034280936454849,\n",
       "  0.50665,\n",
       "  0.5201045296167247,\n",
       "  0.4774319727891157,\n",
       "  0.5169006849315069,\n",
       "  0.48135451505016724,\n",
       "  0.4598305084745764,\n",
       "  0.5046140939597316,\n",
       "  0.4995296167247387,\n",
       "  0.4730333333333334,\n",
       "  0.5130035335689045,\n",
       "  0.48885159010600704,\n",
       "  0.5146610169491525,\n",
       "  0.5254682274247492,\n",
       "  0.4743478260869566,\n",
       "  0.5199662162162163,\n",
       "  0.5057023411371236,\n",
       "  0.5231438127090301,\n",
       "  0.5608614864864865,\n",
       "  0.4971790540540541,\n",
       "  0.48184745762711856,\n",
       "  0.510976430976431,\n",
       "  0.46415224913494807,\n",
       "  0.4749333333333334,\n",
       "  0.4461872909698997,\n",
       "  0.4753741496598639,\n",
       "  0.5235517241379309,\n",
       "  0.4950505050505051,\n",
       "  0.48547457627118634,\n",
       "  0.522003367003367,\n",
       "  0.5081605351170568,\n",
       "  0.5303678929765887,\n",
       "  0.5261577181208055,\n",
       "  0.4836409395973154,\n",
       "  0.5152203389830509,\n",
       "  0.5020302013422818,\n",
       "  0.5234563758389262,\n",
       "  0.49518456375838926,\n",
       "  0.5230267558528428,\n",
       "  0.5089057239057239,\n",
       "  0.48107744107744105,\n",
       "  0.518030303030303,\n",
       "  0.5238737201365188,\n",
       "  0.5152173913043478,\n",
       "  0.535200668896321,\n",
       "  0.5139298245614035,\n",
       "  0.5151351351351352,\n",
       "  0.4877554744525547,\n",
       "  0.46309364548494986,\n",
       "  0.48726510067114087,\n",
       "  0.450551839464883,\n",
       "  0.514061433447099,\n",
       "  0.5691891891891891,\n",
       "  0.4673578595317726,\n",
       "  0.4878082191780822,\n",
       "  0.4987458193979933,\n",
       "  0.5289965986394558,\n",
       "  0.5219377162629757,\n",
       "  0.5110508474576271,\n",
       "  0.49447635135135126,\n",
       "  0.4701333333333334,\n",
       "  0.5135304054054054,\n",
       "  0.48963087248322146,\n",
       "  0.5198160535117057,\n",
       "  0.5050841750841751,\n",
       "  0.5108557046979866,\n",
       "  0.4918750000000001,\n",
       "  0.482265100671141,\n",
       "  0.4949488054607508,\n",
       "  0.5188006756756757,\n",
       "  0.45106060606060616,\n",
       "  0.462809364548495,\n",
       "  0.5123400673400674,\n",
       "  0.4654406779661017,\n",
       "  0.49234113712374583,\n",
       "  0.4596153846153846,\n",
       "  0.4907094594594595,\n",
       "  0.4710906040268456,\n",
       "  0.49494999999999995,\n",
       "  0.49275337837837835,\n",
       "  0.49436026936026933,\n",
       "  0.5196127946127946,\n",
       "  0.49658862876254184,\n",
       "  0.4872742474916388,\n",
       "  0.5145959595959595,\n",
       "  0.5061616161616161,\n",
       "  0.49749152542372876,\n",
       "  0.5254560810810811,\n",
       "  0.47545,\n",
       "  0.48472426470588237,\n",
       "  0.4987919463087248,\n",
       "  0.47237458193979937,\n",
       "  0.5615084745762712,\n",
       "  0.4822727272727273,\n",
       "  0.48645,\n",
       "  0.5070439189189189,\n",
       "  0.5053220338983051,\n",
       "  0.49305,\n",
       "  0.5112583892617449,\n",
       "  0.4890969899665552,\n",
       "  0.510484429065744,\n",
       "  0.5253620689655172,\n",
       "  0.5191973244147158,\n",
       "  0.5092976588628763,\n",
       "  0.5131313131313131,\n",
       "  0.4773630136986301,\n",
       "  0.5302684563758389,\n",
       "  0.5491525423728814,\n",
       "  0.48425423728813566,\n",
       "  0.5106506849315069,\n",
       "  0.455503355704698,\n",
       "  0.48018333333333335,\n",
       "  0.5175084745762711,\n",
       "  0.4454713804713805,\n",
       "  0.5266952054794521,\n",
       "  0.5041,\n",
       "  0.4888758389261745,\n",
       "  0.5248793103448276,\n",
       "  0.5214527027027027,\n",
       "  0.4889115646258504,\n",
       "  0.4946476510067113,\n",
       "  0.5109060402684564,\n",
       "  0.4905841924398625,\n",
       "  0.5415136054421769,\n",
       "  0.4739225589225589,\n",
       "  0.4820066889632107,\n",
       "  0.5071717171717172,\n",
       "  0.48118243243243247,\n",
       "  0.4839799331103678,\n",
       "  0.5100692041522491,\n",
       "  0.5274406779661018,\n",
       "  0.48291666666666666,\n",
       "  0.5007023411371236,\n",
       "  0.5142424242424243,\n",
       "  0.5137244897959184,\n",
       "  0.47630508474576266,\n",
       "  0.512235494880546,\n",
       "  0.47639261744966444,\n",
       "  0.5735374149659864,\n",
       "  0.4953547297297297,\n",
       "  0.4865239726027397,\n",
       "  0.4729109589041096,\n",
       "  0.5132570422535211,\n",
       "  0.5226500000000001,\n",
       "  0.5301858108108108,\n",
       "  0.5030808080808081,\n",
       "  0.519639175257732,\n",
       "  0.49681666666666674,\n",
       "  0.5193166666666666,\n",
       "  0.5241385135135135,\n",
       "  0.5038552188552189,\n",
       "  0.5026858108108109,\n",
       "  0.5410942760942761,\n",
       "  0.487457627118644,\n",
       "  0.526411149825784,\n",
       "  0.4636409395973154,\n",
       "  0.4996644295302013,\n",
       "  0.5118150684931506,\n",
       "  0.4806944444444444,\n",
       "  0.5002188552188552,\n",
       "  0.5079124579124579,\n",
       "  0.5134395973154362,\n",
       "  0.5162331081081082,\n",
       "  0.4864093959731543,\n",
       "  0.515135593220339,\n",
       "  0.5190102389078498,\n",
       "  0.48917508417508415,\n",
       "  0.47930000000000006,\n",
       "  0.4974657534246575,\n",
       "  0.4927966101694915,\n",
       "  0.44914141414141406,\n",
       "  0.46631205673758863,\n",
       "  0.5044763513513513,\n",
       "  0.5261166666666667,\n",
       "  0.5181333333333333,\n",
       "  0.5524581939799331,\n",
       "  0.46637583892617446,\n",
       "  0.44101666666666667,\n",
       "  0.4919520547945206,\n",
       "  0.5063210702341137,\n",
       "  0.471075085324232,\n",
       "  0.5543959731543624,\n",
       "  0.4787668918918919,\n",
       "  0.5108333333333334,\n",
       "  0.4379010238907849,\n",
       "  0.4526174496644295,\n",
       "  0.4366779661016949,\n",
       "  0.516904761904762,\n",
       "  0.5107357859531773,\n",
       "  0.5177609427609426,\n",
       "  0.487929292929293,\n",
       "  0.474765100671141,\n",
       "  0.5301174496644295,\n",
       "  0.5184406779661017,\n",
       "  0.49900673400673395,\n",
       "  0.4600344827586207,\n",
       "  0.4804865771812081,\n",
       "  0.528293918918919,\n",
       "  0.4983166666666667,\n",
       "  0.46097972972972967,\n",
       "  0.5179,\n",
       "  0.46648014440433216,\n",
       "  0.5118600682593857,\n",
       "  0.4416271186440679,\n",
       "  0.4818855218855219,\n",
       "  0.4699,\n",
       "  0.5159698996655517,\n",
       "  0.4958249158249158,\n",
       "  0.5030743243243243,\n",
       "  0.49486440677966104,\n",
       "  0.4897972972972974,\n",
       "  0.5147610921501706,\n",
       "  0.5106,\n",
       "  0.4856020066889632,\n",
       "  0.5267676767676768,\n",
       "  0.4672390572390573,\n",
       "  0.48638513513513515,\n",
       "  0.514006734006734,\n",
       "  0.5192809364548495,\n",
       "  0.5135467128027681,\n",
       "  0.4753523489932886,\n",
       "  0.4998653198653199,\n",
       "  0.46045150501672244,\n",
       "  0.5404529616724739,\n",
       "  0.5074155405405405,\n",
       "  0.4665100671140939,\n",
       "  0.5163356164383561,\n",
       "  0.5099319727891157,\n",
       "  0.5407070707070707,\n",
       "  0.5230442176870749,\n",
       "  0.5133503401360544,\n",
       "  0.5255369127516779,\n",
       "  0.5067905405405405,\n",
       "  0.45671821305841925,\n",
       "  0.4676949152542373,\n",
       "  0.465456081081081,\n",
       "  0.5139795918367347,\n",
       "  0.43610921501706484,\n",
       "  0.5139333333333334,\n",
       "  0.5273986486486487,\n",
       "  0.4956040268456376,\n",
       "  ...]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_trainer._eval_save_prefix = OUTPUT_PREFIX + \"-test-pos-biased\"\n",
    "model_trainer._evaluate_partial(test_dataset_pos_biased)\n",
    "\n",
    "model_trainer._eval_save_prefix = OUTPUT_PREFIX +  \"-test-neg-biased\"\n",
    "model_trainer._evaluate_partial(test_dataset_neg_biased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unbiased Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2296/2296 [00:01<00:00, 1305.78it/s]\n",
      "100%|██████████| 2296/2296 [00:03<00:00, 754.67it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AUC': [0.5816666666666667,\n",
       "  0.525,\n",
       "  0.46749999999999997,\n",
       "  0.5138888888888888,\n",
       "  0.43555555555555553,\n",
       "  0.5342857142857143,\n",
       "  0.37166666666666676,\n",
       "  0.5528571428571428,\n",
       "  0.41874999999999996,\n",
       "  0.4511111111111111,\n",
       "  0.3425,\n",
       "  0.47285714285714286,\n",
       "  0.5835714285714287,\n",
       "  0.403125,\n",
       "  0.54,\n",
       "  0.5227777777777778,\n",
       "  0.47611111111111115,\n",
       "  0.516875,\n",
       "  0.49357142857142866,\n",
       "  0.5322222222222223,\n",
       "  0.51625,\n",
       "  0.5199999999999999,\n",
       "  0.3577777777777778,\n",
       "  0.489375,\n",
       "  0.6537499999999999,\n",
       "  0.294375,\n",
       "  0.59,\n",
       "  0.583125,\n",
       "  0.411875,\n",
       "  0.6044444444444443,\n",
       "  0.2621428571428571,\n",
       "  0.48888888888888893,\n",
       "  0.45999999999999996,\n",
       "  0.4927777777777778,\n",
       "  0.45500000000000007,\n",
       "  0.6407142857142858,\n",
       "  0.678125,\n",
       "  0.4683333333333333,\n",
       "  0.5394444444444445,\n",
       "  0.3585714285714286,\n",
       "  0.54375,\n",
       "  0.41125,\n",
       "  0.661875,\n",
       "  0.655,\n",
       "  0.41111111111111115,\n",
       "  0.491875,\n",
       "  0.6221428571428572,\n",
       "  0.3499999999999999,\n",
       "  0.5194444444444444,\n",
       "  0.47888888888888886,\n",
       "  0.5294444444444445,\n",
       "  0.485,\n",
       "  0.37999999999999995,\n",
       "  0.44055555555555553,\n",
       "  0.4294444444444444,\n",
       "  0.5214285714285715,\n",
       "  0.6074999999999999,\n",
       "  0.6625,\n",
       "  0.4600000000000001,\n",
       "  0.501,\n",
       "  0.5433333333333333,\n",
       "  0.7361111111111112,\n",
       "  0.4511111111111112,\n",
       "  0.5107142857142858,\n",
       "  0.3922222222222222,\n",
       "  0.7216666666666668,\n",
       "  0.4166666666666667,\n",
       "  0.6325000000000001,\n",
       "  0.4155555555555555,\n",
       "  0.5083333333333333,\n",
       "  0.5311111111111111,\n",
       "  0.444375,\n",
       "  0.355,\n",
       "  0.27499999999999997,\n",
       "  0.345,\n",
       "  0.42055555555555557,\n",
       "  0.44777777777777783,\n",
       "  0.5127777777777778,\n",
       "  0.5016666666666667,\n",
       "  0.435625,\n",
       "  0.5625,\n",
       "  0.625,\n",
       "  0.319375,\n",
       "  0.3875,\n",
       "  0.6477777777777778,\n",
       "  0.49833333333333335,\n",
       "  0.41666666666666674,\n",
       "  0.535,\n",
       "  0.583125,\n",
       "  0.48000000000000004,\n",
       "  0.5594444444444444,\n",
       "  0.5566666666666666,\n",
       "  0.44875,\n",
       "  0.40388888888888885,\n",
       "  0.6741666666666667,\n",
       "  0.4444444444444444,\n",
       "  0.4427777777777777,\n",
       "  0.6133333333333333,\n",
       "  0.39444444444444443,\n",
       "  0.5042857142857143,\n",
       "  0.474375,\n",
       "  0.5433333333333334,\n",
       "  0.42777777777777776,\n",
       "  0.5661111111111112,\n",
       "  0.6238888888888888,\n",
       "  0.616111111111111,\n",
       "  0.6991666666666667,\n",
       "  0.304375,\n",
       "  0.6499999999999999,\n",
       "  0.6207142857142857,\n",
       "  0.5394444444444445,\n",
       "  0.5444444444444444,\n",
       "  0.3877777777777778,\n",
       "  0.4275,\n",
       "  0.519375,\n",
       "  0.523125,\n",
       "  0.405625,\n",
       "  0.49944444444444447,\n",
       "  0.45375,\n",
       "  0.32928571428571424,\n",
       "  0.36833333333333335,\n",
       "  0.5994444444444444,\n",
       "  0.4844444444444444,\n",
       "  0.5675,\n",
       "  0.5016666666666667,\n",
       "  0.43333333333333335,\n",
       "  0.37357142857142855,\n",
       "  0.4366666666666667,\n",
       "  0.43124999999999997,\n",
       "  0.4421428571428571,\n",
       "  0.715,\n",
       "  0.47375,\n",
       "  0.43333333333333335,\n",
       "  0.59875,\n",
       "  0.58375,\n",
       "  0.4427777777777777,\n",
       "  0.43388888888888894,\n",
       "  0.4825,\n",
       "  0.48444444444444446,\n",
       "  0.48777777777777787,\n",
       "  0.5155555555555557,\n",
       "  0.5166666666666666,\n",
       "  0.577,\n",
       "  0.5705555555555556,\n",
       "  0.4872222222222222,\n",
       "  0.421875,\n",
       "  0.6294444444444444,\n",
       "  0.4211111111111111,\n",
       "  0.350625,\n",
       "  0.3483333333333334,\n",
       "  0.6427777777777778,\n",
       "  0.4244444444444444,\n",
       "  0.510625,\n",
       "  0.43375,\n",
       "  0.5188888888888888,\n",
       "  0.5321428571428571,\n",
       "  0.3933333333333333,\n",
       "  0.3716666666666667,\n",
       "  0.385625,\n",
       "  0.4122222222222222,\n",
       "  0.27166666666666667,\n",
       "  0.4983333333333333,\n",
       "  0.5511111111111112,\n",
       "  0.5094444444444445,\n",
       "  0.47,\n",
       "  0.631875,\n",
       "  0.49714285714285705,\n",
       "  0.2225,\n",
       "  0.5477777777777778,\n",
       "  0.314375,\n",
       "  0.5675,\n",
       "  0.56,\n",
       "  0.433125,\n",
       "  0.37833333333333335,\n",
       "  0.6372222222222222,\n",
       "  0.42499999999999993,\n",
       "  0.45999999999999996,\n",
       "  0.5625,\n",
       "  0.5272222222222224,\n",
       "  0.5522222222222223,\n",
       "  0.49,\n",
       "  0.5718749999999999,\n",
       "  0.49285714285714277,\n",
       "  0.5800000000000001,\n",
       "  0.45428571428571424,\n",
       "  0.4011111111111111,\n",
       "  0.4855555555555556,\n",
       "  0.48125,\n",
       "  0.4927777777777778,\n",
       "  0.6166666666666667,\n",
       "  0.504375,\n",
       "  0.4144444444444445,\n",
       "  0.36062500000000003,\n",
       "  0.3422222222222222,\n",
       "  0.3157142857142857,\n",
       "  0.5462499999999999,\n",
       "  0.47333333333333333,\n",
       "  0.4125,\n",
       "  0.33111111111111113,\n",
       "  0.748125,\n",
       "  0.5005555555555556,\n",
       "  0.520625,\n",
       "  0.4138888888888889,\n",
       "  0.6472222222222223,\n",
       "  0.5788888888888888,\n",
       "  0.4228571428571429,\n",
       "  0.2683333333333333,\n",
       "  0.5755555555555557,\n",
       "  0.4433333333333333,\n",
       "  0.38,\n",
       "  0.5222222222222221,\n",
       "  0.47277777777777774,\n",
       "  0.416875,\n",
       "  0.51875,\n",
       "  0.47277777777777774,\n",
       "  0.49125,\n",
       "  0.48888888888888893,\n",
       "  0.5188888888888888,\n",
       "  0.4822222222222222,\n",
       "  0.5077777777777778,\n",
       "  0.47000000000000003,\n",
       "  0.45944444444444443,\n",
       "  0.47,\n",
       "  0.6172222222222222,\n",
       "  0.4955555555555555,\n",
       "  0.3933333333333333,\n",
       "  0.615,\n",
       "  0.51,\n",
       "  0.2857142857142857,\n",
       "  0.490625,\n",
       "  0.3005555555555556,\n",
       "  0.32333333333333336,\n",
       "  0.45055555555555554,\n",
       "  0.6244444444444445,\n",
       "  0.37444444444444447,\n",
       "  0.6058333333333333,\n",
       "  0.3705555555555555,\n",
       "  0.3214285714285715,\n",
       "  0.52125,\n",
       "  0.6638888888888889,\n",
       "  0.5225,\n",
       "  0.44555555555555554,\n",
       "  0.7999999999999998,\n",
       "  0.39375,\n",
       "  0.4788888888888889,\n",
       "  0.6928571428571428,\n",
       "  0.43666666666666665,\n",
       "  0.40928571428571425,\n",
       "  0.53625,\n",
       "  0.4640000000000001,\n",
       "  0.526875,\n",
       "  0.5894444444444444,\n",
       "  0.5155555555555555,\n",
       "  0.5383333333333333,\n",
       "  0.5338888888888889,\n",
       "  0.5716666666666668,\n",
       "  0.4672222222222222,\n",
       "  0.37888888888888894,\n",
       "  0.49500000000000005,\n",
       "  0.4683333333333333,\n",
       "  0.36722222222222217,\n",
       "  0.5942857142857143,\n",
       "  0.445625,\n",
       "  0.4855555555555556,\n",
       "  0.3933333333333333,\n",
       "  0.5588888888888889,\n",
       "  0.44722222222222224,\n",
       "  0.3494444444444444,\n",
       "  0.5633333333333334,\n",
       "  0.47000000000000003,\n",
       "  0.390625,\n",
       "  0.3277777777777778,\n",
       "  0.34277777777777785,\n",
       "  0.5392857142857143,\n",
       "  0.47000000000000003,\n",
       "  0.4342857142857143,\n",
       "  0.681875,\n",
       "  0.6588888888888889,\n",
       "  0.5155555555555555,\n",
       "  0.6816666666666668,\n",
       "  0.6312500000000001,\n",
       "  0.46888888888888897,\n",
       "  0.46222222222222215,\n",
       "  0.508125,\n",
       "  0.4192857142857143,\n",
       "  0.4766666666666667,\n",
       "  0.454375,\n",
       "  0.44875000000000004,\n",
       "  0.45722222222222225,\n",
       "  0.3994444444444445,\n",
       "  0.40812499999999996,\n",
       "  0.4816666666666667,\n",
       "  0.4627777777777778,\n",
       "  0.39888888888888885,\n",
       "  0.5962500000000001,\n",
       "  0.39499999999999996,\n",
       "  0.29625,\n",
       "  0.6992857142857142,\n",
       "  0.6883333333333332,\n",
       "  0.51125,\n",
       "  0.4514285714285714,\n",
       "  0.5575,\n",
       "  0.46,\n",
       "  0.7888888888888889,\n",
       "  0.485625,\n",
       "  0.5618749999999999,\n",
       "  0.3661111111111111,\n",
       "  0.5794444444444444,\n",
       "  0.44499999999999995,\n",
       "  0.44416666666666665,\n",
       "  0.4677777777777778,\n",
       "  0.5957142857142858,\n",
       "  0.6468750000000001,\n",
       "  0.59875,\n",
       "  0.38166666666666665,\n",
       "  0.31,\n",
       "  0.34375,\n",
       "  0.5705555555555555,\n",
       "  0.415,\n",
       "  0.3188888888888889,\n",
       "  0.525,\n",
       "  0.3127777777777778,\n",
       "  0.2383333333333333,\n",
       "  0.5750000000000001,\n",
       "  0.47500000000000003,\n",
       "  0.44249999999999995,\n",
       "  0.43500000000000005,\n",
       "  0.37375,\n",
       "  0.64125,\n",
       "  0.3633333333333333,\n",
       "  0.40611111111111114,\n",
       "  0.37666666666666665,\n",
       "  0.47611111111111115,\n",
       "  0.635,\n",
       "  0.6266666666666666,\n",
       "  0.4192857142857143,\n",
       "  0.24300000000000002,\n",
       "  0.5731250000000001,\n",
       "  0.45444444444444443,\n",
       "  0.4555555555555555,\n",
       "  0.6012500000000001,\n",
       "  0.5462499999999999,\n",
       "  0.3778571428571428,\n",
       "  0.6458333333333334,\n",
       "  0.40555555555555556,\n",
       "  0.4192857142857142,\n",
       "  0.43333333333333335,\n",
       "  0.49888888888888894,\n",
       "  0.5233333333333333,\n",
       "  0.5544444444444445,\n",
       "  0.5762499999999999,\n",
       "  0.5477777777777777,\n",
       "  0.5391666666666667,\n",
       "  0.31777777777777777,\n",
       "  0.5605555555555556,\n",
       "  0.50375,\n",
       "  0.42277777777777775,\n",
       "  0.4577777777777778,\n",
       "  0.5061111111111112,\n",
       "  0.329375,\n",
       "  0.35687499999999994,\n",
       "  0.4344444444444444,\n",
       "  0.584,\n",
       "  0.39444444444444443,\n",
       "  0.6155555555555555,\n",
       "  0.5306250000000001,\n",
       "  0.3805555555555556,\n",
       "  0.6427777777777778,\n",
       "  0.27785714285714286,\n",
       "  0.41000000000000003,\n",
       "  0.38,\n",
       "  0.5672222222222222,\n",
       "  0.4577777777777778,\n",
       "  0.5155555555555555,\n",
       "  0.5114285714285715,\n",
       "  0.42874999999999996,\n",
       "  0.42944444444444446,\n",
       "  0.389,\n",
       "  0.3705555555555555,\n",
       "  0.46222222222222226,\n",
       "  0.6287499999999999,\n",
       "  0.491,\n",
       "  0.5622222222222222,\n",
       "  0.48888888888888893,\n",
       "  0.3672222222222222,\n",
       "  0.4466666666666667,\n",
       "  0.6577777777777779,\n",
       "  0.5166666666666667,\n",
       "  0.3711111111111111,\n",
       "  0.67375,\n",
       "  0.22999999999999998,\n",
       "  0.6466666666666667,\n",
       "  0.49250000000000005,\n",
       "  0.47500000000000003,\n",
       "  0.43124999999999997,\n",
       "  0.6055555555555556,\n",
       "  0.489375,\n",
       "  0.36388888888888893,\n",
       "  0.5905555555555555,\n",
       "  0.6838888888888889,\n",
       "  0.4042857142857143,\n",
       "  0.49799999999999994,\n",
       "  0.4472222222222222,\n",
       "  0.32611111111111113,\n",
       "  0.409375,\n",
       "  0.5405555555555556,\n",
       "  0.558125,\n",
       "  0.50125,\n",
       "  0.3727777777777778,\n",
       "  0.6125,\n",
       "  0.43900000000000006,\n",
       "  0.375625,\n",
       "  0.5271428571428571,\n",
       "  0.48055555555555557,\n",
       "  0.5728571428571428,\n",
       "  0.3777777777777778,\n",
       "  0.5222222222222223,\n",
       "  0.4722222222222222,\n",
       "  0.61,\n",
       "  0.4783333333333333,\n",
       "  0.5435714285714285,\n",
       "  0.5158333333333333,\n",
       "  0.556111111111111,\n",
       "  0.375,\n",
       "  0.4838888888888888,\n",
       "  0.38071428571428567,\n",
       "  0.5775,\n",
       "  0.395,\n",
       "  0.43666666666666665,\n",
       "  0.6171428571428572,\n",
       "  0.37000000000000005,\n",
       "  0.48444444444444446,\n",
       "  0.3638888888888889,\n",
       "  0.5625,\n",
       "  0.4494444444444444,\n",
       "  0.37277777777777776,\n",
       "  0.4785714285714286,\n",
       "  0.4583333333333333,\n",
       "  0.3125,\n",
       "  0.5655555555555556,\n",
       "  0.39222222222222225,\n",
       "  0.42714285714285716,\n",
       "  0.61375,\n",
       "  0.32833333333333337,\n",
       "  0.3416666666666666,\n",
       "  0.48625,\n",
       "  0.39166666666666666,\n",
       "  0.4928571428571428,\n",
       "  0.56125,\n",
       "  0.47777777777777775,\n",
       "  0.55875,\n",
       "  0.4225,\n",
       "  0.5022222222222222,\n",
       "  0.5162499999999999,\n",
       "  0.36250000000000004,\n",
       "  0.44555555555555565,\n",
       "  0.54,\n",
       "  0.44055555555555553,\n",
       "  0.4875,\n",
       "  0.5422222222222223,\n",
       "  0.515,\n",
       "  0.46875,\n",
       "  0.46611111111111114,\n",
       "  0.4427777777777778,\n",
       "  0.47611111111111115,\n",
       "  0.3888888888888889,\n",
       "  0.385,\n",
       "  0.580625,\n",
       "  0.45722222222222225,\n",
       "  0.47083333333333327,\n",
       "  0.546111111111111,\n",
       "  0.4316666666666667,\n",
       "  0.5461111111111111,\n",
       "  0.33,\n",
       "  0.4799999999999999,\n",
       "  0.5057142857142857,\n",
       "  0.474375,\n",
       "  0.485625,\n",
       "  0.44222222222222224,\n",
       "  0.5072222222222222,\n",
       "  0.5544444444444445,\n",
       "  0.4783333333333333,\n",
       "  0.53125,\n",
       "  0.3575,\n",
       "  0.365,\n",
       "  0.6275000000000001,\n",
       "  0.5177777777777778,\n",
       "  0.1527777777777778,\n",
       "  0.5672222222222222,\n",
       "  0.381,\n",
       "  0.23500000000000001,\n",
       "  0.6375,\n",
       "  0.5766666666666667,\n",
       "  0.5666666666666667,\n",
       "  0.4325,\n",
       "  0.4928571428571429,\n",
       "  0.54125,\n",
       "  0.41999999999999993,\n",
       "  0.42,\n",
       "  0.5477777777777777,\n",
       "  0.6355555555555555,\n",
       "  0.549375,\n",
       "  0.6241666666666666,\n",
       "  0.30666666666666664,\n",
       "  0.45055555555555554,\n",
       "  0.6911111111111111,\n",
       "  0.336875,\n",
       "  0.35333333333333333,\n",
       "  0.6021428571428571,\n",
       "  0.5433333333333333,\n",
       "  0.608125,\n",
       "  0.588888888888889,\n",
       "  0.45583333333333337,\n",
       "  0.38499999999999995,\n",
       "  0.43,\n",
       "  0.5072222222222222,\n",
       "  0.541111111111111,\n",
       "  0.3794444444444445,\n",
       "  0.6827777777777778,\n",
       "  0.36375,\n",
       "  0.5114285714285715,\n",
       "  0.5388888888888889,\n",
       "  0.3125,\n",
       "  0.48277777777777786,\n",
       "  0.44,\n",
       "  0.696875,\n",
       "  0.5631250000000001,\n",
       "  0.521875,\n",
       "  0.5138888888888888,\n",
       "  0.4744444444444444,\n",
       "  0.39166666666666666,\n",
       "  0.40125,\n",
       "  0.5388888888888889,\n",
       "  0.4422222222222223,\n",
       "  0.7057142857142856,\n",
       "  0.505,\n",
       "  0.505625,\n",
       "  0.43416666666666665,\n",
       "  0.5672222222222223,\n",
       "  0.5744444444444444,\n",
       "  0.43200000000000005,\n",
       "  0.36833333333333335,\n",
       "  0.2877777777777778,\n",
       "  0.3764285714285714,\n",
       "  0.5972222222222222,\n",
       "  0.4,\n",
       "  0.41888888888888887,\n",
       "  0.44999999999999996,\n",
       "  0.59,\n",
       "  0.37214285714285716,\n",
       "  0.5125,\n",
       "  0.34714285714285714,\n",
       "  0.5825,\n",
       "  0.5311111111111111,\n",
       "  0.025000000000000005,\n",
       "  0.5437500000000001,\n",
       "  0.32666666666666666,\n",
       "  0.4221428571428571,\n",
       "  0.37611111111111106,\n",
       "  0.44833333333333325,\n",
       "  0.4822222222222222,\n",
       "  0.48277777777777786,\n",
       "  0.6727777777777777,\n",
       "  0.4608333333333334,\n",
       "  0.4355555555555555,\n",
       "  0.5750000000000001,\n",
       "  0.4655555555555556,\n",
       "  0.4457142857142857,\n",
       "  0.6535714285714286,\n",
       "  0.6250000000000001,\n",
       "  0.38399999999999995,\n",
       "  0.5744444444444444,\n",
       "  0.5255555555555556,\n",
       "  0.5238888888888888,\n",
       "  0.6985714285714286,\n",
       "  0.615,\n",
       "  0.5818749999999999,\n",
       "  0.32,\n",
       "  0.68,\n",
       "  0.41,\n",
       "  0.3938888888888889,\n",
       "  0.4122222222222222,\n",
       "  0.526875,\n",
       "  0.4561111111111111,\n",
       "  0.385625,\n",
       "  0.3722222222222222,\n",
       "  0.5242857142857142,\n",
       "  0.4861111111111111,\n",
       "  0.6305555555555556,\n",
       "  0.6322222222222221,\n",
       "  0.4538888888888889,\n",
       "  0.601111111111111,\n",
       "  0.5616666666666666,\n",
       "  0.6031249999999999,\n",
       "  0.4666666666666667,\n",
       "  0.6414285714285715,\n",
       "  0.5827777777777778,\n",
       "  0.44812500000000005,\n",
       "  0.4933333333333333,\n",
       "  0.6168750000000001,\n",
       "  0.40222222222222226,\n",
       "  0.2799999999999999,\n",
       "  0.45222222222222214,\n",
       "  0.4478571428571429,\n",
       "  0.42666666666666664,\n",
       "  0.629375,\n",
       "  0.5672222222222223,\n",
       "  0.375,\n",
       "  0.5838888888888889,\n",
       "  0.41899999999999993,\n",
       "  0.35888888888888887,\n",
       "  0.5918749999999999,\n",
       "  0.36833333333333335,\n",
       "  0.3983333333333333,\n",
       "  0.40944444444444444,\n",
       "  0.5327777777777778,\n",
       "  0.449375,\n",
       "  0.555,\n",
       "  0.5072222222222221,\n",
       "  0.43000000000000005,\n",
       "  0.5014285714285714,\n",
       "  0.541875,\n",
       "  0.29874999999999996,\n",
       "  0.68375,\n",
       "  0.535,\n",
       "  0.6522222222222223,\n",
       "  0.42625,\n",
       "  0.44375,\n",
       "  0.3077777777777777,\n",
       "  0.6361111111111111,\n",
       "  0.4883333333333333,\n",
       "  0.39937500000000004,\n",
       "  0.46611111111111114,\n",
       "  0.511875,\n",
       "  0.325,\n",
       "  0.5428571428571429,\n",
       "  0.5427777777777778,\n",
       "  0.4222222222222222,\n",
       "  0.46875,\n",
       "  0.533,\n",
       "  0.54375,\n",
       "  0.5866666666666666,\n",
       "  0.42388888888888887,\n",
       "  0.45722222222222214,\n",
       "  0.29888888888888887,\n",
       "  0.4655555555555556,\n",
       "  0.458,\n",
       "  0.5177777777777778,\n",
       "  0.44625000000000004,\n",
       "  0.65125,\n",
       "  0.3561111111111111,\n",
       "  0.46125,\n",
       "  0.5,\n",
       "  0.45833333333333326,\n",
       "  0.5211111111111112,\n",
       "  0.51,\n",
       "  0.43,\n",
       "  0.39875000000000005,\n",
       "  0.45611111111111113,\n",
       "  0.2841666666666666,\n",
       "  0.5083333333333333,\n",
       "  0.4471428571428572,\n",
       "  0.39142857142857146,\n",
       "  0.51,\n",
       "  0.545,\n",
       "  0.6012500000000001,\n",
       "  0.5914285714285715,\n",
       "  0.41555555555555557,\n",
       "  0.37500000000000006,\n",
       "  0.5874999999999999,\n",
       "  0.47333333333333333,\n",
       "  0.5755555555555556,\n",
       "  0.5555555555555556,\n",
       "  0.43750000000000006,\n",
       "  0.22777777777777775,\n",
       "  0.4666666666666667,\n",
       "  0.32916666666666666,\n",
       "  0.3244444444444444,\n",
       "  0.39375,\n",
       "  0.5422222222222222,\n",
       "  0.4605555555555556,\n",
       "  0.5233333333333333,\n",
       "  0.4425,\n",
       "  0.24428571428571427,\n",
       "  0.5822222222222222,\n",
       "  0.72,\n",
       "  0.44999999999999996,\n",
       "  0.48750000000000004,\n",
       "  0.3722222222222222,\n",
       "  0.41444444444444445,\n",
       "  0.5566666666666666,\n",
       "  0.35888888888888887,\n",
       "  0.495,\n",
       "  0.43166666666666664,\n",
       "  0.34777777777777774,\n",
       "  0.54125,\n",
       "  0.45222222222222225,\n",
       "  0.711111111111111,\n",
       "  0.6144444444444445,\n",
       "  0.3805555555555556,\n",
       "  0.471875,\n",
       "  0.309375,\n",
       "  0.5109999999999999,\n",
       "  0.4511111111111111,\n",
       "  0.4194444444444445,\n",
       "  0.44749999999999995,\n",
       "  0.5422222222222222,\n",
       "  0.5862499999999999,\n",
       "  0.38625,\n",
       "  0.41333333333333333,\n",
       "  0.645,\n",
       "  0.3178571428571429,\n",
       "  0.6266666666666666,\n",
       "  0.35777777777777775,\n",
       "  0.45999999999999996,\n",
       "  0.33222222222222225,\n",
       "  0.49,\n",
       "  0.44625,\n",
       "  0.393125,\n",
       "  0.4600000000000001,\n",
       "  0.496875,\n",
       "  0.5299999999999999,\n",
       "  0.5072222222222221,\n",
       "  0.4605555555555555,\n",
       "  0.521875,\n",
       "  0.6757142857142858,\n",
       "  0.7314285714285714,\n",
       "  0.4033333333333334,\n",
       "  0.37750000000000006,\n",
       "  0.3025,\n",
       "  0.535,\n",
       "  0.31937499999999996,\n",
       "  0.39357142857142857,\n",
       "  0.5738888888888889,\n",
       "  0.3721428571428572,\n",
       "  0.5700000000000001,\n",
       "  0.540625,\n",
       "  0.6077777777777778,\n",
       "  0.4478571428571429,\n",
       "  0.4883333333333334,\n",
       "  0.5164285714285713,\n",
       "  0.4221428571428571,\n",
       "  0.4294444444444444,\n",
       "  0.34800000000000003,\n",
       "  0.6016666666666667,\n",
       "  0.5244444444444444,\n",
       "  0.44777777777777783,\n",
       "  0.7238888888888889,\n",
       "  0.4655555555555556,\n",
       "  0.57,\n",
       "  0.400625,\n",
       "  0.34500000000000003,\n",
       "  0.411875,\n",
       "  0.4183333333333334,\n",
       "  0.28444444444444444,\n",
       "  0.47187500000000004,\n",
       "  0.45,\n",
       "  0.4491666666666667,\n",
       "  0.5107142857142857,\n",
       "  0.35833333333333334,\n",
       "  0.5138888888888888,\n",
       "  0.4066666666666667,\n",
       "  0.3627777777777778,\n",
       "  0.43937499999999996,\n",
       "  0.4633333333333333,\n",
       "  0.5005555555555555,\n",
       "  0.5033333333333333,\n",
       "  0.3192857142857143,\n",
       "  0.5861111111111111,\n",
       "  0.45428571428571424,\n",
       "  0.38625,\n",
       "  0.6244444444444445,\n",
       "  0.6491666666666666,\n",
       "  0.5125,\n",
       "  0.5485714285714286,\n",
       "  0.45357142857142857,\n",
       "  0.5144444444444445,\n",
       "  0.33428571428571424,\n",
       "  0.560625,\n",
       "  0.4733333333333334,\n",
       "  0.42375,\n",
       "  0.48055555555555557,\n",
       "  0.32333333333333336,\n",
       "  0.3994444444444445,\n",
       "  0.288125,\n",
       "  0.29437500000000005,\n",
       "  0.42277777777777775,\n",
       "  0.4844444444444444,\n",
       "  0.2725,\n",
       "  0.5122222222222221,\n",
       "  0.6994444444444444,\n",
       "  0.5822222222222222,\n",
       "  0.44500000000000006,\n",
       "  0.48055555555555557,\n",
       "  0.48874999999999996,\n",
       "  0.43277777777777776,\n",
       "  0.54,\n",
       "  0.565,\n",
       "  0.403125,\n",
       "  0.34187500000000004,\n",
       "  0.5083333333333333,\n",
       "  0.26799999999999996,\n",
       "  0.5472222222222222,\n",
       "  0.49888888888888894,\n",
       "  0.58125,\n",
       "  0.4275,\n",
       "  0.43374999999999997,\n",
       "  0.40199999999999997,\n",
       "  0.29833333333333334,\n",
       "  0.4861111111111111,\n",
       "  0.4175,\n",
       "  0.7124999999999999,\n",
       "  0.47000000000000003,\n",
       "  0.4933333333333333,\n",
       "  0.3514285714285714,\n",
       "  0.46125,\n",
       "  0.5583333333333333,\n",
       "  0.6116666666666667,\n",
       "  0.5227777777777778,\n",
       "  0.3383333333333333,\n",
       "  0.4494444444444444,\n",
       "  0.5085714285714286,\n",
       "  0.3661111111111111,\n",
       "  0.3557142857142857,\n",
       "  0.5157142857142857,\n",
       "  0.32083333333333336,\n",
       "  0.2314285714285714,\n",
       "  0.516875,\n",
       "  0.6061111111111112,\n",
       "  0.41812499999999997,\n",
       "  0.3772222222222222,\n",
       "  0.5678571428571428,\n",
       "  0.541875,\n",
       "  0.3764285714285715,\n",
       "  0.46222222222222226,\n",
       "  0.4283333333333334,\n",
       "  0.49099999999999994,\n",
       "  0.6433333333333333,\n",
       "  0.52,\n",
       "  0.455,\n",
       "  0.38687499999999997,\n",
       "  0.414375,\n",
       "  0.6499999999999999,\n",
       "  0.5494444444444444,\n",
       "  0.5011111111111111,\n",
       "  0.274,\n",
       "  0.41687500000000005,\n",
       "  0.5327777777777778,\n",
       "  0.4749999999999999,\n",
       "  0.640625,\n",
       "  0.5085714285714286,\n",
       "  0.35000000000000003,\n",
       "  0.5625,\n",
       "  0.41777777777777775,\n",
       "  0.3472222222222222,\n",
       "  0.535625,\n",
       "  0.5216666666666667,\n",
       "  0.44999999999999996,\n",
       "  0.5605555555555556,\n",
       "  0.5175000000000001,\n",
       "  0.5116666666666666,\n",
       "  0.3658333333333334,\n",
       "  0.468125,\n",
       "  0.38333333333333336,\n",
       "  0.6641666666666667,\n",
       "  0.575,\n",
       "  0.5294444444444444,\n",
       "  0.2525,\n",
       "  0.4425,\n",
       "  0.2992857142857143,\n",
       "  0.526875,\n",
       "  0.6641666666666667,\n",
       "  0.4388888888888889,\n",
       "  0.57875,\n",
       "  0.6864285714285714,\n",
       "  0.4864285714285715,\n",
       "  0.531875,\n",
       "  0.3742857142857142,\n",
       "  0.4958333333333333,\n",
       "  0.391875,\n",
       "  0.5694444444444443,\n",
       "  0.538,\n",
       "  0.5892857142857143,\n",
       "  0.6988888888888889,\n",
       "  0.5725,\n",
       "  0.5275,\n",
       "  0.5827777777777778,\n",
       "  0.4861111111111111,\n",
       "  0.576111111111111,\n",
       "  0.5449999999999999,\n",
       "  0.6437499999999999,\n",
       "  0.4577777777777778,\n",
       "  0.4075,\n",
       "  0.41777777777777775,\n",
       "  0.5638888888888889,\n",
       "  0.4044444444444445,\n",
       "  0.46111111111111114,\n",
       "  0.50125,\n",
       "  0.3627777777777778,\n",
       "  0.33999999999999997,\n",
       "  0.32666666666666666,\n",
       "  0.4066666666666667,\n",
       "  0.5194444444444444,\n",
       "  0.5527777777777778,\n",
       "  0.2791666666666666,\n",
       "  0.4007142857142857,\n",
       "  0.4694444444444444,\n",
       "  0.5933333333333333,\n",
       "  0.5938888888888889,\n",
       "  0.4135714285714286,\n",
       "  0.414375,\n",
       "  0.339375,\n",
       "  0.5387500000000001,\n",
       "  0.3085714285714286,\n",
       "  0.3161111111111111,\n",
       "  0.460625,\n",
       "  0.4222222222222222,\n",
       "  0.5077777777777778,\n",
       "  0.5416666666666666,\n",
       "  0.355,\n",
       "  0.375,\n",
       "  0.42583333333333334,\n",
       "  0.4988888888888888,\n",
       "  0.3861111111111111,\n",
       "  0.2983333333333334,\n",
       "  0.5471428571428572,\n",
       "  0.6477777777777778,\n",
       "  0.4588888888888889,\n",
       "  0.4566666666666667,\n",
       "  0.5543750000000001,\n",
       "  0.53875,\n",
       "  0.38312500000000005,\n",
       "  0.62,\n",
       "  0.43555555555555553,\n",
       "  0.4444444444444444,\n",
       "  0.18944444444444444,\n",
       "  0.48,\n",
       "  0.4858333333333333,\n",
       "  0.6411111111111112,\n",
       "  0.44785714285714284,\n",
       "  0.5333333333333333,\n",
       "  0.4772222222222222,\n",
       "  0.3605555555555556,\n",
       "  0.2792857142857143,\n",
       "  0.454375,\n",
       "  0.45500000000000007,\n",
       "  0.6105555555555555,\n",
       "  0.48277777777777775,\n",
       "  0.5627777777777777,\n",
       "  0.5877777777777777,\n",
       "  0.36749999999999994,\n",
       "  0.41333333333333333,\n",
       "  0.37166666666666665,\n",
       "  0.4744444444444444,\n",
       "  0.565,\n",
       "  0.5683333333333332,\n",
       "  0.3192857142857143,\n",
       "  0.39555555555555555,\n",
       "  0.415625,\n",
       "  0.6122222222222222,\n",
       "  0.52625,\n",
       "  0.545625,\n",
       "  0.31875000000000003,\n",
       "  0.5011111111111111,\n",
       "  0.501,\n",
       "  0.54,\n",
       "  0.343125,\n",
       "  0.5627777777777778,\n",
       "  0.608125,\n",
       "  0.44875,\n",
       "  0.36166666666666664,\n",
       "  0.43583333333333335,\n",
       "  0.5842857142857143,\n",
       "  0.57,\n",
       "  0.41625,\n",
       "  0.515,\n",
       "  0.635625,\n",
       "  0.4027777777777778,\n",
       "  0.3416666666666667,\n",
       "  0.5644444444444444,\n",
       "  0.44357142857142856,\n",
       "  0.441875,\n",
       "  0.44799999999999995,\n",
       "  0.34444444444444444,\n",
       "  0.4535714285714287,\n",
       "  0.49666666666666665,\n",
       "  0.48250000000000004,\n",
       "  0.591111111111111,\n",
       "  0.434,\n",
       "  0.371875,\n",
       "  0.5416666666666666,\n",
       "  0.43083333333333335,\n",
       "  0.575,\n",
       "  0.5371428571428571,\n",
       "  0.3955555555555555,\n",
       "  0.6221428571428572,\n",
       "  0.41388888888888886,\n",
       "  0.6328571428571428,\n",
       "  0.4425,\n",
       "  0.6288888888888889,\n",
       "  ...]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_trainer._eval_save_prefix = OUTPUT_PREFIX + \"-test-pos-unbiased\"\n",
    "model_trainer._evaluate_partial(test_dataset_pos_unbiased)\n",
    "\n",
    "model_trainer._eval_save_prefix = OUTPUT_PREFIX +  \"-test-neg-unbiased\"\n",
    "model_trainer._evaluate_partial(test_dataset_neg_unbiased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DEFINING FUNCTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def eq(infilename, infilename_neg, trainfilename, gamma=-1.0, K=1):\n",
    "\n",
    "    # Read pickles\n",
    "    infile = open(infilename, 'rb')\n",
    "    infile_neg = open(infilename_neg, 'rb')\n",
    "    P = pickle.load(infile)\n",
    "    infile.close()\n",
    "    P_neg = pickle.load(infile_neg)\n",
    "    infile_neg.close()\n",
    "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
    "    \n",
    "    # Merge P and P_neg\n",
    "    for theuser in P[\"users\"]:\n",
    "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "    \n",
    "    Zui = dict()\n",
    "    Ni = dict()\n",
    "    \n",
    "    # Compute frequencies of items in the training set\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "    del trainset\n",
    "\n",
    "    # Count #users with non-zero item frequencies\n",
    "    nonzero_user_count = 0\n",
    "    for theuser in P[\"users\"]:\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for pos_item in pos_items:\n",
    "            if pos_item in Ni:\n",
    "                nonzero_user_count += 1\n",
    "                break\n",
    "    \n",
    "    # Compute recommendations for each user\n",
    "    for theuser in P[\"users\"]:\n",
    "        all_scores = np.array(P[\"results\"][theuser])\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for i, pos_item in enumerate(pos_items):\n",
    "            pos_score = pos_scores[i]\n",
    "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
    "\n",
    "    \n",
    "    # Calculate per-user scores\n",
    "    sum_user_auc = 0.0\n",
    "    sum_user_recall = 0.0\n",
    "    for theuser in P[\"users\"]:\n",
    "        numerator_auc = 0.0\n",
    "        numerator_recall = 0.0\n",
    "        denominator = 0.0\n",
    "\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            if theitem not in Ni:\n",
    "                continue\n",
    "            pui = np.power(Ni[theitem], (gamma + 1) / 2.0)\n",
    "\n",
    "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser])) / pui\n",
    "            \n",
    "            if Zui[(theuser, theitem)] < K:\n",
    "                numerator_recall += 1.0 / pui\n",
    "            denominator += 1 / pui\n",
    "                \n",
    "        if denominator > 0:\n",
    "            sum_user_auc += numerator_auc / denominator\n",
    "            sum_user_recall += numerator_recall / denominator \n",
    "\n",
    "    # Return\n",
    "    return {\n",
    "        \"auc\"       : sum_user_auc / nonzero_user_count,\n",
    "        \"recall\"    : sum_user_recall / nonzero_user_count,\n",
    "        \"bias\"      : \"Nope\",\n",
    "        \"concentration\" : \"Nope\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def aoa(infilename, infilename_neg, trainfilename, K=1):\n",
    "\n",
    "    # Read pickles\n",
    "    infile = open(infilename, 'rb')\n",
    "    infile_neg = open(infilename_neg, 'rb')\n",
    "    P = pickle.load(infile)\n",
    "    infile.close()\n",
    "    P_neg = pickle.load(infile_neg)\n",
    "    infile_neg.close()\n",
    "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
    "    \n",
    "    # Merge P and P_neg\n",
    "    for theuser in P[\"users\"]:\n",
    "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "    \n",
    "    Zui = dict()\n",
    "    Ni = dict()\n",
    "\n",
    "    # Compute frequencies of items in the training set\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "    del trainset\n",
    "    \n",
    "    # Count #users with non-zero item frequencies\n",
    "    nonzero_user_count = 0\n",
    "    for theuser in P[\"users\"]:\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for pos_item in pos_items:\n",
    "            if pos_item in Ni:\n",
    "                nonzero_user_count += 1\n",
    "                break\n",
    "    \n",
    "    # Compute recommendations for each user\n",
    "    for theuser in P[\"users\"]:\n",
    "        all_scores = np.array(P[\"results\"][theuser])\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for i, pos_item in enumerate(pos_items):\n",
    "            pos_score = pos_scores[i]\n",
    "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
    "\n",
    "    # Calculate per-user scores\n",
    "    sum_user_auc = 0.0\n",
    "    sum_user_recall = 0.0\n",
    "    for theuser in P[\"users\"]:\n",
    "        numerator_auc = 0.0\n",
    "        numerator_recall = 0.0\n",
    "        denominator = 0.0\n",
    "\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            if theitem not in Ni:\n",
    "                continue\n",
    "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser]))\n",
    "            # Calcolo il Recall a 30, vedi nota 6 paper\n",
    "            if Zui[(theuser, theitem)] < K:\n",
    "                numerator_recall += 1.0\n",
    "            denominator += 1 \n",
    "\n",
    "        if denominator > 0:\n",
    "            sum_user_auc += numerator_auc / denominator\n",
    "            sum_user_recall += numerator_recall / denominator\n",
    "\n",
    "    # Return\n",
    "    return {\n",
    "        \"auc\"       : sum_user_auc / nonzero_user_count,\n",
    "        \"recall\"    : sum_user_recall / nonzero_user_count,\n",
    "        \"bias\"      : \"To be compute\",\n",
    "        \"concentration\" : \"Nope\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def stratified(infilename, infilename_neg, trainfilename, gamma=1.0, K=30, partition=10, delta=0.1):\n",
    "\n",
    "    # Read pickles\n",
    "    infile = open(infilename, 'rb')\n",
    "    infile_neg = open(infilename_neg, 'rb')\n",
    "    P = pickle.load(infile)\n",
    "    infile.close()\n",
    "    P_neg = pickle.load(infile_neg)\n",
    "    infile_neg.close()\n",
    "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
    "\n",
    "    # Merge P and P_neg\n",
    "    for theuser in P[\"users\"]:\n",
    "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "\n",
    "    Zui = dict()\n",
    "    Ni = dict()\n",
    "\n",
    "    # Compute frequencies of items in the training set\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "    #del trainset\n",
    "\n",
    "    # Compute recommendations for each user\n",
    "    for theuser in P[\"users\"]:\n",
    "        all_scores = np.array(P[\"results\"][theuser])\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for i, pos_item in enumerate(pos_items):\n",
    "            pos_score = pos_scores[i]\n",
    "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
    "\n",
    "    pui = dict()\n",
    "    w = dict()\n",
    "\n",
    "    # Compute dictionary of propensity scores\n",
    "    for theuser in P[\"users\"]:\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            if theitem not in Ni:\n",
    "                continue\n",
    "            if theitem in pui:\n",
    "                continue\n",
    "            pui[theitem] = np.power(Ni[theitem], (gamma + 1) / 2.0)\n",
    "\n",
    "    # Take the list of items (not tuples) in pui sorted by value\n",
    "    items_sorted_by_value = sorted(pui, key=pui.get, reverse=True)\n",
    "\n",
    "    # Compute linspace between the pui[0] and pui[-1] \n",
    "    linspace = np.linspace(pui[items_sorted_by_value[0]], pui[items_sorted_by_value[-1]], partition+1)\n",
    "   \n",
    "    # Compute dictionary w, that is, for each item, assigns the average of the puis in the partition it belongs to\n",
    "    i=0\n",
    "    j = 0\n",
    "    while i < len(items_sorted_by_value):\n",
    "                            \n",
    "        avg = 0\n",
    "        start = i\n",
    "        end = i\n",
    "    \n",
    "        while i < len(items_sorted_by_value) and pui[items_sorted_by_value[i]] >= linspace[j+1]:\n",
    "            avg += 1.0 / pui[items_sorted_by_value[i]]\n",
    "            end = i\n",
    "            i += 1\n",
    "        avg = avg / (end - start + 1)\n",
    "\n",
    "        for k in range(start, end+1):\n",
    "            w[items_sorted_by_value[k]] = avg\n",
    "\n",
    "        j += 1\n",
    "\n",
    "    # Compute bias' numerator\n",
    "    bias = 0.0\n",
    "    for k in pui.keys():\n",
    "        # add |pui*w - 1!|\n",
    "        bias += abs(pui[k] * w[k] - 1)\n",
    "    # Multiply by number of users\n",
    "    bias *= len(P[\"users\"])\n",
    "\n",
    "    # Compute concentrations numerator (for each user)\n",
    "    concentrations = {}\n",
    "    max_w = max(w.values())\n",
    "    # ... by computing the sum of squares of w for each user\n",
    "    for user, item in zip(trainset['user_id'], trainset['item_id']):\n",
    "        # Iterate over the trainset to compute the sum of squares for each user\n",
    "        if item in w:\n",
    "            if user not in concentrations:\n",
    "                concentrations[user] = 0\n",
    "            concentrations[user] += w[item] ** 2\n",
    "    # ... and then applying the formula\n",
    "    for user in concentrations:\n",
    "        concentrations[user] = math.sqrt(concentrations[user] * 2 * math.log(2/delta)) + max_w * 7 * math.log(2/delta)\n",
    "    # Now sum all the concentrations\n",
    "    concentration = sum(concentrations.values())\n",
    "\n",
    "    # Calculate per-user scores\n",
    "    nonzero_user_count = 0\n",
    "    sum_user_auc = 0.0\n",
    "    sum_user_recall = 0.0\n",
    "    for theuser in P[\"users\"]:\n",
    "        numerator_auc = 0.0\n",
    "        numerator_recall = 0.0\n",
    "        denominator = 0.0\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            # Skip items with null frequency\n",
    "            if  theitem not in Ni:\n",
    "                continue\n",
    "            # Add things to be summed for each item\n",
    "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser])) * w[theitem]\n",
    "            # Add things for recall\n",
    "            if Zui[(theuser, theitem)] < K:\n",
    "                numerator_recall += 1.0 * w[theitem] # spetta\n",
    "            # Increment denominator that the sum must be divided by \n",
    "            denominator += 1 / pui[theitem]\n",
    "\n",
    "\n",
    "        # If there was at least one item for the user, count the user and sum the results\n",
    "        if denominator > 0:\n",
    "            nonzero_user_count += 1\n",
    "            sum_user_auc += numerator_auc / denominator\n",
    "            sum_user_recall += numerator_recall / denominator \n",
    "\n",
    "    #del trainset\n",
    "\n",
    "    # Return\n",
    "    return {\n",
    "        \"auc\"       : sum_user_auc / nonzero_user_count, \n",
    "        \"recall\"    : sum_user_recall / nonzero_user_count,\n",
    "        \"bias\"      : bias,\n",
    "        \"concentration\" : concentration\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def stratified_logspace(infilename, infilename_neg, trainfilename, gamma=1.0, K=30, partition=10, delta=0.1):\n",
    "\n",
    "    # Read pickles\n",
    "    infile = open(infilename, 'rb')\n",
    "    infile_neg = open(infilename_neg, 'rb')\n",
    "    P = pickle.load(infile)\n",
    "    infile.close()\n",
    "    P_neg = pickle.load(infile_neg)\n",
    "    infile_neg.close()\n",
    "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
    "\n",
    "    # Merge P and P_neg\n",
    "    for theuser in P[\"users\"]:\n",
    "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "\n",
    "    Zui = dict()\n",
    "    Ni = dict()\n",
    "\n",
    "    # Compute frequencies of items in the training set\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "    del trainset\n",
    "\n",
    "    # Compute recommendations for each user\n",
    "    for theuser in P[\"users\"]:\n",
    "        all_scores = np.array(P[\"results\"][theuser])\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for i, pos_item in enumerate(pos_items):\n",
    "            pos_score = pos_scores[i]\n",
    "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
    "\n",
    "    pui = dict()\n",
    "    w = dict()\n",
    "\n",
    "    # Compute dictionary of propensity scores\n",
    "    for theuser in P[\"users\"]:\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            if theitem not in Ni:\n",
    "                continue\n",
    "            if theitem in pui:\n",
    "                continue\n",
    "            pui[theitem] = np.power(Ni[theitem], (gamma + 1) / 2.0)\n",
    "\n",
    "    # Take the list of items (not tuples) in pui sorted by value\n",
    "    items_sorted_by_value = sorted(pui, key=pui.get, reverse=True)\n",
    "\n",
    "    # Compute linspace between the pui[0] and pui[-1] \n",
    "\n",
    "    # Maybe try to split the logspace instead of the linspace?\n",
    "    logspace = np.logspace(pui[items_sorted_by_value[0]], pui[items_sorted_by_value[-1]], partition+1)\n",
    "   \n",
    "    # Compute dictionary w, that is, for each item, assigns the average of the puis in the partition it belongs to\n",
    "    i=0\n",
    "    j = 0\n",
    "    while i < len(items_sorted_by_value):\n",
    "                            \n",
    "        avg = 0\n",
    "        start = i\n",
    "        end = i\n",
    "    \n",
    "        while i < len(items_sorted_by_value) and pui[items_sorted_by_value[i]] >= logspace[j+1]:\n",
    "            avg += 1.0 / pui[items_sorted_by_value[i]]\n",
    "            end = i\n",
    "            i += 1\n",
    "        \n",
    "        # Is the average the only good choice? even with the log space split?\n",
    "        avg = avg / (end - start + 1)\n",
    "\n",
    "        for k in range(start, end+1):\n",
    "            w[items_sorted_by_value[k]] = avg\n",
    "\n",
    "        j += 1\n",
    "\n",
    "        # Compute bias' numerator\n",
    "        bias = 0.0\n",
    "        for k in pui.keys():\n",
    "            # add |pui*w - 1!|\n",
    "            bias += abs(pui[k] * w[k] - 1)\n",
    "        # Multiply by number of users\n",
    "        bias *= len(P[\"users\"])\n",
    "\n",
    "        # Compute concentrations numerator (for each user)\n",
    "        concentrations = {}\n",
    "        max_w = max(w.values())\n",
    "        # ... by computing the sum of squares of w for each user\n",
    "        for user, item in zip(trainset['user_id'], trainset['item_id']):\n",
    "            # Iterate over the trainset to compute the sum of squares for each user\n",
    "            if item in w:\n",
    "                if user not in concentrations:\n",
    "                    concentrations[user] = 0\n",
    "                concentrations[user] += w[item] ** 2\n",
    "        # ... and then applying the formula\n",
    "        for user in concentrations:\n",
    "            concentrations[user] = math.sqrt(concentrations[user] * 2 * math.log(2/delta)) + max_w * 7 * math.log(2/delta)\n",
    "        # Now sum all the concentrations\n",
    "        concentration = sum(concentrations.values())\n",
    "\n",
    "    # Compute score with AUC and compute Recall\n",
    "    nonzero_user_count = 0\n",
    "    sum_user_auc = 0.0\n",
    "    sum_user_recall = 0.0\n",
    "    for theuser in P[\"users\"]:\n",
    "        numerator_auc = 0.0\n",
    "        numerator_recall = 0.0\n",
    "        denominator = 0.0\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            # Skip items with null frequency\n",
    "            if  theitem not in Ni:\n",
    "                continue\n",
    "            # Add things to be summed for each item\n",
    "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser])) * w[theitem]\n",
    "            # Add things for recall\n",
    "            if Zui[(theuser, theitem)] < K:\n",
    "                numerator_recall += 1.0 * w[theitem] # spetta\n",
    "            # Increment denominator that the sum must be divided by \n",
    "            denominator += 1 / pui[theitem]\n",
    "\n",
    "\n",
    "        # If there was at least one item for the user, count the user and sum the results\n",
    "        if denominator > 0:\n",
    "            nonzero_user_count += 1\n",
    "            sum_user_auc += numerator_auc / denominator\n",
    "            sum_user_recall += numerator_recall / denominator \n",
    "\n",
    "    # Return\n",
    "    return {\n",
    "        \"auc\"       : sum_user_auc / nonzero_user_count, \n",
    "        \"recall\"    : sum_user_recall / nonzero_user_count,\n",
    "        \"bias\"      : bias,\n",
    "        \"concentration\" : concentration\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# This version uses the linspace of the number of number of items used for evaluation, not of the propensities\n",
    "def stratified_2(infilename, infilename_neg, trainfilename, gamma=1.0, K=30, partition=10, delta=0.1):\n",
    "\n",
    "    # Read pickles\n",
    "    infile = open(infilename, 'rb')\n",
    "    infile_neg = open(infilename_neg, 'rb')\n",
    "    P = pickle.load(infile)\n",
    "    infile.close()\n",
    "    P_neg = pickle.load(infile_neg)\n",
    "    infile_neg.close()\n",
    "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
    "\n",
    "    # Merge P and P_neg\n",
    "    for theuser in P[\"users\"]:\n",
    "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
    "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
    "\n",
    "    Zui = dict()\n",
    "    Ni = dict()\n",
    "\n",
    "    # Compute frequencies of items in the training set\n",
    "    trainset = np.load(trainfilename)\n",
    "    for i in trainset['item_id']:\n",
    "        if i in Ni:\n",
    "            Ni[i] += 1\n",
    "        else:\n",
    "            Ni[i] = 1\n",
    "    #del trainset\n",
    "\n",
    "    # Compute recommendations for each user\n",
    "    for theuser in P[\"users\"]:\n",
    "        all_scores = np.array(P[\"results\"][theuser])\n",
    "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
    "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
    "        for i, pos_item in enumerate(pos_items):\n",
    "            pos_score = pos_scores[i]\n",
    "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
    "\n",
    "    pui = dict()\n",
    "    w = dict()\n",
    "\n",
    "    # Compute dictionary of propensity scores\n",
    "    for theuser in P[\"users\"]:\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            if theitem not in Ni:\n",
    "                continue\n",
    "            if theitem in pui:\n",
    "                continue\n",
    "            pui[theitem] = np.power(Ni[theitem], (gamma + 1) / 2.0)\n",
    "\n",
    "    # Take the list of items (not tuples) in pui sorted by value\n",
    "    items_sorted_by_value = sorted(pui, key=pui.get, reverse=True)\n",
    "\n",
    "    # Compute linspace between the 0 to len(item_sorted...)\n",
    "    linspace = np.linspace(0, len(items_sorted_by_value), partition+1)\n",
    "   \n",
    "    # Compute dictionary w, that is, for each item, assigns the average of the puis in the partition it belongs to\n",
    "    i=0\n",
    "    j = 0\n",
    "    while i < len(items_sorted_by_value):\n",
    "                            \n",
    "        avg = 0\n",
    "        start = i\n",
    "        end = i\n",
    "    \n",
    "        while i < len(items_sorted_by_value) and i < linspace[j+1]:\n",
    "            avg += 1.0 / pui[items_sorted_by_value[i]]\n",
    "            end = i\n",
    "            i += 1\n",
    "        \n",
    "        avg = avg / (end - start + 1)\n",
    "\n",
    "        for k in range(start, end+1):\n",
    "            w[items_sorted_by_value[k]] = avg\n",
    "\n",
    "        j += 1\n",
    "\n",
    "    # Compute bias' numerator\n",
    "    bias = 0.0\n",
    "    for k in pui.keys():\n",
    "        # add |pui*w - 1!|\n",
    "        bias += abs(pui[k] * w[k] - 1)\n",
    "    # Multiply by number of users\n",
    "    bias *= len(P[\"users\"])\n",
    "\n",
    "    # Compute concentrations numerator (for each user)\n",
    "    concentrations = {}\n",
    "    max_w = max(w.values())\n",
    "    # ... by computing the sum of squares of w for each user\n",
    "    for user, item in zip(trainset['user_id'], trainset['item_id']):\n",
    "        # Iterate over the trainset to compute the sum of squares for each user\n",
    "        if item in w:\n",
    "            if user not in concentrations:\n",
    "                concentrations[user] = 0\n",
    "            concentrations[user] += w[item] ** 2\n",
    "    # ... and then applying the formula\n",
    "    for user in concentrations:\n",
    "        concentrations[user] = math.sqrt(concentrations[user] * 2 * math.log(2/delta)) + max_w * 7 * math.log(2/delta)\n",
    "    # Now sum all the concentrations\n",
    "    concentration = sum(concentrations.values())\n",
    "\n",
    "    # Compute score with AUC and compute Recall\n",
    "    nonzero_user_count = 0\n",
    "    sum_user_auc = 0.0\n",
    "    sum_user_recall = 0.0\n",
    "    for theuser in P[\"users\"]:\n",
    "        numerator_auc = 0.0\n",
    "        numerator_recall = 0.0\n",
    "        denominator = 0.0\n",
    "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
    "            # Skip items with null frequency\n",
    "            if  theitem not in Ni:\n",
    "                continue\n",
    "            # Add things to be summed for each item\n",
    "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser])) * w[theitem]\n",
    "            # Add things for recall\n",
    "            if Zui[(theuser, theitem)] < K:\n",
    "                numerator_recall += 1.0 * w[theitem] # spetta\n",
    "            # Increment denominator that the sum must be divided by \n",
    "            denominator += 1 / pui[theitem]\n",
    "\n",
    "\n",
    "        # If there was at least one item for the user, count the user and sum the results\n",
    "        if denominator > 0:\n",
    "            nonzero_user_count += 1\n",
    "            sum_user_auc += numerator_auc / denominator\n",
    "            sum_user_recall += numerator_recall / denominator \n",
    "\n",
    "    return {\n",
    "        \"auc\"       : sum_user_auc / nonzero_user_count, \n",
    "        \"recall\"    : sum_user_recall / nonzero_user_count,\n",
    "        \"bias\"      : bias,\n",
    "        \"concentration\" : concentration\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **COMPUTE RESULTS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute AOA and unbiased evaluator metrics with biased testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "biased_results = dict()\n",
    "\n",
    "# biased_results[\"STRATIFIED_15\"] = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=1.5, K=30, partition=100)\n",
    "biased_results[\"AOA\"] = aoa(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", K=30)\n",
    "biased_results[\"UB_15\"] = eq(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=1.5, K=30)\n",
    "biased_results[\"UB_2\"] =  eq(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2, K=30)\n",
    "biased_results[\"UB_25\"] =  eq(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2.5, K=30)\n",
    "biased_results[\"UB_3\"] =  eq(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=3, K=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute AOA and unbiased evaluator metrics with unbiased testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "unbiased_results = dict()\n",
    "\n",
    "# unbiased_results[\"STRATIFIED_15\"] = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=1.5, K=1, partition=100)\n",
    "unbiased_results[\"AOA\"] = aoa(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", K=1)\n",
    "unbiased_results[\"UB_15\"] = eq(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=1.5, K=1)\n",
    "unbiased_results[\"UB_2\"] =  eq(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2, K=1)\n",
    "unbiased_results[\"UB_25\"] =  eq(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2.5, K=1)\n",
    "unbiased_results[\"UB_3\"] =  eq(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=3, K=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 742,  977,  141,  707,  751,   24,  273,  959,  193,  200, 1001,\n",
       "        206,  783,  662,  295,  384,  228,  975,  340,  122,  146,  740,\n",
       "        818,  747,  346,  852,  849,  579,   20,  284,  197,  727,  910,\n",
       "        332,  251,  599,  529,  463,  192,  873,  502,  309,  731,   11,\n",
       "        366,  581,  108,  400,  294,  524,  486,  265,  530,  445,  883,\n",
       "        476,  168,  672,   38,  666,  984,  848,    7,  874,  356,    2,\n",
       "        490,  545,  955,  365,  189,  701,  993,   99,  823,  885,  128,\n",
       "        832,  680,  804,  654,  329,  521,  746,  691,  318,   55,  936,\n",
       "        715,  429,  768,   40,   51,  506,  458,   18,  244,  648,  409,\n",
       "        548,  694,  567,  730,  411,  864,  172,  272,   15,  227,  427,\n",
       "        606,  917,  809,  646,  515,  838,  651,  511,  266,  420,  861,\n",
       "         62,   46,  290,  522,  372,   93,   19,  773,  793,  302,  886,\n",
       "         26,  705,  781,  395,  456,  958,  785,  483,  334,  644,  536,\n",
       "        937,  156,  577,  297,  704,  561,  617,  813,  836,  215,  584,\n",
       "        538,  230,  117,  510,  650,  303,  786,  754,  957,  489,  630,\n",
       "        758,  393,  216,  824,  620,  299,    5,  360,   22,    3,  353,\n",
       "        593,  323,  494,  424,  982,  656,  512,  941,  976,  956,  573,\n",
       "        177,  321,   33,  422,  847,  412,  794,  713,  822,  869,  428,\n",
       "        776,  513,  167,  635,  571,  225,  853,  383,  300,  878,  798,\n",
       "        653,  286,  276,  324,  724,  390,  600,  540,  460,  526,  981,\n",
       "        118,  922,  622,  113,  639,  663,   43,  363,  222,  152,  433,\n",
       "        994,  597,  891,  898,   96,  854,   30,  771,  948,  909,  929,\n",
       "        963,  979,  739,  110,  440,  238,  734,  721,  382,  188,  357,\n",
       "        661,  142,  839,  634,  178,  461,  765,  888,  399,  166,  815,\n",
       "        528,  313,  638,  418,  518,  692,  209,  889,  665,  190,   67,\n",
       "        223,   97,  718,  689,   12,  902,  415,  392,  467,  289,  601,\n",
       "        348,  441,  271,  444,  492,  471,  884,  780,   63,    9,  155,\n",
       "        589,  554,  173])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get number of items\n",
    "num_items = raw_data['max_item']\n",
    "\n",
    "# Get the n_p partitions\n",
    "n_p = 300\n",
    "nums = np.arange(1, num_items+1)\n",
    "partitions = np.random.choice(nums, n_p, replace=False)\n",
    "\n",
    "# Visualize\n",
    "partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the partition which minimizes the sum of AUC and Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75becf72437f48f8bff877dd14ee4dbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now I have partition  742 -> Bias is now:  86769.13386926711  and concentration is now:  29597.936320479897  for unbiased and  61158.51359095633  and  28333.82152569018  for biased.\n",
      "Now I have partition  977 -> Bias is now:  60431.176879551356  and concentration is now:  29597.1994832529  for unbiased and  37798.534062019135  and  28333.111189114257  for biased.\n"
     ]
    }
   ],
   "source": [
    "# Compute biased and unbiased results with stratified for each partition\n",
    "# and store biased and unbiased results such that the sum of AUC and Recall is minimized\n",
    "\n",
    "# Value of gamma to use for minimization\n",
    "gamma = 1.5\n",
    "key = \"STRATIFIED_\" + str(gamma).replace(\".\",\"\")\n",
    "\n",
    "# Initialize results\n",
    "unbiased_results[key] = dict()\n",
    "biased_results[key] = dict()\n",
    "best_partition = np.random.choice(nums, 1)[0]\n",
    "\n",
    "# For each partition\n",
    "for p in tqdm(partitions):\n",
    "    # Compute the results (AUC and Recall) for both biased and unbiased test sets\n",
    "    temp_unbiased = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=gamma, K=1, partition=p)\n",
    "    temp_biased = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=gamma, K=30, partition=p)\n",
    "    # If first iteration\n",
    "    # Or if a better partition was found, update the results\n",
    "    if not unbiased_results[key] or ( temp_unbiased['bias'] + temp_unbiased['concentration'] + temp_biased['bias'] + temp_biased['concentration'] < biased_results[key]['bias'] + biased_results[key]['concentration'] + unbiased_results[key]['bias'] + unbiased_results[key]['concentration'] ) :\n",
    "        print(\"Now I have partition \", p, \"-> Bias is now: \", temp_unbiased['bias'], \" and concentration is now: \", temp_unbiased['concentration'], \" for unbiased and \", temp_biased['bias'], \" and \", temp_biased['concentration'], \" for biased.\") \n",
    "        unbiased_results[key] = temp_unbiased\n",
    "        biased_results[key] = temp_biased\n",
    "        best_partition = p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, for the chosen value of gamma, the best partition is..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "977"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize\n",
    "best_partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute stratified metrics with biased and unbiased testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "unbiased_results[\"STRATIFIED_15\"] = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=1.5, K=1, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_15\"] = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=1.5, K=30, partition=best_partition)\n",
    "\n",
    "unbiased_results[\"STRATIFIED_2\"] = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2, K=1, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_2\"] = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2, K=30, partition=best_partition)\n",
    "\n",
    "unbiased_results[\"STRATIFIED_25\"] = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2.5, K=1, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_25\"] = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2.5, K=30, partition=best_partition)\n",
    "\n",
    "unbiased_results[\"STRATIFIED_3\"] = stratified(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=3, K=1, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_3\"] = stratified(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=3, K=30, partition=best_partition)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version uses the linspace of items instead of linspace of propensities to make the partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "unbiased_results[\"STRATIFIED_v2_15\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=1.5, K=1, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_v2_15\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=1.5, K=30, partition=best_partition)\n",
    "\n",
    "unbiased_results[\"STRATIFIED_v2_2\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2, K=1, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_v2_2\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2, K=30, partition=best_partition)\n",
    "\n",
    "unbiased_results[\"STRATIFIED_v2_25\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2.5, K=1, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_v2_25\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=2.5, K=30, partition=best_partition)\n",
    "\n",
    "unbiased_results[\"STRATIFIED_v2_3\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-unbiased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-unbiased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=3, K=1, partition=best_partition)\n",
    "biased_results[\"STRATIFIED_v2_3\"] = stratified_2(OUTPUT_PREFIX+\"-test-pos-biased_evaluate_partial.pickle\", OUTPUT_PREFIX+\"-test-neg-biased_evaluate_partial.pickle\", output_name+\"training_arr.npy\", gamma=3, K=30, partition=best_partition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare table for results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#len(list(value.keys()))\n",
    "rows = 4\n",
    "#len(list(biased_results.items()))\n",
    "columns = 13\n",
    "\n",
    "# Init results\n",
    "results_array = np.zeros((rows,columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the table with the MAE results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# For better readability\n",
    "def round_if_number(value):\n",
    "    if isinstance(value, (int, float)):\n",
    "        return round(value, 5)\n",
    "    return value\n",
    "\n",
    "# Get the metrics excluding bias and concentration\n",
    "metrics = list(biased_results[list(biased_results.keys())[0]].keys())\n",
    "metrics_for_mae = [metric for metric in metrics if metric not in ['bias', 'concentration']]\n",
    "num_metrics_for_mae = len(metrics_for_mae)\n",
    "\n",
    "# Initialize the results array with object dtype to accommodate mixed data types\n",
    "results_array = np.empty((num_metrics_for_mae + 4, len(biased_results)), dtype=object)  # +4 for 2 bias and 2 concentration\n",
    "\n",
    "# Get the names of the rows\n",
    "list_biased_res = list(biased_results.keys())\n",
    "\n",
    "# For each key in biased_results\n",
    "for i in range(len(list_biased_res)):\n",
    "    key = list_biased_res[i]\n",
    "\n",
    "    # Compute MAE for the metrics (AUC and Recall)\n",
    "    for j in range(num_metrics_for_mae):\n",
    "        results_array[j][i] = round_if_number(abs(biased_results[key][metrics_for_mae[j]] - unbiased_results[key][metrics_for_mae[j]]))\n",
    "\n",
    "    # Add bias and concentration values directly\n",
    "    results_array[num_metrics_for_mae][i] = round_if_number(biased_results[key]['bias'])\n",
    "    results_array[num_metrics_for_mae + 1][i] = round_if_number(biased_results[key]['concentration'])\n",
    "    results_array[num_metrics_for_mae + 2][i] = round_if_number(unbiased_results[key]['bias'])\n",
    "    results_array[num_metrics_for_mae + 3][i] = round_if_number(unbiased_results[key]['concentration'])\n",
    "\n",
    "# Add metric names\n",
    "metric_values = metrics_for_mae + ['biased_bias', 'biased_concentration', 'unbiased_bias', 'unbiased_concentration']\n",
    "\n",
    "# Make it a DataFrame\n",
    "mae_df = pd.DataFrame(columns=list_biased_res, data=results_array)\n",
    "mae_df.insert(0, \"metric\", metric_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **RESULTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>AOA</th>\n",
       "      <th>UB_15</th>\n",
       "      <th>UB_2</th>\n",
       "      <th>UB_25</th>\n",
       "      <th>UB_3</th>\n",
       "      <th>STRATIFIED_15</th>\n",
       "      <th>STRATIFIED_2</th>\n",
       "      <th>STRATIFIED_25</th>\n",
       "      <th>STRATIFIED_3</th>\n",
       "      <th>STRATIFIED_v2_15</th>\n",
       "      <th>STRATIFIED_v2_2</th>\n",
       "      <th>STRATIFIED_v2_25</th>\n",
       "      <th>STRATIFIED_v2_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>auc</td>\n",
       "      <td>0.15324</td>\n",
       "      <td>0.1279</td>\n",
       "      <td>0.12466</td>\n",
       "      <td>0.12218</td>\n",
       "      <td>0.1203</td>\n",
       "      <td>0.12485</td>\n",
       "      <td>0.10516</td>\n",
       "      <td>0.04351</td>\n",
       "      <td>0.24531</td>\n",
       "      <td>0.1279</td>\n",
       "      <td>0.12466</td>\n",
       "      <td>0.12218</td>\n",
       "      <td>0.1203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>recall</td>\n",
       "      <td>0.37667</td>\n",
       "      <td>0.25649</td>\n",
       "      <td>0.24488</td>\n",
       "      <td>0.2363</td>\n",
       "      <td>0.22989</td>\n",
       "      <td>0.25618</td>\n",
       "      <td>0.24952</td>\n",
       "      <td>0.2421</td>\n",
       "      <td>0.21967</td>\n",
       "      <td>0.25649</td>\n",
       "      <td>0.24488</td>\n",
       "      <td>0.2363</td>\n",
       "      <td>0.22989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>biased_bias</td>\n",
       "      <td>To be compute</td>\n",
       "      <td>Nope</td>\n",
       "      <td>Nope</td>\n",
       "      <td>Nope</td>\n",
       "      <td>Nope</td>\n",
       "      <td>37798.53406</td>\n",
       "      <td>257576.20112</td>\n",
       "      <td>897026.38874</td>\n",
       "      <td>2914954.6877</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>biased_concentration</td>\n",
       "      <td>Nope</td>\n",
       "      <td>Nope</td>\n",
       "      <td>Nope</td>\n",
       "      <td>Nope</td>\n",
       "      <td>Nope</td>\n",
       "      <td>28333.11119</td>\n",
       "      <td>14507.48042</td>\n",
       "      <td>7027.49807</td>\n",
       "      <td>3828.31383</td>\n",
       "      <td>48114.64423</td>\n",
       "      <td>47931.59355</td>\n",
       "      <td>47825.96046</td>\n",
       "      <td>47762.24962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>unbiased_bias</td>\n",
       "      <td>To be compute</td>\n",
       "      <td>Nope</td>\n",
       "      <td>Nope</td>\n",
       "      <td>Nope</td>\n",
       "      <td>Nope</td>\n",
       "      <td>60431.17688</td>\n",
       "      <td>325108.41124</td>\n",
       "      <td>1226209.15539</td>\n",
       "      <td>4108874.27145</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>unbiased_concentration</td>\n",
       "      <td>Nope</td>\n",
       "      <td>Nope</td>\n",
       "      <td>Nope</td>\n",
       "      <td>Nope</td>\n",
       "      <td>Nope</td>\n",
       "      <td>29597.19948</td>\n",
       "      <td>16750.75416</td>\n",
       "      <td>8395.61329</td>\n",
       "      <td>4734.34158</td>\n",
       "      <td>48125.01414</td>\n",
       "      <td>47945.8319</td>\n",
       "      <td>47842.37303</td>\n",
       "      <td>47779.84369</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   metric            AOA    UB_15     UB_2    UB_25     UB_3  \\\n",
       "0                     auc        0.15324   0.1279  0.12466  0.12218   0.1203   \n",
       "1                  recall        0.37667  0.25649  0.24488   0.2363  0.22989   \n",
       "2             biased_bias  To be compute     Nope     Nope     Nope     Nope   \n",
       "3    biased_concentration           Nope     Nope     Nope     Nope     Nope   \n",
       "4           unbiased_bias  To be compute     Nope     Nope     Nope     Nope   \n",
       "5  unbiased_concentration           Nope     Nope     Nope     Nope     Nope   \n",
       "\n",
       "  STRATIFIED_15  STRATIFIED_2  STRATIFIED_25   STRATIFIED_3 STRATIFIED_v2_15  \\\n",
       "0       0.12485       0.10516        0.04351        0.24531           0.1279   \n",
       "1       0.25618       0.24952         0.2421        0.21967          0.25649   \n",
       "2   37798.53406  257576.20112   897026.38874   2914954.6877              0.0   \n",
       "3   28333.11119   14507.48042     7027.49807     3828.31383      48114.64423   \n",
       "4   60431.17688  325108.41124  1226209.15539  4108874.27145              0.0   \n",
       "5   29597.19948   16750.75416     8395.61329     4734.34158      48125.01414   \n",
       "\n",
       "  STRATIFIED_v2_2 STRATIFIED_v2_25 STRATIFIED_v2_3  \n",
       "0         0.12466          0.12218          0.1203  \n",
       "1         0.24488           0.2363         0.22989  \n",
       "2             0.0              0.0             0.0  \n",
       "3     47931.59355      47825.96046     47762.24962  \n",
       "4             0.0              0.0             0.0  \n",
       "5      47945.8319      47842.37303     47779.84369  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize\n",
    "mae_df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RecSys-Evaluation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
