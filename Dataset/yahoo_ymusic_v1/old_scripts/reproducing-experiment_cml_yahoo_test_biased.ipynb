{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **SETUP**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjYyDBmbcEL9"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nUxhCz5MkDZk"
      },
      "outputs": [],
      "source": [
        "from __future__ import division\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os\n",
        "from openrec.tf1.legacy import ImplicitModelTrainer\n",
        "from openrec.tf1.legacy.utils import ImplicitDataset\n",
        "from openrec.tf1.legacy.utils.evaluators import ImplicitEvalManager\n",
        "from openrec.tf1.legacy.recommenders import CML\n",
        "from openrec.tf1.legacy.utils.evaluators import AUC\n",
        "from openrec.tf1.legacy.utils.samplers import PairwiseSampler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "auc_results = []\n",
        "recall_results = []\n",
        "\n",
        "seed = 76424236\n",
        "np.random.seed(seed=seed)\n",
        "\n",
        "folder_name = f\"./generated_data/\"\n",
        "\n",
        "if os.path.exists(folder_name) == False:\n",
        "    os.makedirs(folder_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "g0C--vI7lUe2"
      },
      "outputs": [],
      "source": [
        "raw_data = dict()\n",
        "raw_data['train_data'] = np.load(folder_name + \"training_arr.npy\")\n",
        "raw_data['test_data_pos'] = np.load(folder_name + \"biased-test_arr_pos.npy\")\n",
        "raw_data['test_data_neg'] = np.load(folder_name + \"biased-test_arr_neg.npy\")\n",
        "raw_data['max_user'] = 15401\n",
        "raw_data['max_item'] = 1001\n",
        "batch_size = 8000\n",
        "test_batch_size = 1000\n",
        "display_itr = 1000\n",
        "\n",
        "train_dataset = ImplicitDataset(raw_data['train_data'], raw_data['max_user'], raw_data['max_item'], name='Train')\n",
        "test_dataset_pos = ImplicitDataset(raw_data['test_data_pos'], raw_data['max_user'], raw_data['max_item'])\n",
        "test_dataset_neg = ImplicitDataset(raw_data['test_data_neg'], raw_data['max_user'], raw_data['max_item'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c61lBOIqcawA"
      },
      "source": [
        "## Define Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:391: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:31: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:41: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:43: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:33: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/interactions/pairwise_eu_dist.py:71: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:596: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/modules/extractions/latent_factor.py:75: The name tf.scatter_update is deprecated. Please use tf.compat.v1.scatter_update instead.\n",
            "\n",
            "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:144: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:365: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /home/japo/miniconda3/envs/RecSys-Evaluation/lib/python3.7/site-packages/openrec/tf1/legacy/recommenders/recommender.py:148: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-04-24 10:10:00.647422: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
            "2024-04-24 10:10:00.649710: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 4192050000 Hz\n",
            "2024-04-24 10:10:00.650104: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55624cceaa60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2024-04-24 10:10:00.650120: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./generated_data/cml-yahoo\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf                     # Code to avoid tf using cached embeddings\n",
        "tf.compat.v1.reset_default_graph()\n",
        "\n",
        "cml_model = CML(batch_size=batch_size, max_user=train_dataset.max_user(), max_item=train_dataset.max_item(),\n",
        "    dim_embed=50, l2_reg=0.001, opt='Adam', sess_config=None)\n",
        "sampler = PairwiseSampler(batch_size=batch_size, dataset=train_dataset, num_process=4)\n",
        "model_trainer = ImplicitModelTrainer(batch_size=batch_size, test_batch_size=test_batch_size,\n",
        "                                     train_dataset=train_dataset, model=cml_model, sampler=sampler,\n",
        "                                     eval_save_prefix=folder_name+\"yahoo\",\n",
        "                                     item_serving_size=500)\n",
        "auc_evaluator = AUC()\n",
        "\n",
        "cml_model.load(folder_name+\"cml-yahoo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **DEFINE FUNCTION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def eq(infilename, infilename_neg, trainfilename, gamma=1.0, K=30):\n",
        "\n",
        "    # Load the pickles\n",
        "    infile = open(infilename, 'rb')\n",
        "    infile_neg = open(infilename_neg, 'rb')\n",
        "    P = pickle.load(infile)\n",
        "    infile.close()\n",
        "    P_neg = pickle.load(infile_neg)\n",
        "    infile_neg.close()\n",
        "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
        "    \n",
        "    # Merge the two dictionaries\n",
        "    for theuser in P[\"users\"]:\n",
        "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
        "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
        "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
        "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
        "    \n",
        "    Zui = dict()\n",
        "    Ni = dict()\n",
        "\n",
        "    # Compute the frequency of each item in the training set\n",
        "    trainset = np.load(trainfilename)\n",
        "    for i in trainset['item_id']:\n",
        "        if i in Ni:\n",
        "            Ni[i] += 1\n",
        "        else:\n",
        "            Ni[i] = 1\n",
        "    del trainset\n",
        "\n",
        "    # Count the number of users with at least one positive item in the test set\n",
        "    # (not the smartest way)\n",
        "    nonzero_user_count = 0\n",
        "    for theuser in P[\"users\"]:\n",
        "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
        "        for pos_item in pos_items:\n",
        "            if pos_item in Ni:\n",
        "                nonzero_user_count += 1\n",
        "                break\n",
        "\n",
        "    # Compute the recommendations\n",
        "    for theuser in P[\"users\"]:\n",
        "        all_scores = np.array(P[\"results\"][theuser])\n",
        "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
        "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
        "        for i, pos_item in enumerate(pos_items):\n",
        "            pos_score = pos_scores[i]\n",
        "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
        "\n",
        "    # Compute the scores using AUC and compute the recall\n",
        "    sum_user_auc = 0.0\n",
        "    sum_user_recall = 0.0\n",
        "    for theuser in P[\"users\"]:\n",
        "        numerator_auc = 0.0\n",
        "        numerator_recall = 0.0\n",
        "        denominator = 0.0\n",
        "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
        "            if theitem not in Ni:\n",
        "                continue\n",
        "            pui = np.power(Ni[theitem], (gamma + 1) / 2.0)\n",
        "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser])) / pui\n",
        "            if Zui[(theuser, theitem)] < K:\n",
        "                numerator_recall += 1.0 / pui\n",
        "            denominator += 1 / pui\n",
        "        if denominator > 0:\n",
        "            sum_user_auc += numerator_auc / denominator\n",
        "            sum_user_recall += numerator_recall / denominator\n",
        "\n",
        "    return {\n",
        "        \"auc\"       : sum_user_auc / nonzero_user_count,\n",
        "        \"recall\"    : sum_user_recall / nonzero_user_count\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def aoa(infilename, infilename_neg, trainfilename, K=1):\n",
        "\n",
        "    # Load pickles\n",
        "    infile = open(infilename, 'rb')\n",
        "    infile_neg = open(infilename_neg, 'rb')\n",
        "    P = pickle.load(infile)\n",
        "    infile.close()\n",
        "    P_neg = pickle.load(infile_neg)\n",
        "    infile_neg.close()\n",
        "    NUM_NEGATIVES = P[\"num_negatives\"]\n",
        "    \n",
        "    # Merge the two dictionaries\n",
        "    for theuser in P[\"users\"]:\n",
        "        neg_items = list(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
        "        neg_scores = list(P_neg[\"results\"][theuser][NUM_NEGATIVES:])\n",
        "        P[\"user_items\"][theuser] = list(neg_items) + list(P[\"user_items\"][theuser][NUM_NEGATIVES:])\n",
        "        P[\"results\"][theuser] = list(neg_scores) + list(P[\"results\"][theuser][NUM_NEGATIVES:])\n",
        "        \n",
        "    Zui = dict()\n",
        "    Ni = dict()\n",
        "    \n",
        "    # Count item frequencies in the training set\n",
        "    trainset = np.load(trainfilename)\n",
        "    for i in trainset['item_id']:\n",
        "        if i in Ni:\n",
        "            Ni[i] += 1\n",
        "        else:\n",
        "            Ni[i] = 1\n",
        "    del trainset\n",
        "    \n",
        "    # Count the number of users with at least one positive item in the test set \n",
        "    # (not the smartest way)\n",
        "    nonzero_user_count = 0\n",
        "    for theuser in P[\"users\"]:\n",
        "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
        "        for pos_item in pos_items:\n",
        "            if pos_item in Ni:\n",
        "                nonzero_user_count += 1\n",
        "                break\n",
        "\n",
        "    # Compute the recommendations for each user\n",
        "    for theuser in P[\"users\"]:\n",
        "        all_scores = np.array(P[\"results\"][theuser])\n",
        "        pos_items = P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]\n",
        "        pos_scores = P[\"results\"][theuser][len(P_neg[\"results\"][theuser][NUM_NEGATIVES:]):]\n",
        "        for i, pos_item in enumerate(pos_items):\n",
        "            pos_score = pos_scores[i]\n",
        "            Zui[(theuser, pos_item)] = float(np.sum(all_scores > pos_score))\n",
        "            \n",
        "    sum_user_auc = 0.0\n",
        "    sum_user_recall = 0.0\n",
        "\n",
        "    # Compute the scores using AUC and compute the recall\n",
        "    for theuser in P[\"users\"]:\n",
        "        numerator_auc = 0.0\n",
        "        numerator_recall = 0.0\n",
        "        denominator = 0.0\n",
        "        for theitem in P[\"user_items\"][theuser][len(P_neg[\"user_items\"][theuser][NUM_NEGATIVES:]):]:\n",
        "            if theitem not in Ni:\n",
        "                continue\n",
        "            numerator_auc += (1 - Zui[(theuser, theitem)] / len(P[\"user_items\"][theuser]))\n",
        "            if Zui[(theuser, theitem)] < K:\n",
        "                numerator_recall += 1.0\n",
        "            denominator += 1 \n",
        "        if denominator > 0:\n",
        "            sum_user_auc += numerator_auc / denominator\n",
        "            sum_user_recall += numerator_recall / denominator\n",
        "\n",
        "    return {\n",
        "        \"auc\"       : sum_user_auc / nonzero_user_count,\n",
        "        \"recall\"    : sum_user_recall / nonzero_user_count\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **TEST**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTBMf1BPcvgL"
      },
      "source": [
        "## Generate Raw Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Subsampling negative items]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 11692/11692 [00:03<00:00, 3075.29it/s] \n",
            "100%|██████████| 15400/15400 [02:48<00:00, 91.31it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'AUC': [0.4520270270270271,\n",
              "  0.5277104377104377,\n",
              "  0.5513758389261745,\n",
              "  0.44965000000000005,\n",
              "  0.5109866220735785,\n",
              "  0.49216442953020134,\n",
              "  0.5300510204081632,\n",
              "  0.5020790378006873,\n",
              "  0.5079264214046822,\n",
              "  0.4634615384615384,\n",
              "  0.5082274247491638,\n",
              "  0.4895547945205479,\n",
              "  0.4874579124579125,\n",
              "  0.5040939597315436,\n",
              "  0.41154237288135587,\n",
              "  0.5231756756756757,\n",
              "  0.4790666666666666,\n",
              "  0.49193877551020404,\n",
              "  0.503843537414966,\n",
              "  0.49515100671140944,\n",
              "  0.5273400673400673,\n",
              "  0.4414715719063545,\n",
              "  0.48308080808080817,\n",
              "  0.49555000000000005,\n",
              "  0.5111486486486487,\n",
              "  0.4914965986394558,\n",
              "  0.5199828178694158,\n",
              "  0.48954128440366973,\n",
              "  0.49142140468227424,\n",
              "  0.47784246575342465,\n",
              "  0.5318833333333334,\n",
              "  0.48486577181208057,\n",
              "  0.4823154362416108,\n",
              "  0.5074664429530202,\n",
              "  0.47377516778523493,\n",
              "  0.49820945945945955,\n",
              "  0.5091778523489934,\n",
              "  0.46560200668896323,\n",
              "  0.4942,\n",
              "  0.5095150501672241,\n",
              "  0.5053,\n",
              "  0.4895286195286195,\n",
              "  0.5376440677966102,\n",
              "  0.5072895622895621,\n",
              "  0.45561666666666667,\n",
              "  0.4736317567567567,\n",
              "  0.4841833333333333,\n",
              "  0.5129833333333333,\n",
              "  0.5212372881355932,\n",
              "  0.5425254237288135,\n",
              "  0.49836666666666674,\n",
              "  0.47303691275167786,\n",
              "  0.5351839464882944,\n",
              "  0.46586956521739126,\n",
              "  0.48083050847457626,\n",
              "  0.4988833333333333,\n",
              "  0.49578595317725754,\n",
              "  0.4865268456375839,\n",
              "  0.4944983277591973,\n",
              "  0.5260869565217392,\n",
              "  0.524799331103679,\n",
              "  0.4303678929765887,\n",
              "  0.4995101351351351,\n",
              "  0.5015824915824916,\n",
              "  0.5229530201342281,\n",
              "  0.4982586206896552,\n",
              "  0.5380369127516779,\n",
              "  0.5045101351351351,\n",
              "  0.4961317567567568,\n",
              "  0.5351333333333333,\n",
              "  0.4954545454545455,\n",
              "  0.5358361204013378,\n",
              "  0.49520202020202014,\n",
              "  0.5057939189189189,\n",
              "  0.5324074074074074,\n",
              "  0.5639000000000001,\n",
              "  0.4073657718120805,\n",
              "  0.4618074324324324,\n",
              "  0.4630500000000001,\n",
              "  0.4425833333333333,\n",
              "  0.5118561872909699,\n",
              "  0.49536082474226795,\n",
              "  0.48553691275167793,\n",
              "  0.48931438127090304,\n",
              "  0.5383333333333333,\n",
              "  0.4502333333333333,\n",
              "  0.512483108108108,\n",
              "  0.5018959731543624,\n",
              "  0.5078260869565219,\n",
              "  0.5207118644067797,\n",
              "  0.5218013468013469,\n",
              "  0.5148160535117057,\n",
              "  0.5019112627986348,\n",
              "  0.5008833333333332,\n",
              "  0.4821959459459459,\n",
              "  0.4639057239057238,\n",
              "  0.4839833333333333,\n",
              "  0.5263666666666668,\n",
              "  0.5349499999999999,\n",
              "  0.4983724832214765,\n",
              "  0.504581881533101,\n",
              "  0.47313758389261745,\n",
              "  0.49083333333333334,\n",
              "  0.5243939393939394,\n",
              "  0.5138833333333334,\n",
              "  0.4844539249146758,\n",
              "  0.49187919463087243,\n",
              "  0.531744966442953,\n",
              "  0.5205166666666666,\n",
              "  0.5093311036789299,\n",
              "  0.47464646464646465,\n",
              "  0.4988758389261745,\n",
              "  0.48576666666666674,\n",
              "  0.5129833333333332,\n",
              "  0.5138,\n",
              "  0.4417617449664429,\n",
              "  0.5224831081081082,\n",
              "  0.4781,\n",
              "  0.5155254237288136,\n",
              "  0.4704026845637584,\n",
              "  0.5091186440677966,\n",
              "  0.5375925925925925,\n",
              "  0.47978187919463094,\n",
              "  0.493,\n",
              "  0.5109060402684564,\n",
              "  0.4956354515050167,\n",
              "  0.4879431438127091,\n",
              "  0.49935,\n",
              "  0.5023141891891892,\n",
              "  0.49547781569965876,\n",
              "  0.49677852348993295,\n",
              "  0.5281772575250836,\n",
              "  0.4354639175257732,\n",
              "  0.458561872909699,\n",
              "  0.4652675585284281,\n",
              "  0.5227759197324415,\n",
              "  0.5051360544217687,\n",
              "  0.5213926174496645,\n",
              "  0.5277133105802048,\n",
              "  0.4807482993197279,\n",
              "  0.49277397260273975,\n",
              "  0.5237792642140467,\n",
              "  0.5323,\n",
              "  0.4996632996632997,\n",
              "  0.4972315436241611,\n",
              "  0.46607382550335563,\n",
              "  0.4655833333333333,\n",
              "  0.5366442953020134,\n",
              "  0.4842592592592592,\n",
              "  0.4780369127516778,\n",
              "  0.47855218855218856,\n",
              "  0.49376712328767125,\n",
              "  0.5134511784511785,\n",
              "  0.49166666666666664,\n",
              "  0.46008389261744975,\n",
              "  0.524949494949495,\n",
              "  0.4908862876254181,\n",
              "  0.5,\n",
              "  0.5081481481481482,\n",
              "  0.4509060402684563,\n",
              "  0.5310606060606061,\n",
              "  0.5042592592592592,\n",
              "  0.4602516778523489,\n",
              "  0.45612040133779264,\n",
              "  0.466510067114094,\n",
              "  0.5112040133779264,\n",
              "  0.47993265993266,\n",
              "  0.5094816053511706,\n",
              "  0.4917666666666667,\n",
              "  0.4972895622895623,\n",
              "  0.48582214765100673,\n",
              "  0.5182775919732442,\n",
              "  0.46725000000000005,\n",
              "  0.5089297658862877,\n",
              "  0.47114093959731534,\n",
              "  0.4613926174496644,\n",
              "  0.49148464163822525,\n",
              "  0.49211666666666665,\n",
              "  0.49584999999999996,\n",
              "  0.46838333333333326,\n",
              "  0.4955333333333333,\n",
              "  0.5028451178451178,\n",
              "  0.46964765100671135,\n",
              "  0.5475250836120401,\n",
              "  0.5418394648829432,\n",
              "  0.4801672240802675,\n",
              "  0.4979194630872483,\n",
              "  0.5088590604026845,\n",
              "  0.49370629370629365,\n",
              "  0.46038333333333337,\n",
              "  0.5420338983050846,\n",
              "  0.5417353951890035,\n",
              "  0.47979865771812086,\n",
              "  0.5051515151515151,\n",
              "  0.5197297297297298,\n",
              "  0.4620666666666667,\n",
              "  0.5336166666666667,\n",
              "  0.5131438127090301,\n",
              "  0.5229166666666667,\n",
              "  0.45785000000000003,\n",
              "  0.4684333333333333,\n",
              "  0.46796666666666664,\n",
              "  0.4700167224080267,\n",
              "  0.5397466216216216,\n",
              "  0.47797658862876263,\n",
              "  0.4934666666666667,\n",
              "  0.48193333333333327,\n",
              "  0.5023825503355704,\n",
              "  0.47374161073825505,\n",
              "  0.5151845637583893,\n",
              "  0.47878424657534246,\n",
              "  0.5384060402684564,\n",
              "  0.4836655405405406,\n",
              "  0.4554166666666667,\n",
              "  0.49171140939597313,\n",
              "  0.46901337792642134,\n",
              "  0.48363175675675674,\n",
              "  0.5183166666666666,\n",
              "  0.5297157190635451,\n",
              "  0.46785,\n",
              "  0.48136666666666666,\n",
              "  0.49785000000000007,\n",
              "  0.5137751677852349,\n",
              "  0.532347972972973,\n",
              "  0.4822222222222222,\n",
              "  0.46396666666666664,\n",
              "  0.5017229729729731,\n",
              "  0.4957508532423208,\n",
              "  0.49396666666666667,\n",
              "  0.4999821428571429,\n",
              "  0.4572315436241611,\n",
              "  0.4771571906354515,\n",
              "  0.5072895622895623,\n",
              "  0.47583333333333333,\n",
              "  0.4278,\n",
              "  0.4884931506849315,\n",
              "  0.4992114093959731,\n",
              "  0.5183728813559322,\n",
              "  0.4857718120805369,\n",
              "  0.5146616541353384,\n",
              "  0.5413758389261745,\n",
              "  0.506744966442953,\n",
              "  0.44405405405405407,\n",
              "  0.5000844594594595,\n",
              "  0.49922558922558924,\n",
              "  0.4534628378378379,\n",
              "  0.49595,\n",
              "  0.4919006849315068,\n",
              "  0.49444999999999995,\n",
              "  0.4813210702341137,\n",
              "  0.517030201342282,\n",
              "  0.4982441471571906,\n",
              "  0.5181333333333333,\n",
              "  0.49724489795918364,\n",
              "  0.4642929292929293,\n",
              "  0.5294833333333333,\n",
              "  0.5120777027027027,\n",
              "  0.4945791245791245,\n",
              "  0.53365,\n",
              "  0.49293220338983057,\n",
              "  0.46071186440677964,\n",
              "  0.5022852233676975,\n",
              "  0.5042114093959731,\n",
              "  0.4703846153846154,\n",
              "  0.48315878378378374,\n",
              "  0.50295,\n",
              "  0.49462962962962964,\n",
              "  0.47823232323232323,\n",
              "  0.4958390410958905,\n",
              "  0.5290816326530612,\n",
              "  0.482876254180602,\n",
              "  0.4969127516778523,\n",
              "  0.535752508361204,\n",
              "  0.5064166666666667,\n",
              "  0.46021666666666666,\n",
              "  0.5274496644295302,\n",
              "  0.5082817869415808,\n",
              "  0.524244966442953,\n",
              "  0.48569727891156467,\n",
              "  0.501996644295302,\n",
              "  0.5031481481481481,\n",
              "  0.5232943143812708,\n",
              "  0.44978260869565223,\n",
              "  0.47287878787878784,\n",
              "  0.5461204013377926,\n",
              "  0.48508417508417506,\n",
              "  0.5107939189189189,\n",
              "  0.5225,\n",
              "  0.5380201342281878,\n",
              "  0.4820833333333333,\n",
              "  0.5225252525252525,\n",
              "  0.48951178451178445,\n",
              "  0.513013468013468,\n",
              "  0.4900336700336701,\n",
              "  0.4908833333333333,\n",
              "  0.5080808080808081,\n",
              "  0.5017833333333334,\n",
              "  0.48679054054054055,\n",
              "  0.532231543624161,\n",
              "  0.47976588628762545,\n",
              "  0.5158724832214765,\n",
              "  0.46619863013698626,\n",
              "  0.48357382550335576,\n",
              "  0.5466385135135136,\n",
              "  0.524949494949495,\n",
              "  0.47963333333333336,\n",
              "  0.5179530201342282,\n",
              "  0.5589130434782608,\n",
              "  0.517056856187291,\n",
              "  0.4713833333333333,\n",
              "  0.48927609427609425,\n",
              "  0.5158833333333332,\n",
              "  0.557742474916388,\n",
              "  0.4733053691275168,\n",
              "  0.5194127516778524,\n",
              "  0.46089830508474566,\n",
              "  0.507043918918919,\n",
              "  0.5312751677852349,\n",
              "  0.4453716216216216,\n",
              "  0.47161073825503363,\n",
              "  0.5404166666666667,\n",
              "  0.5206040268456376,\n",
              "  0.4350675675675676,\n",
              "  0.5609364548494983,\n",
              "  0.5301003344481605,\n",
              "  0.46866666666666673,\n",
              "  0.47324414715719065,\n",
              "  0.5056499999999999,\n",
              "  0.49395,\n",
              "  0.5264776632302406,\n",
              "  0.5163926174496644,\n",
              "  0.5041919191919192,\n",
              "  0.5648976109215017,\n",
              "  0.48860000000000003,\n",
              "  0.5110535117056857,\n",
              "  0.4965547703180212,\n",
              "  0.49138047138047136,\n",
              "  0.5134561403508772,\n",
              "  0.5271404682274248,\n",
              "  0.5145819397993312,\n",
              "  0.45424496644295304,\n",
              "  0.47552364864864866,\n",
              "  0.45617449664429527,\n",
              "  0.5251190476190477,\n",
              "  0.5077104377104377,\n",
              "  0.46521666666666667,\n",
              "  0.46511904761904765,\n",
              "  0.5426,\n",
              "  0.5442833333333333,\n",
              "  0.4958666666666666,\n",
              "  0.5270068027210885,\n",
              "  0.4864765100671142,\n",
              "  0.506472602739726,\n",
              "  0.4648809523809524,\n",
              "  0.5101515151515152,\n",
              "  0.5113666666666667,\n",
              "  0.5284833333333333,\n",
              "  0.4771114864864865,\n",
              "  0.5006521739130435,\n",
              "  0.45227424749163886,\n",
              "  0.4942833333333334,\n",
              "  0.5047297297297296,\n",
              "  0.5149832214765102,\n",
              "  0.47304713804713805,\n",
              "  0.45651515151515154,\n",
              "  0.5028595317725754,\n",
              "  0.4639833333333333,\n",
              "  0.5051,\n",
              "  0.5252525252525253,\n",
              "  0.5229931972789116,\n",
              "  0.4759259259259258,\n",
              "  0.4572073578595317,\n",
              "  0.5218181818181817,\n",
              "  0.5190878378378379,\n",
              "  0.49793333333333334,\n",
              "  0.5125838926174496,\n",
              "  0.5391833333333333,\n",
              "  0.50575,\n",
              "  0.5291471571906355,\n",
              "  0.5320166666666667,\n",
              "  0.4825335570469798,\n",
              "  0.5016,\n",
              "  0.4785117056856187,\n",
              "  0.50835,\n",
              "  0.5101677852348993,\n",
              "  0.4700668896321071,\n",
              "  0.4796127946127946,\n",
              "  0.5011206896551724,\n",
              "  0.5050671140939597,\n",
              "  0.4828187919463087,\n",
              "  0.5004040404040404,\n",
              "  0.4870637583892617,\n",
              "  0.5281925675675676,\n",
              "  0.5058361204013379,\n",
              "  0.5296735395189004,\n",
              "  0.47248310810810806,\n",
              "  0.519054054054054,\n",
              "  0.5029096989966555,\n",
              "  0.5171666666666666,\n",
              "  0.5239666666666667,\n",
              "  0.4858249158249159,\n",
              "  0.4861447811447811,\n",
              "  0.4786287625418061,\n",
              "  0.5557214765100671,\n",
              "  0.5286166666666667,\n",
              "  0.47929530201342274,\n",
              "  0.5059090909090909,\n",
              "  0.5231818181818182,\n",
              "  0.49856666666666666,\n",
              "  0.5542809364548495,\n",
              "  0.5124915254237288,\n",
              "  0.5081818181818182,\n",
              "  0.5071812080536914,\n",
              "  0.4879530201342282,\n",
              "  0.5202006688963211,\n",
              "  0.5067114093959731,\n",
              "  0.4852852348993289,\n",
              "  0.48852842809364544,\n",
              "  0.5253344481605351,\n",
              "  0.4436026936026936,\n",
              "  0.4927739726027397,\n",
              "  0.49053511705685615,\n",
              "  0.49853535353535355,\n",
              "  0.5105743243243243,\n",
              "  0.5236842105263158,\n",
              "  0.4455574324324324,\n",
              "  0.49289297658862874,\n",
              "  0.4557885906040268,\n",
              "  0.47861204013377934,\n",
              "  0.4551839464882943,\n",
              "  0.5073076923076923,\n",
              "  0.532,\n",
              "  0.49893581081081084,\n",
              "  0.5181144781144781,\n",
              "  0.5019023569023568,\n",
              "  0.5049833333333332,\n",
              "  0.49795986622073574,\n",
              "  0.49191275167785237,\n",
              "  0.4915924657534247,\n",
              "  0.5177441077441077,\n",
              "  0.47086666666666666,\n",
              "  0.4659228187919463,\n",
              "  0.472190635451505,\n",
              "  0.553597972972973,\n",
              "  0.48465517241379313,\n",
              "  0.5300838926174497,\n",
              "  0.4822727272727273,\n",
              "  0.5141385135135135,\n",
              "  0.4751346801346802,\n",
              "  0.4652173913043478,\n",
              "  0.5059666666666668,\n",
              "  0.4690338983050848,\n",
              "  0.4487625418060201,\n",
              "  0.5059833333333333,\n",
              "  0.4982943143812709,\n",
              "  0.5079697986577181,\n",
              "  0.48994880546075076,\n",
              "  0.5144999999999998,\n",
              "  0.4880333333333333,\n",
              "  0.4472333333333333,\n",
              "  0.5384333333333333,\n",
              "  0.5016833333333334,\n",
              "  0.4735833333333333,\n",
              "  0.5145,\n",
              "  0.5345134228187919,\n",
              "  0.5415816326530613,\n",
              "  0.5056711409395973,\n",
              "  0.48588135593220344,\n",
              "  0.4944983277591973,\n",
              "  0.5450499999999999,\n",
              "  0.4416722408026756,\n",
              "  0.5322986577181208,\n",
              "  0.48521812080536914,\n",
              "  0.49540268456375836,\n",
              "  0.5049333333333333,\n",
              "  0.4908361204013378,\n",
              "  0.54001677852349,\n",
              "  0.4416891891891892,\n",
              "  0.51075,\n",
              "  0.5036577181208054,\n",
              "  0.534391891891892,\n",
              "  0.4780333333333334,\n",
              "  0.5215939597315437,\n",
              "  0.4678333333333333,\n",
              "  0.496026936026936,\n",
              "  0.5203198653198653,\n",
              "  0.4836409395973154,\n",
              "  0.5454,\n",
              "  0.5541582491582492,\n",
              "  0.5155236486486486,\n",
              "  0.499244966442953,\n",
              "  0.4763166666666666,\n",
              "  0.4816053511705686,\n",
              "  0.525819397993311,\n",
              "  0.5525426621160409,\n",
              "  0.48865,\n",
              "  0.48210801393728225,\n",
              "  0.5161148648648649,\n",
              "  0.4993684210526316,\n",
              "  0.4757705479452055,\n",
              "  0.49390000000000006,\n",
              "  0.4975510204081633,\n",
              "  0.4460367892976589,\n",
              "  0.5603367003367004,\n",
              "  0.5692140468227425,\n",
              "  0.5140969899665552,\n",
              "  0.4902842809364548,\n",
              "  0.490472972972973,\n",
              "  0.4495150501672241,\n",
              "  0.49530201342281877,\n",
              "  0.43765100671140933,\n",
              "  0.5311,\n",
              "  0.49355704697986574,\n",
              "  0.5222542372881356,\n",
              "  0.49149999999999994,\n",
              "  0.5346822742474917,\n",
              "  0.5333108108108109,\n",
              "  0.5070134228187919,\n",
              "  0.48064999999999997,\n",
              "  0.48484899328859066,\n",
              "  0.4953846153846154,\n",
              "  0.49741496598639456,\n",
              "  0.53685,\n",
              "  0.548922558922559,\n",
              "  0.5192499999999999,\n",
              "  0.45025083612040134,\n",
              "  0.5217953020134228,\n",
              "  0.5052166666666666,\n",
              "  0.4881166666666667,\n",
              "  0.5385204081632653,\n",
              "  0.5588552188552188,\n",
              "  0.4794147157190635,\n",
              "  0.4618166666666667,\n",
              "  0.5131666666666667,\n",
              "  0.47387959866220736,\n",
              "  0.4974161073825504,\n",
              "  0.48520066889632113,\n",
              "  0.4938833333333333,\n",
              "  0.5033779264214047,\n",
              "  0.5110067114093959,\n",
              "  0.5182996632996633,\n",
              "  0.485,\n",
              "  0.4753050847457627,\n",
              "  0.5153678929765887,\n",
              "  0.49261666666666665,\n",
              "  0.4850169491525424,\n",
              "  0.5136454849498328,\n",
              "  0.4984175084175083,\n",
              "  0.4666778523489933,\n",
              "  0.48550167224080265,\n",
              "  0.5248662207357859,\n",
              "  0.5010833333333333,\n",
              "  0.5186734693877552,\n",
              "  0.50815,\n",
              "  0.47876254180602007,\n",
              "  0.4927104377104377,\n",
              "  0.5475,\n",
              "  0.5403010033444815,\n",
              "  0.48291525423728804,\n",
              "  0.5010739436619719,\n",
              "  0.5012207357859532,\n",
              "  0.5209044368600683,\n",
              "  0.48966555183946486,\n",
              "  0.5044314381270902,\n",
              "  0.4693143812709031,\n",
              "  0.5554865771812081,\n",
              "  0.46250836120401334,\n",
              "  0.49570234113712375,\n",
              "  0.5,\n",
              "  0.4895652173913043,\n",
              "  0.4606506849315069,\n",
              "  0.5361371237458195,\n",
              "  0.4696101694915255,\n",
              "  0.48345637583892614,\n",
              "  0.4976599326599327,\n",
              "  0.5079264214046822,\n",
              "  0.47123745819397983,\n",
              "  0.5368333333333334,\n",
              "  0.5150677966101694,\n",
              "  0.5032214765100671,\n",
              "  0.5132666666666668,\n",
              "  0.5582166666666667,\n",
              "  0.4917391304347826,\n",
              "  0.4368959731543624,\n",
              "  0.46375000000000005,\n",
              "  0.5162969283276451,\n",
              "  0.46863333333333324,\n",
              "  0.5062876254180602,\n",
              "  0.5015202702702702,\n",
              "  0.5009863945578231,\n",
              "  0.49556856187290965,\n",
              "  0.47451342281879183,\n",
              "  0.4601839464882943,\n",
              "  0.4709698996655518,\n",
              "  0.5598666666666666,\n",
              "  0.5108557046979866,\n",
              "  0.5191,\n",
              "  0.49109797297297303,\n",
              "  0.48393220338983045,\n",
              "  0.5343959731543624,\n",
              "  0.5693771043771044,\n",
              "  0.4844127516778523,\n",
              "  0.5002203389830508,\n",
              "  0.5005218855218855,\n",
              "  0.49423986486486493,\n",
              "  0.5022073578595317,\n",
              "  0.5267687074829932,\n",
              "  0.5262802768166089,\n",
              "  0.49937710437710436,\n",
              "  0.49553763440860216,\n",
              "  0.48906354515050166,\n",
              "  0.4940572390572391,\n",
              "  0.497056856187291,\n",
              "  0.5355166666666666,\n",
              "  0.4783,\n",
              "  0.5074659863945578,\n",
              "  0.5090604026845638,\n",
              "  0.49750836120401337,\n",
              "  0.5085000000000001,\n",
              "  0.47895622895622897,\n",
              "  0.5137416107382551,\n",
              "  0.5098958333333333,\n",
              "  0.4847333333333334,\n",
              "  0.46675,\n",
              "  0.46469491525423734,\n",
              "  0.4824149659863945,\n",
              "  0.5275,\n",
              "  0.5153344481605352,\n",
              "  0.4863166666666666,\n",
              "  0.5350333333333332,\n",
              "  0.5208983050847458,\n",
              "  0.4666107382550336,\n",
              "  0.5352675585284281,\n",
              "  0.4455460750853243,\n",
              "  0.5073400673400673,\n",
              "  0.47151006711409393,\n",
              "  0.4870307167235495,\n",
              "  0.5373817567567567,\n",
              "  0.5044949494949496,\n",
              "  0.5175168918918919,\n",
              "  0.5026599326599327,\n",
              "  0.49570234113712364,\n",
              "  0.4876333333333332,\n",
              "  0.49728187919463085,\n",
              "  0.5316000000000001,\n",
              "  0.462190635451505,\n",
              "  0.458939393939394,\n",
              "  0.47920875420875425,\n",
              "  0.5025083612040134,\n",
              "  0.5084949832775919,\n",
              "  0.5166610738255034,\n",
              "  0.5312751677852349,\n",
              "  0.49911864406779666,\n",
              "  0.5074242424242423,\n",
              "  0.49508333333333326,\n",
              "  0.48567010309278347,\n",
              "  0.46096666666666675,\n",
              "  0.5013087248322147,\n",
              "  0.4509375,\n",
              "  0.4998484848484848,\n",
              "  0.46687290969899675,\n",
              "  0.4924496644295302,\n",
              "  0.47678451178451176,\n",
              "  0.5042398648648648,\n",
              "  0.4859090909090909,\n",
              "  0.49096666666666666,\n",
              "  0.4869127516778524,\n",
              "  0.47312709030100336,\n",
              "  0.517946127946128,\n",
              "  0.49613333333333326,\n",
              "  0.49203448275862066,\n",
              "  0.53,\n",
              "  0.48074999999999996,\n",
              "  0.4645819397993311,\n",
              "  0.45239130434782604,\n",
              "  0.4977257525083612,\n",
              "  0.49498316498316497,\n",
              "  0.4806040268456376,\n",
              "  0.48100000000000004,\n",
              "  0.4962374581939799,\n",
              "  0.47183501683501683,\n",
              "  0.4981925675675676,\n",
              "  0.47621621621621624,\n",
              "  0.5277871621621623,\n",
              "  0.4819360269360269,\n",
              "  0.4248166666666666,\n",
              "  0.46158862876254175,\n",
              "  0.48533444816053517,\n",
              "  0.5098327759197324,\n",
              "  0.5464166666666667,\n",
              "  0.48371621621621624,\n",
              "  0.5972,\n",
              "  0.5053187919463087,\n",
              "  0.4865833333333334,\n",
              "  0.4861148648648648,\n",
              "  0.48235785953177246,\n",
              "  0.5408862876254181,\n",
              "  0.49587328767123295,\n",
              "  0.5327684563758389,\n",
              "  0.4952842809364549,\n",
              "  0.5001170568561872,\n",
              "  0.5279081632653062,\n",
              "  0.5266220735785953,\n",
              "  0.4715050167224081,\n",
              "  0.46974048442906574,\n",
              "  0.5196308724832215,\n",
              "  0.45498322147651,\n",
              "  0.4859866220735786,\n",
              "  0.5439464882943144,\n",
              "  0.5340235690235691,\n",
              "  0.5038999999999999,\n",
              "  0.47834494773519165,\n",
              "  0.5483053691275168,\n",
              "  0.5293120805369128,\n",
              "  0.5088422818791946,\n",
              "  0.5128040540540542,\n",
              "  0.504054054054054,\n",
              "  0.4842140468227425,\n",
              "  0.46775167785234895,\n",
              "  0.5213926174496645,\n",
              "  0.5122166666666665,\n",
              "  0.5528282828282828,\n",
              "  0.5134931506849315,\n",
              "  0.4839932885906041,\n",
              "  0.4524829931972789,\n",
              "  0.5398829431438128,\n",
              "  0.49804347826086953,\n",
              "  0.5421452702702703,\n",
              "  0.5216387959866221,\n",
              "  0.5293166666666668,\n",
              "  0.4835953177257525,\n",
              "  0.483265306122449,\n",
              "  0.4716666666666667,\n",
              "  0.5198825503355705,\n",
              "  0.5042033898305085,\n",
              "  0.5101006711409396,\n",
              "  0.5078282828282827,\n",
              "  0.4720101351351351,\n",
              "  0.45248333333333335,\n",
              "  0.4953898305084746,\n",
              "  0.4595000000000001,\n",
              "  0.5088850174216029,\n",
              "  0.5492642140468228,\n",
              "  0.47869127516778526,\n",
              "  0.4845376712328768,\n",
              "  0.5137751677852349,\n",
              "  0.4872818791946308,\n",
              "  0.5045484949832776,\n",
              "  0.4721666666666667,\n",
              "  0.47778523489932895,\n",
              "  0.5187205387205387,\n",
              "  0.4866387959866221,\n",
              "  0.49541806020066886,\n",
              "  0.5304333333333333,\n",
              "  0.49907718120805367,\n",
              "  0.514054054054054,\n",
              "  0.5462207357859532,\n",
              "  0.5069,\n",
              "  0.55555,\n",
              "  0.5144087837837837,\n",
              "  0.44573578595317725,\n",
              "  0.48872053872053867,\n",
              "  0.4979,\n",
              "  0.5007,\n",
              "  0.48668896321070226,\n",
              "  0.5057570422535211,\n",
              "  0.5008108108108108,\n",
              "  0.4520547945205479,\n",
              "  0.5360238907849829,\n",
              "  0.5127257525083613,\n",
              "  0.48574576271186437,\n",
              "  0.5151337792642141,\n",
              "  0.5094444444444445,\n",
              "  0.4790301003344482,\n",
              "  0.4712794612794613,\n",
              "  0.5433833333333333,\n",
              "  0.4924328859060403,\n",
              "  0.47842372881355927,\n",
              "  0.49222972972972967,\n",
              "  0.5425833333333333,\n",
              "  0.5139799331103679,\n",
              "  0.5167666666666666,\n",
              "  0.5160234899328859,\n",
              "  0.47501677852348984,\n",
              "  0.48429292929292933,\n",
              "  0.49881666666666663,\n",
              "  0.5214666666666666,\n",
              "  0.5282775919732441,\n",
              "  0.5286148648648649,\n",
              "  0.5252861952861952,\n",
              "  0.45096989966555184,\n",
              "  0.47125000000000006,\n",
              "  0.4827424749163879,\n",
              "  0.5063758389261744,\n",
              "  0.5054054054054054,\n",
              "  0.5210033444816053,\n",
              "  0.5358053691275168,\n",
              "  0.49134680134680137,\n",
              "  0.49376666666666663,\n",
              "  0.5248154362416108,\n",
              "  0.5090166666666667,\n",
              "  0.45603333333333335,\n",
              "  0.5184782608695653,\n",
              "  0.4732423208191126,\n",
              "  0.4753065134099617,\n",
              "  0.45268581081081083,\n",
              "  0.5073322147651007,\n",
              "  0.5067892976588628,\n",
              "  0.4960774410774411,\n",
              "  0.48355218855218857,\n",
              "  0.4917676767676768,\n",
              "  0.49728040540540536,\n",
              "  0.4831786941580756,\n",
              "  0.46923728813559323,\n",
              "  0.5027068965517241,\n",
              "  0.5281711409395974,\n",
              "  0.5258249158249159,\n",
              "  0.49692953020134223,\n",
              "  0.5083946488294314,\n",
              "  0.5189966555183947,\n",
              "  0.48678787878787877,\n",
              "  0.5278451178451178,\n",
              "  0.5155833333333334,\n",
              "  0.4789864864864865,\n",
              "  0.48179530201342285,\n",
              "  0.5450833333333333,\n",
              "  0.47678333333333334,\n",
              "  0.5064381270903009,\n",
              "  0.4351858108108108,\n",
              "  0.4429833333333333,\n",
              "  0.5131587837837838,\n",
              "  0.4540666666666667,\n",
              "  0.4477627118644068,\n",
              "  0.5133108108108108,\n",
              "  0.5118288590604027,\n",
              "  0.48288395904436865,\n",
              "  0.45571906354515046,\n",
              "  0.4899305555555554,\n",
              "  0.48375,\n",
              "  0.5357166666666667,\n",
              "  0.4923666666666667,\n",
              "  0.5033221476510067,\n",
              "  0.5359666666666666,\n",
              "  0.5194500000000001,\n",
              "  0.5411631944444445,\n",
              "  0.46488255033557047,\n",
              "  0.5242,\n",
              "  0.5424915254237288,\n",
              "  0.49251677852348985,\n",
              "  0.4497666666666667,\n",
              "  0.4429560810810811,\n",
              "  0.545066889632107,\n",
              "  0.544732441471572,\n",
              "  0.5143833333333333,\n",
              "  0.48251677852349,\n",
              "  0.5411016949152543,\n",
              "  0.49657190635451515,\n",
              "  0.45871186440677963,\n",
              "  0.4767003367003366,\n",
              "  0.5175418060200669,\n",
              "  0.5482608695652175,\n",
              "  0.5369587628865979,\n",
              "  0.4785185185185185,\n",
              "  0.4848160535117056,\n",
              "  0.4586348122866894,\n",
              "  0.5130267558528427,\n",
              "  0.5206208053691275,\n",
              "  0.49807046979865777,\n",
              "  0.4896644295302014,\n",
              "  0.5237,\n",
              "  0.4845000000000001,\n",
              "  0.5054013377926422,\n",
              "  0.4922222222222222,\n",
              "  0.4892166666666667,\n",
              "  0.5120408163265306,\n",
              "  0.4411774744027303,\n",
              "  0.5222909698996655,\n",
              "  0.46765100671140947,\n",
              "  0.48769230769230765,\n",
              "  0.516418918918919,\n",
              "  0.4807692307692308,\n",
              "  0.4958108108108108,\n",
              "  0.4928355704697986,\n",
              "  0.5476166666666665,\n",
              "  0.5075337837837839,\n",
              "  0.4943918918918919,\n",
              "  0.5283166666666667,\n",
              "  0.47913333333333336,\n",
              "  0.511476510067114,\n",
              "  0.54325,\n",
              "  0.5107357859531771,\n",
              "  0.5165500000000001,\n",
              "  0.5344314381270904,\n",
              "  0.47787162162162167,\n",
              "  0.5226174496644295,\n",
              "  0.5165500000000001,\n",
              "  0.5010869565217391,\n",
              "  0.5348829431438127,\n",
              "  0.47580536912751675,\n",
              "  0.4911872909698997,\n",
              "  0.4765719063545151,\n",
              "  0.5855872483221476,\n",
              "  0.5164576271186441,\n",
              "  0.5140133779264214,\n",
              "  0.4892241379310345,\n",
              "  0.51435,\n",
              "  0.531438127090301,\n",
              "  0.4783164983164983,\n",
              "  0.4846167247386759,\n",
              "  0.5054227941176471,\n",
              "  0.4895622895622895,\n",
              "  0.5027833333333334,\n",
              "  0.4589322033898305,\n",
              "  0.478561872909699,\n",
              "  0.5015999999999999,\n",
              "  0.5164236111111111,\n",
              "  0.4902356902356902,\n",
              "  0.5523801369863014,\n",
              "  0.5506418918918919,\n",
              "  0.5201525423728813,\n",
              "  0.46968333333333323,\n",
              "  0.477027027027027,\n",
              "  0.47223905723905724,\n",
              "  0.4752693602693603,\n",
              "  0.48274999999999996,\n",
              "  0.49021739130434777,\n",
              "  0.480728813559322,\n",
              "  0.4738963210702341,\n",
              "  0.45738175675675674,\n",
              "  0.5481818181818182,\n",
              "  0.47074074074074074,\n",
              "  0.5254347826086957,\n",
              "  0.5011538461538462,\n",
              "  0.5043624161073826,\n",
              "  0.5177833333333333,\n",
              "  0.5393645484949832,\n",
              "  0.485793918918919,\n",
              "  0.5234899328859061,\n",
              "  0.505721476510067,\n",
              "  0.5108333333333334,\n",
              "  0.5137792642140468,\n",
              "  0.5272909698996656,\n",
              "  0.4820735785953178,\n",
              "  0.5151736111111112,\n",
              "  0.4991,\n",
              "  0.46765886287625413,\n",
              "  0.5238963210702341,\n",
              "  0.5306228956228957,\n",
              "  0.5362794612794612,\n",
              "  0.5220833333333333,\n",
              "  0.5508,\n",
              "  0.5324324324324325,\n",
              "  0.5275752508361204,\n",
              "  0.522751677852349,\n",
              "  0.46474747474747474,\n",
              "  0.4806166666666667,\n",
              "  0.508186440677966,\n",
              "  0.48355,\n",
              "  0.5292642140468228,\n",
              "  0.4606440677966102,\n",
              "  0.4768707482993198,\n",
              "  0.48419732441471564,\n",
              "  0.48393220338983045,\n",
              "  0.456989966555184,\n",
              "  0.5154362416107382,\n",
              "  0.579515050167224,\n",
              "  0.4871499999999999,\n",
              "  0.5054166666666666,\n",
              "  0.5443333333333332,\n",
              "  0.5145986622073578,\n",
              "  0.4836744966442953,\n",
              "  0.4783110367892976,\n",
              "  0.4561952861952862,\n",
              "  0.5540301003344481,\n",
              "  0.51405,\n",
              "  0.47263422818791945,\n",
              "  0.48761666666666664,\n",
              "  0.5268918918918918,\n",
              "  0.4553472222222222,\n",
              "  0.4844781144781145,\n",
              "  0.45175585284280945,\n",
              "  0.5287123745819398,\n",
              "  0.51585,\n",
              "  0.5207499999999999,\n",
              "  0.5117068965517242,\n",
              "  0.5217833333333333,\n",
              "  0.5103020134228188,\n",
              "  0.4941304347826087,\n",
              "  0.5405518394648829,\n",
              "  0.4833892617449665,\n",
              "  0.526304347826087,\n",
              "  0.47356666666666664,\n",
              "  0.49568791946308727,\n",
              "  0.5491946308724833,\n",
              "  0.5268791946308724,\n",
              "  0.4914814814814816,\n",
              "  0.48760563380281696,\n",
              "  0.5470338983050848,\n",
              "  0.5703333333333334,\n",
              "  0.5180479452054794,\n",
              "  ...]}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_trainer._eval_manager = ImplicitEvalManager(evaluators=[auc_evaluator])\n",
        "model_trainer._num_negatives = 200\n",
        "model_trainer._exclude_positives([train_dataset, test_dataset_pos, test_dataset_neg])\n",
        "model_trainer._sample_negatives(seed=10)\n",
        "\n",
        "model_trainer._eval_save_prefix = folder_name+\"cml-yahoo-test-pos-biased\"\n",
        "model_trainer._evaluate_partial(test_dataset_pos)\n",
        "\n",
        "model_trainer._eval_save_prefix = folder_name+\"cml-yahoo-test-neg-biased\"\n",
        "model_trainer._evaluate_partial(test_dataset_neg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'auc': 0.8439872027026072, 'recall': 0.5972519221896899}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eq(folder_name+\"cml-yahoo-test-pos-biased_evaluate_partial.pickle\", folder_name+\"cml-yahoo-test-neg-biased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", gamma=1.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'auc': 0.8398511463738416, 'recall': 0.5870412214563767}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eq(folder_name+\"cml-yahoo-test-pos-biased_evaluate_partial.pickle\", folder_name+\"cml-yahoo-test-neg-biased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", gamma=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'auc': 0.8345355174867299, 'recall': 0.5741340450757577}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eq(folder_name+\"cml-yahoo-test-pos-biased_evaluate_partial.pickle\", folder_name+\"cml-yahoo-test-neg-biased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\", gamma=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'auc': 0.8965086694921884, 'recall': 0.08820303463610504}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "aoa(folder_name+\"cml-yahoo-test-pos-biased_evaluate_partial.pickle\", folder_name+\"cml-yahoo-test-neg-biased_evaluate_partial.pickle\", folder_name+\"training_arr.npy\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "RecSysEvaluation",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
